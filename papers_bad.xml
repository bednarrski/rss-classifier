<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06349"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1409.8498"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04520"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06423"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06490"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06504"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1602.02915"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.01672"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.02703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08683"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.03353"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04153"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.06274">
<title>Mobile Machine Learning Hardware at ARM: A Systems-on-Chip (SoC) Perspective. (arXiv:1801.06274v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.06274</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is playing an increasingly significant role in emerging
mobile application domains such as AR/VR, ADAS, etc. Accordingly, hardware
architects have designed customized hardware for machine learning algorithms,
especially neural networks, to improve compute efficiency. However, machine
learning is typically just one processing stage in complex end-to-end
applications, which involve multiple components in a mobile Systems-on-a-chip
(SoC). Focusing on just ML accelerators loses bigger optimization opportunity
at the system (SoC) level. This paper argues that hardware architects should
expand the optimization scope to the entire SoC. We demonstrate one particular
case-study in the domain of continuous computer vision where camera sensor,
image signal processor (ISP), memory, and NN accelerator are synergistically
co-designed to achieve optimal system-level efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1&quot;&gt;Matthew Mattina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1&quot;&gt;Paul Whatmough&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06434">
<title>EffNet: An Efficient Structure for Convolutional Neural Networks. (arXiv:1801.06434v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.06434</link>
<description rdf:parseType="Literal">&lt;p&gt;With the ever increasing application of Convolutional Neural Networks to
costumer products the need emerges for models which can efficiently run on
embedded, mobile hardware. Slimmer models have therefore become a hot research
topic with multiple different approaches which vary from binary networks to
revised convolution layers. We offer our contribution to the latter and propose
a novel convolution block which significantly reduces the computational burden
while surpassing the current state-of-the-art. Our model, dubbed EffNet, is
optimised for models which are slim to begin with and is created to tackle
issues in existing models such as MobileNet and ShuffleNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1&quot;&gt;Ido Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roese_Koerner_L/0/1/0/all/0/1&quot;&gt;Lutz Roese-Koerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1&quot;&gt;Anton Kummert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06349">
<title>Proceedings of eNTERFACE 2015 Workshop on Intelligent Interfaces. (arXiv:1801.06349v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1801.06349</link>
<description rdf:parseType="Literal">&lt;p&gt;The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted
by the Numediart Institute of Creative Technologies of the University of Mons
from August 10th to September 2015. During the four weeks, students and
researchers from all over the world came together in the Numediart Institute of
the University of Mons to work on eight selected projects structured around
intelligent interfaces. Eight projects were selected and their reports are
shown here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mancas_M/0/1/0/all/0/1&quot;&gt;Matei Mancas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frisson_C/0/1/0/all/0/1&quot;&gt;Christian Frisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tilmanne_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;lle Tilmanne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+dAlessandro_N/0/1/0/all/0/1&quot;&gt;Nicolas d&amp;#x27;Alessandro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barborka_P/0/1/0/all/0/1&quot;&gt;Petr Barborka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayansar_F/0/1/0/all/0/1&quot;&gt;Furkan Bayansar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1&quot;&gt;Francisco Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiebrink_R/0/1/0/all/0/1&quot;&gt;Rebecca Fiebrink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heloir_A/0/1/0/all/0/1&quot;&gt;Alexis Heloir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemery_E/0/1/0/all/0/1&quot;&gt;Edgar Hemery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laraba_S/0/1/0/all/0/1&quot;&gt;Sohaib Laraba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moinet_A/0/1/0/all/0/1&quot;&gt;Alexis Moinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunnari_F/0/1/0/all/0/1&quot;&gt;Fabrizio Nunnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravet_T/0/1/0/all/0/1&quot;&gt;Thierry Ravet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reboursiere_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Reboursi&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarasua_A/0/1/0/all/0/1&quot;&gt;Alvaro Sarasua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tits_M/0/1/0/all/0/1&quot;&gt;Micka&amp;#xeb;l Tits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tits_N/0/1/0/all/0/1&quot;&gt;No&amp;#xe9; Tits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zajega_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Zaj&amp;#xe9;ga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alborno_P/0/1/0/all/0/1&quot;&gt;Paolo Alborno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolykhalova_K/0/1/0/all/0/1&quot;&gt;Ksenia Kolykhalova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frid_E/0/1/0/all/0/1&quot;&gt;Emma Frid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malafronte_D/0/1/0/all/0/1&quot;&gt;Damiano Malafronte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veld_L/0/1/0/all/0/1&quot;&gt;Lisanne Huis in&amp;#x27;t Veld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cakmak_H/0/1/0/all/0/1&quot;&gt;H&amp;#xfc;seyin Cakmak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haddad_K/0/1/0/all/0/1&quot;&gt;Kevin El Haddad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riche_N/0/1/0/all/0/1&quot;&gt;Nicolas Riche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leroy_J/0/1/0/all/0/1&quot;&gt;Julien Leroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marighetto_P/0/1/0/all/0/1&quot;&gt;Pierre Marighetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turker_B/0/1/0/all/0/1&quot;&gt;Bekir Berker T&amp;#xfc;rker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaki_H/0/1/0/all/0/1&quot;&gt;Hossein Khaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pulisci_R/0/1/0/all/0/1&quot;&gt;Roberto Pulisci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilmartin_E/0/1/0/all/0/1&quot;&gt;Emer Gilmartin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haider_F/0/1/0/all/0/1&quot;&gt;Fasih Haider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cengiz_K/0/1/0/all/0/1&quot;&gt;K&amp;#xfc;bra Cengiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulir_M/0/1/0/all/0/1&quot;&gt;Martin Sulir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_I/0/1/0/all/0/1&quot;&gt;Ilaria Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1&quot;&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazici_R/0/1/0/all/0/1&quot;&gt;Ramazan Yaz&amp;#x131;c&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_F/0/1/0/all/0/1&quot;&gt;Furkan Burak B&amp;#xe2;gc&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kili_V/0/1/0/all/0/1&quot;&gt;Vedat Gazi K&amp;#x131;l&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sezer_H/0/1/0/all/0/1&quot;&gt;Hilal Sezer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yenge_S/0/1/0/all/0/1&quot;&gt;Sena B&amp;#xfc;sra Yenge&lt;/a&gt;, et al. (31 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06481">
<title>Active Learning of Strict Partial Orders: A Case Study on Concept Prerequisite Relations. (arXiv:1801.06481v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.06481</link>
<description rdf:parseType="Literal">&lt;p&gt;Strict partial order is a mathematical structure commonly seen in relational
data. One obstacle to extracting such type of relations at scale is the lack of
large-scale labels for building effective data-driven solutions. We develop an
active learning framework for mining such relations subject to a strict order.
Our approach incorporates relational reasoning not only in finding new
unlabeled pairs whose labels can be deduced from an existing label set, but
also in devising new query strategies that consider the relational structure of
labels. Our experiments on concept prerequisite relations show our proposed
framework can substantially improve the classification performance with the
same query budget compared to other baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jianbo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Han Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pursel_B/0/1/0/all/0/1&quot;&gt;Bart Pursel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1&quot;&gt;C. Lee Giles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1409.8498">
<title>Non-myopic learning in repeated stochastic games. (arXiv:1409.8498v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1409.8498</link>
<description rdf:parseType="Literal">&lt;p&gt;In repeated stochastic games (RSGs), an agent must quickly adapt to the
behavior of previously unknown associates, who may themselves be learning. This
machine-learning problem is particularly challenging due, in part, to the
presence of multiple (even infinite) equilibria and inherently large strategy
spaces. In this paper, we introduce a method to reduce the strategy space of
two-player general-sum RSGs to a handful of expert strategies. This process,
called Mega, effectually reduces an RSG to a bandit problem. We show that the
resulting strategy space preserves several important properties of the original
RSG, thus enabling a learner to produce robust strategies within a reasonably
small number of interactions. To better establish strengths and weaknesses of
this approach, we empirically evaluate the resulting learning system against
other algorithms in three different RSGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_J/0/1/0/all/0/1&quot;&gt;Jacob W. Crandall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02896">
<title>Recurrent Network-based Deterministic Policy Gradient for Solving Bipedal Walking Challenge on Rugged Terrains. (arXiv:1710.02896v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02896</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the learning algorithm based on the Recurrent
Network-based Deterministic Policy Gradient. The Long-Short Term Memory is
utilized to enable the Partially Observed Markov Decision Process framework.
The novelty are improvements of LSTM networks: update of multi-step temporal
difference, removal of backpropagation through time on actor, initialisation of
hidden state using past trajectory scanning, and injection of external
experiences learned by other agents. Our methods benefit the reinforcement
learning agent on inferring the desirable action by referring the trajectories
of both past observations and actions. The proposed algorithm was implemented
to solve the Bipedal-Walker challenge in OpenAI virtual environment where only
partial state information is available. The validation on the extremely rugged
terrain demonstrates the effectiveness of the proposed algorithm by achieving a
new record of highest rewards in the challenge. The autonomous behaviors
generated by our agent are highly adaptive to a variety of obstacles as shown
in the simulation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Doo Re Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chuanyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGreavy_C/0/1/0/all/0/1&quot;&gt;Christopher McGreavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03225">
<title>Large-scale Cloze Test Dataset Designed by Teachers. (arXiv:1711.03225v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03225</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloze test is widely adopted in language exams to evaluate students&apos; language
proficiency. In this paper, we propose the first large-scale human-designed
cloze test dataset CLOTH, in which the questions were used in middle-school and
high-school language exams. With the missing blanks carefully created by
teachers and candidate choices purposely designed to be confusing, CLOTH
requires a deeper language understanding and a wider attention span than
previous automatically generated cloze datasets. We show humans outperform
dedicated designed baseline models by a significant margin, even when the model
is trained on sufficiently large external data. We investigate the source of
the performance gap, trace model deficiencies to some distinct properties of
CLOTH, and identify the limited ability of comprehending a long-term context to
be the key bottleneck.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qizhe Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_G/0/1/0/all/0/1&quot;&gt;Guokun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zihang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1&quot;&gt;Eduard Hovy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06924">
<title>Safe Policy Improvement with Baseline Bootstrapping. (arXiv:1712.06924v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06924</link>
<description rdf:parseType="Literal">&lt;p&gt;A common goal in Reinforcement Learning is to derive a good strategy given a
limited batch of data. In this paper, we adopt the safe policy improvement
(SPI) approach: we compute a target policy guaranteed to perform at least as
well as a given baseline policy. Our SPI strategy, inspired by the
knows-what-it-knows paradigms, consists in bootstrapping the target policy with
the baseline policy when it does not know. We develop two computationally
efficient bootstrapping algorithms, a value-based and a policy-based, both
accompanied with theoretical SPI bounds. Three algorithm variants are proposed.
We empirically show the literature algorithms limits on a small stochastic
gridworld problem, and then demonstrate that our five algorithms not only
improve the worst case scenarios, but also the mean performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trichelair_P/0/1/0/all/0/1&quot;&gt;Paul Trichelair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04520">
<title>Non-Parametric Transformation Networks. (arXiv:1801.04520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04520</link>
<description rdf:parseType="Literal">&lt;p&gt;ConvNets have been very effective in many applications where it is required
to learn invariances to within-class nuisance transformations. However, through
their architecture, ConvNets only enforce invariance to translation. In this
paper, we introduce a new class of convolutional architectures called
Non-Parametric Transformation Networks (NPTNs) which can learn general
invariances and symmetries directly from data. NPTNs are a direct and natural
generalization of ConvNets and can be optimized directly using gradient
descent. They make no assumption regarding structure of the invariances present
in the data and in that aspect are very flexible and powerful. We also model
ConvNets and NPTNs under a unified framework called Transformation Networks
which establishes the natural connection between the two. We demonstrate the
efficacy of NPTNs on natural data such as MNIST and CIFAR 10 where it
outperforms ConvNet baselines with the same number of parameters. We show it is
effective in learning invariances unknown apriori directly from data from
scratch. Finally, we apply NPTNs to Capsule Networks and show that they enable
them to perform even better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_D/0/1/0/all/0/1&quot;&gt;Dipan K. Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1&quot;&gt;Marios Savvides&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09644">
<title>PyPhi: A toolbox for integrated information theory. (arXiv:1712.09644v2 [q-bio.NC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.09644</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrated information theory provides a mathematical framework to fully
characterize the cause-effect structure of a physical system. Here, we
introduce PyPhi, a Python software package that implements this framework for
causal analysis and unfolds the full cause-effect structure of discrete
dynamical systems of binary elements. The software allows users to easily study
these structures, serves as an up-to-date reference implementation of the
formalisms of integrated information theory, and has been applied in research
on complexity, emergence, and certain biological questions. We first provide an
overview of the main algorithm and demonstrate PyPhi&apos;s functionality in the
course of analyzing an example system, and then describe details of the
algorithm&apos;s design and implementation.
&lt;/p&gt;
&lt;p&gt;PyPhi can be installed with Python&apos;s package manager via the command &apos;pip
install pyphi&apos; on Linux and macOS systems equipped with Python 3.4 or higher.
PyPhi is open-source and licensed under the GPLv3; the source code is hosted on
GitHub at https://github.com/wmayner/pyphi . Comprehensive and
continually-updated documentation is available at https://pyphi.readthedocs.io/
. The pyphi-users mailing list can be joined at
https://groups.google.com/forum/#!forum/pyphi-users . A web-based graphical
interface to the software is available at
&lt;a href=&quot;http://integratedinformationtheory.org/calculate.html&quot;&gt;this http URL&lt;/a&gt; .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mayner_W/0/1/0/all/0/1&quot;&gt;William G. P. Mayner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Marshall_W/0/1/0/all/0/1&quot;&gt;William Marshall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Albantakis_L/0/1/0/all/0/1&quot;&gt;Larissa Albantakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Findlay_G/0/1/0/all/0/1&quot;&gt;Graham Findlay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Marchman_R/0/1/0/all/0/1&quot;&gt;Robert Marchman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tononi_G/0/1/0/all/0/1&quot;&gt;Giulio Tononi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06202">
<title>A graph-embedded deep feedforward network for disease outcome classification and feature selection using gene expression data. (arXiv:1801.06202v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06202</link>
<description rdf:parseType="Literal">&lt;p&gt;Gene expression data represents a unique challenge in predictive model
building, because of the small number of samples $(n)$ compared to the huge
amount of features $(p)$. This &quot;$n&amp;lt;&amp;lt;p$&quot; property has hampered application of
deep learning techniques for disease outcome classification. Sparse learning by
incorporating external gene network information could be a potential solution
to this issue. Still, the problem is very challenging because (1) there are
tens of thousands of features and only hundreds of training samples, (2) the
scale-free structure of the gene network is unfriendly to the setup of
convolutional neural networks. To address these issues and build a robust
classification model, we propose the Graph-Embedded Deep Feedforward Networks
(GEDFN), to integrate external relational information of features into the deep
neural network architecture. The method is able to achieve sparse connection
between network layers to prevent overfitting. To validate the method&apos;s
capability, we conducted both simulation experiments and a real data analysis
using a breast cancer RNA-seq dataset from The Cancer Genome Atlas (TCGA). The
resulting high classification accuracy and easily interpretable feature
selection results suggest the method is a useful addition to the current
classification models and feature selection procedures. The method is available
at https://github.com/yunchuankong/NetworkNeuralNetwork.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Yunchuan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianwei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06230">
<title>Overpruning in Variational Bayesian Neural Networks. (arXiv:1801.06230v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06230</link>
<description rdf:parseType="Literal">&lt;p&gt;The motivations for using variational inference (VI) in neural networks
differ significantly from those in latent variable models. This has a
counter-intuitive consequence; more expressive variational approximations can
provide significantly worse predictions as compared to those with less
expressive families. In this work we make two contributions. First, we identify
a cause of this performance gap, variational over-pruning. Second, we introduce
a theoretically grounded explanation for this phenomenon. Our perspective sheds
light on several related published results and provides intuition into the
design of effective variational approximations of neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trippe_B/0/1/0/all/0/1&quot;&gt;Brian Trippe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06296">
<title>A Dirichlet Process Mixture Model of Discrete Choice. (arXiv:1801.06296v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1801.06296</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a mixed multinomial logit (MNL) model, which leverages the
truncated stick-breaking process representation of the Dirichlet process as a
flexible nonparametric mixing distribution. The proposed model is a Dirichlet
process mixture model and accommodates discrete representations of
heterogeneity, like a latent class MNL model. Yet, unlike a latent class MNL
model, the proposed discrete choice model does not require the analyst to fix
the number of mixture components prior to estimation, as the complexity of the
discrete mixing distribution is inferred from the evidence. For posterior
inference in the proposed Dirichlet process mixture model of discrete choice,
we derive an expectation maximisation algorithm. In a simulation study, we
demonstrate that the proposed model framework can flexibly capture
differently-shaped taste parameter distributions. Furthermore, we empirically
validate the model framework in a case study on motorists&apos; route choice
preferences and find that the proposed Dirichlet process mixture model of
discrete choice outperforms a latent class MNL model and mixed MNL models with
common parametric mixing distributions in terms of both in-sample fit and
out-of-sample predictive ability. Compared to extant modelling approaches, the
proposed discrete choice model substantially abbreviates specification
searches, as it relies on less restrictive parametric assumptions and does not
require the analyst to specify the complexity of the discrete mixing
distribution prior to estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krueger_R/0/1/0/all/0/1&quot;&gt;Rico Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vij_A/0/1/0/all/0/1&quot;&gt;Akshay Vij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rashidi_T/0/1/0/all/0/1&quot;&gt;Taha H. Rashidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06378">
<title>Introducing ReQuEST: an Open Platform for Reproducible and Quality-Efficient Systems-ML Tournaments. (arXiv:1801.06378v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06378</link>
<description rdf:parseType="Literal">&lt;p&gt;Co-designing efficient machine learning based systems across the whole
hardware/software stack to trade off speed, accuracy, energy and costs is
becoming extremely complex and time consuming. Researchers often struggle to
evaluate and compare different published works across rapidly evolving software
frameworks, heterogeneous hardware platforms, compilers, libraries, algorithms,
data sets, models, and environments.
&lt;/p&gt;
&lt;p&gt;We present our community effort to develop an open co-design tournament
platform with an online public scoreboard. It will gradually incorporate best
research practices while providing a common way for multidisciplinary
researchers to optimize and compare the quality vs. efficiency Pareto
optimality of various workloads on diverse and complete hardware/software
systems. We want to leverage the open-source Collective Knowledge framework and
the ACM artifact evaluation methodology to validate and share the complete
machine learning system implementations in a standardized, portable, and
reproducible fashion. We plan to hold regular multi-objective optimization and
co-design tournaments for emerging workloads such as deep learning, starting
with ASPLOS&apos;18 (ACM conference on Architectural Support for Programming
Languages and Operating Systems - the premier forum for multidisciplinary
systems research spanning computer architecture and hardware, programming
languages and compilers, operating systems and networking) to build a public
repository of the most efficient machine learning algorithms and systems which
can be easily customized, reused and built upon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moreau_T/0/1/0/all/0/1&quot;&gt;Thierry Moreau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lokhmotov_A/0/1/0/all/0/1&quot;&gt;Anton Lokhmotov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fursin_G/0/1/0/all/0/1&quot;&gt;Grigori Fursin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06423">
<title>Minimum spanning tree release under differential privacy constraints. (arXiv:1801.06423v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1801.06423</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of nodes clustering under privacy constraints when
representing a dataset as a graph. Our contribution is threefold. First we
formally define the concept of differential privacy for structured databases
such as graphs, and give an alternative definition based on a new neighborhood
notion between graphs. This definition is adapted to particular frameworks that
can be met in various application fields such as genomics, world wide web,
population survey, etc. Second, we introduce a new algorithm to tackle the
issue of privately releasing an approximated minimum spanning tree topology for
a simple-undirected-weighted graph. It provides a simple way of producing the
topology of a private almost minimum spanning tree which outperforms, in most
cases, the state of the art &quot;Laplace mechanism&quot; in terms of
weight-approximation error.
&lt;/p&gt;
&lt;p&gt;Finally, we propose a theoretically motivated method combining a sanitizing
mechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree
(MST)-based clustering algorithm. It provides an accurate method for nodes
clustering in a graph while keeping the sensitive information contained in the
edges weights of the private graph. We provide some theoretical results on the
robustness of an almost minimum spanning tree construction for Laplace
sanitizing mechanisms. These results exhibit which conditions the graph weights
should respect in order to consider that the nodes form well separated clusters
both for Laplace and our algorithm as sanitizing mechanism. The method has been
experimentally evaluated on simulated data, and preliminary results show the
good behavior of the algorithm while identifying well separated clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinot_R/0/1/0/all/0/1&quot;&gt;Rafael Pinot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06432">
<title>Robust Kronecker Component Analysis. (arXiv:1801.06432v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06432</link>
<description rdf:parseType="Literal">&lt;p&gt;Dictionary learning and component analysis models are fundamental in learning
compact representations that are relevant to a given task (feature extraction,
dimensionality reduction, denoising, etc.). The model complexity is encoded by
means of specific structure, such as sparsity, low-rankness, or nonnegativity.
Unfortunately, approaches like K-SVD - that learn dictionaries for sparse
coding via Singular Value Decomposition (SVD) - are hard to scale to
high-volume and high-dimensional visual data, and fragile in the presence of
outliers. Conversely, robust component analysis methods such as the Robust
Principle Component Analysis (RPCA) are able to recover low-complexity (e.g.,
low-rank) representations from data corrupted with noise of unknown magnitude
and support, but do not provide a dictionary that respects the structure of the
data (e.g., images), and also involve expensive computations. In this paper, we
propose a novel Kronecker-decomposable component analysis model, coined as
Robust Kronecker Component Analysis (RKCA), that combines ideas from sparse
dictionary learning and robust component analysis. RKCA has several appealing
properties, including robustness to gross corruption; it can be used for
low-rank modeling, and leverages separability to solve significantly smaller
problems. We design an efficient learning algorithm by drawing links with a
restricted form of tensor factorization, and analyze its optimality and
low-rankness properties. The effectiveness of the proposed approach is
demonstrated on real-world applications, namely background subtraction and
image denoising and completion, by performing a thorough comparison with the
current state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bahri_M/0/1/0/all/0/1&quot;&gt;Mehdi Bahri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panagakis_Y/0/1/0/all/0/1&quot;&gt;Yannis Panagakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06490">
<title>Worst-case Optimal Submodular Extensions for Marginal Estimation. (arXiv:1801.06490v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.06490</link>
<description rdf:parseType="Literal">&lt;p&gt;Submodular extensions of an energy function can be used to efficiently
compute approximate marginals via variational inference. The accuracy of the
marginals depends crucially on the quality of the submodular extension. To
identify the best possible extension, we show an equivalence between the
submodular extensions of the energy and the objective functions of linear
programming (LP) relaxations for the corresponding MAP estimation problem. This
allows us to (i) establish the worst-case optimality of the submodular
extension for Potts model used in the literature; (ii) identify the worst-case
optimal submodular extension for the more general class of metric labeling; and
(iii) efficiently compute the marginals for the widely used dense CRF model
with the help of a recently proposed Gaussian filtering method. Using synthetic
and real data, we show that our approach provides comparable upper bounds on
the log-partition function to those obtained using tree-reweighted message
passing (TRW) in cases where the latter is computationally feasible.
Importantly, unlike TRW, our approach provides the first practical algorithm to
compute an upper bound on the dense CRF model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pansari_P/0/1/0/all/0/1&quot;&gt;Pankaj Pansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1&quot;&gt;Chris Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;M.Pawan Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06503">
<title>Global overview of Imitation Learning. (arXiv:1801.06503v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06503</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation Learning is a sequential task where the learner tries to mimic an
expert&apos;s action in order to achieve the best performance. Several algorithms
have been proposed recently for this task. In this project, we aim at proposing
a wide review of these algorithms, presenting their main features and comparing
them on their performance and their regret bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Attia_A/0/1/0/all/0/1&quot;&gt;Alexandre Attia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dayan_S/0/1/0/all/0/1&quot;&gt;Sharone Dayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06504">
<title>Detecting and counting tiny faces. (arXiv:1801.06504v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.06504</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding Tiny Faces by Hu and Ramanan - and released at CVPR 2017 - proposes a
novel approach to find small objects in an image. Our contribution consists in
deeply understanding the choices of the paper together with applying and
extending a similar method to a real world subject which is the counting of
people in a public demonstration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attia_A/0/1/0/all/0/1&quot;&gt;Alexandre Attia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayan_S/0/1/0/all/0/1&quot;&gt;Sharone Dayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1602.02915">
<title>Calculus of the exponent of Kurdyka-{\L}ojasiewicz inequality and its applications to linear convergence of first-order methods. (arXiv:1602.02915v5 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1602.02915</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the Kurdyka-{\L}ojasiewicz (KL) exponent, an
important quantity for analyzing the convergence rate of first-order methods.
Specifically, we develop various calculus rules to deduce the KL exponent of
new (possibly nonconvex and nonsmooth) functions formed from functions with
known KL exponents. In addition, we show that the well-studied Luo-Tseng error
bound together with a mild assumption on the separation of stationary values
implies that the KL exponent is $\frac12$. The Luo-Tseng error bound is known
to hold for a large class of concrete structured optimization problems, and
thus we deduce the KL exponent of a large class of functions whose exponents
were previously unknown. Building upon this and the calculus rules, we are then
able to show that for many convex or nonconvex optimization models for
applications such as sparse recovery, their objective function&apos;s KL exponent is
$\frac12$. This includes the least squares problem with smoothly clipped
absolute deviation (SCAD) regularization or minimax concave penalty (MCP)
regularization and the logistic regression problem with $\ell_1$
regularization. Since many existing local convergence rate analysis for
first-order methods in the nonconvex scenario relies on the KL exponent, our
results enable us to obtain explicit convergence rate for various first-order
methods when they are applied to a large variety of practical optimization
models. Finally, we further illustrate how our results can be applied to
establishing local linear convergence of the proximal gradient algorithm and
the inertial proximal algorithm with constant step-sizes for some specific
models that arise in sparse recovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoyin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pong_T/0/1/0/all/0/1&quot;&gt;Ting Kei Pong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.01672">
<title>Connectome Smoothing via Low-rank Approximations. (arXiv:1609.01672v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1609.01672</link>
<description rdf:parseType="Literal">&lt;p&gt;In statistical connectomics, the quantitative study of brain networks,
estimating the mean of a population of graphs based on a sample is a core
problem. Often, this problem is especially difficult because the sample or
cohort size is relatively small, sometimes even a single subject. While using
the element-wise sample mean of the adjacency matrices is a common approach,
this method does not exploit any underlying structural properties of the
graphs. We propose using a low-rank method which incorporates tools for
dimension selection and diagonal augmentation to smooth the estimates and
improve performance over the naive methodology for small sample sizes.
Theoretical results for the stochastic blockmodel show that this method offers
major improvements when there are many vertices. Similarly, we demonstrate that
the low-rank methods outperform the standard sample mean for a variety of
independent edge distributions as well as human connectome data derived from
magnetic resonance imaging, especially when sample sizes are small. Moreover,
the low-rank methods yield &quot;eigen-connectomes&quot;, which correlate with the
lobe-structure of the human brain and superstructures of the mouse brain. These
results indicate that low-rank methods are an important part of the tool box
for researchers studying populations of graphs in general, and statistical
connectomics in particular.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Runze Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ketcha_M/0/1/0/all/0/1&quot;&gt;Michael Ketcha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Badea_A/0/1/0/all/0/1&quot;&gt;Alexandra Badea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Calabrese_E/0/1/0/all/0/1&quot;&gt;Evan D. Calabrese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Margulies_D/0/1/0/all/0/1&quot;&gt;Daniel S. Margulies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sussman_D/0/1/0/all/0/1&quot;&gt;Daniel L. Sussman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.02703">
<title>Nonparametric Bayesian inference of the microcanonical stochastic block model. (arXiv:1610.02703v3 [physics.data-an] UPDATED)</title>
<link>http://arxiv.org/abs/1610.02703</link>
<description rdf:parseType="Literal">&lt;p&gt;A principled approach to characterize the hidden structure of networks is to
formulate generative models, and then infer their parameters from data. When
the desired structure is composed of modules or &quot;communities&quot;, a suitable
choice for this task is the stochastic block model (SBM), where nodes are
divided into groups, and the placement of edges is conditioned on the group
memberships. Here, we present a nonparametric Bayesian method to infer the
modular structure of empirical networks, including the number of modules and
their hierarchical organization. We focus on a microcanonical variant of the
SBM, where the structure is imposed via hard constraints, i.e. the generated
networks are not allowed to violate the patterns imposed by the model. We show
how this simple model variation allows simultaneously for two important
improvements over more traditional inference approaches: 1. Deeper Bayesian
hierarchies, with noninformative priors replaced by sequences of priors and
hyperpriors, that not only remove limitations that seriously degrade the
inference on large networks, but also reveal structures at multiple scales; 2.
A very efficient inference algorithm that scales well not only for networks
with a large number of nodes and edges, but also with an unlimited number of
modules. We show also how this approach can be used to sample modular
hierarchies from the posterior distribution, as well as to perform model
selection. We discuss and analyze the differences between sampling from the
posterior and simply finding the single parameter estimate that maximizes it.
Furthermore, we expose a direct equivalence between our microcanonical approach
and alternative derivations based on the canonical SBM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08683">
<title>Matrix Completion and Related Problems via Strong Duality. (arXiv:1704.08683v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1704.08683</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies the strong duality of non-convex matrix factorization
problems: we show that under certain dual conditions, these problems and its
dual have the same optimum. This has been well understood for convex
optimization, but little was known for non-convex problems. We propose a novel
analytical framework and show that under certain dual conditions, the optimal
solution of the matrix factorization program is the same as its bi-dual and
thus the global optimality of the non-convex program can be achieved by solving
its bi-dual which is convex. These dual conditions are satisfied by a wide
class of matrix factorization problems, although matrix factorization problems
are hard to solve in full generality. This analytical framework may be of
independent interest to non-convex optimization more broadly.
&lt;/p&gt;
&lt;p&gt;We apply our framework to two prototypical matrix factorization problems:
matrix completion and robust Principal Component Analysis (PCA). These are
examples of efficiently recovering a hidden matrix given limited reliable
observations of it. Our framework shows that exact recoverability and strong
duality hold with nearly-optimal sample complexity guarantees for matrix
completion and robust PCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1&quot;&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10225">
<title>Bayesian stochastic blockmodeling. (arXiv:1705.10225v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter provides a self-contained introduction to the use of Bayesian
inference to extract large-scale modular structures from network data, based on
the stochastic block model (SBM), as well as its degree-corrected and
overlapping generalizations. We focus on nonparametric formulations that allow
their inference in a manner that prevents overfitting, and enables model
selection. We discuss aspects of the choice of priors, in particular how to
avoid underfitting via increased Bayesian hierarchies, and we contrast the task
of sampling network partitions from the posterior distribution with finding the
single point estimate that maximizes it, while describing efficient algorithms
to perform either one. We also show how inferring the SBM can be used to
predict missing and spurious links, and shed light on the fundamental
limitations of the detectability of modular structures in networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.03353">
<title>Low Complexity Gaussian Latent Factor Models and a Blessing of Dimensionality. (arXiv:1706.03353v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.03353</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning the structure of graphical models from data usually incurs a heavy
curse of dimensionality that renders this problem intractable in many
real-world situations. The rare cases where the curse becomes a blessing
provide insight into the limits of the efficiently computable and augment the
scarce options for treating very under-sampled, high-dimensional data. We study
a special class of Gaussian latent factor models where each (non-iid) observed
variable depends on at most one of a set of latent variables. We derive
information-theoretic lower bounds on the sample complexity for structure
recovery that suggest complexity actually decreases as the dimensionality
increases. Contrary to this prediction, we observe that existing structure
recovery methods deteriorate with increasing dimension. Therefore, we design a
new approach to learning Gaussian latent factor models that benefits from
dimensionality. Our approach relies on an unconstrained information-theoretic
objective whose global optima correspond to structured latent factor generative
models. In addition to improved structure recovery, we also show that we are
able to outperform state-of-the-art approaches for covariance estimation on
both synthetic and real data in the very under-sampled, high-dimensional
regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Galstyan_A/0/1/0/all/0/1&quot;&gt;Aram Galstyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01432">
<title>Nonparametric weighted stochastic block models. (arXiv:1708.01432v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01432</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a Bayesian formulation of weighted stochastic block models that
can be used to infer the large-scale modular structure of weighted networks,
including their hierarchical organization. Our method is nonparametric, and
thus does not require the prior knowledge of the number of groups or other
dimensions of the model, which are instead inferred from data. We give a
comprehensive treatment of different kinds of edge weights (i.e. continuous or
discrete, signed or unsigned, bounded or unbounded), as well as arbitrary
weight transformations, and describe an unsupervised model selection approach
to choose the best network description. We illustrate the application of our
method to a variety of empirical weighted networks, such as global migrations,
voting patterns in congress, and neural connections in the human brain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04153">
<title>Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then demonstrate its efficiency in the context of multi-fidelity models for
complex engineering systems, as well as a problem of global illumination in
computer graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item></rdf:RDF>