<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01929"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01937"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01941"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01947"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07031"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01956"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01960"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02205"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02264"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02408"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.04068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.08862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02285"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02338"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02590"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.07986"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.02588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.04389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.01604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08247"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08289"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10988"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.01929">
<title>Superconducting Optoelectronic Neurons I: General Principles. (arXiv:1805.01929v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01929</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of neural hardware is informed by the prominence of differentiated
processing and information integration in cognitive systems. The central role
of communication leads to the principal assumption of the hardware platform:
signals between neurons should be optical to enable fanout and communication
with minimal delay. The requirement of energy efficiency leads to the
utilization of superconducting detectors to receive single-photon signals. We
discuss the potential of superconducting optoelectronic hardware to achieve the
spatial and temporal information integration advantageous for cognitive
processing, and we consider physical scaling limits based on light-speed
communication. We introduce the superconducting optoelectronic neurons and
networks that are the subject of the subsequent papers in this series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiles_J/0/1/0/all/0/1&quot;&gt;Jeff Chiles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01937">
<title>Superconducting Optoelectronic Neurons III: Synaptic Plasticity. (arXiv:1805.01937v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01937</link>
<description rdf:parseType="Literal">&lt;p&gt;As a means of dynamically reconfiguring the synaptic weight of a
superconducting optoelectronic loop neuron, a superconducting flux storage loop
is inductively coupled to the synaptic current bias of the neuron. A standard
flux memory cell is used to achieve a binary synapse, and loops capable of
storing many flux quanta are used to enact multi-stable synapses. Circuits are
designed to implement supervised learning wherein current pulses add or remove
flux from the loop to strengthen or weaken the synaptic weight. Designs are
presented for circuits with hundreds of intermediate synaptic weights between
minimum and maximum strengths. Circuits for implementing unsupervised learning
are modeled using two photons to strengthen and two photons to weaken the
synaptic weight via Hebbian and anti-Hebbian learning rules, and techniques are
proposed to control the learning rate. Implementation of short-term plasticity,
homeostatic plasticity, and metaplasticity in loop neurons is discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donnelly_C/0/1/0/all/0/1&quot;&gt;Christine A. Donnelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castellanos_Beltran_M/0/1/0/all/0/1&quot;&gt;Manuel Castellanos-Beltran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_M/0/1/0/all/0/1&quot;&gt;Michael L. Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01941">
<title>Superconducting Optoelectronic Neurons IV: Transmitter Circuits. (arXiv:1805.01941v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01941</link>
<description rdf:parseType="Literal">&lt;p&gt;A superconducting optoelectronic neuron will produce a small current pulse
upon reaching threshold. We present an amplifier chain that converts this small
current pulse to a voltage pulse sufficient to produce light from a
semiconductor diode. This light is the signal used to communicate between
neurons in the network. The amplifier chain comprises a thresholding Josephson
junction, a relaxation oscillator Josephson junction, a superconducting
thin-film current-gated current amplifier, and a superconducting thin-film
current-gated voltage amplifier. We analyze the performance of the elements in
the amplifier chain in the time domain to calculate the energy consumption per
photon created for several values of light-emitting diode capacitance and
efficiency. The speed of the amplification sequence allows neuronal firing up
to at least 20\,MHz with power density low enough to be cooled easily with
standard $^4$He cryogenic systems operating at 4.2\,K.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01942">
<title>Superconducting Optoelectronic Neurons V: Networks and Scaling. (arXiv:1805.01942v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01942</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks of superconducting optoelectronic neurons are investigated for their
near-term technological potential and long-term physical limitations. Networks
with short average path length, high clustering coefficient, and power-law
degree distribution are designed using a growth model that assigns connections
between new and existing nodes based on spatial distance as well as degree of
existing nodes. The network construction algorithm is scalable to arbitrary
levels of network hierarchy and achieves systems with fractal spatial
properties and efficient wiring. By modeling the physical size of
superconducting optoelectronic neurons, we calculate the area of these
networks. A system with 8100 neurons and 330,430 total synapses will fit on a
1\,cm $\times$ 1\,cm die. Systems of millions of neurons with hundreds of
millions of synapses will fit on a 300\,mm wafer. For multi-wafer assemblies,
communication at light speed enables a neuronal pool the size of a large data
center comprising 100 trillion neurons with coherent oscillations at 1\,MHz.
Assuming a power law frequency distribution, as is necessary for self-organized
criticality, we calculate the power consumption of the networks. We find the
use of single photons for communication and superconducting circuits for
computation leads to power density low enough to be cooled by liquid $^4$He for
networks of any scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiles_J/0/1/0/all/0/1&quot;&gt;Jeff Chiles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01947">
<title>Light for communication and superconductors for efficiency in neural computing. (arXiv:1805.01947v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical communication achieves high fanout and short delay advantageous for
information integration in neural systems. Superconducting detectors enable
signaling with single photons for maximal energy efficiency. We present designs
of superconducting optoelectronic neurons based on superconducting
single-photon detectors, Josephson junctions, semiconductor light sources, and
multi-planar dielectric waveguides. These circuits achieve complex synaptic and
neuronal functions with high energy efficiency, leveraging the strengths of
light for communication and superconducting electronics for computation. The
neurons send few-photon signals to synaptic connections. These signals
communicate neuronal firing events as well as update synaptic weights.
Spike-timing-dependent plasticity is implemented with a single photon
triggering each step of the process. Microscale light-emitting diodes and
waveguide networks enable connectivity from a neuron to thousands of synaptic
connections, and the use of light for communication enables synchronization of
neurons across an area limited only by the distance light can travel within the
period of a network oscillation. Experimentally, each of the requisite circuit
elements has been demonstrated, yet a hardware platform combining them all has
not been attempted. Compared to digital logic or quantum computing, device
tolerances are relaxed. For this neural application, sources providing
incoherent pulses with 10,000 photons produced with efficiency of 10$^{-3}$
operating at 20\,MHz at 4.2\,K are sufficient to enable a massively scalable
neural computing platform with connectivity comparable to the brain and thirty
thousand times higher speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiles_J/0/1/0/all/0/1&quot;&gt;Jeff Chiles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07031">
<title>Protein Folding Optimization using Differential Evolution Extended with Local Search and Component Reinitialization. (arXiv:1710.07031v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07031</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel Differential Evolution algorithm for protein
folding optimization that is applied to a three-dimensional AB off-lattice
model. The proposed algorithm includes two new mechanisms. A local search is
used to improve convergence speed and to reduce the runtime complexity of the
energy calculation. For this purpose, a local movement is introduced within the
local search. The designed evolutionary algorithm has fast convergence speed
and, therefore, when it is trapped into the local optimum or a relatively good
solution is located, it is hard to locate a better similar solution. The
similar solution is different from the good solution in only a few components.
A component reinitialization method is designed to mitigate this problem. Both
the new mechanisms and the proposed algorithm were analyzed on well-known amino
acid sequences that are used frequently in the literature. Experimental results
show that the employed new mechanisms improve the efficiency of our algorithm
and that the proposed algorithm is superior to other state-of-the-art
algorithms. It obtained a hit ratio of 100% for sequences up to 18 monomers,
within a budget of $10^{11}$ solution evaluations. New best-known solutions
were obtained for most of the sequences. The existence of the symmetric
best-known solutions is also demonstrated in the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boskovic_B/0/1/0/all/0/1&quot;&gt;Borko Bo&amp;#x161;kovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brest_J/0/1/0/all/0/1&quot;&gt;Janez Brest&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06928">
<title>Memcomputing: Leveraging memory and physics to compute efficiently. (arXiv:1802.06928v2 [cs.ET] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06928</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well known that physical phenomena may be of great help in computing
some difficult problems efficiently. A typical example is prime factorization
that may be solved in polynomial time by exploiting quantum entanglement on a
quantum computer. There are, however, other types of (non-quantum) physical
properties that one may leverage to compute efficiently a wide range of hard
problems. In this perspective we discuss how to employ one such property,
memory (time non-locality), in a novel physics-based approach to computation:
Memcomputing. In particular, we focus on digital memcomputing machines (DMMs)
that are scalable. DMMs can be realized with non-linear dynamical systems with
memory. The latter property allows the realization of a new type of Boolean
logic, one that is self-organizing. Self-organizing logic gates are
&quot;terminal-agnostic&quot;, namely they do not distinguish between input and output
terminals. When appropriately assembled to represent a given
combinatorial/optimization problem, the corresponding self-organizing circuit
converges to the equilibrium points that express the solutions of the problem
at hand. In doing so, DMMs take advantage of the long-range order that develops
during the transient dynamics. This collective dynamical behavior, reminiscent
of a phase transition, or even the &quot;edge of chaos&quot;, is mediated by families of
classical trajectories (instantons) that connect critical points of increasing
stability in the system&apos;s phase space. The topological character of the
solution search renders DMMs robust against noise and structural disorder.
Since DMMs are non-quantum systems described by ordinary differential
equations, not only can they be built in hardware with available technology,
they can also be simulated efficiently on modern classical computers. As an
example, we will show the polynomial-time solution of the subset-sum problem
for the worst...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventra_M/0/1/0/all/0/1&quot;&gt;Massimiliano Di Ventra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traversa_F/0/1/0/all/0/1&quot;&gt;Fabio L. Traversa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07234">
<title>Limited Evaluation Cooperative Co-evolutionary Differential Evolution for Large-scale Neuroevolution. (arXiv:1804.07234v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07234</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world control and classification tasks involve a large number of
features. When artificial neural networks (ANNs) are used for modeling these
tasks, the network architectures tend to be large. Neuroevolution is an
effective approach for optimizing ANNs; however, there are two bottlenecks that
make their application challenging in case of high-dimensional networks using
direct encoding. First, classic evolutionary algorithms tend not to scale well
for searching large parameter spaces; second, the network evaluation over a
large number of training instances is in general time-consuming. In this work,
we propose an approach called the Limited Evaluation Cooperative
Co-evolutionary Differential Evolution algorithm (LECCDE) to optimize
high-dimensional ANNs.
&lt;/p&gt;
&lt;p&gt;The proposed method aims to optimize the pre-synaptic weights of each
post-synaptic neuron in different subpopulations using a Cooperative
Co-evolutionary Differential Evolution algorithm, and employs a limited
evaluation scheme where fitness evaluation is performed on a relatively small
number of training instances based on fitness inheritance. We test LECCDE on
three datasets with various sizes, and our results show that cooperative
co-evolution significantly improves the test error comparing to standard
Differential Evolution, while the limited evaluation scheme facilitates a
significant reduction in computing time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaman_A/0/1/0/all/0/1&quot;&gt;Anil Yaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1&quot;&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iacca_G/0/1/0/all/0/1&quot;&gt;Giovanni Iacca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fletcher_G/0/1/0/all/0/1&quot;&gt;George Fletcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01907">
<title>Exploration by Distributional Reinforcement Learning. (arXiv:1805.01907v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01907</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a framework based on distributional reinforcement learning and
recent attempts to combine Bayesian parameter updates with deep reinforcement
learning. We show that our proposed framework conceptually unifies multiple
previous methods in exploration. We also derive a practical algorithm that
achieves efficient exploration on challenging control tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Shipra Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01954">
<title>Behavioral Cloning from Observation. (arXiv:1805.01954v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.01954</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans often learn how to perform tasks via imitation: they observe others
perform a task, and then very quickly infer the appropriate actions to take
based on their observations. While extending this paradigm to autonomous agents
is a well-studied problem in general, there are two particular aspects that
have largely been overlooked: (1) that the learning is done from observation
only (i.e., without explicit action information), and (2) that the learning is
typically done very quickly. In this work, we propose a two-phase, autonomous
imitation learning technique called behavioral cloning from observation (BCO),
that aims to provide improved performance with respect to both of these
aspects. First, we allow the agent to acquire experience in a self-supervised
fashion. This experience is used to develop a model which is then utilized to
learn a particular task by observing an expert perform that task without the
knowledge of the specific actions taken. We experimentally compare BCO to
imitation learning methods, including the state-of-the-art, generative
adversarial imitation learning (GAIL) technique, and we show comparable task
performance in several different simulation domains while exhibiting increased
learning speed after expert trajectories become available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1&quot;&gt;Faraz Torabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1&quot;&gt;Garrett Warnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01956">
<title>Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning. (arXiv:1805.01956v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.01956</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots that navigate among pedestrians use collision avoidance algorithms to
enable safe and efficient operation. Recent works present deep reinforcement
learning as a framework to model the complex interactions and cooperation.
However, they are implemented using key assumptions about other agents&apos;
behavior that deviate from reality as the number of agents in the environment
increases. This work extends our previous approach to develop an algorithm that
learns collision avoidance among a variety of types of dynamic agents without
assuming they follow any particular behavior rules. This work also introduces a
strategy using LSTM that enables the algorithm to use observations of an
arbitrary number of other agents, instead of previous methods that have a fixed
observation size. The proposed algorithm outperforms our previous approach in
simulation as the number of agents increases, and the algorithm is demonstrated
on a fully autonomous robotic vehicle traveling at human walking speed, without
the use of a 3D Lidar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Everett_M/0/1/0/all/0/1&quot;&gt;Michael Everett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Fan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01960">
<title>Causal programming: inference with structural causal models as finding instances of a relation. (arXiv:1805.01960v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1805.01960</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a causal inference relation and causal programming as
general frameworks for causal inference with structural causal models. A tuple,
$\langle M, I, Q, F \rangle$, is an instance of the relation if a formula, $F$,
computes a causal query, $Q$, as a function of known population probabilities,
$I$, in every model entailed by a set of model assumptions, $M$. Many problems
in causal inference can be viewed as the problem of enumerating instances of
the relation that satisfy given criteria. This unifies a number of previously
studied problems, including causal effect identification, causal discovery and
recovery from selection bias. In addition, the relation supports formalizing
new problems in causal inference with structural causal models, such as the
problem of research design. Causal programming is proposed as a further
generalization of causal inference as the problem of finding optimal instances
of the relation, with respect to a cost function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brule_J/0/1/0/all/0/1&quot;&gt;Joshua Brul&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01987">
<title>Designing the Game to Play: Optimizing Payoff Structure in Security Games. (arXiv:1805.01987v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1805.01987</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective game-theoretic modeling of defender-attacker behavior is becoming
increasingly important. In many domains, the defender functions not only as a
player but also the designer of the game&apos;s payoff structure. We study
Stackelberg Security Games where the defender, in addition to allocating
defensive resources to protect targets from the attacker, can strategically
manipulate the attacker&apos;s payoff under budget constraints in weighted L^p-norm
form regarding the amount of change. Focusing on problems with weighted
L^1-norm form constraint, we present (i) a mixed integer linear program-based
algorithm with approximation guarantee; (ii) a branch-and-bound based algorithm
with improved efficiency achieved by effective pruning; (iii) a polynomial time
approximation scheme for a special but practical class of problems. In
addition, we show that problems under budget constraints in L^0-norm form and
weighted L^\infty-norm form can be solved in polynomial time. We provide an
extensive experimental evaluation of our proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Ryan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Ziye Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_Thanh_L/0/1/0/all/0/1&quot;&gt;Long Tran-Thanh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rohit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fei Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02102">
<title>Cluster-based trajectory segmentation with local noise. (arXiv:1805.02102v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.02102</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for the partitioning of a spatial trajectory in a
sequence of segments based on spatial density and temporal criteria. The result
is a set of temporally separated clusters interleaved by sub-sequences of
unclustered points. A major novelty is the proposal of an outlier or noise
model based on the distinction between intra-cluster (local noise) and
inter-cluster noise (transition): the local noise models the temporary absence
from a residence while the transition the definitive departure towards a next
residence. We analyze in detail the properties of the model and present a
comprehensive solution for the extraction of temporally ordered clusters. The
effectiveness of the solution is evaluated first qualitatively and next
quantitatively by contrasting the segmentation with ground truth. The ground
truth consists of a set of trajectories of labeled points simulating animal
movement. Moreover, we show that the approach can streamline the discovery of
additional derived patterns, by presenting a novel technique for the analysis
of periodic movement. From a methodological perspective, a valuable aspect of
this research is that it combines the theoretical investigation with the
application and external validation of the segmentation framework. This paves
the way to an effective deployment of the solution in broad and challenging
fields such as e-science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damiani_M/0/1/0/all/0/1&quot;&gt;Maria Luisa Damiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hachem_F/0/1/0/all/0/1&quot;&gt;Fatima Hachem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamza_I/0/1/0/all/0/1&quot;&gt;Issa Hamza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranc_N/0/1/0/all/0/1&quot;&gt;Nathan Ranc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moorcroft_P/0/1/0/all/0/1&quot;&gt;Paul Moorcroft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagnacci_F/0/1/0/all/0/1&quot;&gt;Francesca Cagnacci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02114">
<title>An Accelerated Approach to Safely and Efficiently Test Pre-produced Autonomous Vehicles on Public Streets. (arXiv:1805.02114v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.02114</link>
<description rdf:parseType="Literal">&lt;p&gt;Various automobile and mobility companies, for instance, Ford, Uber, and
Waymo, are currently testing their pre-produced autonomous vehicle (AV) fleets
on the public roads. However, due to the rareness of the safety-critical cases
and, effectively, unlimited number of possible traffic scenarios, these on-road
testing efforts have been acknowledged as tedious, costly, and risky. In this
study, we propose Accelerated Deployment framework to safely and efficiently
estimate the AVs performance on public streets. We showed that by appropriately
addressing the gradual accuracy improvement and adaptively selecting meaningful
and safe environment under which the AV is deployed, the proposed framework
yield to highly accurate estimation with much faster evaluation time, and more
importantly, lower deployment risk. Our findings provide an answer to the
currently heated and active discussions on how to properly test AV performance
on public roads so as to achieve safe, efficient, and statistically-reliable
testing framework for AV technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arief_M/0/1/0/all/0/1&quot;&gt;Mansur Arief&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1&quot;&gt;Peter Glynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02181">
<title>Context Spaces as the Cornerstone of a Near-Transparent &amp; Self-Reorganizing Semantic Desktop. (arXiv:1805.02181v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.02181</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing Semantic Desktops are still reproached for being too complicated to
use or not scaling well. Besides, a real &quot;killer app&quot; is still missing. In this
paper, we present a new prototype inspired by NEPOMUK and its successors having
a semantic graph and ontologies as its basis. In addition, we introduce the
idea of context spaces that users can directly interact with and work on. To
make them available in all applications without further ado, the system is
transparently integrated using mostly standard protocols complemented by a
sidebar for advanced features. By exploiting collected context information and
applying Managed Forgetting features (like hiding, condensation or deletion),
the system is able to dynamically reorganize itself, which also includes a kind
of tidy-up-itself functionality. We therefore expect it to be more scalable
while providing new levels of user support. An early prototype has been
implemented and is presented in this demo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jilek_C/0/1/0/all/0/1&quot;&gt;Christian Jilek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Markus Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_S/0/1/0/all/0/1&quot;&gt;Sven Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maus_H/0/1/0/all/0/1&quot;&gt;Heiko Maus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02205">
<title>Correlation Heuristics for Constraint Programming. (arXiv:1805.02205v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.02205</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective general-purpose search strategies are an important component in
Constraint Programming. We introduce a new idea, namely, using correlations
between variables to guide search. Variable correlations are measured and
maintained by using domain changes during constraint propagation. We propose
two variable heuristics based on the correlation matrix, \emph{crbs-sum} and
\emph{crbs-max}. We evaluate our correlation heuristics with well known
heuristics, namely, \emph{dom/wdeg}, impact-based search and activity-based
search. Experiments on a large set of benchmarks show that our correlation
heuristics are competitive with the other heuristics, and can be the fastest on
many series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruiwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1&quot;&gt;Wei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yap_R/0/1/0/all/0/1&quot;&gt;Roland H. C. Yap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02264">
<title>Automated Diagnosis of Clinic Workflows. (arXiv:1805.02264v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.02264</link>
<description rdf:parseType="Literal">&lt;p&gt;Outpatient clinics often run behind schedule due to patients who arrive late
or appointments that run longer than expected. We sought to develop a
generalizable method that would allow healthcare providers to diagnose problems
in workflow that disrupt the schedule on any given provider clinic day. We use
a constraint optimization problem to identify the least number of appointment
modifications that make the rest of the schedule run on-time. We apply this
method to an outpatient clinic at Vanderbilt. For patient seen in this clinic
between March 27, 2017 and April 21, 2017, long cycle times tended to affect
the overall schedule more than late patients. Results from this workflow
diagnosis method could be used to inform interventions to help clinics run
smoothly, thus decreasing patient wait times and increasing provider
utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1&quot;&gt;Alex Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1&quot;&gt;Jules White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02363">
<title>Planning and Learning with Stochastic Action Sets. (arXiv:1805.02363v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.02363</link>
<description rdf:parseType="Literal">&lt;p&gt;In many practical uses of reinforcement learning (RL) the set of actions
available at a given state is a random variable, with realizations governed by
an exogenous stochastic process. Somewhat surprisingly, the foundations for
such sequential decision processes have been unaddressed. In this work, we
formalize and investigate MDPs with stochastic action sets (SAS-MDPs) to
provide these foundations. We show that optimal policies and value functions in
this model have a structure that admits a compact representation. From an RL
perspective, we show that Q-learning with sampled action sets is sound. In
model-based settings, we consider two important special cases: when individual
actions are available with independent probabilities; and a sampling-based
model for unknown distributions. We develop poly-time value and policy
iteration methods for both cases; and in the first, we offer a poly-time linear
programming solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1&quot;&gt;Craig Boutilier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Alon Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1&quot;&gt;Amit Daniely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1&quot;&gt;Avinatan Hassidim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1&quot;&gt;Yishay Mansour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meshi_O/0/1/0/all/0/1&quot;&gt;Ofer Meshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mladenov_M/0/1/0/all/0/1&quot;&gt;Martin Mladenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02408">
<title>Improving Knowledge Graph Embedding Using Simple Constraints. (arXiv:1805.02408v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.02408</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of
current research. Early works performed this task via simple models developed
over KG triples. Recent attempts focused on either designing more complicated
triple scoring models, or incorporating extra information beyond triples. This
paper, by contrast, investigates the potential of using very simple constraints
to improve KG embedding. We examine {\it non-negativity constraints} on entity
representations and {\it approximate entailment constraints} on relation
representations. The former help to learn compact and interpretable
representations for entities. The latter further encode regularities of logical
entailment between relations into their distributed representations. These
constraints impose prior beliefs upon the structure of the embedding space,
without negative impacts on efficiency or scalability. Evaluation on WordNet,
Freebase, and DBpedia shows that our approach is simple yet surprisingly
effective, significantly and consistently outperforming competitive baselines.
The constraints imposed indeed improve model interpretability, leading to a
substantially increased structuring of the embedding space. Code and data are
available at \url{https://github.com/iieir-km/ComplEx-NNE_AER}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Boyang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Li Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.04068">
<title>A Signaling Game Approach to Databases Querying and Interaction. (arXiv:1603.04068v5 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1603.04068</link>
<description rdf:parseType="Literal">&lt;p&gt;As most users do not precisely know the structure and/or the content of
databases, their queries do not exactly reflect their information needs. The
database management systems (DBMS) may interact with users and use their
feedback on the returned results to learn the information needs behind their
queries. Current query interfaces assume that users do not learn and modify the
way way they express their information needs in form of queries during their
interaction with the DBMS. Using a real-world interaction workload, we show
that users learn and modify how to express their information needs during their
interactions with the DBMS and their learning is accurately modeled by a
well-known reinforcement learning mechanism. As current data interaction
systems assume that users do not modify their strategies, they cannot discover
the information needs behind users&apos; queries effectively. We model the
interaction between users and DBMS as a game with identical interest between
two rational agents whose goal is to establish a common language for
representing information needs in form of queries. We propose a reinforcement
learning method that learns and answers the information needs behind queries
and adapts to the changes in users&apos; strategies and prove that it improves the
effectiveness of answering queries stochastically speaking. We propose two
efficient implementation of this method over large relational databases. Our
extensive empirical studies over real-world query workloads indicate that our
algorithms are efficient and effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCamish_B/0/1/0/all/0/1&quot;&gt;Ben McCamish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghadakchi_V/0/1/0/all/0/1&quot;&gt;Vahid Ghadakchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Termehchy_A/0/1/0/all/0/1&quot;&gt;Arash Termehchy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touri_B/0/1/0/all/0/1&quot;&gt;Behrouz Touri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.08862">
<title>Socially Aware Motion Planning with Deep Reinforcement Learning. (arXiv:1703.08862v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1703.08862</link>
<description rdf:parseType="Literal">&lt;p&gt;For robotic vehicles to navigate safely and efficiently in pedestrian-rich
environments, it is important to model subtle human behaviors and navigation
rules (e.g., passing on the right). However, while instinctive to humans,
socially compliant navigation is still difficult to quantify due to the
stochasticity in people&apos;s behaviors. Existing works are mostly focused on using
feature-matching techniques to describe and imitate human paths, but often do
not generalize well since the feature values can vary from person to person,
and even run to run. This work notes that while it is challenging to directly
specify the details of what to do (precise mechanisms of human navigation), it
is straightforward to specify what not to do (violations of social norms).
Specifically, using deep reinforcement learning, this work develops a
time-efficient navigation policy that respects common social norms. The
proposed method is shown to enable fully autonomous navigation of a robotic
vehicle moving at human walking speed in an environment with many pedestrians.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Fan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Everett_M/0/1/0/all/0/1&quot;&gt;Michael Everett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02896">
<title>Recurrent Deterministic Policy Gradient Method for Bipedal Locomotion on Rough Terrain Challenge. (arXiv:1710.02896v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02896</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep learning framework that is capable of solving
partially observable locomotion tasks based on our novel Recurrent
Deterministic Policy Gradient (RDPG). Three major improvements are applied in
our RDPG based learning framework: asynchronized backup of interpolated
temporal difference, initialisation of hidden state using past trajectory
scanning, and injection of external experiences learned by other agents. The
proposed learning framework was implemented to solve the Bipedal-Walker
challenge in OpenAI&apos;s gym simulation environment where only partial state
information is available. Our simulation study shows that the autonomous
behaviors generated by the RDPG agent are highly adaptive to a variety of
obstacles and enables the agent to traverse rugged terrains effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Doo Re Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chuanyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGreavy_C/0/1/0/all/0/1&quot;&gt;Christopher McGreavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00740">
<title>Learning to Represent Programs with Graphs. (arXiv:1711.00740v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00740</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning tasks on source code (i.e., formal languages) have been considered
recently, but most work has tried to transfer natural language methods and does
not capitalize on the unique opportunities offered by code&apos;s known syntax. For
example, long-range dependencies induced by using the same variable or function
in distant locations are often not considered. We propose to use graphs to
represent both the syntactic and semantic structure of code and use graph-based
deep learning methods to learn to reason over program structures.
&lt;/p&gt;
&lt;p&gt;In this work, we present how to construct graphs from source code and how to
scale Gated Graph Neural Networks training to such large graphs. We evaluate
our method on two tasks: VarNaming, in which a network attempts to predict the
name of a variable given its usage, and VarMisuse, in which the network learns
to reason about selecting the correct variable that should be used at a given
program location. Our comparison to methods that use less structured program
representations shows the advantages of modeling known structure, and suggests
that our models learn to infer meaningful names and to solve the VarMisuse task
in many cases. Additionally, our testing showed that VarMisuse identifies a
number of bugs in mature open-source projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1&quot;&gt;Miltiadis Allamanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1&quot;&gt;Marc Brockschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khademi_M/0/1/0/all/0/1&quot;&gt;Mahmoud Khademi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07875">
<title>Constructive Preference Elicitation over Hybrid Combinatorial Spaces. (arXiv:1711.07875v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07875</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference elicitation is the task of suggesting a highly preferred
configuration to a decision maker. The preferences are typically learned by
querying the user for choice feedback over pairs or sets of objects. In its
constructive variant, new objects are synthesized &quot;from scratch&quot; by maximizing
an estimate of the user utility over a combinatorial (possibly infinite) space
of candidates. In the constructive setting, most existing elicitation
techniques fail because they rely on exhaustive enumeration of the candidates.
A previous solution explicitly designed for constructive tasks comes with no
formal performance guarantees, and can be very expensive in (or unapplicable
to) problems with non-Boolean attributes. We propose the Choice Perceptron, a
Perceptron-like algorithm for learning user preferences from set-wise choice
feedback over constructive domains and hybrid Boolean-numeric feature spaces.
We provide a theoretical analysis on the attained regret that holds for a large
class of query selection strategies, and devise a heuristic strategy that aims
at optimizing the regret in practice. Finally, we demonstrate its effectiveness
by empirical evaluation against existing competitors on constructive scenarios
of increasing complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragone_P/0/1/0/all/0/1&quot;&gt;Paolo Dragone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1&quot;&gt;Stefano Teso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10495">
<title>Lifted Filtering via Exchangeable Decomposition. (arXiv:1801.10495v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10495</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a model for exact recursive Bayesian filtering based on lifted
multiset states. Combining multisets with lifting makes it possible to
simultaneously exploit multiple strategies for reducing inference complexity
when compared to list-based grounded state representations. The core idea is to
borrow the concept of Maximally Parallel Multiset Rewriting Systems and to
enhance it by concepts from Rao-Blackwellization and Lifted Inference, giving a
representation of state distributions that enables efficient inference. In
worlds where the random variables that define the system state are exchangeable
-- where the identity of entities does not matter -- it automatically uses a
representation that abstracts from ordering (achieving an exponential reduction
in complexity) -- and it automatically adapts when observations or system
dynamics destroy exchangeability by breaking symmetry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludtke_S/0/1/0/all/0/1&quot;&gt;Stefan L&amp;#xfc;dtke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Max Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bader_S/0/1/0/all/0/1&quot;&gt;Sebastian Bader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirste_T/0/1/0/all/0/1&quot;&gt;Thomas Kirste&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09158">
<title>Surrogate Scoring Rules and a Dominant Truth Serum for Information Elicitation. (arXiv:1802.09158v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09158</link>
<description rdf:parseType="Literal">&lt;p&gt;We study information elicitation without verification (IEWV) and ask the
following question: Can we achieve truthfulness in dominant strategy in IEWV?
This paper considers two elicitation settings. The first setting is when the
mechanism designer has access to a random variable that is a noisy or proxy
version of the ground truth, with known biases. The second setting is the
standard peer prediction setting where agents&apos; reports are the only source of
information that the mechanism designer has. We introduce surrogate scoring
rules (SSR) for the first setting, which use the noisy ground truth to evaluate
quality of elicited information, and show that SSR achieve truthful elicitation
in dominant strategy. Built upon SSR, we develop a multi-task mechanism,
dominant truth serum (DTS), to achieve truthful elicitation in dominant
strategy when the mechanism designer only has access to agents&apos; reports (the
second setting). The method relies on an estimation procedure to accurately
estimate the average bias in the reports of other agents. With the accurate
estimation, a random peer agent&apos;s report serves as a noisy ground truth and SSR
can then be applied to achieve truthfulness in dominant strategy. A salient
feature of SSR and DTS is that they both quantify the quality or value of
information despite lack of ground truth, just as proper scoring rules do for
the with verification setting. Our work complements both the strictly proper
scoring rule literature by solving the case where the mechanism designer only
has access to a noisy or proxy version of the ground truth, and the peer
prediction literature by achieving truthful elicitation in dominant strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiling Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09066">
<title>ECO: Efficient Convolutional Network for Online Video Understanding. (arXiv:1804.09066v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09066</link>
<description rdf:parseType="Literal">&lt;p&gt;The state of the art in video understanding suffers from two problems: (1)
The major part of reasoning is performed locally in the video, therefore, it
misses important relationships within actions that span several seconds. (2)
While there are local methods with fast per-frame processing, the processing of
the whole video is not efficient and hampers fast video retrieval or online
classification of long-term activities. In this paper, we introduce a network
architecture that takes long-term content into account and enables fast
per-video processing at the same time. The architecture is based on merging
long-term content already in the network rather than in a post-hoc fusion.
Together with a sampling strategy, which exploits that neighboring frames are
largely redundant, this yields high-quality action classification and video
captioning at up to 230 videos per second, where each video can consist of a
few hundred frames. The approach achieves competitive performance across all
datasets while being 10x to 80x faster than state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zolfaghari_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Zolfaghari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Kamaljeet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00912">
<title>Fast Directional Self-Attention Mechanism. (arXiv:1805.00912v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00912</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a self-attention mechanism, dubbed &quot;fast
directional self-attention (Fast-DiSA)&quot;, which is a fast and light extension of
&quot;directional self-attention (DiSA)&quot;. The proposed Fast-DiSA performs as
expressively as the original DiSA but only uses much less computation time and
memory, in which 1) both token2token and source2token dependencies are modeled
by a joint compatibility function designed for a hybrid of both dot-product and
multi-dim ways; 2) both multi-head and multi-dim attention combined with
bi-directional temporal information captured by multiple positional masks are
in consideration without heavy time and memory consumption appearing in the
DiSA. The experiment results show that the proposed Fast-DiSA can achieve
state-of-the-art performance as fast and memory-friendly as CNNs. The code for
Fast-DiSA is released at
\url{https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1&quot;&gt;Tao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01889">
<title>t-PINE: Tensor-based Predictable and Interpretable Node Embeddings. (arXiv:1805.01889v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01889</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph representations have increasingly grown in popularity during the last
years. Existing representation learning approaches explicitly encode network
structure. Despite their good performance in downstream processes (e.g., node
classification, link prediction), there is still room for improvement in
different aspects, like efficacy, visualization, and interpretability. In this
paper, we propose, t-PINE, a method that addresses these limitations. Contrary
to baseline methods, which generally learn explicit graph representations by
solely using an adjacency matrix, t-PINE avails a multi-view information graph,
the adjacency matrix represents the first view, and a nearest neighbor
adjacency, computed over the node features, is the second view, in order to
learn explicit and implicit node representations, using the Canonical Polyadic
(a.k.a. CP) decomposition. We argue that the implicit and the explicit mapping
from a higher-dimensional to a lower-dimensional vector space is the key to
learn more useful, highly predictable, and gracefully interpretable
representations. Having good interpretable representations provides a good
guidance to understand how each view contributes to the representation learning
process. In addition, it helps us to exclude unrelated dimensions. Extensive
experiments show that t-PINE drastically outperforms baseline methods by up to
158.6% with respect to Micro-F1, in several multi-label classification
problems, while it has high visualization and interpretability utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Sayouri_S/0/1/0/all/0/1&quot;&gt;Saba A. Al-Sayouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gujral_E/0/1/0/all/0/1&quot;&gt;Ekta Gujral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1&quot;&gt;Danai Koutra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_S/0/1/0/all/0/1&quot;&gt;Sarah S. Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01955">
<title>Improve Uncertainty Estimation for Unknown Classes in Bayesian Neural Networks with Semi-Supervised /One Set Classification. (arXiv:1805.01955v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep neural network (DNN) has achieved many state-of-the-art
results, estimating the uncertainty presented in the DNN model and the data is
a challenging task. Problems related to uncertainty such as classifying unknown
classes (class which does not appear in the training data) data as known class
with high confidence, is critically concerned in the safety domain area (e.g,
autonomous driving, medical diagnosis). In this paper, we show that applying
current Bayesian Neural Network (BNN) techniques alone does not effectively
capture the uncertainty. To tackle this problem, we introduce a simple way to
improve the BNN by using one class classification (in this paper, we use the
term &quot;set classification&quot; instead). We empirically show the result of our
method on an experiment which involves three datasets: MNIST, notMNIST and
FMNIST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_B/0/1/0/all/0/1&quot;&gt;Buu Phan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02087">
<title>A Constraint-Based Algorithm For Causal Discovery with Cycles, Latent Variables and Selection Bias. (arXiv:1805.02087v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.02087</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal processes in nature may contain cycles, and real datasets may violate
causal sufficiency as well as contain selection bias. No constraint-based
causal discovery algorithm can currently handle cycles, latent variables and
selection bias (CLS) simultaneously. I therefore introduce an algorithm called
Cyclic Causal Inference (CCI) that makes sound inferences with a conditional
independence oracle under CLS, provided that we can represent the cyclic causal
process as a non-recursive linear structural equation model with independent
errors. Empirical results show that CCI outperforms CCD in the cyclic case as
well as rivals FCI and RFCI in the acyclic case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strobl_E/0/1/0/all/0/1&quot;&gt;Eric V. Strobl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02123">
<title>Population Anomaly Detection through Deep Gaussianization. (arXiv:1805.02123v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.02123</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an algorithmic method for population anomaly detection based on
gaussianization through an adversarial autoencoder. This method is applicable
to detection of `soft&apos; anomalies in arbitrarily distributed highly-dimensional
data.
&lt;/p&gt;
&lt;p&gt;A soft, or population, anomaly is characterized by a shift in the
distribution of the data set, where certain elements appear with higher
probability than anticipated. Such anomalies must be detected by considering a
sufficiently large sample set rather than a single sample.
&lt;/p&gt;
&lt;p&gt;Applications include, but not limited to, payment fraud trends, data
exfiltration, disease clusters and epidemics, and social unrests. We evaluate
the method on several domains and obtain both quantitative results and
qualitative insights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tolpin_D/0/1/0/all/0/1&quot;&gt;David Tolpin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02136">
<title>Private Sequential Learning. (arXiv:1805.02136v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.02136</link>
<description rdf:parseType="Literal">&lt;p&gt;We formulate a private learning model to study an intrinsic tradeoff between
privacy and query complexity in sequential learning. Our model involves a
learner who aims to determine a scalar value, $v^*$, by sequentially querying
an external database and receiving binary responses. In the meantime, an
adversary observes the learner&apos;s queries, though not the responses, and tries
to infer from them the value of $v^*$. The objective of the learner is to
obtain an accurate estimate of $v^*$ using only a small number of queries,
while simultaneously protecting her privacy by making $v^*$ provably difficult
to learn for the adversary. Our main results provide tight upper and lower
bounds on the learner&apos;s query complexity as a function of desired levels of
privacy and estimation accuracy. We also construct explicit query strategies
whose complexity is optimal up to an additive constant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsitsiklis_J/0/1/0/all/0/1&quot;&gt;John N. Tsitsiklis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kuang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02176">
<title>Predicting clinical significance of BRCA1 and BRCA2 single nucleotide substitution variants with unknown clinical significance using probabilistic neural network and deep neural network-stacked autoencoder. (arXiv:1805.02176v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/1805.02176</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-synonymous single nucleotide polymorphisms (nsSNPs) are single nucleotide
substitution occurring in the coding region of a gene and leads to a change in
amino-acid sequence of protein. The studies have shown these variations may be
associated with disease. Thus, investigating the effects of nsSNPs on protein
function will give a greater insight on how nsSNPs can lead into disease.
Breast cancer is the most common cancer among women causing highest cancer
death every year. BRCA1 and BRCA2 tumor suppressor genes are two main
candidates of which, mutations in them can increase the risk of developing
breast cancer. For prediction and detection of the cancer one can use
experimental or computational methods, but the experimental method is very
costly and time consuming in comparison with the computational method. The
computer and computational methods have been used for more than 30 years. Here
we try to predict the clinical significance of BRCA1 and BRCA2 nsSNPs as well
as the unknown clinical significances. Nearly 500 BRCA1 and BRCA2 nsSNPs with
known clinical significances retrieved from NCBI database. Based on
hydrophobicity or hydrophilicity and their role in proteins&apos; second structure,
they are divided into 6 groups, each assigned with scores. The data are
prepared in the acceptable form to the automated prediction mechanisms,
Probabilistic Neural Network (PNN) and Deep Neural NetworkStacked AutoEncoder
(DNN). With Jackknife cross validation we show that the prediction accuracy
achieved for BRCA1 and BRCA2 using PNN are 87.97% and 82.17% respectively,
while 95.41% and 92.80% accuracies achieved using DNN. The total required
processing time for the training and testing the PNN is 0.9 second and DNN
requires about 7 hours of training and it can predict instantly. both methods
show great improvement in accuracy and speed compared to previous attempts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+KhajePasha_E/0/1/0/all/0/1&quot;&gt;Ehsan Rahmatizad KhajePasha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bazarghan_M/0/1/0/all/0/1&quot;&gt;Mahdi Bazarghan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Manjili_H/0/1/0/all/0/1&quot;&gt;Hamidreza Kheiri Manjili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mohammadkhani_R/0/1/0/all/0/1&quot;&gt;Ramin Mohammadkhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Amandi_R/0/1/0/all/0/1&quot;&gt;Ruhallah Amandi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02232">
<title>Discrete Factorization Machines for Fast Feature-based Recommendation. (arXiv:1805.02232v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1805.02232</link>
<description rdf:parseType="Literal">&lt;p&gt;User and item features of side information are crucial for accurate
recommendation. However, the large number of feature dimensions, e.g., usually
larger than 10^7, results in expensive storage and computational cost. This
prohibits fast recommendation especially on mobile applications where the
computational resource is very limited. In this paper, we develop a generic
feature-based recommendation model, called Discrete Factorization Machine
(DFM), for fast and accurate recommendation. DFM binarizes the real-valued
model parameters (e.g., float32) of every feature embedding into binary codes
(e.g., boolean), and thus supports efficient storage and fast user-item score
computation. To avoid the severe quantization loss of the binarization, we
propose a convergent updating rule that resolves the challenging discrete
optimization of DFM. Through extensive experiments on two real-world datasets,
we show that 1) DFM consistently outperforms state-of-the-art binarized
recommendation models, and 2) DFM shows very competitive performance compared
to its real-valued version (FM), demonstrating the minimized quantization loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Rui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02242">
<title>Reachability Analysis of Deep Neural Networks with Provable Guarantees. (arXiv:1805.02242v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.02242</link>
<description rdf:parseType="Literal">&lt;p&gt;Verifying correctness of deep neural networks (DNNs) is challenging. We study
a generic reachability problem for feed-forward DNNs which, for a given set of
inputs to the network and a Lipschitz-continuous function over its outputs,
computes the lower and upper bound on the function values. Because the network
and the function are Lipschitz continuous, all values in the interval between
the lower and upper bound are reachable. We show how to obtain the safety
verification problem, the output range analysis problem and a robustness
measure by instantiating the reachability problem. We present a novel algorithm
based on adaptive nested optimisation to solve the reachability problem. The
technique has been implemented and evaluated on a range of DNNs, demonstrating
its efficiency, scalability and ability to handle a broader class of networks
than state-of-the-art verification approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1&quot;&gt;Marta Kwiatkowska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02257">
<title>Bayesian Regularization for Graphical Models with Unequal Shrinkage. (arXiv:1805.02257v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.02257</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a Bayesian framework for estimating a high-dimensional sparse
precision matrix, in which adaptive shrinkage and sparsity are induced by a
mixture of Laplace priors. Besides discussing our formulation from the Bayesian
standpoint, we investigate the MAP (maximum a posteriori) estimator from a
penalized likelihood perspective that gives rise to a new non-convex penalty
approximating the $\ell_0$ penalty. Optimal error rates for estimation
consistency in terms of various matrix norms along with selection consistency
for sparse structure recovery are shown for the unique MAP estimator under mild
conditions. For fast and efficient computation, an EM algorithm is proposed to
compute the MAP estimator of the precision matrix and (approximate) posterior
probabilities on the edges of the underlying sparse structure. Through
extensive simulation studies and a real application to a call center data, we
have demonstrated the fine performance of our method compared with existing
alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gan_L/0/1/0/all/0/1&quot;&gt;Lingrui Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narisetty_N/0/1/0/all/0/1&quot;&gt;Naveen N. Narisetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_F/0/1/0/all/0/1&quot;&gt;Feng Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02285">
<title>Clustering With Pairwise Relationships: A Generative Approach. (arXiv:1805.02285v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.02285</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning (SSL) has become important in current data analysis
applications, where the amount of unlabeled data is growing exponentially and
user input remains limited by logistics and expense. Constrained clustering, as
a subclass of SSL, makes use of user input in the form of relationships between
data points (e.g., pairs of data points belonging to the same class or
different classes) and can remarkably improve the performance of unsupervised
clustering in order to reflect user-defined knowledge of the relationships
between particular data points. Existing algorithms incorporate such user
input, heuristically, as either hard constraints or soft penalties, which are
separate from any generative or statistical aspect of the clustering model;
this results in formulations that are suboptimal and not sufficiently general.
In this paper, we propose a principled, generative approach to
probabilistically model, without ad hoc penalties, the joint distribution given
by user-defined pairwise relations. The proposed model accounts for general
underlying distributions without assuming a specific form and relies on
expectation-maximization for model fitting. For distributions in a standard
form, the proposed approach results in a closed-form solution for updated
parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yen-Yun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Y. Elhabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitaker_R/0/1/0/all/0/1&quot;&gt;Ross T. Whitaker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02306">
<title>Semi-Orthogonal Non-Negative Matrix Factorization. (arXiv:1805.02306v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1805.02306</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-negative Matrix Factorization (NMF) is a popular clustering and dimension
reduction method by decomposing a non-negative matrix into the product of two
lower dimension matrices composed of basis vectors. In this paper, we propose a
semi-orthogonal NMF method that enforces one of the matrices to be orthogonal
with mixed signs, thereby guarantees the rank of the factorization. Our method
preserves strict orthogonality by implementing the Cayley transformation to
force the solution path to be exactly on the Stiefel manifold, as opposed to
the approximated orthogonality solutions in existing literature. We apply a
line search update scheme along with an SVD-based initialization which produces
a rapid convergence of the algorithm compared to other existing approaches. In
addition, we present formulations of our method to incorporate both continuous
and binary design matrices. Through various simulation studies, we show that
our model has an advantage over other NMF variations regarding the accuracy of
the factorization, rate of convergence, and the degree of orthogonality while
being computationally competitive. We also apply our method to a text-mining
data on classifying triage notes, and show the effectiveness of our model in
reducing classification error compared to the conventional bag-of-words model
and other alternative matrix factorization approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jack Yutong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Ruoqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qu_A/0/1/0/all/0/1&quot;&gt;Annie Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Han Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhankun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02338">
<title>Implementation of Stochastic Quasi-Newton&apos;s Method in PyTorch. (arXiv:1805.02338v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.02338</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we implement the Stochastic Damped LBFGS (SdLBFGS) for
stochastic non-convex optimization. We make two important modifications to the
original SdLBFGS algorithm. First, by initializing the Hessian at each step
using an identity matrix, the algorithm converges better than original
algorithm. Second, by performing direction normalization we could gain stable
optimization procedure without line search. Experiments on minimizing a 2D
non-convex function shows that our improved algorithm converges better than
original algorithm, and experiments on the CIFAR10 and MNIST datasets show that
our improved algorithm works stably and gives comparable or even better testing
accuracies than first order optimizers SGD, Adagrad, and second order
optimizers LBFGS in PyTorch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingkai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huidong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02396">
<title>Billion-scale Network Embedding with Iterative Random Projection. (arXiv:1805.02396v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1805.02396</link>
<description rdf:parseType="Literal">&lt;p&gt;Network embedding has attracted considerable research attention recently.
However, the existing methods are incapable of handling billion-scale networks,
because they are computationally expensive and, at the same time, difficult to
be accelerated by distributed computing schemes. To address these problems, we
propose RandNE, a novel and simple billion-scale network embedding method.
Specifically, we propose a Gaussian random projection approach to map the
network into a low-dimensional embedding space while preserving the high-order
proximities between nodes. To reduce the time complexity, we design an
iterative projection procedure to avoid the explicit calculation of the
high-order proximities. Theoretical analysis shows that our method is extremely
efficient, and friendly to distributed computing schemes without any
communication cost in the calculation. We demonstrate the efficacy of RandNE
over state-of-the-art methods in network reconstruction and link prediction
tasks on multiple datasets with different scales, ranging from thousands to
billions of nodes and edges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenwu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02587">
<title>Complete Analysis of a Random Forest Model. (arXiv:1805.02587v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.02587</link>
<description rdf:parseType="Literal">&lt;p&gt;Random forests have become an important tool for improving accuracy in
regression problems since their popularization by (Breiman, 2001) and others.
In this paper, we revisit a random forest model originally proposed by
(Breiman, 2004) and later studied by (Biau, 2012), where a feature is selected
at random and the split occurs at the midpoint of the block containing the
chosen feature. If the regression function is sparse and depends only on a
small, unknown subset of $ S $ out of $ d $ features, we show that given $ n $
observations, this random forest model outputs a predictor that has a
mean-squared prediction error of order $ \left(n\sqrt{\log^{S-1}
n}\right)^{-\frac{1}{S\log2+1}} $. When $ S \leq \lfloor 0.72 d \rfloor $, this
rate is better than the minimax optimal rate $ n^{-\frac{2}{d+2}} $ for $ d
$-dimensional, Lipschitz function classes.
&lt;/p&gt;
&lt;p&gt;As a consequence of our analysis, we show that the variance of the forest
decays with the depth of the tree at a rate that is independent of the ambient
dimension, even when the trees are fully grown. In particular, if $ \ell_{avg}
$ (resp. $ \ell_{max} $) is the average (resp. maximum) number of observations
per leaf node, we show that the variance of this forest is $
\Theta\left(\ell^{-1}_{avg}(\sqrt{\log n})^{-(S-1)}\right) $, which for the
case of $ S = d $, is similar in form to the lower bound $
\Omega\left(\ell^{-1}_{max}(\log n)^{-(d-1)}\right) $ of (Lin and Jeon, 2006)
for any random forest model with a nonadaptive splitting scheme. We also show
that the bias is tight for any linear model with nonzero parameter vector.
Thus, we completely characterize the fundamental limits of this random forest
model. Our new analysis also implies that better theoretical performance can be
achieved if the trees are grown less aggressively (i.e., grown to a shallower
depth) than previous work would otherwise recommend.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1&quot;&gt;Jason M. Klusowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02590">
<title>Modeling Dengue Vector Population Using Remotely Sensed Data and Machine Learning. (arXiv:1805.02590v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.02590</link>
<description rdf:parseType="Literal">&lt;p&gt;Mosquitoes are vectors of many human diseases. In particular, Aedes \ae gypti
(Linnaeus) is the main vector for Chikungunya, Dengue, and Zika viruses in
Latin America and it represents a global threat. Public health policies that
aim at combating this vector require dependable and timely information, which
is usually expensive to obtain with field campaigns. For this reason, several
efforts have been done to use remote sensing due to its reduced cost. The
present work includes the temporal modeling of the oviposition activity
(measured weekly on 50 ovitraps in a north Argentinean city) of Aedes \ae gypti
(Linnaeus), based on time series of data extracted from operational earth
observation satellite images. We use are NDVI, NDWI, LST night, LST day and
TRMM-GPM rain from 2012 to 2016 as predictive variables. In contrast to
previous works which use linear models, we employ Machine Learning techniques
using completely accessible open source toolkits. These models have the
advantages of being non-parametric and capable of describing nonlinear
relationships between variables. Specifically, in addition to two linear
approaches, we assess a Support Vector Machine, an Artificial Neural Networks,
a K-nearest neighbors and a Decision Tree Regressor. Considerations are made on
parameter tuning and the validation and training approach. The results are
compared to linear models used in previous works with similar data sets for
generating temporal predictive models. These new tools perform better than
linear approaches, in particular Nearest Neighbor Regression (KNNR) performs
the best. These results provide better alternatives to be implemented
operatively on the Argentine geospatial Risk system that is running since 2012.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scavuzzo_J/0/1/0/all/0/1&quot;&gt;J. M. Scavuzzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trucco_F/0/1/0/all/0/1&quot;&gt;F. Trucco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Espinosa_M/0/1/0/all/0/1&quot;&gt;M. Espinosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tauro_C/0/1/0/all/0/1&quot;&gt;C. B. Tauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abril_M/0/1/0/all/0/1&quot;&gt;M. Abril&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scavuzzo_C/0/1/0/all/0/1&quot;&gt;C. M. Scavuzzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frery_A/0/1/0/all/0/1&quot;&gt;A. C. Frery&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02608">
<title>Anticipating contingengies in power grids using fast neural net screening. (arXiv:1805.02608v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/1805.02608</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of maintaining high voltage power transmission
networks in security at all time. This requires that power flowing through all
lines remain below a certain nominal thermal limit above which lines might
melt, break or cause other damages. Current practices include enforcing the
deterministic &quot;N-1&quot; reliability criterion, namely anticipating exceeding of
thermal limit for any eventual single line disconnection (whatever its cause
may be) by running a slow, but accurate, physical grid simulator. New
conceptual frameworks are calling for a probabilistic risk based security
criterion and are in need of new methods to assess the risk. To tackle this
difficult assessment, we address in this paper the problem of rapidly ranking
higher order contingencies including all pairs of line disconnections, to
better prioritize simulations. We present a novel method based on neural
networks, which ranks &quot;N-1&quot; and &quot;N-2&quot; contingencies in decreasing order of
presumed severity. We demonstrate on a classical benchmark problem that the
residual risk of contingencies decreases dramatically compared to considering
solely all &quot;N-1&quot; cases, at no additional computational cost. We evaluate that
our method scales up to power grids of the size of the French high voltage
power grid (over 1000 power lines).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Donnot_B/0/1/0/all/0/1&quot;&gt;Benjamin Donnot&lt;/a&gt; (TAU, LRI), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Guyon_I/0/1/0/all/0/1&quot;&gt;Isabelle Guyon&lt;/a&gt; (TAU, LRI, UP11), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Schoenauer_M/0/1/0/all/0/1&quot;&gt;Marc Schoenauer&lt;/a&gt; (TAU, LRI), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Marot_A/0/1/0/all/0/1&quot;&gt;Antoine Marot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Panciatici_P/0/1/0/all/0/1&quot;&gt;Patrick Panciatici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02627">
<title>Computing the Shattering Coefficient of Supervised Learning Algorithms. (arXiv:1805.02627v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.02627</link>
<description rdf:parseType="Literal">&lt;p&gt;The Statistical Learning Theory (SLT) provides the theoretical guarantees for
supervised machine learning based on the Empirical Risk Minimization Principle
(ERMP). Such principle defines an upper bound to ensure the uniform convergence
of the empirical risk Remp(f), i.e., the error measured on a given data sample,
to the expected value of risk R(f) (a.k.a. actual risk), which depends on the
Joint Probability Distribution P(X x Y) mapping input examples x in X to class
labels y in Y. The uniform convergence is only ensured when the Shattering
coefficient N(F,2n) has a polynomial growing behavior. This paper proves the
Shattering coefficient for any Hilbert space H containing the input space X and
discusses its effects in terms of learning guarantees for supervised machine
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1&quot;&gt;Rodrigo Fernandes de Mello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1&quot;&gt;Moacir Antonelli Ponti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_C/0/1/0/all/0/1&quot;&gt;Carlos Henrique Grossi Ferreira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02642">
<title>Wavelet Decomposition of Gradient Boosting. (arXiv:1805.02642v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.02642</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a significant improvement to the popular
tree-based Stochastic Gradient Boosting algorithm using a wavelet decomposition
of the trees. This approach is based on harmonic analysis and approximation
theoretical elements, and as we show through extensive experimentation, our
wavelet based method generally outperforms existing methods, particularly in
difficult scenarios of class unbalance and mislabeling in the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dekel_S/0/1/0/all/0/1&quot;&gt;Shai Dekel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elisha_O/0/1/0/all/0/1&quot;&gt;Oren Elisha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_O/0/1/0/all/0/1&quot;&gt;Ohad Morgan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.07986">
<title>Geometric adaptive Monte Carlo in random environment. (arXiv:1608.07986v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1608.07986</link>
<description rdf:parseType="Literal">&lt;p&gt;Manifold Markov chain Monte Carlo algorithms have been introduced to sample
more effectively from challenging target densities exhibiting multiple modes or
strong correlations. Such algorithms exploit the local geometry of the
parameter space, thus enabling chains to achieve a faster convergence rate when
measured in number of steps. However, acquiring local geometric information can
often increase computational complexity per step to the extent that sampling
from high-dimensional targets becomes inefficient in terms of total
computational time. This paper analyzes the computational complexity of
manifold Langevin Monte Carlo and proposes a geometric adaptive Monte Carlo
sampler aimed at balancing the benefits of exploiting local geometry with
computational cost to achieve a high effective sample size for a given
computational cost. The suggested sampler is a discrete-time stochastic process
in random environment. The random environment allows to switch between local
geometric and adaptive proposal kernels with the help of a schedule. An
exponential schedule is put forward that enables more frequent use of geometric
information in early transient phases of the chain, while saving computational
time in late stationary phases. The average complexity can be manually set
depending on the need for geometric exploitation posed by the underlying model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papamarkou_T/0/1/0/all/0/1&quot;&gt;Theodore Papamarkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lindo_A/0/1/0/all/0/1&quot;&gt;Alexey Lindo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ford_E/0/1/0/all/0/1&quot;&gt;Eric B. Ford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.02588">
<title>Iterative proportional scaling revisited: a modern optimization perspective. (arXiv:1610.02588v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1610.02588</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper revisits the classic iterative proportional scaling (IPS) from a
modern optimization perspective. In contrast to the criticisms made in the
literature, we show that based on a coordinate descent characterization, IPS
can be slightly modified to deliver coefficient estimates, and from a
majorization-minimization standpoint, IPS can be extended to handle log-affine
models with features not necessarily binary-valued or nonnegative. Furthermore,
some state-of-the-art optimization techniques such as block-wise computation,
randomization and momentum-based acceleration can be employed to provide more
scalable IPS algorithms, as well as some regularized variants of IPS for
concurrent feature selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+She_Y/0/1/0/all/0/1&quot;&gt;Yiyuan She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shao Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.04389">
<title>Real-Time Energy Disaggregation of a Distribution Feeder&apos;s Demand Using Online Learning. (arXiv:1701.04389v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1701.04389</link>
<description rdf:parseType="Literal">&lt;p&gt;Though distribution system operators have been adding more sensors to their
networks, they still often lack an accurate real-time picture of the behavior
of distributed energy resources such as demand responsive electric loads and
residential solar generation. Such information could improve system
reliability, economic efficiency, and environmental impact. Rather than
installing additional, costly sensing and communication infrastructure to
obtain additional real-time information, it may be possible to use existing
sensing capabilities and leverage knowledge about the system to reduce the need
for new infrastructure. In this paper, we disaggregate a distribution feeder&apos;s
demand measurements into: 1) the demand of a population of air conditioners,
and 2) the demand of the remaining loads connected to the feeder. We use an
online learning algorithm, Dynamic Fixed Share (DFS), that uses the real-time
distribution feeder measurements as well as models generated from historical
building- and device-level data. We develop two implementations of the
algorithm and conduct case studies using real demand data from households and
commercial buildings to investigate the effectiveness of the algorithm. The
case studies demonstrate that DFS can effectively perform online disaggregation
and the choice and construction of models included in the algorithm affects its
accuracy, which is comparable to that of a set of Kalman filters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ledva_G/0/1/0/all/0/1&quot;&gt;Gregory S. Ledva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balzano_L/0/1/0/all/0/1&quot;&gt;Laura Balzano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mathieu_J/0/1/0/all/0/1&quot;&gt;Johanna L. Mathieu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.01604">
<title>Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. (arXiv:1709.01604v5 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.01604</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms, when applied to sensitive data, pose a distinct
threat to privacy. A growing body of prior work demonstrates that models
produced by these algorithms may leak specific private information in the
training data to an attacker, either through the models&apos; structure or their
observable behavior. However, the underlying cause of this privacy risk is not
well understood beyond a handful of anecdotal accounts that suggest overfitting
and influence might play a role.
&lt;/p&gt;
&lt;p&gt;This paper examines the effect that overfitting and influence have on the
ability of an attacker to learn information about the training data from
machine learning models, either through training set membership inference or
attribute inference attacks. Using both formal and empirical analyses, we
illustrate a clear relationship between these factors and the privacy risk that
arises in several popular machine learning algorithms. We find that overfitting
is sufficient to allow an attacker to perform membership inference and, when
the target attribute meets certain conditions about its influence, attribute
inference attacks. Interestingly, our formal analysis also shows that
overfitting is not necessary for these attacks and begins to shed light on what
other factors may be in play. Finally, we explore the connection between
membership inference and attribute inference, showing that there are deep
connections between the two that lead to effective new attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_S/0/1/0/all/0/1&quot;&gt;Samuel Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomelli_I/0/1/0/all/0/1&quot;&gt;Irene Giacomelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1&quot;&gt;Matt Fredrikson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Somesh Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09997">
<title>Zeroth Order Nonconvex Multi-Agent Optimization over Networks. (arXiv:1710.09997v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we consider distributed optimization problems over a
multi-agent network, where each agent can only partially evaluate the objective
function, and it is allowed to exchange messages with its immediate neighbors.
Differently from all existing works on distributed optimization, our focus is
given to optimizing a class of difficult non-convex problems, and under the
challenging setting where each agent can only access the zeroth-order
information (i.e., the functional values) of its local functions. For different
types of network topologies such as undirected connected networks or star
networks, we develop efficient distributed algorithms and rigorously analyze
their convergence and rate of convergence (to the set of stationary solutions).
Numerical results are provided to demonstrate the efficiency of the proposed
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hajinezhad_D/0/1/0/all/0/1&quot;&gt;Davood Hajinezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mingyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Alfredo Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08247">
<title>Decomposition Strategies for Constructive Preference Elicitation. (arXiv:1711.08247v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08247</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle the problem of constructive preference elicitation, that is the
problem of learning user preferences over very large decision problems,
involving a combinatorial space of possible outcomes. In this setting, the
suggested configuration is synthesized on-the-fly by solving a constrained
optimization problem, while the preferences are learned itera tively by
interacting with the user. Previous work has shown that Coactive Learning is a
suitable method for learning user preferences in constructive scenarios. In
Coactive Learning the user provides feedback to the algorithm in the form of an
improvement to a suggested configuration. When the problem involves many
decision variables and constraints, this type of interaction poses a
significant cognitive burden on the user. We propose a decomposition technique
for large preference-based decision problems relying exclusively on inference
and feedback over partial configurations. This has the clear advantage of
drastically reducing the user cognitive load. Additionally, part-wise inference
can be (up to exponentially) less computationally demanding than inference over
full configurations. We discuss the theoretical implications of working with
parts and present promising empirical results on one synthetic and two
realistic constructive problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dragone_P/0/1/0/all/0/1&quot;&gt;Paolo Dragone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teso_S/0/1/0/all/0/1&quot;&gt;Stefano Teso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Mohit Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10589">
<title>Contextual Outlier Interpretation. (arXiv:1711.10589v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10589</link>
<description rdf:parseType="Literal">&lt;p&gt;Outlier detection plays an essential role in many data-driven applications to
identify isolated instances that are different from the majority. While many
statistical learning and data mining techniques have been used for developing
more effective outlier detection algorithms, the interpretation of detected
outliers does not receive much attention. Interpretation is becoming
increasingly important to help people trust and evaluate the developed models
through providing intrinsic reasons why the certain outliers are chosen. It is
difficult, if not impossible, to simply apply feature selection for explaining
outliers due to the distinct characteristics of various detection models,
complicated structures of data in certain applications, and imbalanced
distribution of outliers and normal instances. In addition, the role of
contrastive contexts where outliers locate, as well as the relation between
outliers and contexts, are usually overlooked in interpretation. To tackle the
issues above, in this paper, we propose a novel Contextual Outlier
INterpretation (COIN) method to explain the abnormality of existing outliers
spotted by detectors. The interpretability for an outlier is achieved from
three aspects: outlierness score, attributes that contribute to the
abnormality, and contextual description of its neighborhoods. Experimental
results on various types of datasets demonstrate the flexibility and
effectiveness of the proposed framework compared with existing interpretation
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1&quot;&gt;Donghwa Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08289">
<title>Learning and Transferring IDs Representation in E-commerce. (arXiv:1712.08289v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08289</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine intelligence techniques are developed in E-commerce and one of
the most essential components is the representation of IDs, including user ID,
item ID, product ID, store ID, brand ID, category ID etc. The classical
encoding based methods (like one-hot encoding) are inefficient in that it
suffers sparsity problems due to its high dimension, and it cannot reflect the
relationships among IDs, either homogeneous or heterogeneous ones. In this
paper, we propose an embedding based framework to learn and transfer the
representation of IDs. As the the implicit feedbacks of users, a tremendous
amount of item ID sequences can be easily collected from the interactive
sessions. By jointly using these informative sequences and the structural
connections among IDs, all types of IDs can be embedded into one
low-dimensional semantic space. Subsequently, the learned representations are
utilized and transferred in four scenarios: (i) measuring the similarity
between items, (ii) transferring from seen items to unseen items, (iii)
transferring across different domains, (iv) transferring across different
tasks. We deploy and evaluate the proposed approach in Hema App and the results
validate its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuechuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuai_Z/0/1/0/all/0/1&quot;&gt;Zhaoqian Shuai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08881">
<title>Correlated Components Analysis --- Extracting Reliable Dimensions in Multivariate Data. (arXiv:1801.08881v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08881</link>
<description rdf:parseType="Literal">&lt;p&gt;How does one find dimensions in multivariate data that are reliably expressed
across repetitions? For example, in a brain imaging study one may want to
identify combinations of neural signals that are reliably expressed across
multiple trials or subjects. For a behavioral assessment with multiple ratings,
one may want to identify an aggregate score that is reliably reproduced across
raters. Correlated Components Analysis (CorrCA) addresses this problem by
identifying components that are maximally correlated between repetitions (e.g.
trials, subjects, raters). Here we formalize this as the maximization of the
ratio of between-repetition to within-repetition covariance. We show that this
criterion maximizes repeat-reliability, defined as mean over variance across
repeats, and that it leads to CorrCA or to multi-set Canonical Correlation
Analysis, depending on the constraints. Surprisingly, we also find that CorrCA
is equivalent to linear discriminant analysis for zero-mean signals. For data
with independent samples, we provided an exact parametric test for statistical
significance based on the F-statistic. For non-independent samples we present
and validate shuffle statistics. Regularization and extension to non-linear
mappings using kernels are also presented. The algorithms are demonstrated on a
series of applications, and we provide all code and data required to reproduce
the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parra_L/0/1/0/all/0/1&quot;&gt;Lucas C. Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haufe_S/0/1/0/all/0/1&quot;&gt;Stefan Haufe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dmochowski_J/0/1/0/all/0/1&quot;&gt;Jacek P. Dmochowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07200">
<title>Training Recurrent Neural Networks as a Constraint Satisfaction Problem. (arXiv:1803.07200v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07200</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new approach for training artificial neural networks
using techniques for solving the constraint satisfaction problem (CSP). The
quotient gradient system (QGS) is a trajectory based method for solving the
CSP. This study converts the training set of a neural network into a CSP and
uses the QGS to find its solutions. The QGS finds the global minimum of the
optimization problem by tracking trajectories of a nonlinear dynamical system
and does not stop at a local minimum of the optimization problem. Lyapunov
theory is used to prove the asymptotic stability of the solutions with and
without the presence of measurement errors. Numerical examples illustrate the
effectiveness of the proposed methodology and compare it to a genetic algorithm
and error backpropagation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodabandehlou_H/0/1/0/all/0/1&quot;&gt;Hamid Khodabandehlou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadali_M/0/1/0/all/0/1&quot;&gt;Mohammad Sami Fadali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10801">
<title>A Cost-Sensitive Deep Belief Network for Imbalanced Classification. (arXiv:1804.10801v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10801</link>
<description rdf:parseType="Literal">&lt;p&gt;Imbalanced data with a skewed class distribution are common in many
real-world applications. Deep Belief Network (DBN) is a machine learning
technique that is effective in classification tasks. However, conventional DBN
does not work well for imbalanced data classification because it assumes equal
costs for each class. To deal with this problem, cost-sensitive approaches
assign different misclassification costs for different classes without
disrupting the true data sample distributions. However, due to lack of prior
knowledge, the misclassification costs are usually unknown and hard to choose
in practice. Moreover, it has not been well studied as to how cost-sensitive
learning could improve DBN performance on imbalanced data problems. This paper
proposes an evolutionary cost-sensitive deep belief network (ECS-DBN) for
imbalanced classification. ECS-DBN uses adaptive differential evolution to
optimize the misclassification costs based on training data, that presents an
effective approach to incorporating the evaluation measure (i.e. G-mean) into
the objective function. We first optimize the misclassification costs, then
apply them to deep belief network. Adaptive differential evolution optimization
is implemented as the optimization algorithm that automatically updates its
corresponding parameters without the need of prior domain knowledge. The
experiments have shown that the proposed approach consistently outperforms the
state-of-the-art on both benchmark datasets and real-world dataset for fault
diagnosis in tool condition monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kay Chen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haizhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_G/0/1/0/all/0/1&quot;&gt;Geok Soon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10988">
<title>SHADE: Information-Based Regularization for Deep Learning. (arXiv:1804.10988v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10988</link>
<description rdf:parseType="Literal">&lt;p&gt;Regularization is a big issue for training deep neural networks. In this
paper, we propose a new information-theory-based regularization scheme named
SHADE for SHAnnon DEcay. The originality of the approach is to define a prior
based on conditional entropy, which explicitly decouples the learning of
invariant representations in the regularizer and the learning of correlations
between inputs and labels in the data fitting term. Our second contribution is
to derive a stochastic version of the regularizer compatible with deep
learning, resulting in a tractable training scheme. We empirically validate the
efficiency of our approach to improve classification performances compared to
common regularization schemes on several standard architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blot_M/0/1/0/all/0/1&quot;&gt;Michael Blot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robert_T/0/1/0/all/0/1&quot;&gt;Thomas Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item></rdf:RDF>