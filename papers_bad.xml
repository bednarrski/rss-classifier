<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03958"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03736"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04228"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04247"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04327"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04355"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.03875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03598"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03698"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03749"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03753"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03873"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03874"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04022"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04262"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04281"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04302"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04308"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.06423"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.04530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05327"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04072"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10377"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00552"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09865"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03333"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.03920">
<title>Multimodal Language Analysis with Recurrent Multistage Fusion. (arXiv:1808.03920v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03920</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational modeling of human multimodal language is an emerging research
area in natural language processing spanning the language, visual and acoustic
modalities. Comprehending multimodal language requires modeling not only the
interactions within each modality (intra-modal interactions) but more
importantly the interactions between modalities (cross-modal interactions). In
this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which
decomposes the fusion problem into multiple stages, each of them focused on a
subset of multimodal signals for specialized, effective fusion. Cross-modal
interactions are modeled using this multistage fusion approach which builds
upon intermediate representations of previous stages. Temporal and intra-modal
interactions are modeled by integrating our proposed fusion approach with a
system of recurrent neural networks. The RMFN displays state-of-the-art
performance in modeling human multimodal language across three public datasets
relating to multimodal sentiment analysis, emotion recognition, and speaker
traits recognition. We provide visualizations to show that each stage of fusion
focuses on a different subset of multimodal signals, learning increasingly
discriminative multimodal representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziyin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadeh_A/0/1/0/all/0/1&quot;&gt;Amir Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03958">
<title>Characterizing Neuronal Circuits with Spike-triggered Non-negative Matrix Factorization. (arXiv:1808.03958v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1808.03958</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuronal circuits formed in the brain are complex with intricate connection
patterns. Such a complexity is also observed in the retina as a relatively
simple neuronal circuit. A retinal ganglion cell receives excitatory inputs
from neurons in previous layers as driving forces to fire spikes. Analytical
methods are required that can decipher these components in a systematic manner.
Recently a method termed spike-triggered non-negative matrix factorization
(STNMF) has been proposed for this purpose. In this study, we extend the scope
of the STNMF method. By using the retinal ganglion cell as a model system, we
show that STNMF can detect various biophysical properties of upstream bipolar
cells, including spatial receptive fields, temporal filters, and transfer
nonlinearity. In addition, we recover synaptic connection strengths from the
weight matrix of STNMF. Furthermore, we show that STNMF can separate spikes of
a ganglion cell into a few subsets of spikes where each subset is contributed
by one presynaptic bipolar cell. Taken together, these results corroborate that
STNMF is a useful method for deciphering the structure of neuronal circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jia_S/0/1/0/all/0/1&quot;&gt;Shanshan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1&quot;&gt;Arno Onken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian K. Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07165">
<title>Scale-invariant temporal history (SITH): optimal slicing of the past in an uncertain world. (arXiv:1712.07165v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07165</link>
<description rdf:parseType="Literal">&lt;p&gt;In both the human brain and any general artificial intelligence (AI), a
representation of the past is necessary to predict the future. However, perfect
storage of all experiences is not feasible. One possibility, utilized in many
applications, is to retain information about the past in a buffer. A limitation
of this approach is that, although events in the buffer are represented with
perfect accuracy, the resources necessary to represent information at multiple
time scales go up rapidly. Here we present a neurally-plausible, compressed,
scale-free memory representation we call Scale-Invariant Temporal History
(SITH). This representation covers an exponentially large period of time at the
cost of sacrificing temporal accuracy for events further in the past. The form
of this decay is scale-invariant and can be shown to be optimal, in that it is
able to respond to worlds with a wide range of relevant time scales. We
demonstrate the utility of this representation in learning to play video games
at different levels of complexity. In these environments, SITH exhibits better
learning performance than both a fixed-size buffer history representation and a
representation with exponentially decaying features. Whereas the buffer
performs well as long as the temporal dependencies can be represented within
the buffer, SITH performs well over a much larger range of time scales with the
same amount of resources. Finally, we discuss how the application of SITH,
along with other human-inspired models of cognition, could improve
reinforcement and machine learning algorithms in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spears_T/0/1/0/all/0/1&quot;&gt;Tyler A. Spears&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacques_B/0/1/0/all/0/1&quot;&gt;Brandon G. Jacques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_M/0/1/0/all/0/1&quot;&gt;Marc W. Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sederberg_P/0/1/0/all/0/1&quot;&gt;Per B. Sederberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03736">
<title>An Implementation, Empirical Evaluation and Proposed Improvement for Bidirectional Splitting Method for Argumentation Frameworks under Stable Semantics. (arXiv:1808.03736v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.03736</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstract argumentation frameworks are formal systems that facilitate
obtaining conclusions from non-monotonic knowledge systems. Within such a
system, an argumentation semantics is defined as a set of arguments with some
desired qualities, for example, that the elements are not in conflict with each
other. Splitting an argumentation framework can efficiently speed up the
computation of argumentation semantics. With respect to stable semantics, two
methods have been proposed to split an argumentation framework either in a
unidirectional or bidirectional fashion. The advantage of bidirectional
splitting is that it is not structure-dependent and, unlike unidirectional
splitting, it can be used for frameworks consisting of a single strongly
connected component. Bidirectional splitting makes use of a minimum cut. In
this paper, we implement and test the performance of the bidirectional
splitting method, along with two types of graph cut algorithms. Experimental
data suggest that using a minimum cut will not improve the performance of
computing stable semantics in most cases. Hence, instead of a minimum cut, we
propose to use a balanced cut, where the framework is split into two
sub-frameworks of equal size. Experimental results conducted on bidirectional
splitting using the balanced cut show a significant improvement in the
performance of computing semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1&quot;&gt;Renata Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03737">
<title>Learning Multi-touch Conversion Attribution with Dual-attention Mechanisms for Online Advertising. (arXiv:1808.03737v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.03737</link>
<description rdf:parseType="Literal">&lt;p&gt;In online advertising, the Internet users may be exposed to a sequence of
different ad campaigns, i.e., display ads, search, or referrals from multiple
channels, before led up to any final sales conversion and transaction. For both
campaigners and publishers, it is fundamentally critical to estimate the
contribution from ad campaign touch-points during the customer journey
(conversion funnel) and assign the right credit to the right ad exposure
accordingly. However, the existing research on the multi-touch attribution
problem lacks a principled way of utilizing the users&apos; pre-conversion actions
(i.e., clicks), and quite often fails to model the sequential patterns among
the touch points from a user&apos;s behavior data. To make it worse, the current
industry practice is merely employing a set of arbitrary rules as the
attribution model, e.g., the popular last-touch model assigns 100% credit to
the final touch-point regardless of actual attributions. In this paper, we
propose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch
attribution problem. It learns the attribution values through an attention
mechanism directly from the conversion estimation objective. To achieve this,
we utilize sequence-to-sequence prediction for user clicks, and combine both
post-view and post-click attribution patterns together for the final conversion
estimation. To quantitatively benchmark attribution models, we also propose a
novel yet practical attribution evaluation scheme through the proxy of budget
allocation (under the estimated attributions) over ad channels. The
experimental results on two real datasets demonstrate the significant
performance gains of our attribution model against the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03852">
<title>A Parameterized Complexity View on Description Logic Reasoning. (arXiv:1808.03852v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1808.03852</link>
<description rdf:parseType="Literal">&lt;p&gt;Description logics are knowledge representation languages that have been
designed to strike a balance between expressivity and computational
tractability. Many different description logics have been developed, and
numerous computational problems for these logics have been studied for their
computational complexity. However, essentially all complexity analyses of
reasoning problems for description logics use the one-dimensional framework of
classical complexity theory. The multi-dimensional framework of parameterized
complexity theory is able to provide a much more detailed image of the
complexity of reasoning problems.
&lt;/p&gt;
&lt;p&gt;In this paper we argue that the framework of parameterized complexity has a
lot to offer for the complexity analysis of description logic reasoning
problems---when one takes a progressive and forward-looking view on
parameterized complexity tools. We substantiate our argument by means of three
case studies. The first case study is about the problem of concept
satisfiability for the logic ALC with respect to nearly acyclic TBoxes. The
second case study concerns concept satisfiability for ALC concepts
parameterized by the number of occurrences of union operators and the number of
occurrences of full existential quantification. The third case study offers a
critical look at data complexity results from a parameterized complexity point
of view. These three case studies are representative for the wide range of uses
for parameterized complexity methods for description logic problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haan_R/0/1/0/all/0/1&quot;&gt;Ronald de Haan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03944">
<title>Unsupervised learning for cross-domain medical image synthesis using deformation invariant cycle consistency networks. (arXiv:1808.03944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.03944</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the cycle-consistent generative adversarial networks (CycleGAN) has
been widely used for synthesis of multi-domain medical images. The
domain-specific nonlinear deformations captured by CycleGAN make the
synthesized images difficult to be used for some applications, for example,
generating pseudo-CT for PET-MR attenuation correction. This paper presents a
deformation-invariant CycleGAN (DicycleGAN) method using deformable
convolutional layers and new cycle-consistency losses. Its robustness dealing
with data that suffer from domain-specific nonlinear deformations has been
evaluated through comparison experiments performed on a multi-sequence brain MR
dataset and a multi-modality abdominal dataset. Our method has displayed its
ability to generate synthesized data that is aligned with the source while
maintaining a proper quality of signal compared to CycleGAN-generated data. The
proposed model also obtained comparable performance with CycleGAN when data
from the source and target domains are alignable through simple affine
transformations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macnaught_G/0/1/0/all/0/1&quot;&gt;Gillian Macnaught&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papanastasiou_G/0/1/0/all/0/1&quot;&gt;Giorgos Papanastasiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacGillivray_T/0/1/0/all/0/1&quot;&gt;Tom MacGillivray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newby_D/0/1/0/all/0/1&quot;&gt;David Newby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04043">
<title>Faster and More Robust Mesh-based Algorithms for Obstacle k-Nearest Neighbour. (arXiv:1808.04043v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04043</link>
<description rdf:parseType="Literal">&lt;p&gt;We are interested in the problem of finding $k$ nearest neighbours in the
plane and in the presence of polygonal obstacles ($\textit{OkNN}$). Widely used
algorithms for OkNN are based on incremental visibility graphs, which means
they require costly and online visibility checking and have worst-case
quadratic running time. Recently $\mathbf{Polyanya}$, a fast point-to-point
pathfinding algorithm was proposed which avoids the disadvantages of visibility
graphs by searching over an alternative data structure known as a navigation
mesh. Previously, we adapted $\mathbf{Polyanya}$ to multi-target scenarios by
developing two specialised heuristic functions: the $\mathbf{Interval
heuristic}$ $h_v$ and the $\mathbf{Target heuristic}$ $h_t$. Though these
methods outperform visibility graph algorithms by orders of magnitude in all
our experiments they are not robust: $h_v$ expands many redundant nodes when
the set of neighbours is small while $h_t$ performs poorly when the set of
neighbours is large. In this paper, we propose new algorithms and heuristics
for OkNN which perform well regardless of neighbour density.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shizhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harabor_D/0/1/0/all/0/1&quot;&gt;Daniel D. Harabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniar_D/0/1/0/all/0/1&quot;&gt;David Taniar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04228">
<title>DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human Activity Recognition. (arXiv:1808.04228v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04228</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Convolutional Neural Networks (DCNNs) are currently popular in human
activity recognition applications. However, in the face of modern artificial
intelligence sensor-based games, many research achievements cannot be
practically applied on portable devices. DCNNs are typically resource-intensive
and too large to be deployed on portable devices, thus this limits the
practical application of complex activity detection. In addition, since
portable devices do not possess high-performance Graphic Processing Units
(GPUs), there is hardly any improvement in Action Game (ACT) experience.
Besides, in order to deal with multi-sensor collaboration, all previous human
activity recognition models typically treated the representations from
different sensor signal sources equally. However, distinct types of activities
should adopt different fusion strategies. In this paper, a novel scheme is
proposed. This scheme is used to train 2-bit Convolutional Neural Networks with
weights and activations constrained to {-0.5,0,0.5}. It takes into account the
correlation between different sensor signal sources and the activity types.
This model, which we refer to as DFTerNet, aims at producing a more reliable
inference and better trade-offs for practical applications. Our basic idea is
to exploit quantization of weights and activations directly in pre-trained
filter banks and adopt dynamic fusion strategies for different activity types.
Experiments demonstrate that by using dynamic fusion strategy can exceed the
baseline model performance by up to ~5% on activity recognition like
OPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we
were able to achieve performances closer to that of full-precision counterpart.
These results were also verified using the UniMiB-SHAR dataset. In addition,
the proposed method can achieve ~9x acceleration on CPUs and ~11x memory
saving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raymond_O/0/1/0/all/0/1&quot;&gt;Osolo Ian Raymond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;ChengYuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Ying Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;Jun Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04247">
<title>Relational dynamic memory networks. (arXiv:1808.04247v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;Working memory is an essential component of reasoning -- the capacity to
answer a new question by manipulating acquired knowledge. Current
memory-augmented neural networks offer a differentiable method to realize
limited reasoning with support of a working memory module. Memory modules are
often implemented as a set of memory slots without explicit relational exchange
of content. This does not naturally match multi-relational domains in which
data is structured. We design a new model dubbed Relational Dynamic Memory
Network (RDMN) to fill this gap. The memory can have a single or multiple
components, each of which realizes a multi-relational graph of memory slots.
The memory is dynamically updated in the reasoning process controlled by the
central controller. We evaluate the capability of RDMN on several important
application domains: software vulnerability, molecular bioactivity and chemical
reaction. Results demonstrate the efficacy of the proposed model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Trang Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Truyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04327">
<title>Hidden Fluid Mechanics: A Navier-Stokes Informed Deep Learning Framework for Assimilating Flow Visualization Data. (arXiv:1808.04327v1 [cs.CE])</title>
<link>http://arxiv.org/abs/1808.04327</link>
<description rdf:parseType="Literal">&lt;p&gt;We present hidden fluid mechanics (HFM), a physics informed deep learning
framework capable of encoding an important class of physical laws governing
fluid motions, namely the Navier-Stokes equations. In particular, we seek to
leverage the underlying conservation laws (i.e., for mass, momentum, and
energy) to infer hidden quantities of interest such as velocity and pressure
fields merely from spatio-temporal visualizations of a passive scaler (e.g.,
dye or smoke), transported in arbitrarily complex domains (e.g., in human
arteries or brain aneurysms). Our approach towards solving the aforementioned
data assimilation problem is unique as we design an algorithm that is agnostic
to the geometry or the initial and boundary conditions. This makes HFM highly
flexible in choosing the spatio-temporal domain of interest for data
acquisition as well as subsequent training and predictions. Consequently, the
predictions made by HFM are among those cases where a pure machine learning
strategy or a mere scientific computing approach simply cannot reproduce. The
proposed algorithm achieves accurate predictions of the pressure and velocity
fields in both two and three dimensional flows for several benchmark problems
motivated by real-world applications. Our results demonstrate that this
relatively simple methodology can be used in physical and biomedical problems
to extract valuable quantitative information (e.g., lift and drag forces or
wall shear stresses in arteries) for which direct measurements may not be
possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raissi_M/0/1/0/all/0/1&quot;&gt;Maziar Raissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1&quot;&gt;Alireza Yazdani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04355">
<title>Large-Scale Study of Curiosity-Driven Learning. (arXiv:1808.04355v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04355</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning algorithms rely on carefully engineering environment
rewards that are extrinsic to the agent. However, annotating each environment
with hand-designed, dense rewards is not scalable, motivating the need for
developing reward functions that are intrinsic to the agent. Curiosity is a
type of intrinsic reward function which uses prediction error as reward signal.
In this paper: (a) We perform the first large-scale study of purely
curiosity-driven learning, i.e. without any extrinsic rewards, across 54
standard benchmark environments, including the Atari game suite. Our results
show surprisingly good performance, and a high degree of alignment between the
intrinsic curiosity objective and the hand-designed extrinsic rewards of many
game environments. (b) We investigate the effect of using different feature
spaces for computing prediction error and show that random features are
sufficient for many popular RL game benchmarks, but learned features appear to
generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We
demonstrate limitations of the prediction-based rewards in stochastic setups.
Game-play videos and code are at
https://pathak22.github.io/large-scale-curiosity/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1&quot;&gt;Yuri Burda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1&quot;&gt;Harri Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1&quot;&gt;Amos Storkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.03875">
<title>Learning Task Specifications from Demonstrations. (arXiv:1710.03875v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.03875</link>
<description rdf:parseType="Literal">&lt;p&gt;Real world applications often naturally decompose into several sub-tasks. In
many settings (e.g., robotics) demonstrations provide a natural way to specify
the sub-tasks. However, most methods for learning from demonstrations either do
not provide guarantees that the artifacts learned for the subtasks can be
safely recombined or limit the types of composition available. Motivated by
this deficit, we consider the problem of inferring binary non-Markovian
rewards, also known as logical trace properties or \emph{specifications}, from
demonstrations provided by an agent operating in an uncertain, stochastic
environment. Crucially, specifications admit well-defined composition rules
that are typically easy to interpret. In this paper, we formulate the
specification inference task as a maximum a posteriori (MAP) probability
inference problem, apply the principle of maximum entropy to derive an analytic
demonstration likelihood model and give an efficient approach to search for the
most likely specification in a large candidate pool of specifications. In our
experiments, we demonstrate how learning specifications can help avoid common
bugs that often occur due to ad-hoc reward composition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_Chanlatte_M/0/1/0/all/0/1&quot;&gt;Marcell Vazquez-Chanlatte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Susmit Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1&quot;&gt;Ashish Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1&quot;&gt;Sanjit A. Seshia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08033">
<title>Learning from the experts: From expert systems to machine-learned diagnosis models. (arXiv:1804.08033v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08033</link>
<description rdf:parseType="Literal">&lt;p&gt;Expert diagnostic support systems have been extensively studied. The
practical applications of these systems in real-world scenarios have been
somewhat limited due to well-understood shortcomings, such as lack of
extensibility. More recently, machine-learned models for medical diagnosis have
gained momentum, since they can learn and generalize patterns found in very
large datasets like electronic health records. These models also have
shortcomings - in particular, there is no easy way to incorporate prior
knowledge from existing literature or experts. In this paper, we present a
method to merge both approaches by using expert systems as generative models
that create simulated data on which models can be learned. We demonstrate that
such a learned model not only preserves the original properties of the expert
systems but also addresses some of their limitations. Furthermore, we show how
this approach can also be used as the starting point to combine expert
knowledge with knowledge extracted from other data sources, such as electronic
health records.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravuri_M/0/1/0/all/0/1&quot;&gt;Murali Ravuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anitha Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tso_G/0/1/0/all/0/1&quot;&gt;Geoffrey J. Tso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1&quot;&gt;Xavier Amatriain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03598">
<title>Overarching Computation Model (OCM). (arXiv:1808.03598v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03598</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing models of computation, such as a Turing machine (hereafter, TM), do
not consider the agent involved in interpreting the outcome of the computation.
We argue that a TM, or any other computation model, has no significance if its
output is not interpreted by some agent. Furthermore, we argue that including
the interpreter in the model definition sheds light on some of the difficult
problems faced in computation and mathematics. We provide an analytic process
framework to address this limitation. The framework can be overlaid on existing
concepts of computation to address many practical and philosophical concerns
such as the P vs NP problem. In addition, we argue that the P vs NP problem is
reminiscent of existing computation model which does not account for the person
that initiates the computation and interprets the intermediate and final
output. We utilize the observation that deterministic computational procedures
lack fundamental capacity to fully simulate their non-deterministic variant to
conclude that the set NP cannot be fully contained in P. Deterministic
procedure can approximate non-deterministic variant to some degree. However,
the logical implication of the fundamental differences between determinism and
non-determinism is that equivalence of the two classes is impossible to
establish.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghebrechristos_H/0/1/0/all/0/1&quot;&gt;Henok Ghebrechristos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1&quot;&gt;Drew Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03679">
<title>Machine Learning Promoting Extreme Simplification of Spectroscopy Equipment. (arXiv:1808.03679v1 [physics.ins-det])</title>
<link>http://arxiv.org/abs/1808.03679</link>
<description rdf:parseType="Literal">&lt;p&gt;The spectroscopy measurement is one of main pathways for exploring and
understanding the nature. Today, it seems that racing artificial intelligence
will remould its styles. The algorithms contained in huge neural networks are
capable of substituting many of expensive and complex components of spectrum
instruments. In this work, we presented a smart machine learning strategy on
the measurement of absorbance curves, and also initially verified that an
exceedingly-simplified equipment is sufficient to meet the needs for this
strategy. Further, with its simplicity, the setup is expected to infiltrate
into many scientific areas in versatile forms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jianchao Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Duan_Q/0/1/0/all/0/1&quot;&gt;Qiannan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Sifan Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Ruen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lian_Y/0/1/0/all/0/1&quot;&gt;Yachao Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Ruixing Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guodong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jinhong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03698">
<title>BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear Regressions. (arXiv:1808.03698v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.03698</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a new machine learning (ML) model for nonlinear
regression called Boosting Smooth Transition Regression Tree (BooST). The main
advantage of the BooST is that it estimates the derivatives (partial effects)
of very general nonlinear models, providing more interpretation than other tree
based models concerning the mapping between the covariates and the dependent
variable. We provide some asymptotic theory that shows consistency of the
partial derivatives and we present some examples on simulated and empirical
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fonseca_Y/0/1/0/all/0/1&quot;&gt;Yuri Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Medeiros_M/0/1/0/all/0/1&quot;&gt;Marcelo Medeiros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vasconcelos_G/0/1/0/all/0/1&quot;&gt;Gabriel Vasconcelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veiga_A/0/1/0/all/0/1&quot;&gt;Alvaro Veiga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03749">
<title>Neural Network Encapsulation. (arXiv:1808.03749v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03749</link>
<description rdf:parseType="Literal">&lt;p&gt;A capsule is a collection of neurons which represents different variants of a
pattern in the network. The routing scheme ensures only certain capsules which
resemble lower counterparts in the higher layer should be activated. However,
the computational complexity becomes a bottleneck for scaling up to larger
networks, as lower capsules need to correspond to each and every higher
capsule. To resolve this limitation, we approximate the routing process with
two branches: a master branch which collects primary information from its
direct contact in the lower layer and an aide branch that replenishes master
based on pattern variants encoded in other lower capsules. Compared with
previous iterative and unsupervised routing scheme, these two branches are
communicated in a fast, supervised and one-time pass fashion. The complexity
and runtime of the model are therefore decreased by a large margin. Motivated
by the routing to make higher capsule have agreement with lower capsule, we
extend the mechanism as a compensation for the rapid loss of information in
nearby layers. We devise a feedback agreement unit to send back higher capsules
as feedback. It could be regarded as an additional regularization to the
network. The feedback agreement is achieved by comparing the optimal transport
divergence between two distributions (lower and higher capsules). Such an
add-on witnesses a unanimous gain in both capsule and vanilla networks. Our
proposed EncapNet performs favorably better against previous state-of-the-arts
on CIFAR10/100, SVHN and a subset of ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03753">
<title>MARVIN: An Open Machine Learning Corpus and Environment for Automated Machine Learning Primitive Annotation and Execution. (arXiv:1808.03753v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03753</link>
<description rdf:parseType="Literal">&lt;p&gt;In this demo paper, we introduce the DARPA D3M program for automatic machine
learning (ML) and JPL&apos;s MARVIN tool that provides an environment to locate,
annotate, and execute machine learning primitives for use in ML pipelines.
MARVIN is a web-based application and associated back-end interface written in
Python that enables composition of ML pipelines from hundreds of primitives
from the world of Scikit-Learn, Keras, DL4J and other widely used libraries.
MARVIN allows for the creation of Docker containers that run on Kubernetes
clusters within DARPA to provide an execution environment for automated machine
learning. MARVIN currently contains over 400 datasets and challenge problems
from a wide array of ML domains including routine classification and regression
to advanced video/image classification and remote sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattmann_C/0/1/0/all/0/1&quot;&gt;Chris A. Mattmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Sujen Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1&quot;&gt;Brian Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03856">
<title>Neural Importance Sampling. (arXiv:1808.03856v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03856</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to use deep neural networks for generating samples in Monte Carlo
integration. Our work is based on non-linear independent component analysis,
which we extend in numerous ways to improve performance and enable its
application to integration problems. First, we introduce piecewise-polynomial
coupling transforms that greatly increase the modeling power of individual
coupling layers. Second, we propose to preprocess the inputs of neural networks
using one-blob encoding, which stimulates localization of computation and
improves inference. Third, we derive a gradient-descent-based optimization for
the KL and the $\chi^2$ divergence for the specific application of Monte Carlo
integration with stochastic estimates of the target distribution. Our approach
enables fast and accurate inference and efficient sample generation independent
of the dimensionality of the integration domain. We demonstrate the benefits of
our approach for generating natural images and in two applications to
light-transport simulation. First, we show how to learn joint path-sampling
densities in primary sample space and how to importance sample
multi-dimensional path prefixes thereof. Second, we use our technique to
extract conditional directional densities driven by the triple product of the
rendering equation, and leverage them for path guiding. In all applications,
our approach yields on-par or higher performance at equal sample count than
competing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McWilliams_B/0/1/0/all/0/1&quot;&gt;Brian McWilliams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousselle_F/0/1/0/all/0/1&quot;&gt;Fabrice Rousselle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1&quot;&gt;Markus Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novak_J/0/1/0/all/0/1&quot;&gt;Jan Nov&amp;#xe1;k&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03857">
<title>Ranking with Features: Algorithm and A Graph Theoretic Analysis. (arXiv:1808.03857v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03857</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of ranking a set of items from pairwise comparisons
in the presence of features associated with the items. Recent works have
established that $O(n\log(n))$ samples are needed to rank well when there is no
feature information present. However, this might be sub-optimal in the presence
of associated features. We introduce a new probabilistic preference model
called feature-Bradley-Terry-Luce (f-BTL) model that generalizes the standard
BTL model to incorporate feature information. We present a new least squares
based algorithm called fBTL-LS which we show requires much lesser than
$O(n\log(n))$ pairs to obtain a good ranking -- precisely our new sample
complexity bound is of $O(\alpha\log \alpha)$, where $\alpha$ denotes the
number of `independent items&apos; of the set, in general $\alpha &amp;lt;&amp;lt; n$. Our
analysis is novel and makes use of tools from classical graph matching theory
to provide tighter bounds that sheds light on the true complexity of the
ranking problem, capturing the item dependencies in terms of their feature
representations. This was not possible with earlier matrix completion based
tools used for this problem. We also prove an information theoretic lower bound
on the required sample complexity for recovering the underlying ranking, which
essentially shows the tightness of our proposed algorithms. The efficacy of our
proposed algorithms are validated through extensive experimental evaluations on
a variety of synthetic and real world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajkumar_A/0/1/0/all/0/1&quot;&gt;Arun Rajkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03873">
<title>A Consistent Method for Learning OOMs from Asymptotically Stationary Time Series Data Containing Missing Values. (arXiv:1808.03873v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03873</link>
<description rdf:parseType="Literal">&lt;p&gt;In the traditional framework of spectral learning of stochastic time series
models, model parameters are estimated based on trajectories of fully recorded
observations. However, real-world time series data often contain missing
values, and worse, the distributions of missingness events over time are often
not independent of the visible process. Recently, a spectral OOM learning
algorithm for time series with missing data was introduced and proved to be
consistent, albeit under quite strong conditions. Here we refine the algorithm
and prove that the original strong conditions can be very much relaxed. We
validate our theoretical findings by numerical experiments, showing that the
algorithm can consistently handle missingness patterns whose dynamic interacts
with the visible process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianlin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03874">
<title>Orders-of-magnitude speedup in atmospheric chemistry modeling through neural network-based emulation. (arXiv:1808.03874v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/1808.03874</link>
<description rdf:parseType="Literal">&lt;p&gt;Chemical transport models (CTMs), which simulate air pollution transport,
transformation, and removal, are computationally expensive, largely because of
the computational intensity of the chemical mechanisms: systems of coupled
differential equations representing atmospheric chemistry. Here we investigate
the potential for machine learning to reproduce the behavior of a chemical
mechanism, yet with reduced computational expense. We create a 17-layer
residual multi-target regression neural network to emulate the Carbon Bond
Mechanism Z (CBM-Z) gas-phase chemical mechanism. We train the network to match
CBM-Z predictions of changes in concentrations of 77 chemical species after one
hour, given a range of chemical and meteorological input conditions, which it
is able to do with root-mean-square error (RMSE) of less than 1.97 ppb (median
RMSE = 0.02 ppb), while achieving a 250x computational speedup. An additional
17x speedup (total 4250x speedup) is achieved by running the neural network on
a graphics-processing unit (GPU). The neural network is able to reproduce the
emergent behavior of the chemical system over diurnal cycles using Euler
integration, but additional work is needed to constrain the propagation of
errors as simulation time progresses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kelp_M/0/1/0/all/0/1&quot;&gt;Makoto M. Kelp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tessum_C/0/1/0/all/0/1&quot;&gt;Christopher W. Tessum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Marshall_J/0/1/0/all/0/1&quot;&gt;Julian D. Marshall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03889">
<title>Robust high dimensional factor models with applications to statistical machine learning. (arXiv:1808.03889v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1808.03889</link>
<description rdf:parseType="Literal">&lt;p&gt;Factor models are a class of powerful statistical models that have been
widely used to deal with dependent measurements that arise frequently from
various applications from genomics and neuroscience to economics and finance.
As data are collected at an ever-growing scale, statistical machine learning
faces some new challenges: high dimensionality, strong dependence among
observed variables, heavy-tailed variables and heterogeneity. High-dimensional
robust factor analysis serves as a powerful toolkit to conquer these
challenges.
&lt;/p&gt;
&lt;p&gt;This paper gives a selective overview on recent advance on high-dimensional
factor models and their applications to statistics including Factor-Adjusted
Robust Model selection (FarmSelect) and Factor-Adjusted Robust Multiple testing
(FarmTest). We show that classical methods, especially principal component
analysis (PCA), can be tailored to many new problems and provide powerful tools
for statistical estimation and inference. We highlight PCA and its connections
to matrix perturbation theory, robust statistics, random projection, false
discovery rate, etc., and illustrate through several applications how insights
from these fields yield solutions to modern challenges. We also present
far-reaching connections between factor models and popular statistical learning
problems, including network analysis and low-rank matrix recovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianqing Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaizheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiqiao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03953">
<title>A Fourier View of REINFORCE. (arXiv:1808.03953v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03953</link>
<description rdf:parseType="Literal">&lt;p&gt;We show a connection between the Fourier spectrum of Boolean functions and
the REINFORCE gradient estimator for binary latent variable models. We show
that REINFORCE estimates (up to a factor) the degree-1 Fourier coefficients of
a Boolean function. Using this connection we offer a new perspective on
variance reduction in gradient estimation for latent variable models: namely,
that variance reduction involves eliminating or reducing Fourier coefficients
that do not have degree 1. We then use this connection to develop low-variance
unbiased gradient estimators for binary latent variable models such as sigmoid
belief networks. The estimator is based upon properties of the noise operator
from Boolean Fourier theory and involves a sample-dependent baseline added to
the REINFORCE estimator in a way that keeps the estimator unbiased. The
baseline can be plugged into existing gradient estimators for further variance
reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pervez_A/0/1/0/all/0/1&quot;&gt;Adeel Pervez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04008">
<title>PAC-Battling Bandits with Plackett-Luce: Tradeoff between Sample Complexity and Subset Size. (arXiv:1808.04008v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04008</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the probably approximately correct (PAC) version of the problem
of {Battling-bandits} with the Plackett-Luce (PL) model -- an online learning
framework where in each trial, the learner chooses a subset of $k \le n$ arms
from a pool of fixed set of $n$ arms, and subsequently observes a stochastic
feedback indicating preference information over the items in the chosen subset;
e.g., the most preferred item or ranking of the top $m$ most preferred items
etc. The objective is to recover an `approximate-best&apos; item of the underlying
PL model with high probability. This framework is motivated by practical
settings such as recommendation systems and information retrieval, where it is
easier and more efficient to collect relative feedback for multiple arms at
once. Our framework can be seen as a generalization of the well-studied
PAC-{Dueling-Bandit} problem over set of $n$ arms. We propose two different
feedback models: just the winner information (WI), and ranking of top-$m$ items
(TR), for any $2\le m \le k$. We show that with just the winner information
(WI), one cannot recover the `approximate-best&apos; item with sample complexity
lesser than $\Omega\bigg( \frac{n}{\epsilon^2} \ln \frac{1}{\delta}\bigg)$,
which is independent of $k$, and same as the one required for standard dueling
bandit setting ($k=2$). However with top-$m$ ranking (TR) feedback, our lower
analysis proves an improved sample complexity guarantee of $\Omega\bigg(
\frac{n}{m\epsilon^2} \ln \frac{1}{\delta}\bigg)$, which shows a relative
improvement of $\frac{1}{m}$ factor compared to WI feedback, rightfully
justifying the additional information gain due to the knowledge of ranking of
topmost $m$ items. We also provide algorithms for each of the above feedback
models, our theoretical analyses proves the {optimality} of their sample
complexities which matches the derived lower bounds (upto logarithmic factors).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalan_A/0/1/0/all/0/1&quot;&gt;Aditya Gopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Aadirupa Saha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04022">
<title>Interpretable Time Series Classification using All-Subsequence Learning and Symbolic Representations in Time and Frequency Domains. (arXiv:1808.04022v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04022</link>
<description rdf:parseType="Literal">&lt;p&gt;The time series classification literature has expanded rapidly over the last
decade, with many new classification approaches published each year. The
research focus has mostly been on improving the accuracy and efficiency of
classifiers, while their interpretability has been somewhat neglected.
Classifier interpretability has become a critical constraint for many
application domains and the introduction of the &apos;right to explanation&apos; GDPR EU
legislation in May 2018 is likely to further emphasize the importance of
explainable learning algorithms. In this work we analyse the state-of-the-art
for time series classification, and propose new algorithms that aim to maintain
the classifier accuracy and efficiency, but keep interpretability as a key
design constraint. We present new time series classification algorithms that
advance the state-of-the-art by implementing the following three key ideas: (1)
Multiple resolutions of symbolic approximations: we combine symbolic
representations obtained using different parameters; (2) Multiple domain
representations: we combine symbolic approximations in time (e.g., SAX) and
frequency (e.g., SFA) domains; (3) Efficient navigation of a huge
symbolic-words space: we adapt a symbolic sequence classifier named SEQL, to
make it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL),
and use its greedy feature selection strategy to effectively filter the best
features for each representation. We show that a multi-resolution multi-domain
linear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the
state-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but
uses a fraction of the time required by either COTE or FCN. We discuss the
accuracy, efficiency and interpretability of our proposed algorithms. To
further analyse the interpretability aspect of our classifiers, we present a
case study on an ecology benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thach Le Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gsponer_S/0/1/0/all/0/1&quot;&gt;Severin Gsponer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilie_I/0/1/0/all/0/1&quot;&gt;Iulia Ilie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ifrim_G/0/1/0/all/0/1&quot;&gt;Georgiana Ifrim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04048">
<title>Relax, and Accelerate: A Continuous Perspective on ADMM. (arXiv:1808.04048v1 [math.OC])</title>
<link>http://arxiv.org/abs/1808.04048</link>
<description rdf:parseType="Literal">&lt;p&gt;The acceleration technique first introduced by Nesterov for gradient descent
is widely used in many machine learning applications, however it is not yet
well-understood. Recently, significant progress has been made to close this
understanding gap by using a continuous-time dynamical system perspective
associated with gradient-based methods. In this paper, we extend this
perspective by considering the continuous limit of the family of relaxed
Alternating Direction Method of Multipliers (ADMM). We also introduce two new
families of relaxed and accelerated ADMM algorithms, one follows Nesterov&apos;s
acceleration approach and the other is inspired by Polyak&apos;s Heavy Ball method,
and derive the continuous limit of these families of relaxed and accelerated
algorithms as differential equations. Then, using a Lyapunov stability analysis
of the dynamical systems, we obtain rate-of-convergence results for convex and
strongly convex objective functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Franca_G/0/1/0/all/0/1&quot;&gt;Guilherme Fran&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Robinson_D/0/1/0/all/0/1&quot;&gt;Daniel P. Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vidal_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04216">
<title>Effective Unsupervised Author Disambiguation with Relative Frequencies. (arXiv:1808.04216v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.04216</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses the problem of author name homonymy in the Web of
Science. Aiming for an efficient, simple and straightforward solution, we
introduce a novel probabilistic similarity measure for author name
disambiguation based on feature overlap. Using the researcher-ID available for
a subset of the Web of Science, we evaluate the application of this measure in
the context of agglomeratively clustering author mentions. We focus on a
concise evaluation that shows clearly for which problem setups and at which
time during the clustering process our approach works best. In contrast to most
other works in this field, we are sceptical towards the performance of author
name disambiguation methods in general and compare our approach to the trivial
single-cluster baseline. Our results are presented separately for each correct
clustering size as we can explain that, when treating all cases together, the
trivial baseline and more sophisticated approaches are hardly distinguishable
in terms of evaluation results. Our model shows state-of-the-art performance
for all correct clustering sizes without any discriminative training and with
tuning only one convergence parameter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backes_T/0/1/0/all/0/1&quot;&gt;Tobias Backes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04244">
<title>Affect Estimation in 3D Space Using Multi-Task Active Learning for Regression. (arXiv:1808.04244v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04244</link>
<description rdf:parseType="Literal">&lt;p&gt;Acquisition of labeled training samples for affective computing is usually
costly and time-consuming, as affects are intrinsically subjective, subtle and
uncertain, and hence multiple human assessors are needed to evaluate each
affective sample. Particularly, for affect estimation in the 3D space of
valence, arousal and dominance, each assessor has to perform the evaluations in
three dimensions, which makes the labeling problem even more challenging. Many
sophisticated machine learning approaches have been proposed to reduce the data
labeling requirement in various other domains, but so far few have considered
affective computing. This paper proposes two multi-task active learning for
regression approaches, which select the most beneficial samples to label, by
considering the three affect primitives simultaneously. Experimental results on
the VAM corpus demonstrated that our optimal sample selection approaches can
result in better estimation performance than random selection and several
traditional single-task active learning approaches. Thus, they can help
alleviate the data labeling problem in affective computing, i.e., better
estimation performance can be obtained from fewer labeling queries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongrui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jian Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04256">
<title>CT Super-resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble(GAN-CIRCLE). (arXiv:1808.04256v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1808.04256</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is a popular medical imaging modality for screening,
diagnosis, and image-guided therapy. However, CT has its limitations,
especially involved ionizing radiation dose. Practically, it is highly
desirable to have ultrahigh quality CT imaging for fine structural details at a
minimized radiation dosage. In this paper, we propose a semi-supervised deep
learning approach to recover high-resolution (HR) CT images from low-resolution
(LR) counterparts. Especially, with the generative adversarial network (GAN) as
the basic component, we enforce the cycle-consistency in terms of the
Wasserstein distance to establish a nonlinear end-to-end mapping from noisy LR
input images to denoised HR outputs. In this deep imaging process, we
incorporate deep convolutional neural network (CNNs), residual learning, and
network in network techniques for feature extraction and restoration. In
contrast to the current trend of increasing network depth and complexity to
boost the CT imaging performance, which limit its real-world applications by
imposing considerable computational and memory overheads, we apply a parallel
1x1 CNN to reduce the dimensionality of the output of the hidden layer.
Furthermore, we optimize the number of layers and the number of filters for
each CNN layer. Quantitative and qualitative evaluations demonstrate that our
proposed model is accurate, efficient and robust for SR image restoration from
noisy LR input images. In particular, we validate our composite SR networks on
two large-scale CT datasets, and obtain very encouraging results as compared to
the other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoliu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_S/0/1/0/all/0/1&quot;&gt;Shenghong Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wenxiang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saha_P/0/1/0/all/0/1&quot;&gt;Punam K. Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04262">
<title>Connectivity-Driven Brain Parcellation via Consensus Clustering. (arXiv:1808.04262v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1808.04262</link>
<description rdf:parseType="Literal">&lt;p&gt;We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kurmukov_A/0/1/0/all/0/1&quot;&gt;Anvar Kurmukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mussabayeva_A/0/1/0/all/0/1&quot;&gt;Ayagoz Mussabayeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Denisova_Y/0/1/0/all/0/1&quot;&gt;Yulia Denisova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Moyer_D/0/1/0/all/0/1&quot;&gt;Daniel Moyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gutman_B/0/1/0/all/0/1&quot;&gt;Boris Gutman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04281">
<title>Estimating Heterogeneous Causal Effects in the Presence of Irregular Assignment Mechanisms. (arXiv:1808.04281v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04281</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a link between causal inference and machine learning
techniques - specifically, Classification and Regression Trees (CART) - in
observational studies where the receipt of the treatment is not randomized, but
the assignment to the treatment can be assumed to be randomized (irregular
assignment mechanism). The paper contributes to the growing applied machine
learning literature on causal inference, by proposing a modified version of the
Causal Tree (CT) algorithm to draw causal inference from an irregular
assignment mechanism. The proposed method is developed by merging the CT
approach with the instrumental variable framework to causal inference, hence
the name Causal Tree with Instrumental Variable (CT-IV). As compared to CT, the
main strength of CT-IV is that it can deal more efficiently with the
heterogeneity of causal effects, as demonstrated by a series of numerical
results obtained on synthetic data. Then, the proposed algorithm is used to
evaluate a public policy implemented by the Tuscan Regional Administration
(Italy), which aimed at easing the access to credit for small firms. In this
context, CT-IV breaks fresh ground for target-based policies, identifying
interesting heterogeneous causal effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoffi_F/0/1/0/all/0/1&quot;&gt;Falco J. Bargagli Stoffi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gnecco_G/0/1/0/all/0/1&quot;&gt;Giorgio Gnecco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04293">
<title>Fast, Better Training Trick --- Random Gradient. (arXiv:1808.04293v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04293</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we will show an unprecedented method to accelerate training
and improve performance, which called random gradient (RG). This method can be
easier to the training of any model without extra calculation cost, we use
Image classification, Semantic segmentation, and GANs to confirm this method
can improve speed which is training model in computer vision. The central idea
is using the loss multiplied by a random number to random reduce the
back-propagation gradient. We can use this method to produce a better result in
Pascal VOC, Cifar, Cityscapes datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jiakai Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04302">
<title>Simple Root Cause Analysis by Separable Likelihoods. (arXiv:1808.04302v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04302</link>
<description rdf:parseType="Literal">&lt;p&gt;Root Cause Analysis for Anomalies is challenging because of the trade-off
between the accuracy and its explanatory friendliness, required for industrial
applications. In this paper we propose a framework for simple and friendly RCA
within the Bayesian regime under certain restrictions (that Hessian at the mode
is diagonal, here referred to as \emph{separability}) imposed on the predictive
posterior. We show that this assumption is satisfied for important base models,
including Multinomal, Dirichlet-Multinomial and Naive Bayes. To demonstrate the
usefulness of the framework, we embed it into the Bayesian Net and validate on
web server error logs (real world data set).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skorski_M/0/1/0/all/0/1&quot;&gt;Maciej Skorski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04308">
<title>What is Unique in Individual Gait Patterns? Understanding and Interpreting Deep Learning in Gait Analysis. (arXiv:1808.04308v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04308</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) techniques such as (deep) artificial neural networks
(DNN) are solving very successfully a plethora of tasks and provide new
predictive models for complex physical, chemical, biological and social
systems. However, in most cases this comes with the disadvantage of acting as a
black box, rarely providing information about what made them arrive at a
particular prediction. This black box aspect of ML techniques can be
problematic especially in medical diagnoses, so far hampering a clinical
acceptance. The present paper studies the uniqueness of individual gait
patterns in clinical biomechanics using DNNs. By attributing portions of the
model predictions back to the input variables (ground reaction forces and
full-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique
reliably demonstrates which variables at what time windows of the gait cycle
are most relevant for the characterisation of gait patterns from a certain
individual. By measuring the timeresolved contribution of each input variable
to the prediction of ML techniques such as DNNs, our method describes the first
general framework that enables to understand and interpret non-linear ML
methods in (biomechanical) gait analysis and thereby supplies a powerful tool
for analysis, diagnosis and treatment of human gait.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horst_F/0/1/0/all/0/1&quot;&gt;Fabian Horst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1&quot;&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schollhorn_W/0/1/0/all/0/1&quot;&gt;Wolfgang I. Sch&amp;#xf6;llhorn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.06423">
<title>Optimization as Estimation with Gaussian Processes in Bandit Settings. (arXiv:1510.06423v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1510.06423</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there has been rising interest in Bayesian optimization -- the
optimization of an unknown function with assumptions usually expressed by a
Gaussian Process (GP) prior. We study an optimization strategy that directly
uses an estimate of the argmax of the function. This strategy offers both
practical and theoretical advantages: no tradeoff parameter needs to be
selected, and, moreover, we establish close connections to the popular GP-UCB
and GP-PI strategies. Our approach can be understood as automatically and
adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We
illustrate the effects of this adaptive tuning via bounds on the regret as well
as an extensive empirical evaluation on robotics and vision tasks,
demonstrating the robustness of this strategy for a range of performance
criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bolei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.04530">
<title>Domain based classification. (arXiv:1601.04530v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1601.04530</link>
<description rdf:parseType="Literal">&lt;p&gt;The majority of traditional classification ru les minimizing the expected
probability of error (0-1 loss) are inappropriate if the class probability
distributions are ill-defined or impossible to estimate. We argue that in such
cases class domains should be used instead of class distributions or densities
to construct a reliable decision function. Proposals are presented for some
evaluation criteria and classifier learning schemes, illustrated by an example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duin_R/0/1/0/all/0/1&quot;&gt;Robert P.W. Duin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pekalska_E/0/1/0/all/0/1&quot;&gt;Elzbieta Pekalska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05327">
<title>Solving Equations of Random Convex Functions via Anchored Regression. (arXiv:1702.05327v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05327</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the question of estimating a solution to a system of equations
that involve convex nonlinearities, a problem that is common in machine
learning and signal processing. Because of these nonlinearities, conventional
estimators based on empirical risk minimization generally involve solving a
non-convex optimization program. We propose anchored regression, a new approach
based on convex programming that amounts to maximizing a linear functional
(perhaps augmented by a regularizer) over a convex set. The proposed convex
program is formulated in the natural space of the problem, and avoids the
introduction of auxiliary variables, making it computationally favorable.
Working in the native space also provides great flexibility as structural
priors (e.g., sparsity) can be seamlessly incorporated.
&lt;/p&gt;
&lt;p&gt;For our analysis, we model the equations as being drawn from a fixed set
according to a probability law. Our main results provide guarantees on the
accuracy of the estimator in terms of the number of equations we are solving,
the amount of noise present, a measure of statistical complexity of the random
equations, and the geometry of the regularizer at the true solution. We also
provide recipes for constructing the anchor vector (that determines the linear
functional to maximize) directly from the observed data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1&quot;&gt;Sohail Bahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romberg_J/0/1/0/all/0/1&quot;&gt;Justin Romberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04072">
<title>A convergence framework for inexact nonconvex and nonsmooth algorithms and its applications to several iterations. (arXiv:1709.04072v5 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04072</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the convergence of an abstract inexact nonconvex
and nonsmooth algorithm. We promise a pseudo sufficient descent condition and a
pseudo relative error condition, which are both related to an auxiliary
sequence, for the algorithm; and a continuity condition is assumed to hold. In
fact, a lot of classical inexact nonconvex and nonsmooth algorithms allow these
three conditions. Under a special kind of summable assumption on the auxiliary
sequence, we prove the sequence generated by the general algorithm converges to
a critical point of the objective function if being assumed Kurdyka-
Lojasiewicz property. The core of the proofs lies in building a new Lyapunov
function, whose successive difference provides a bound for the successive
difference of the points generated by the algorithm. And then, we apply our
findings to several classical nonconvex iterative algorithms and derive the
corresponding convergence results
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lizhi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05114">
<title>Arbitrage-Free Regularization. (arXiv:1710.05114v3 [q-fin.MF] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05114</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an unsupervised and non-anticipative machine learning algorithm
which is able to detect and remove arbitrage from a wide variety models. In
this framework, fundamental results and techniques from risk-neutral pricing
theory such as NFLVR, market completeness, and changes of measure are given an
equivalent formulation and extended to models which are deformable into
arbitrage-free models. We use this scheme to construct a meta-algorithm which
ensures that a wide range of factor estimation schemes return arbitrage-free
estimates and incorporate this additional information into their estimation
procedure. We show that using our meta-algorithm we are able to produce more
accurate estimates of forward-rate curves, specifically at the long-end. The
spread between a model and its arbitrage-free regularization is then used to
construct a mis-pricing detection or classification algorithm, which is in turn
used to develop a pairs trading strategy. Our theory provides a sound
theoretical foundation for a risk-neutral pricing theory capable of handling
models which potentially admit arbitrage but which can which can be deformed
into arbitrage-free models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Kratsios_A/0/1/0/all/0/1&quot;&gt;Anastasis Kratsios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Hyndman_C/0/1/0/all/0/1&quot;&gt;Cody B. Hyndman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06396">
<title>Reviving and Improving Recurrent Back-Propagation. (arXiv:1803.06396v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06396</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we revisit the recurrent back-propagation (RBP) algorithm,
discuss the conditions under which it applies as well as how to satisfy them in
deep neural networks. We show that RBP can be unstable and propose two variants
based on conjugate gradient on the normal equations (CG-RBP) and Neumann series
(Neumann-RBP). We further investigate the relationship between Neumann-RBP and
back propagation through time (BPTT) and its truncated version (TBPTT). Our
Neumann-RBP has the same time complexity as TBPTT but only requires constant
memory, whereas TBPTT&apos;s memory cost scales linearly with the number of
truncation steps. We examine all RBP variants along with BPTT and TBPTT in
three different application domains: associative memory with continuous
Hopfield networks, document classification in citation networks using graph
neural networks and hyperparameter optimization for fully connected networks.
All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are
efficient and effective for optimizing convergent recurrent neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lisa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;KiJung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1&quot;&gt;Xaq Pitkow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09704">
<title>MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time Series Forecasting. (arXiv:1803.09704v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09704</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series forecasting is ubiquitous in the modern world. Applications range
from health care to astronomy, and include climate modelling, financial trading
and monitoring of critical engineering equipment. To offer value over this
range of activities, models must not only provide accurate forecasts, but also
quantify and adjust their uncertainty over time. In this work, we directly
tackle this task with a novel, fully end-to-end deep learning method for time
series forecasting. By recasting time series forecasting as an ordinal
regression task, we develop a principled methodology to assess long-term
predictive uncertainty and describe rich multimodal, non-Gaussian behaviour,
which arises regularly in applied settings.
&lt;/p&gt;
&lt;p&gt;Notably, our framework is a wholly general-purpose approach that requires
little to no user intervention to be used. We showcase this key feature in a
large-scale benchmark test with 45 datasets drawn from both, a wide range of
real-world application domains, as well as a comprehensive list of synthetic
maps. This wide comparison encompasses state-of-the-art methods in both the
Machine Learning and Statistics modelling literature, such as the Gaussian
Process. We find that our approach does not only provide excellent predictive
forecasts, shadowing true future values, but also allows us to infer valuable
information, such as the predictive distribution of the occurrence of critical
events of interest, accurately and reliably even over long time horizons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Orozco_B/0/1/0/all/0/1&quot;&gt;Bernardo P&amp;#xe9;rez Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbati_G/0/1/0/all/0/1&quot;&gt;Gabriele Abbati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10377">
<title>Ergodic Measure Preserving Flows. (arXiv:1805.10377v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10377</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic modelling is a general and elegant framework to capture the
uncertainty, ambiguity and diversity of data. Probabilistic inference is the
core technique for developing training and simulation algorithms on
probabilistic models. However, the classic inference methods, like Markov chain
Monte Carlo (MCMC) methods and mean-field variational inference (VI), are not
computationally scalable for the recent developed probabilistic models with
neural networks (NNs). This motivates many recent works on improving classic
inference methods using NNs, especially, NN empowered VI. However, even with
powerful NNs, VI still suffers its fundamental limitations. In this work, we
propose a novel computational scalable general inference framework. With the
theoretical foundation in ergodic theory, the proposed methods are not only
computationally scalable like NN-based VI methods but also asymptotically
accurate like MCMC. We test our method on popular benchmark problems and the
results suggest that our methods can outperform NN-based VI and MCMC on deep
generative models and Bayesian neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jose Miguel Hernandez-Lobato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11048">
<title>Scalable Spectral Clustering Using Random Binning Features. (arXiv:1805.11048v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11048</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is one of the most effective clustering approaches that
capture hidden cluster structures in the data. However, it does not scale well
to large-scale problems due to its quadratic complexity in constructing
similarity graphs and computing subsequent eigendecomposition. Although a
number of methods have been proposed to accelerate spectral clustering, most of
them compromise considerable information loss in the original data for reducing
computational bottlenecks. In this paper, we present a novel scalable spectral
clustering method using Random Binning features (RB) to simultaneously
accelerate both similarity graph construction and the eigendecomposition.
Specifically, we implicitly approximate the graph similarity (kernel) matrix by
the inner product of a large sparse feature matrix generated by RB. Then we
introduce a state-of-the-art SVD solver to effectively compute eigenvectors of
this large matrix for spectral clustering. Using these two building blocks, we
reduce the computational cost from quadratic to linear in the number of data
points while achieving similar accuracy. Our theoretical analysis shows that
spectral clustering via RB converges faster to the exact spectral clustering
than the standard Random Feature approximation. Extensive experiments on 8
benchmarks show that the proposed method either outperforms or matches the
state-of-the-art methods in both accuracy and runtime. Moreover, our method
exhibits linear scalability in both the number of data samples and the number
of RB features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lingfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1&quot;&gt;Ian En-Hsu Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Fangli Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yinglong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1&quot;&gt;Charu Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00552">
<title>Bayesian approach to model-based extrapolation of nuclear observables. (arXiv:1806.00552v2 [nucl-th] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00552</link>
<description rdf:parseType="Literal">&lt;p&gt;The mass, or binding energy, is the basis property of the atomic nucleus. It
determines its stability, and reaction and decay rates. Quantifying the nuclear
binding is important for understanding the origin of elements in the universe.
The astrophysical processes responsible for the nucleosynthesis in stars often
take place far from the valley of stability, where experimental masses are not
known. In such cases, missing nuclear information must be provided by
theoretical predictions using extreme extrapolations. Bayesian machine learning
techniques can be applied to improve predictions by taking full advantage of
the information contained in the deviations between experimental and calculated
masses. We consider 10 global models based on nuclear Density Functional Theory
as well as two more phenomenological mass models. The emulators of S2n
residuals and credibility intervals defining theoretical error bars are
constructed using Bayesian Gaussian processes and Bayesian neural networks. We
consider a large training dataset pertaining to nuclei whose masses were
measured before 2003. For the testing datasets, we considered those exotic
nuclei whose masses have been determined after 2003. We then carried out
extrapolations towards the 2n dripline. While both Gaussian processes and
Bayesian neural networks reduce the rms deviation from experiment
significantly, GP offers a better and much more stable performance. The
increase in the predictive power is quite astonishing: the resulting rms
deviations from experiment on the testing dataset are similar to those of more
phenomenological models. The empirical coverage probability curves we obtain
match very well the reference values which is highly desirable to ensure
honesty of uncertainty quantification, and the estimated credibility intervals
on predictions make it possible to evaluate predictive power of individual
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Neufcourt_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;o Neufcourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuchen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Nazarewicz_W/0/1/0/all/0/1&quot;&gt;Witold Nazarewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Viens_F/0/1/0/all/0/1&quot;&gt;Frederi Viens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06007">
<title>On Lebesgue Integral Quadrature. (arXiv:1807.06007v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06007</link>
<description rdf:parseType="Literal">&lt;p&gt;A new type of quadrature is developed. The Gauss quadrature, for a given
measure, finds optimal values of a function&apos;s argument (nodes) and the
corresponding weights. In contrast, the Lebesgue quadrature developed in this
paper, finds optimal values of function (value-nodes) and the corresponding
weights. The Gauss quadrature groups sums by function argument, it can be
viewed as a $n$-point discrete measure, producing the Riemann integral. The
Lebesgue quadrature groups sums by function value, it can be viewed as a
$n$-point discrete distribution, producing the Lebesgue integral.
Mathematically, the problem is reduced to a generalized eigenvalue problem:
Lebesgue quadrature value-nodes are the eigenvalues and the corresponding
weights are the square of the averaged eigenvectors. A numerical estimation of
an integral as the Lebesgue integral is especially advantageous when analyzing
irregular and stochastic processes. The approach separates the outcome
(value-nodes) and the probability of the outcome (weight). For this reason, it
is especially well-suited for the study of non--Gaussian processes. The
software implementing the theory is available from the authors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malyshkin_V/0/1/0/all/0/1&quot;&gt;Vladislav Gennadievich Malyshkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09865">
<title>Predicting Acute Kidney Injury at Hospital Re-entry Using High-dimensional Electronic Health Record Data. (arXiv:1807.09865v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.09865</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Kidney Injury (AKI), a sudden decline in kidney function, is associated
with increased mortality, morbidity, length of stay, and hospital cost. Since
AKI is sometimes preventable, there is great interest in prediction. Most
existing studies consider all patients and therefore restrict to features
available in the first hours of hospitalization. Here, the focus is instead on
rehospitalized patients, a cohort in which rich longitudinal features from
prior hospitalizations can be analyzed. Our objective is to provide a risk
score directly at hospital re-entry. Gradient boosting, penalized logistic
regression (with and without stability selection), and a recurrent neural
network are trained on two years of adult inpatient EHR data (3,387 attributes
for 34,505 patients who generated 90,013 training samples with 5,618 cases and
84,395 controls). Predictions are internally evaluated with 50 iterations of
5-fold grouped cross-validation with special emphasis on calibration, an
analysis of which is performed at the patient as well as hospitalization level.
Error is assessed with respect to diagnosis, race, age, gender, AKI
identification method, and hospital utilization. In an additional experiment,
the regularization penalty is severely increased to induce parsimony and
interpretability. Predictors identified for rehospitalized patients are also
reported with a special analysis of medications that might be modifiable risk
factors. Insights from this study might be used to construct a predictive tool
for AKI in rehospitalized patients. An accurate estimate of AKI risk at
hospital entry might serve as a prior for an admitting provider or another
predictive algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisenthal_S/0/1/0/all/0/1&quot;&gt;Samuel J. Weisenthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quill_C/0/1/0/all/0/1&quot;&gt;Caroline Quill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_S/0/1/0/all/0/1&quot;&gt;Samir Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_H/0/1/0/all/0/1&quot;&gt;Henry Kautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1&quot;&gt;Martin S. Zand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03333">
<title>Linked Causal Variational Autoencoder for Inferring Paired Spillover Effects. (arXiv:1808.03333v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03333</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling spillover effects from observational data is an important problem in
economics, business, and other fields of research. % It helps us infer the
causality between two seemingly unrelated set of events. For example, if
consumer spending in the United States declines, it has spillover effects on
economies that depend on the U.S. as their largest export market. In this
paper, we aim to infer the causation that results in spillover effects between
pairs of entities (or units), we call this effect as \textit{paired spillover}.
To achieve this, we leverage the recent developments in variational inference
and deep learning techniques to propose a generative model called Linked Causal
Variational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA
incorporates an encoder neural network to learn the latent attributes and a
decoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the
\textit{latent attributes as confounders that are assumed to affect both the
treatment and the outcome of units}. Specifically, given a pair of units $u$
and $\bar{u}$, their individual treatment and outcomes, the encoder network of
LCVA samples the confounders by conditioning on the observed covariates of $u$,
the treatments of both $u$ and $\bar{u}$ and the outcome of $u$. Once inferred,
the latent attributes (or confounders) of $u$ captures the spillover effect of
$\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde
(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that
LCVA is significantly more robust than existing methods in capturing spillover
effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakesh_V/0/1/0/all/0/1&quot;&gt;Vineeth Rakesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Ruocheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1&quot;&gt;Raha Moraffah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1&quot;&gt;Nitin Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>