<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01425"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01683"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07949"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06591"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01092"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01252"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01422"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06422"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02741"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01229"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01302"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01442"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01454"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01498"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01575"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01626"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01802"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01814"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1504.06964"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.00967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1512.02063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.04460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.00299"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.03863"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.09165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02631"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04072"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09376"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01649"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07575"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09963"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09129"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.01425">
<title>On the Effectiveness of Simple Success-Based Parameter Selection Mechanisms for Two Classical Discrete Black-Box Optimization Benchmark Problems. (arXiv:1803.01425v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.01425</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant empirical and theoretically supported evidence that
non-static parameter choices can be strongly beneficial in evolutionary
computation, the question how to best adjust parameter values plays only a
marginal role in contemporary research on discrete black-box optimization. This
has led to the unsatisfactory situation in which feedback-free parameter
selection rules such as the cooling schedule of Simulated Annealing are
predominant in state-of-the-art heuristics, while, at the same time, we
understand very well that such time-dependent selection rules can only perform
worse than adjustment rules that do take into account the evolution of the
optimization process. A number of adaptive and self-adaptive parameter control
strategies have been proposed in the literature, but did not (yet) make their
way to a broader public. A key obstacle seems to lie in their rather complex
update rules.
&lt;/p&gt;
&lt;p&gt;The purpose of our work is to demonstrate that high-performing online
parameter selection rules do not have to be very complicated. More precisely,
we experiment with a multiplicative, comparison-based update rule to adjust the
mutation probability of a (1+1)~Evolutionary Algorithm. We show that this
simple self-adjusting rule outperforms the best static unary unbiased black-box
algorithm on LeadingOnes, achieving an almost optimal speedup of about~$18\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1&quot;&gt;Carola Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Markus Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01683">
<title>Investigating the Evolvability of Web Page Load Time. (arXiv:1803.01683v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1803.01683</link>
<description rdf:parseType="Literal">&lt;p&gt;Client-side Javascript execution environments (browsers) allow anonymous
functions and event-based programming concepts such as callbacks. We
investigate whether a mutate-and-test approach can be used to optimise web page
load time in these environments. First, we characterise a web page load issue
in a benchmark web page and derive performance metrics from page load event
traces. We parse Javascript source code to an AST and make changes to method
calls which appear in a web page load event trace. We present an operator based
solely on code deletion and evaluate an existing &quot;community-contributed&quot;
performance optimising code transform. By exploring Javascript code changes and
exploiting combinations of non-destructive changes, we can optimise page load
time by 41% in our benchmark web page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cody_Kenny_B/0/1/0/all/0/1&quot;&gt;Brendan Cody-Kenny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manganiello_U/0/1/0/all/0/1&quot;&gt;Umberto Manganiello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrelly_J/0/1/0/all/0/1&quot;&gt;John Farrelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronayne_A/0/1/0/all/0/1&quot;&gt;Adrian Ronayne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Considine_E/0/1/0/all/0/1&quot;&gt;Eoghan Considine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuire_T/0/1/0/all/0/1&quot;&gt;Thomas McGuire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ONeill_M/0/1/0/all/0/1&quot;&gt;Michael O&amp;#x27;Neill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07949">
<title>TraNNsformer: Neural network transformation for memristive crossbar based neuromorphic system design. (arXiv:1708.07949v2 [cs.ET] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07949</link>
<description rdf:parseType="Literal">&lt;p&gt;Implementation of Neuromorphic Systems using post Complementary
Metal-Oxide-Semiconductor (CMOS) technology based Memristive Crossbar Array
(MCA) has emerged as a promising solution to enable low-power acceleration of
neural networks. However, the recent trend to design Deep Neural Networks
(DNNs) for achieving human-like cognitive abilities poses significant
challenges towards the scalable design of neuromorphic systems (due to the
increase in computation/storage demands). Network pruning [7] is a powerful
technique to remove redundant connections for designing optimally connected
(maximally sparse) DNNs. However, such pruning techniques induce irregular
connections that are incoherent to the crossbar structure. Eventually they
produce DNNs with highly inefficient hardware realizations (in terms of area
and energy). In this work, we propose TraNNsformer - an integrated training
framework that transforms DNNs to enable their efficient realization on
MCA-based systems. TraNNsformer first prunes the connectivity matrix while
forming clusters with the remaining connections. Subsequently, it retrains the
network to fine tune the connections and reinforce the clusters. This is done
iteratively to transform the original connectivity into an optimally pruned and
maximally clustered mapping. Without accuracy loss, TraNNsformer reduces the
area (energy) consumption by 28% - 55% (49% - 67%) with respect to the original
network. Compared to network pruning, TraNNsformer achieves 28% - 49% (15% -
29%) area (energy) savings. Furthermore, TraNNsformer is a technology-aware
framework that allows mapping a given DNN to any MCA size permissible by the
memristive technology for reliable operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ankit_A/0/1/0/all/0/1&quot;&gt;Aayush Ankit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1&quot;&gt;Abhronil Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08961">
<title>Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v3 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08961</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, analyzing task-based fMRI (tfMRI) data has become an
essential tool for understanding brain function and networks. However, due to
the sheer size of tfMRI data, its intrinsic complex structure, and lack of
ground truth of underlying neural activities, modeling tfMRI data is hard and
challenging. Previously proposed data-modeling methods including Independent
Component Analysis (ICA) and Sparse Dictionary Learning only provided a weakly
established model based on blind source separation under the strong assumption
that original fMRI signals could be linearly decomposed into time series
components with corresponding spatial maps. Meanwhile, analyzing and learning a
large amount of tfMRI data from a variety of subjects has been shown to be very
demanding but yet challenging even with technological advances in computational
hardware. Given the Convolutional Neural Network (CNN), a robust method for
learning high-level abstractions from low-level data such as tfMRI time series,
in this work we propose a fast and scalable novel framework for distributed
deep Convolutional Autoencoder model. This model aims to both learn the complex
hierarchical structure of the tfMRI data and to leverage the processing power
of multiple GPUs in a distributed fashion. To implement such a model, we have
created an enhanced processing pipeline on the top of Apache Spark and
Tensorflow library, leveraging from a very large cluster of GPU machines.
Experimental data from applying the model on the Human Connectome Project (HCP)
show that the proposed model is efficient and scalable toward tfMRI big data
analytics, thus enabling data-driven extraction of hierarchical neuroscientific
information from massive fMRI big data in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makkie_M/0/1/0/all/0/1&quot;&gt;Milad Makkie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilakos_A/0/1/0/all/0/1&quot;&gt;Athanasios V. Vasilakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06591">
<title>Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration. (arXiv:1802.06591v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06591</link>
<description rdf:parseType="Literal">&lt;p&gt;When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_J/0/1/0/all/0/1&quot;&gt;Jonathan Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisi_G/0/1/0/all/0/1&quot;&gt;German I. Parisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roder_B/0/1/0/all/0/1&quot;&gt;Brigitte R&amp;#xf6;der&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01016">
<title>Model-Free Control for Distributed Stream Data Processing using Deep Reinforcement Learning. (arXiv:1803.01016v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1803.01016</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on general-purpose Distributed Stream Data Processing
Systems (DSDPSs), which deal with processing of unbounded streams of continuous
data at scale distributedly in real or near-real time. A fundamental problem in
a DSDPS is the scheduling problem with the objective of minimizing average
end-to-end tuple processing time. A widely-used solution is to distribute
workload evenly over machines in the cluster in a round-robin manner, which is
obviously not efficient due to lack of consideration for communication delay.
Model-based approaches do not work well either due to the high complexity of
the system environment. We aim to develop a novel model-free approach that can
learn to well control a DSDPS from its experience rather than accurate and
mathematically solvable system models, just as a human learns a skill (such as
cooking, driving, swimming, etc). Specifically, we, for the first time, propose
to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free
control in DSDPSs; and present design, implementation and evaluation of a novel
and highly effective DRL-based control framework, which minimizes average
end-to-end tuple processing time by jointly learning the system environment via
collecting very limited runtime statistics data and making decisions under the
guidance of powerful Deep Neural Networks. To validate and evaluate the
proposed framework, we implemented it based on a widely-used DSDPS, Apache
Storm, and tested it with three representative applications. Extensive
experimental results show 1) Compared to Storm&apos;s default scheduler and the
state-of-the-art model-based method, the proposed framework reduces average
tuple processing by 33.5% and 14.0% respectively on average. 2) The proposed
framework can quickly reach a good scheduling solution during online learning,
which justifies its practicability for online control in DSDPSs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Teng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01092">
<title>Analyzing Business Process Anomalies Using Autoencoders. (arXiv:1803.01092v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.01092</link>
<description rdf:parseType="Literal">&lt;p&gt;Businesses are naturally interested in detecting anomalies in their internal
processes, because these can be indicators for fraud and inefficiencies. Within
the domain of business intelligence, classic anomaly detection is not very
frequently researched. In this paper, we propose a method, using autoencoders,
for detecting and analyzing anomalies occurring in the execution of a business
process. Our method does not rely on any prior knowledge about the process and
can be trained on a noisy dataset already containing the anomalies. We
demonstrate its effectiveness by evaluating it on 700 different datasets and
testing its performance against three state-of-the-art anomaly detection
methods. This paper is an extension of our previous work from 2016 [30].
Compared to the original publication we have further refined the approach in
terms of performance and conducted an elaborate evaluation on more
sophisticated datasets including real-life event logs from the Business Process
Intelligence Challenges of 2012 and 2017. In our experiments our approach
reached an F1 score of 0.87, whereas the best unaltered state-of-the-art
approach reached an F1 score of 0.72. Furthermore, our approach can be used to
analyze the detected anomalies in terms of which event within one execution of
the process causes the anomaly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nolle_T/0/1/0/all/0/1&quot;&gt;Timo Nolle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luettgen_S/0/1/0/all/0/1&quot;&gt;Stefan Luettgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seeliger_A/0/1/0/all/0/1&quot;&gt;Alexander Seeliger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhlhauser_M/0/1/0/all/0/1&quot;&gt;Max M&amp;#xfc;hlh&amp;#xe4;user&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01097">
<title>An Interactive Many Objective Evolutionary Algorithm with Cascade Clustering and Reference Point Incremental Learning. (arXiv:1803.01097v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;Researches have shown difficulties in obtaining proximity while maintaining
diversity for solving many-objective optimization problems (MaOPs). The
complexities of the true Pareto Front (PF) also pose serious challenges for the
pervasive algorithms for their insufficient ability to adapt to the
characteristics of the true PF with no priori. This paper proposes a cascade
Clustering and reference point incremental Learning based Interactive Algorithm
(CLIA) for many-objective optimization. In the cascade clustering process,
using reference lines provided by the learning process, individuals are
clustered and intraclassly sorted in a bi-level cascade style for better
proximity and diversity. In the reference point incremental learning process,
using the feedbacks from the clustering process, the proper generation of
reference points is gradually obtained by incremental learning and the
reference lines are accordingly repositioned. The advantages of the proposed
interactive algorithm CLIA lie not only in the proximity obtainment and
diversity maintenance but also in the versatility for the diverse PFs which
uses only the interactions between the two processes without incurring extra
evaluations. The experimental studies on the CEC&apos;2018 MaOP benchmark functions
have shown that the proposed algorithm CLIA has satisfactory covering of the
true PFs, and is competitive, stable and efficient compared with the
state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_H/0/1/0/all/0/1&quot;&gt;Hongwei Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mingde Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Liang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1&quot;&gt;Guozhen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C.L. Philip Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01206">
<title>On the Power of Over-parametrization in Neural Networks with Quadratic Activation. (arXiv:1803.01206v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01206</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide new theoretical insights on why over-parametrization is effective
in learning neural networks. For a $k$ hidden node shallow network with
quadratic activation and $n$ training data points, we show as long as $ k \ge
\sqrt{2n}$, over-parametrization enables local search algorithms to find a
\emph{globally} optimal solution for general smooth and convex loss functions.
Further, despite that the number of parameters may exceed the sample size,
using theory of Rademacher complexity, we show with weight decay, the solution
also generalizes well if the data is sampled from a regular distribution such
as Gaussian. To prove when $k\ge \sqrt{2n}$, the loss function has benign
landscape properties, we adopt an idea from smoothed analysis, which may have
other applications in studying loss surfaces of neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01252">
<title>A Swift Heuristic Method for Work Order Scheduling under the Skilled-Workforce Constraint. (arXiv:1803.01252v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.01252</link>
<description rdf:parseType="Literal">&lt;p&gt;The considered problem is how to optimally allocate a set of jobs to
technicians of different skills such that the number of technicians of each
skill does not exceed the number of persons with that skill designation. The
key motivation is the quick sensitivity analysis in terms of the workforce size
which is quite necessary in many industries in the presence of unexpected work
orders. A time-indexed mathematical model is proposed to minimize the total
weighted completion time of the jobs. The proposed model is decomposed into a
number of single-skill sub-problems so that each one is a combination of a
series of nested binary Knapsack problems. A heuristic procedure is proposed to
solve the problem. Our experimental results, based on a real-world case study,
reveal that the proposed method quickly produces a schedule statistically close
to the optimal one while the classical optimal procedure is very
time-consuming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safaei_N/0/1/0/all/0/1&quot;&gt;Nima Safaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiassat_C/0/1/0/all/0/1&quot;&gt;Corey Kiassat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01316">
<title>On Cognitive Preferences and the Interpretability of Rule-based Models. (arXiv:1803.01316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01316</link>
<description rdf:parseType="Literal">&lt;p&gt;It is conventional wisdom in machine learning and data mining that logical
models such as rule sets are more interpretable than other models, and that
among such rule-based models, simpler models are more interpretable than more
complex ones. In this position paper, we question this latter assumption, and
recapitulate evidence for and against this postulate. We also report the
results of an evaluation in a crowd-sourcing study, which does not reveal a
strong preference for simple rules, whereas we can observe a weak preference
for longer rules in some domains. We then continue to review criteria for
interpretability from the psychological literature, evaluate some of them, and
briefly discuss their potential use in machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1&quot;&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kliegr_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Kliegr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1&quot;&gt;Heiko Paulheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01364">
<title>SAFE: Spectral Evolution Analysis Feature Extraction for Non-Stationary Time Series Prediction. (arXiv:1803.01364v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01364</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a practical approach for detecting non-stationarity in
time series prediction. This method is called SAFE and works by monitoring the
evolution of the spectral contents of time series through a distance function.
This method is designed to work in combination with state-of-the-art machine
learning methods in real time by informing the online predictors to perform
necessary adaptation when a non-stationarity presents. We also propose an
algorithm to proportionally include some past data in the adaption process to
overcome the Catastrophic Forgetting problem. To validate our hypothesis and
test the effectiveness of our approach, we present comprehensive experiments in
different elements of the approach involving artificial and real-world
datasets. The experiments show that the proposed method is able to
significantly save computational resources in term of processor or GPU cycles
while maintaining high prediction performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koesdwiady_A/0/1/0/all/0/1&quot;&gt;Arief Koesdwiady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1&quot;&gt;Fakhri Karray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01378">
<title>Localization under Topological Uncertainty for Lane Identification of Autonomous Vehicles. (arXiv:1803.01378v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.01378</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles (AVs) require accurate metric and topological location
estimates for safe, effective navigation and decision-making. Although many
high-definition (HD) roadmaps exist, they are not always accurate since public
roads are dynamic, shaped unpredictably by both human activity and nature.
Thus, AVs must be able to handle situations in which the topology specified by
the map does not agree with reality. We present the Variable Structure Multiple
Hidden Markov Model (VSM-HMM) as a framework for localizing in the presence of
topological uncertainty, and demonstrate its effectiveness on an AV where lane
membership is modeled as a topological localization process. VSM-HMMs use a
dynamic set of HMMs to simultaneously reason about location within a set of
most likely current topologies and therefore may also be applied to topological
structure estimation as well as AV lane estimation. In addition, we present an
extension to the Earth Mover&apos;s Distance which allows uncertainty to be taken
into account when computing the distance between belief distributions on
simplices of arbitrary relative sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nashed_S/0/1/0/all/0/1&quot;&gt;Samer B. Nashed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilstrup_D/0/1/0/all/0/1&quot;&gt;David M. Ilstrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_J/0/1/0/all/0/1&quot;&gt;Joydeep Biswas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01412">
<title>A real-time rule-based system for bridge management based on CART decision tree and SMO algorithms. (arXiv:1803.01412v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.01412</link>
<description rdf:parseType="Literal">&lt;p&gt;To real-time management of the bridges under dynamic conditions, this paper
develops a rule-based decision support framework to extract the necessary rules
from simulation results made by Aimsun. In this rule-based system, the
supervised and the unsupervised learning algorithms are applied to generalize
the rules where the initial set of rules are provided by the aid of fuzzy rule
generation algorithms on the results of Aimsun traffic micro-simulation
software. As a pilot case study, Nasr Bridge in Tehran is simulated in Aimsun7
and WEKA data mining software is used to execute the learning algorithms. Based
on this experiment, the accuracy of the supervised algorithms to generalize the
rules is greater than 80%. In addition, CART decision tree and sequential
minimal optimization (SMO) provides 100% accuracy for normal data and so these
algorithms are so reliable for crisis management on bridge. This means that, it
is possible to use such machine learning methods to manage bridges in the
real-time conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abpeykar_S/0/1/0/all/0/1&quot;&gt;Shadi Abpeykar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1&quot;&gt;Mehdi Ghatee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01422">
<title>DAGs with NO TEARS: Smooth Optimization for Structure Learning. (arXiv:1803.01422v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01422</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the structure of directed acyclic graphs (DAGs, also known as
Bayesian networks) is a challenging problem since the search space of DAGs is
combinatorial and scales superexponentially with the number of nodes. Existing
approaches rely on various local heuristics for enforcing the acyclicity
constraint and are not well-suited to general purpose optimization packages for
their solution. In this paper, we introduce a fundamentally different strategy:
We formulate the structure learning problem as a smooth, constrained
optimization problem over real matrices that avoids this combinatorial
constraint entirely. This is achieved by a novel characterization of acyclicity
that is not only smooth but also exact. The resulting nonconvex, constrained
program involves smooth functions whose gradients are easy to compute and only
involve elementary matrix operations. By using existing black-box optimization
routines, our method uses global search to find an optimal DAG and can be
implemented in about 50 lines of Python and outperforms existing methods
without imposing any structural constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aragam_B/0/1/0/all/0/1&quot;&gt;Bryon Aragam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01468">
<title>Improving QED-Tutrix by Automating the Generation of Proofs. (arXiv:1803.01468v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.01468</link>
<description rdf:parseType="Literal">&lt;p&gt;The idea of assisting teachers with technological tools is not new.
Mathematics in general, and geometry in particular, provide interesting
challenges when developing educative softwares, both in the education and
computer science aspects. QED-Tutrix is an intelligent tutor for geometry
offering an interface to help high school students in the resolution of
demonstration problems. It focuses on specific goals: 1) to allow the student
to freely explore the problem and its figure, 2) to accept proofs elements in
any order, 3) to handle a variety of proofs, which can be customized by the
teacher, and 4) to be able to help the student at any step of the resolution of
the problem, if the need arises. The software is also independent from the
intervention of the teacher. QED-Tutrix offers an interesting approach to
geometry education, but is currently crippled by the lengthiness of the process
of implementing new problems, a task that must still be done manually.
Therefore, one of the main focuses of the QED-Tutrix&apos; research team is to ease
the implementation of new problems, by automating the tedious step of finding
all possible proofs for a given problem. This automation must follow
fundamental constraints in order to create problems compatible with QED-Tutrix:
1) readability of the proofs, 2) accessibility at a high school level, and 3)
possibility for the teacher to modify the parameters defining the
&quot;acceptability&quot; of a proof. We present in this paper the result of our
preliminary exploration of possible avenues for this task. Automated theorem
proving in geometry is a widely studied subject, and various provers exist.
However, our constraints are quite specific and some adaptation would be
required to use an existing prover. We have therefore implemented a prototype
of automated prover to suit our needs. The future goal is to compare
performances and usability in our specific use-case between the existing
provers and our implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Font_L/0/1/0/all/0/1&quot;&gt;Ludovic Font&lt;/a&gt; (&amp;#xc9;cole Polytechnique de Montr&amp;#xe9;al), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richard_P/0/1/0/all/0/1&quot;&gt;Philippe R. Richard&lt;/a&gt; (Universit&amp;#xe9; de Montr&amp;#xe9;al), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagnon_M/0/1/0/all/0/1&quot;&gt;Michel Gagnon&lt;/a&gt; (&amp;#xc9;cole Polytechnique de Montr&amp;#xe9;al)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01489">
<title>Recurrent Predictive State Policy Networks. (arXiv:1803.01489v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01489</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Recurrent Predictive State Policy (RPSP) networks, a recurrent
architecture that brings insights from predictive state representations to
reinforcement learning in partially observable environments. Predictive state
policy networks consist of a recursive filter, which keeps track of a belief
about the state of the environment, and a reactive policy that directly maps
beliefs to actions, to maximize the cumulative reward. The recursive filter
leverages predictive state representations (PSRs) (Rosencrantz and Gordon,
2004; Sun et al., 2016) by modeling predictive state-- a prediction of the
distribution of future observations conditioned on history and future actions.
This representation gives rise to a rich class of statistically consistent
algorithms (Hefny et al., 2018) to initialize the recursive filter. Predictive
state serves as an equivalent representation of a belief state. Therefore, the
policy component of the RPSP-network can be purely reactive, simplifying
training while still allowing optimal behaviour. Moreover, we use the PSR
interpretation during training as well, by incorporating prediction error in
the loss function. The entire network (recursive filter and reactive policy) is
still differentiable and can be trained using gradient based methods. We
optimize our policy using a combination of policy gradient based on rewards
(Williams, 1992) and gradient descent based on prediction error. We show the
efficacy of RPSP-networks under partial observability on a set of robotic
control tasks from OpenAI Gym. We empirically show that RPSP-networks perform
well compared with memory-preserving networks such as GRUs, as well as finite
memory models, being the overall best performing method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hefny_A/0/1/0/all/0/1&quot;&gt;Ahmed Hefny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marinho_Z/0/1/0/all/0/1&quot;&gt;Zita Marinho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srinivasa_S/0/1/0/all/0/1&quot;&gt;Siddhartha Srinivasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gordon_G/0/1/0/all/0/1&quot;&gt;Geoffrey Gordon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01571">
<title>Explanatory relations in arbitrary logics based on satisfaction systems, cutting and retraction. (arXiv:1803.01571v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.01571</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of this paper is to introduce a new framework for defining abductive
reasoning operators based on a notion of retraction in arbitrary logics defined
as satisfaction systems. We show how this framework leads to the design of
explanatory relations satisfying properties of abductive reasoning, and discuss
its application to several logics. This extends previous work on propositional
logics where retraction was defined as a morphological erosion. Here weaker
properties are required for retraction, leading to a larger set of suitable
operators for abduction for different logics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aiguier_M/0/1/0/all/0/1&quot;&gt;Marc Aiguier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atif_J/0/1/0/all/0/1&quot;&gt;Jamal Atif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1&quot;&gt;Isabelle Bloch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pino_Perez_R/0/1/0/all/0/1&quot;&gt;Ram&amp;#xf3;n Pino-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01588">
<title>N-body Networks: a Covariant Hierarchical Neural Network Architecture for Learning Atomic Potentials. (arXiv:1803.01588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01588</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe N-body networks, a neural network architecture for learning the
behavior and properties of complex many body physical systems. Our specific
application is to learn atomic potential energy surfaces for use in molecular
dynamics simulations. Our architecture is novel in that (a) it is based on a
hierarchical decomposition of the many body system into subsytems, (b) the
activations of the network correspond to the internal state of each subsystem,
(c) the &quot;neurons&quot; in the network are constructed explicitly so as to guarantee
that each of the activations is covariant to rotations, (d) the neurons operate
entirely in Fourier space, and the nonlinearities are realized by tensor
products followed by Clebsch-Gordan decompositions. As part of the description
of our network, we give a characterization of what way the weights of the
network may interact with the activations so as to ensure that the covariance
property is maintained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondor_R/0/1/0/all/0/1&quot;&gt;Risi Kondor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01690">
<title>Memory Search and Sense from Shallow Hierarchies. (arXiv:1803.01690v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.01690</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes an automatic process for combining patterns and
features, to guide a search process and reason about it. It is based on the
functionality that a human brain might have, which is a highly distributed
network of simple neuronal components that can apply some level of matching and
cross-referencing over retrieved patterns. The process uses memory in a more
dynamic way and it can realise results using a shallow hierarchy, which is a
recognised brain-like construct. The paper gives one example of the process,
using computer chess as a case study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02869">
<title>An Analysis of the Value of Information when Exploring Stochastic, Discrete Multi-Armed Bandits. (arXiv:1710.02869v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02869</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an information-theoretic exploration strategy for
stochastic, discrete multi-armed bandits that achieves optimal regret. Our
strategy is based on the value of information criterion. This criterion
measures the trade-off between policy information and obtainable rewards. High
amounts of policy information are associated with exploration-dominant searches
of the space and yield high rewards. Low amounts of policy information favor
the exploitation of existing knowledge. Information, in this criterion, is
quantified by a parameter that can be varied during search. We demonstrate that
a simulated-annealing-like update of this parameter, with a sufficiently fast
cooling schedule, leads to an optimal regret that is logarithmic with respect
to the number of episodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1&quot;&gt;Isaac J. Sledge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06422">
<title>Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation. (arXiv:1710.06422v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06422</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunfei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinterstoisser_S/0/1/0/all/0/1&quot;&gt;Stefan Hinterstoisser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalakrishnan_M/0/1/0/all/0/1&quot;&gt;Mrinal Kalakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02741">
<title>Recurrent Autoregressive Networks for Online Multi-Object Tracking. (arXiv:1711.02741v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02741</link>
<description rdf:parseType="Literal">&lt;p&gt;The main challenge of online multi-object tracking is to reliably associate
object trajectories with detections in each video frame based on their tracking
history. In this work, we propose the Recurrent Autoregressive Network (RAN), a
temporal generative modeling framework to characterize the appearance and
motion dynamics of multiple objects over time. The RAN couples an external
memory and an internal memory. The external memory explicitly stores previous
inputs of each trajectory in a time window, while the internal memory learns to
summarize long-term tracking history and associate detections by processing the
external memory. We conduct experiments on the MOT 2015 and 2016 datasets to
demonstrate the robustness of our tracking method in highly crowded and
occluded scenes. Our method achieves top-ranked results on the two benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06623">
<title>Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments. (arXiv:1711.06623v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06623</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a self-supervised approach to ignoring &quot;distractors&quot; in camera
images for the purposes of robustly estimating vehicle motion in cluttered
urban environments. We leverage offline multi-session mapping approaches to
automatically generate a per-pixel ephemerality mask and depth map for each
input image, which we use to train a deep convolutional network. At run-time we
use the predicted ephemerality and depth as an input to a monocular visual
odometry (VO) pipeline, using either sparse features or dense photometric
matching. Our approach yields metric-scale VO using only a single camera and
can recover the correct egomotion even when 90% of the image is obscured by
dynamic, independently moving objects. We evaluate our robust VO methods on
more than 400km of driving from the Oxford RobotCar Dataset and demonstrate
reduced odometry drift and significantly improved egomotion estimation in the
presence of large moving vehicles in urban traffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_D/0/1/0/all/0/1&quot;&gt;Dan Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddern_W/0/1/0/all/0/1&quot;&gt;Will Maddern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascoe_G/0/1/0/all/0/1&quot;&gt;Geoffrey Pascoe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09268">
<title>Generalizing Hamiltonian Monte Carlo with Neural Networks. (arXiv:1711.09268v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09268</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a general-purpose method to train Markov chain Monte Carlo
kernels, parameterized by deep neural networks, that converge and mix quickly
to their target distribution. Our method generalizes Hamiltonian Monte Carlo
and is trained to maximize expected squared jumped distance, a proxy for mixing
speed. We demonstrate large empirical gains on a collection of simple but
challenging distributions, for instance achieving a 106x improvement in
effective sample size in one case, and mixing when standard HMC makes no
measurable progress in a second. Finally, we show quantitative and qualitative
gains on a real-world task: latent-variable generative modeling. We release an
open source TensorFlow implementation of the algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Levy_D/0/1/0/all/0/1&quot;&gt;Daniel Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoffman_M/0/1/0/all/0/1&quot;&gt;Matthew D. Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01001">
<title>Characterizing and Computing Causes for Query Answers in Databases from Database Repairs and Repair Programs. (arXiv:1712.01001v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;A correspondence between database tuples as causes for query answers in
databases and tuple-based repairs of inconsistent databases with respect to
denial constraints has already been established. In this work, answer-set
programs that specify repairs of databases are used as a basis for solving
computational and reasoning problems about causes. Here, causes are also
introduced at the attribute level by appealing to a both null-based and
attribute-based repair semantics. The corresponding repair programs are
presented, and they are used as a basis for computation and reasoning about
attribute-level causes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10238">
<title>DeepSOFA: A Real-Time Continuous Acuity Score Framework using Deep Learning. (arXiv:1802.10238v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.10238</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional methods for assessing illness severity and predicting in-hospital
mortality among critically ill patients require manual, time-consuming, and
error-prone calculations that are further hindered by the use of static
variable thresholds derived from aggregate patient populations. These coarse
frameworks do not capture time-sensitive individual physiological patterns and
are not suitable for instantaneous assessment of patients&apos; acuity trajectories,
a critical task for the ICU where conditions often change rapidly. Furthermore,
they are ill-suited to capitalize on the emerging availability of streaming
electronic health record data. We propose a novel acuity score framework
(DeepSOFA) that leverages temporal patient measurements in conjunction with
deep learning models to make accurate assessments of a patient&apos;s illness
severity at any point during their ICU stay. We compare DeepSOFA with SOFA
baseline models using the same predictors and find that at any point during an
ICU admission, DeepSOFA yields more accurate predictions of in-hospital
mortality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftus_T/0/1/0/all/0/1&quot;&gt;Tyler J. Loftus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozrazgat_Baslanti_T/0/1/0/all/0/1&quot;&gt;Tezcan Ozrazgat-Baslanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebadi_A/0/1/0/all/0/1&quot;&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01043">
<title>Building a Telescope to Look Into High-Dimensional Image Spaces. (arXiv:1803.01043v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01043</link>
<description rdf:parseType="Literal">&lt;p&gt;An image pattern can be represented by a probability distribution whose
density is concentrated on different low-dimensional subspaces in the
high-dimensional image space. Such probability densities have an astronomical
number of local modes corresponding to typical pattern appearances. Related
groups of modes can join to form macroscopic image basins that represent
pattern concepts. Recent works use neural networks that capture high-order
image statistics to learn Gibbs models capable of synthesizing realistic images
of many patterns. However, characterizing a learned probability density to
uncover the Hopfield memories of the model, encoded by the structure of the
local modes, remains an open challenge. In this work, we present novel
computational experiments that map and visualize the local mode structure of
Gibbs densities. Efficient mapping requires identifying the global basins
without enumerating the countless modes. Inspired by Grenander&apos;s jump-diffusion
method, we propose a new MCMC tool called Attraction-Diffusion (AD) that can
capture the macroscopic structure of highly non-convex densities by measuring
metastability of local modes. AD involves altering the target density with a
magnetization potential penalizing distance from a known mode and running an
MCMC sample of the altered density to measure the stability of the initial
chain state. Using a low-dimensional generator network to facilitate
exploration, we map image spaces with up to 12,288 dimensions (64 $\times$ 64
pixels in RGB). Our work shows: (1) AD can efficiently map highly non-convex
probability densities, (2) metastable regions of pattern probability densities
contain coherent groups of images, and (3) the perceptibility of differences
between training images influences the metastability of image basins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hill_M/0/1/0/all/0/1&quot;&gt;Mitch Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nijkamp_E/0/1/0/all/0/1&quot;&gt;Erik Nijkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01113">
<title>Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD. (arXiv:1803.01113v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01113</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Stochastic Gradient Descent (SGD) when run in a synchronous
manner, suffers from delays in waiting for the slowest learners (stragglers).
Asynchronous methods can alleviate stragglers, but cause gradient staleness
that can adversely affect convergence. In this work we present the first
theoretical characterization of the speed-up offered by asynchronous methods by
analyzing the trade-off between the error in the trained model and the actual
training runtime (wallclock time). The novelty in our work is that our runtime
analysis considers random straggler delays, which helps us design and compare
distributed SGD algorithms that strike a balance between stragglers and
staleness. We also present a new convergence analysis of asynchronous SGD
variants without bounded or exponential delay assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sanghamitra Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumyadip Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dube_P/0/1/0/all/0/1&quot;&gt;Parijat Dube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagpurkar_P/0/1/0/all/0/1&quot;&gt;Priya Nagpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01203">
<title>Multiresolution Tensor Decomposition for Multiple Spatial Passing Networks. (arXiv:1803.01203v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1803.01203</link>
<description rdf:parseType="Literal">&lt;p&gt;This article is motivated by soccer positional passing networks collected
across multiple games. We refer to these data as replicated spatial passing
networks---to accurately model such data it is necessary to take into account
the spatial positions of the passer and receiver for each passing event. This
spatial registration and replicates that occur across games represent key
differences with usual social network data. As a key step before investigating
how the passing dynamics influence team performance, we focus on developing
methods for summarizing different team&apos;s passing strategies. Our proposed
approach relies on a novel multiresolution data representation framework and
Poisson nonnegative block term decomposition model, which automatically
produces coarse-to-fine low-rank network motifs. The proposed methods are
applied to detailed passing record data collected from the 2014 FIFA World Cup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shaobo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dunson_D/0/1/0/all/0/1&quot;&gt;David B. Dunson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01229">
<title>GAN-based Synthetic Medical Image Augmentation for increased CNN Performance in Liver Lesion Classification. (arXiv:1803.01229v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.01229</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning methods, and in particular convolutional neural networks
(CNNs), have led to an enormous breakthrough in a wide range of computer vision
tasks, primarily by using large-scale annotated datasets. However, obtaining
such datasets in the medical domain remains a challenge. In this paper, we
present methods for generating synthetic medical images using recently
presented deep learning Generative Adversarial Networks (GANs). Furthermore, we
show that generated medical images can be used for synthetic data augmentation,
and improve the performance of CNN for medical image classification. Our novel
method is demonstrated on a limited dataset of computed tomography (CT) images
of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first
exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then
we present a novel scheme for liver lesion classification using CNN. Finally,
we train the CNN using classic data augmentation and our synthetic data
augmentation and compare performance. In addition, we explore the quality of
our synthesized examples using visualization and expert assessment. The
classification performance using only classic data augmentation yielded 78.6%
sensitivity and 88.4% specificity. By adding the synthetic data augmentation
the results increased to 85.7% sensitivity and 92.4% specificity. We believe
that this approach to synthetic data augmentation can generalize to other
medical classification applications and thus support radiologists&apos; efforts to
improve diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frid_Adar_M/0/1/0/all/0/1&quot;&gt;Maayan Frid-Adar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diamant_I/0/1/0/all/0/1&quot;&gt;Idit Diamant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klang_E/0/1/0/all/0/1&quot;&gt;Eyal Klang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amitai_M/0/1/0/all/0/1&quot;&gt;Michal Amitai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1&quot;&gt;Jacob Goldberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_H/0/1/0/all/0/1&quot;&gt;Hayit Greenspan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01233">
<title>Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow. (arXiv:1803.01233v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01233</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit the inductive matrix completion problem that aims to recover a
rank-$r$ matrix with ambient dimension $d$ given $n$ features as the side prior
information. The goal is to make use of the known $n$ features to reduce sample
and computational complexities. We present and analyze a new gradient-based
non-convex optimization algorithm that converges to the true underlying matrix
at a linear rate with sample complexity only linearly depending on $n$ and
logarithmically depending on $d$. To the best of our knowledge, all previous
algorithms either have a quadratic dependency on the number of features in
sample complexity or a sub-linear computational convergence rate. In addition,
we provide experiments on both synthetic and real world data to demonstrate the
effectiveness of our proposed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Quanquan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01257">
<title>Nonnegative Matrix Factorization for Signal and Data Analytics: Identifiability, Algorithms, and Applications. (arXiv:1803.01257v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1803.01257</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonnegative matrix factorization (NMF) has become a workhorse for signal and
data analytics, triggered by its model parsimony and interpretability. Perhaps
a bit surprisingly, the understanding to its model identifiability---the major
reason behind the interpretability in many applications such as topic mining
and hyperspectral imaging---had been rather limited until recent years.
Beginning from the 2010s, the identifiability research of NMF has progressed
considerably: Many interesting and important results have been discovered by
the signal processing (SP) and machine learning (ML) communities. NMF
identifiability has a great impact on many aspects in practice, such as
ill-posed formulation avoidance and performance-guaranteed algorithm design. On
the other hand, there is no tutorial paper that introduces NMF from an
identifiability viewpoint. In this paper, we aim at filling this gap by
offering a comprehensive and deep tutorial on model identifiability of NMF as
well as the connections to algorithms and applications. This tutorial will help
researchers and graduate students grasp the essence and insights of NMF,
thereby avoiding typical `pitfalls&apos; that are often times due to unidentifiable
NMF formulations. This paper will also help practitioners pick/design suitable
factorization tools for their own problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sidiropoulos_N/0/1/0/all/0/1&quot;&gt;Nicholas D. Sidiropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wing-Kin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01302">
<title>Distributed Nonparametric Regression under Communication Constraints. (arXiv:1803.01302v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01302</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of nonparametric estimation of a smooth
function with data distributed across multiple machines. We assume an
independent sample from a white noise model is collected at each machine, and
an estimator of the underlying true function needs to be constructed at a
central machine. We place limits on the number of bits that each machine can
use to transmit information to the central machine. Our results give both
asymptotic lower bounds and matching upper bounds on the statistical risk under
various settings. We identify three regimes, depending on the relationship
among the number of machines, the size of the data available at each machine,
and the communication budget. When the communication budget is small, the
statistical risk depends solely on this communication bottleneck, regardless of
the sample size. In the regime where the communication budget is large, the
classic minimax risk in the non-distributed estimation setting is recovered. In
an intermediate regime, the statistical risk depends on both the sample size
and the communication budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lafferty_J/0/1/0/all/0/1&quot;&gt;John Lafferty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01328">
<title>WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling. (arXiv:1803.01328v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01328</link>
<description rdf:parseType="Literal">&lt;p&gt;To train an inference network jointly with a deep generative topic model,
making it both scalable to big corpora and fast in out-of-sample prediction, we
develop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet
allocation, which infers posterior samples via a hybrid of stochastic-gradient
MCMC and autoencoding variational Bayes. The generative network of WHAI has a
hierarchy of gamma distributions, while the inference network of WHAI is a
Weibull upward-downward variational autoencoder, which integrates a
deterministic-upward deep neural network, and a stochastic-downward deep
generative model based on a hierarchy of Weibull distributions. The Weibull
distribution can be used to well approximate a gamma distribution with an
analytic Kullback-Leibler divergence, and has a simple reparameterization via
the uniform noise, which help efficiently compute the gradients of the evidence
lower bound with respect to the parameters of the inference network. The
effectiveness and efficiency of WHAI are illustrated with experiments on big
corpora.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dandan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01347">
<title>Greedy stochastic algorithms for entropy-regularized optimal transport problems. (arXiv:1803.01347v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01347</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal transport (OT) distances are finding evermore applications in machine
learning and computer vision, but their wide spread use in larger-scale
problems is impeded by their high computational cost. In this work we develop a
family of fast and practical stochastic algorithms for solving the optimal
transport problem with an entropic penalization. This work extends the recently
developed Greenkhorn algorithm, in the sense that, the Greenkhorn algorithm is
a limiting case of this family. We also provide a simple and general
convergence theorem for all algorithms in the class, with rates that match the
best known rates of Greenkorn and the Sinkhorn algorithm, and conclude with
numerical experiments that show under what regime of penalization the new
stochastic methods are faster than the aforementioned methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abid_B/0/1/0/all/0/1&quot;&gt;Brahim Khalil Abid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gower_R/0/1/0/all/0/1&quot;&gt;Robert M. Gower&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01420">
<title>Detecting Correlations with Little Memory and Communication. (arXiv:1803.01420v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01420</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of identifying correlations in multivariate data, under
information constraints: Either on the amount of memory that can be used by the
algorithm, or the amount of communication when the data is distributed across
several machines. We prove a tight trade-off between the memory/communication
complexity and the sample complexity, implying (for example) that to detect
pairwise correlations with optimal sample complexity, the number of required
memory/communication bits is at least quadratic in the dimension. Our results
substantially improve those of Shamir [2014], which studied a similar question
in a much more restricted setting. To the best of our knowledge, these are the
first provable sample/memory/communication trade-offs for a practical
estimation problem, using standard distributions, and in the natural regime
where the memory/communication budget is larger than the size of a single data
point. To derive our theorems, we prove a new information-theoretic result,
which may be relevant for studying other information-constrained learning
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1&quot;&gt;Yuval Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1&quot;&gt;Ohad Shamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01440">
<title>Hierarchical Modeling and Shrinkage for User Session Length Prediction in Media Streaming. (arXiv:1803.01440v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01440</link>
<description rdf:parseType="Literal">&lt;p&gt;An important metric of users&apos; satisfaction and engagement within on-line
streaming services is the user session length, i.e. the amount of time they
spend on a service continuously without interruption. Being able to predict
this value directly benefits the recommendation and ad pacing contexts in music
and video streaming services. Recent research has shown that predicting the
exact amount of time spent is highly nontrivial due to many external factors
for which a user can end a session, and the lack of predictive covariates. Most
of the other related literature on duration based user engagement has focused
on dwell time for websites, for search and display ads, mainly for post-click
satisfaction prediction or ad ranking.
&lt;/p&gt;
&lt;p&gt;In this work we present a novel framework inspired by hierarchical Bayesian
modeling to predict, at the moment of login, the amount of time a user will
spend in the streaming service. The time spent by a user on a platform depends
upon user-specific latent variables which are learned via hierarchical
shrinkage. Our framework enjoys theoretical guarantees, naturally incorporates
flexible parametric/nonparametric models on the covariates and is found to
outperform state-of- the-art estimators in terms of efficiency and predictive
performance on real world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dedieu_A/0/1/0/all/0/1&quot;&gt;Antoine Dedieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1&quot;&gt;Rahul Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vahabi_H/0/1/0/all/0/1&quot;&gt;Hossein Vahabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01442">
<title>Stochastic Activation Pruning for Robust Adversarial Defense. (arXiv:1803.01442v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01442</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are known to be vulnerable to adversarial examples. Carefully
chosen perturbations to real images, while imperceptible to humans, induce
misclassification and threaten the reliability of deep learning systems in the
wild. To guard against adversarial examples, we take inspiration from game
theory and cast the problem as a minimax zero-sum game between the adversary
and the model. In general, for such games, the optimal strategy for both
players requires a stochastic policy, also known as a mixed strategy. In this
light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for
adversarial defense. SAP prunes a random subset of activations (preferentially
pruning those with smaller magnitude) and scales up the survivors to
compensate. We can apply SAP to pretrained networks, including adversarially
trained models, without fine-tuning, providing robustness against adversarial
examples. Experiments demonstrate that SAP confers robustness against attacks,
increasing accuracy and preserving calibration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhillon_G/0/1/0/all/0/1&quot;&gt;Guneet S. Dhillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1&quot;&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_J/0/1/0/all/0/1&quot;&gt;Jeremy Bernstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1&quot;&gt;Jean Kossaifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_A/0/1/0/all/0/1&quot;&gt;Aran Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01454">
<title>Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms. (arXiv:1803.01454v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1803.01454</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the canonical $L_0$-regularized least squares problem (aka best
subsets) which is generally perceived as a `gold-standard&apos; for many sparse
learning regimes. In spite of worst-case computational intractability results,
recent work has shown that advances in mixed integer optimization can be used
to obtain near-optimal solutions to this problem for instances where the number
of features $p \approx 10^3$. While these methods lead to estimators with
excellent statistical properties, often there is a price to pay in terms of a
steep increase in computation times, especially when compared to highly
efficient popular algorithms for sparse learning (e.g., based on
$L_1$-regularization) that scale to much larger problem sizes. Bridging this
gap is a main goal of this paper. We study the computational aspects of a
family of $L_0$-regularized least squares problems with additional convex
penalties. We propose a hierarchy of necessary optimality conditions for these
problems. We develop new algorithms, based on coordinate descent and local
combinatorial optimization schemes, and study their convergence properties. We
demonstrate that the choice of an algorithm determines the quality of solutions
obtained; and local combinatorial optimization-based algorithms generally
result in solutions of superior quality. We show empirically that our proposed
framework is relatively fast for problem instances with $p\approx 10^6$ and
works well, in terms of both optimization and statistical properties (e.g.,
prediction, estimation, and variable selection), compared to simpler heuristic
algorithms. A version of our algorithm reaches up to a three-fold speedup (with
$p$ up to $10^6$) when compared to state-of-the-art schemes for sparse learning
such as glmnet and ncvreg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hazimeh_H/0/1/0/all/0/1&quot;&gt;Hussein Hazimeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1&quot;&gt;Rahul Mazumder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01498">
<title>Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates. (arXiv:1803.01498v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01498</link>
<description rdf:parseType="Literal">&lt;p&gt;In large-scale distributed learning, security issues have become increasingly
important. Particularly in a decentralized environment, some computing units
may behave abnormally, or even exhibit Byzantine failures---arbitrary and
potentially adversarial behavior. In this paper, we develop distributed
learning algorithms that are provably robust against such failures, with a
focus on achieving optimal statistical performance. A main result of this work
is a sharp analysis of two robust distributed gradient descent algorithms based
on median and trimmed mean operations, respectively. We prove statistical error
rates for three kinds of population loss functions: strongly convex,
non-strongly convex, and smooth non-convex. In particular, these algorithms are
shown to achieve order-optimal statistical error rates for strongly convex
losses. To achieve better communication efficiency, we further propose a
median-based distributed algorithm that is provably robust, and uses only one
communication round. For strongly convex quadratic loss, we show that this
algorithm achieves the same optimal error rate as the robust distributed
gradient descent algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1&quot;&gt;Kannan Ramchandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter Bartlett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01500">
<title>Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks. (arXiv:1803.01500v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01500</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approach to address two issues that commonly occur during
training of unsupervised GANs. First, since GANs use only a continuous latent
distribution to embed multiple classes or clusters of data, they often do not
correctly handle the structural discontinuity between disparate classes in a
latent space. Second, discriminators of GANs easily forget about past generated
samples by generators, incurring instability during adversarial training. We
argue that these two infamous problems of unsupervised GAN training can be
largely alleviated by a learnable memory network to which both generators and
discriminators can access. Generators can effectively learn representation of
training samples to understand underlying cluster distributions of data, which
ease the structure discontinuity problem. At the same time, discriminators can
better memorize clusters of previously generated samples, which mitigate the
forgetting problem. We propose a novel end-to-end GAN model named memoryGAN,
which involves a memory network that is unsupervisedly trainable and integrable
to many existing GAN models. With evaluations on multiple datasets such as
Fashion-MNIST, CelebA, CIFAR10, and Chairs, we show that our model is
probabilistically interpretable, and generates realistic image samples of high
visual fidelity. The memoryGAN also achieves the state-of-the-art inception
scores over unsupervised GAN models on the CIFAR10 dataset, without any
optimization tricks and weaker divergences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngjin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minjung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gunhee Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01562">
<title>Local Distance Metric Learning for Nearest Neighbor Algorithm. (arXiv:1803.01562v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.01562</link>
<description rdf:parseType="Literal">&lt;p&gt;Distance metric learning is a successful way to enhance the performance of
the nearest neighbor classifier. In most cases, however, the distribution of
data does not obey a regular form and may change in different parts of the
feature space. Regarding that, this paper proposes a novel local distance
metric learning method, namely Local Mahalanobis Distance Learning (LMDL), in
order to enhance the performance of the nearest neighbor classifier. LMDL
considers the neighborhood influence and learns multiple distance metrics for a
reduced set of input samples. The reduced set is called as prototypes which try
to preserve local discriminative information as much as possible. The proposed
LMDL can be kernelized very easily, which is significantly desirable in the
case of highly nonlinear data. The quality as well as the efficiency of the
proposed method assesses through a set of different experiments on various
datasets and the obtained results show that LDML as well as the kernelized
version is superior to the other related state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabzadeh_H/0/1/0/all/0/1&quot;&gt;Hossein Rajabzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahromi_M/0/1/0/all/0/1&quot;&gt;Mansoor Zolghadri Jahromi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zare_M/0/1/0/all/0/1&quot;&gt;Mohammad Sadegh Zare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakhrahmad_M/0/1/0/all/0/1&quot;&gt;Mostafa Fakhrahmad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01570">
<title>Adversarial Extreme Multi-label Classification. (arXiv:1803.01570v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01570</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal in extreme multi-label classification is to learn a classifier which
can assign a small subset of relevant labels to an instance from an extremely
large set of target labels. Datasets in extreme classification exhibit a long
tail of labels which have small number of positive training instances. In this
work, we pose the learning task in extreme classification with large number of
tail-labels as learning in the presence of adversarial perturbations. This view
motivates a robust optimization framework and equivalence to a corresponding
regularized objective.
&lt;/p&gt;
&lt;p&gt;Under the proposed robustness framework, we demonstrate efficacy of Hamming
loss for tail-label detection in extreme classification. The equivalent
regularized objective, in combination with proximal gradient based
optimization, performs better than state-of-the-art methods on propensity
scored versions of precision@k and nDCG@k(upto 20% relative improvement over
PFastreXML - a leading tree-based approach and 60% relative improvement over
SLEEC - a leading label-embedding approach). Furthermore, we also highlight the
sub-optimality of a sparse solver in a widely used package for large-scale
linear classification, which is interesting in its own right. We also
investigate the spectral properties of label graphs for providing novel
insights towards understanding the conditions governing the performance of
Hamming loss based one-vs-rest scheme vis-\`a-vis label embedding methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Babbar_R/0/1/0/all/0/1&quot;&gt;Rohit Babbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01575">
<title>A Comparative Study of Pairwise Learning Methods based on Kernel Ridge Regression. (arXiv:1803.01575v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01575</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning problems can be formulated as predicting labels for a
pair of objects. Problems of that kind are often referred to as pairwise
learning, dyadic prediction or network inference problems. During the last
decade kernel methods have played a dominant role in pairwise learning. They
still obtain a state-of-the-art predictive performance, but a theoretical
analysis of their behavior has been underexplored in the machine learning
literature.
&lt;/p&gt;
&lt;p&gt;In this work we review and unify existing kernel-based algorithms that are
commonly used in different pairwise learning settings, ranging from matrix
filtering to zero-shot learning. To this end, we focus on closed-form efficient
instantiations of Kronecker kernel ridge regression. We show that independent
task kernel ridge regression, two-step kernel ridge regression and a linear
matrix filter arise naturally as a special case of Kronecker kernel ridge
regression, implying that all these methods implicitly minimize a squared loss.
In addition, we analyze universality, consistency and spectral filtering
properties. Our theoretical results provide valuable insights in assessing the
advantages and limitations of existing pairwise learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stock_M/0/1/0/all/0/1&quot;&gt;Michiel Stock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pahikkala_T/0/1/0/all/0/1&quot;&gt;Tapio Pahikkala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Airola_A/0/1/0/all/0/1&quot;&gt;Antti Airola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baets_B/0/1/0/all/0/1&quot;&gt;Bernard De Baets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Waegeman_W/0/1/0/all/0/1&quot;&gt;Willem Waegeman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01576">
<title>Asymptotic Equivalence of Fixed-size and Varying-size Determinantal Point Processes. (arXiv:1803.01576v1 [math.ST])</title>
<link>http://arxiv.org/abs/1803.01576</link>
<description rdf:parseType="Literal">&lt;p&gt;Determinantal Point Processes (DPPs) are popular models for point processes
with repulsion. They appear in numerous contexts, from physics to graph theory,
and display appealing theoretical properties. On the more practical side of
things, since DPPs tend to select sets of points that are some distance apart
(repulsion), they have been advocated as a way of producing random subsets with
high diversity. DPPs come in two variants: fixed-size and varying-size. A
sample from a varying-size DPP is a subset of random cardinality, while in
fixed-size &quot;$k$-DPPs&quot; the cardinality is fixed. The latter makes more sense in
many applications, but unfortunately their computational properties are less
attractive, since, among other things, inclusion probabilities are harder to
compute. In this work we show that as the size of the ground set grows,
$k$-DPPs and DPPs become equivalent, meaning that their inclusion probabilities
converge. As a by-product, we obtain saddlepoint formulas for inclusion
probabilities in $k$-DPPs. These turn out to be extremely accurate, and suffer
less from numerical difficulties than exact methods do. Our results also
suggest that $k$-DPPs and DPPs also have equivalent maximum likelihood
estimators. Finally, we obtain results on asymptotic approximations of
elementary symmetric polynomials which may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Barthelme_S/0/1/0/all/0/1&quot;&gt;Simon Barthelm&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Amblard_P/0/1/0/all/0/1&quot;&gt;Pierre-Olivier Amblard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tremblay_N/0/1/0/all/0/1&quot;&gt;Nicolas Tremblay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01616">
<title>Tensorial and bipartite block models for link prediction in layered networks and temporal networks. (arXiv:1803.01616v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/1803.01616</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world complex systems are well represented as multilayer networks;
predicting interactions in those systems is one of the most pressing problems
in predictive network science. To address this challenge, we introduce two
stochastic block models for multilayer and temporal networks; one of them uses
nodes as its fundamental unit, whereas the other focuses on links. We also
develop scalable algorithms for inferring the parameters of these models.
Because our models describe all layers simultaneously, our approach takes full
advantage of the information contained in the whole network when making
predictions about any particular layer. We illustrate the potential of our
approach by analyzing two empirical datasets---a temporal network of email
communications, and a network of drug interactions for treating different
cancer types. We find that modeling all layers simultaneously does result, in
general, in more accurate link prediction. However, the most predictive model
depends on the dataset under consideration; whereas the node-based model is
more appropriate for predicting drug interactions, the link-based model is more
appropriate for predicting email communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tarres_Deulofeu_M/0/1/0/all/0/1&quot;&gt;Marc Tarres-Deulofeu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Godoy_Lorite_A/0/1/0/all/0/1&quot;&gt;Antonia Godoy-Lorite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Guimera_R/0/1/0/all/0/1&quot;&gt;Roger Guimera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sales_Pardo_M/0/1/0/all/0/1&quot;&gt;Marta Sales-Pardo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01626">
<title>Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs. (arXiv:1803.01626v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01626</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of reinforcement learning in an unknown and discrete Markov
Decision Process (MDP) under the average-reward criterion is considered, when
the learner interacts with the system in a single stream of observations,
starting from an initial state without any reset. We revisit the minimax lower
bound for that problem by making appear the local variance of the bias function
in place of the diameter of the MDP. Furthermore, we provide a novel analysis
of the KL-UCRL algorithm establishing a high-probability regret bound scaling
as $\widetilde {\mathcal O}\Bigl({\textstyle \sqrt{S\sum_{s,a}{\bf
V}^\star_{s,a}T}}\Big)$ for this algorithm for ergodic MDPs, where $S$ denotes
the number of states and where ${\bf V}^\star_{s,a}$ is the variance of the
bias function with respect to the next-state distribution following action $a$
in state $s$. The resulting bound improves upon the best previously known
regret bound $\widetilde {\mathcal O}(DS\sqrt{AT})$ for that algorithm, where
$A$ and $D$ respectively denote the maximum number of actions (per state) and
the diameter of MDP. We finally compare the leading terms of the two bounds in
some benchmark MDPs indicating that the derived bound can provide an order of
magnitude improvement in some cases. Our analysis leverages novel variations of
the transportation lemma combined with Kullback-Leibler concentration
inequalities, that we believe to be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Talebi_M/0/1/0/all/0/1&quot;&gt;Mohammad Sadegh Talebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maillard_O/0/1/0/all/0/1&quot;&gt;Odalric-Ambrym Maillard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01785">
<title>Differentiable Submodular Maximization. (arXiv:1803.01785v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01785</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider learning of submodular functions from data. These functions are
important in machine learning and have a wide range of applications, e.g. data
summarization, feature selection and active learning. Despite their
combinatorial nature, submodular functions can be maximized approximately with
strong theoretical guarantees in polynomial time. Typically, learning the
submodular function and optimization of that function are treated separately,
i.e. the function is first learned using a proxy objective and subsequently
maximized. In contrast, we show how to perform learning and optimization
jointly. By interpreting the output of greedy maximization algorithms as
distributions over sequences of items and smoothening these distributions, we
obtain a differentiable objective. In this way, we can differentiate through
the maximization algorithms and optimize the model to work well with the
optimization algorithm. We theoretically characterize the error made by our
approach, yielding insights into the trade-off of smoothness and accuracy. We
demonstrate the effectiveness of our approach for jointly learning and
optimizing on synthetic maxcut data, and on a real world product recommendation
application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tschiatschek_S/0/1/0/all/0/1&quot;&gt;Sebastian Tschiatschek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sahin_A/0/1/0/all/0/1&quot;&gt;Aytunc Sahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01802">
<title>Event-triggered Learning for Resource-efficient Networked Control. (arXiv:1803.01802v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1803.01802</link>
<description rdf:parseType="Literal">&lt;p&gt;Common event-triggered state estimation (ETSE) algorithms save communication
in networked control systems by predicting agents&apos; behavior, and transmitting
updates only when the predictions deviate significantly. The effectiveness in
reducing communication thus heavily depends on the quality of the dynamics
models used to predict the agents&apos; states or measurements. Event-triggered
learning is proposed herein as a novel concept to further reduce communication:
whenever poor communication performance is detected, an identification
experiment is triggered and an improved prediction model learned from data.
Effective learning triggers are obtained by comparing the actual communication
rate with the one that is expected based on the current model. By analyzing
statistical properties of the inter-communication times and leveraging powerful
convergence results, the proposed trigger is proven to limit learning
experiments to the necessary instants. Numerical and physical experiments
demonstrate that event-triggered learning improves robustness toward changing
environments and yields lower communication rates than common ETSE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solowjow_F/0/1/0/all/0/1&quot;&gt;Friedrich Solowjow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumann_D/0/1/0/all/0/1&quot;&gt;Dominik Baumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcke_J/0/1/0/all/0/1&quot;&gt;Jochen Garcke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01814">
<title>Norm matters: efficient and accurate normalization schemes in deep networks. (arXiv:1803.01814v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01814</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years batch-normalization has been commonly used in deep
networks, allowing faster training and high performance for a wide variety of
applications. However, the reasons behind its merits remained unanswered, with
several shortcomings that hindered its use for certain tasks. In this work we
present a novel view on the purpose and function of normalization methods and
weight-decay, as tools to decouple weights&apos; norm from the underlying optimized
objective. We also improve the use of weight-normalization and show the
connection between practices such as normalization, weight decay and
learning-rate adjustments. Finally, we suggest several alternatives to the
widely used $L^2$ batch-norm, using normalization in $L^1$ and $L^\infty$
spaces that can substantially improve numerical stability in low-precision
implementations as well as provide computational and memory benefits. We
demonstrate that such methods enable the first batch-norm alternative to work
for half-precision implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoffer_E/0/1/0/all/0/1&quot;&gt;Elad Hoffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Banner_R/0/1/0/all/0/1&quot;&gt;Ron Banner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Golan_I/0/1/0/all/0/1&quot;&gt;Itay Golan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1504.06964">
<title>Modeling Recovery Curves With Application to Prostatectomy. (arXiv:1504.06964v6 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1504.06964</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Bayesian model that predicts recovery curves based on
information available before the disruptive event. A recovery curve of interest
is the quantified sexual function of prostate cancer patients after
prostatectomy surgery. We illustrate the utility of our model as a
pre-treatment medical decision aid, producing personalized predictions that are
both interpretable and accurate. We uncover covariate relationships that agree
with and supplement that in existing medical literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fulton Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCormick_T/0/1/0/all/0/1&quot;&gt;Tyler H. McCormick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gore_J/0/1/0/all/0/1&quot;&gt;John Gore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.00967">
<title>Stable Robbins-Monro approximations through stochastic proximal updates. (arXiv:1510.00967v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1510.00967</link>
<description rdf:parseType="Literal">&lt;p&gt;The need for parameter estimation with massive data has reinvigorated
interest in iterative estimation procedures. Stochastic approximations, such as
stochastic gradient descent, are at the forefront of this recent development
because they yield simple, generic, and extremely fast iterative estimation
procedures. Such stochastic approximations, however, are often numerically
unstable. As a consequence, current practice has turned to proximal operators,
which can induce stable parameter updates within iterations. While the majority
of classical iterative estimation procedures are subsumed by the framework of
Robbins and Monro (1951), there is no such generalization for stochastic
approximations with proximal updates. In this paper, we conceptualize a general
stochastic approximation method with proximal updates. This method can be
applied even in situations where the analytical form of the objective is not
known, and so it generalizes many stochastic gradient procedures with proximal
operators currently in use. Our theoretical analysis indicates that the
proposed method has important stability benefits over the classical stochastic
approximation method. Exact instantiations of the proposed method are
challenging, but we show that approximate instantiations lead to procedures
that are easy to implement, and still dominate classical procedures by
achieving numerical stability without tradeoffs. This last advantage is akin to
that seen in deterministic proximal optimization, where the framework is
typically impossible to instantiate exactly, but where approximate
instantiations lead to new optimization procedures that dominate classical
ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Toulis_P/0/1/0/all/0/1&quot;&gt;Panos Toulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Horel_T/0/1/0/all/0/1&quot;&gt;Thibaut Horel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Airoldi_E/0/1/0/all/0/1&quot;&gt;Edoardo M. Airoldi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1512.02063">
<title>An Explicit Rate Bound for the Over-Relaxed ADMM. (arXiv:1512.02063v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1512.02063</link>
<description rdf:parseType="Literal">&lt;p&gt;The framework of Integral Quadratic Constraints of Lessard et al. (2014)
reduces the computation of upper bounds on the convergence rate of several
optimization algorithms to semi-definite programming (SDP). Followup work by
Nishihara et al. (2015) applies this technique to the entire family of
over-relaxed Alternating Direction Method of Multipliers (ADMM). Unfortunately,
they only provide an explicit error bound for sufficiently large values of some
of the parameters of the problem, leaving the computation for the general case
as a numerical optimization problem. In this paper we provide an exact
analytical solution to this SDP and obtain a general and explicit upper bound
on the convergence rate of the entire family of over-relaxed ADMM. Furthermore,
we demonstrate that it is not possible to extract from this SDP a general bound
better than ours. We end with a few numerical illustrations of our result and a
comparison between the convergence rate we obtain for the ADMM with known
convergence rates for the Gradient Descent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franca_G/0/1/0/all/0/1&quot;&gt;Guilherme Fran&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bento_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Bento&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.04460">
<title>On the Existence of a Sample Mean in Dynamic Time Warping Spaces. (arXiv:1610.04460v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1610.04460</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of sample mean in dynamic time warping (DTW) spaces has been
successfully applied to improve pattern recognition systems and generalize
centroid-based clustering algorithms. Its existence has neither been proved nor
challenged. This article presents sufficient conditions for existence of a
sample mean in DTW spaces. The proposed result justifies prior work on
approximate mean algorithms, sets the stage for constructing exact mean
algorithms, and is a first step towards a statistical theory of DTW spaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Brijnesh J. Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schultz_D/0/1/0/all/0/1&quot;&gt;David Schultz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.00299">
<title>Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution. (arXiv:1701.00299v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1701.00299</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward
deep neural network that allows selective execution. Given an input, only a
subset of D2NN neurons are executed, and the particular subset is determined by
the D2NN itself. By pruning unnecessary computation depending on input, D2NNs
provide a way to improve computational efficiency. To achieve dynamic selective
execution, a D2NN augments a feed-forward deep neural network (directed acyclic
graph of differentiable modules) with controller modules. Each controller
module is a sub-network whose output is a decision that controls whether other
modules can execute. A D2NN is trained end to end. Both regular and controller
modules in a D2NN are learnable and are jointly trained to optimize both
accuracy and efficiency. Such training is achieved by integrating
backpropagation with reinforcement learning. With extensive experiments of
various D2NN architectures on image classification tasks, we demonstrate that
D2NNs are general and flexible, and can effectively optimize
accuracy-efficiency trade-offs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lanlan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jia Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.03863">
<title>Tuning Over-Relaxed ADMM. (arXiv:1703.03863v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.03863</link>
<description rdf:parseType="Literal">&lt;p&gt;The framework of Integral Quadratic Constraints (IQC) reduces the computation
of upper bounds on the convergence rate of several optimization algorithms to a
semi-definite program (SDP). In the case of over-relaxed Alternating Direction
Method of Multipliers (ADMM), an explicit and closed form solution to this SDP
was derived in our recent work [1]. The purpose of this paper is twofold.
First, we summarize these results. Second, we explore one of its consequences
which allows us to obtain general and simple formulas for optimal parameter
selection. These results are valid for arbitrary strongly convex objective
functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franca_G/0/1/0/all/0/1&quot;&gt;Guilherme Fran&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bento_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Bento&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.09165">
<title>PWLS-ULTRA: An Efficient Clustering and Learning-Based Approach for Low-Dose 3D CT Image Reconstruction. (arXiv:1703.09165v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.09165</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of computed tomography (CT) image reconstruction methods that
significantly reduce patient radiation exposure while maintaining high image
quality is an important area of research in low-dose CT (LDCT) imaging. We
propose a new penalized weighted least squares (PWLS) reconstruction method
that exploits regularization based on an efficient Union of Learned TRAnsforms
(PWLS-ULTRA). The union of square transforms is pre-learned from numerous image
patches extracted from a dataset of CT images or volumes. The proposed
PWLS-based cost function is optimized by alternating between a CT image
reconstruction step, and a sparse coding and clustering step. The CT image
reconstruction step is accelerated by a relaxed linearized augmented Lagrangian
method with ordered-subsets that reduces the number of forward and back
projections. Simulations with 2D and 3D axial CT scans of the XCAT phantom and
3D helical chest and abdomen scans show that for both normal-dose and low-dose
levels, the proposed method significantly improves the quality of reconstructed
images compared to PWLS reconstruction with a nonadaptive edge-preserving
regularizer (PWLS-EP). PWLS with regularization based on a union of learned
transforms leads to better image reconstructions than using a single learned
square transform. We also incorporate patch-based weights in PWLS-ULTRA that
enhance image quality and help improve image resolution uniformity. The
proposed approach achieves comparable or better image quality compared to
learned overcomplete synthesis dictionaries, but importantly, is much faster
(computationally more efficient).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xuehang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ravishankar_S/0/1/0/all/0/1&quot;&gt;Saiprasad Ravishankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Long_Y/0/1/0/all/0/1&quot;&gt;Yong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fessler_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Fessler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07562">
<title>On the diffusion approximation of nonconvex stochastic gradient descent. (arXiv:1705.07562v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07562</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the Stochastic Gradient Descent (SGD) method in nonconvex
optimization problems from the point of view of approximating diffusion
processes. We prove rigorously that the diffusion process can approximate the
SGD algorithm weakly using the weak form of master equation for probability
evolution. In the small step size regime and the presence of omnidirectional
noise, our weak approximating diffusion process suggests the following dynamics
for the SGD iteration starting from a local minimizer (resp.~saddle point): it
escapes in a number of iterations exponentially (resp.~almost linearly)
dependent on the inverse stepsize. The results are obtained using the theory
for random perturbations of dynamical systems (theory of large deviations for
local minimizers and theory of exiting for unstable stationary points). In
addition, we discuss the effects of batch size for the deep neural networks,
and we find that small batch size is helpful for SGD algorithms to escape
unstable stationary points and sharp minimizers. Our theory indicates that one
should increase the batch size at later stage for the SGD to be trapped in flat
minimizers for better generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenqing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chris Junchi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian-Guo Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02631">
<title>Sliced Wasserstein Generative Models. (arXiv:1706.02631v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02631</link>
<description rdf:parseType="Literal">&lt;p&gt;In the paper, we introduce a model of sliced optimal transport (SOT), which
measures the distribution affinity with sliced Wasserstein distance (SWD).
Since SWD enjoys the property of factorizing high-dimensional joint
distributions into their multiple one-dimensional marginal distributions, its
dual and primal forms can be approximated easier compared to Wasserstein
distance (WD). Thus, we propose two types of differentiable SOT blocks to equip
modern generative frameworks---Auto-Encoders (AEs) and Generative Adversarial
Networks (GANs)---with the primal and dual forms of SWD. The superiority of our
SWAE and SWGAN over the state-of-the-art generative models is studied both
qualitatively and quantitatively on standard benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiqing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoma_J/0/1/0/all/0/1&quot;&gt;Janine Thoma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00689">
<title>Dirichlet Bayesian Network Scores and the Maximum Relative Entropy Principle. (arXiv:1708.00689v5 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00689</link>
<description rdf:parseType="Literal">&lt;p&gt;A classic approach for learning Bayesian networks from data is to identify a
maximum a posteriori (MAP) network structure. In the case of discrete Bayesian
networks, MAP networks are selected by maximising one of several possible
Bayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet
equivalent uniform (BDeu) score from Heckerman et al (1995). The key properties
of BDeu arise from its uniform prior over the parameters of each local
distribution in the network, which makes structure learning computationally
efficient; it does not require the elicitation of prior knowledge from experts;
and it satisfies score equivalence.
&lt;/p&gt;
&lt;p&gt;In this paper we will review the derivation and the properties of BD scores,
and of BDeu in particular, and we will link them to the corresponding entropy
estimates to study them from an information theoretic perspective. To this end,
we will work in the context of the foundational work of Giffin and Caticha
(2007), who showed that Bayesian inference can be framed as a particular case
of the maximum relative entropy principle. We will use this connection to show
that BDeu should not be used for structure learning from sparse data, since it
violates the maximum relative entropy principle; and that it is also
problematic from a more classic Bayesian model selection perspective, because
it produces Bayes factors that are sensitive to the value of its only
hyperparameter. Using a large simulation study, we found in our previous work
(Scutari, 2016) that the Bayesian Dirichlet sparse (BDs) score seems to provide
better accuracy in structure learning; in this paper we further show that BDs
does not suffer from the issues above, and we recommend to use it for sparse
data instead of BDeu. Finally, will show that these issues are in fact
different aspects of the same problem and a consequence of the distributional
assumptions of the prior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04072">
<title>A convergence frame for inexact nonconvex and nonsmooth algorithms and its applications to several iterations. (arXiv:1709.04072v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04072</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the convergence of an abstract inexact nonconvex
and nonsmooth algorithm. We promise a pseudo sufficient descent condition and a
pseudo relative error condition, which both are related to an auxiliary
sequence, for the algorithm; and a continuity condition is assumed to hold. In
fact, a wide of classical inexact nonconvex and nonsmooth algorithms allow
these three conditions. Under the finite energy assumption on the auxiliary
sequence, we prove the sequence generated by the general algorithm converges to
a critical point of the objective function if being assumed Kurdyka-
Lojasiewicz property. The core of the proofs lies on building a new Lyapunov
function, whose successive difference provides a bound for the successive
difference of the points generated by the algorithm. And then, we apply our
findings to several classical nonconvex iterative algorithms and derive
corresponding convergence results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09376">
<title>Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors. (arXiv:1712.09376v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09376</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning
algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior)
classifier, i.e., a randomized classifier obtained by a risk-sensitive
perturbation of the weights of a learned classifier. Entropy-SGD works by
optimizing the bound&apos;s prior, violating the hypothesis of the PAC-Bayes theorem
that the prior is chosen independently of the data. Indeed, available
implementations of Entropy-SGD rapidly obtain zero training error on random
labels and the same holds of the Gibbs posterior. In order to obtain a valid
generalization bound, we rely on a result showing that data-dependent priors
obtained by stochastic gradient Langevin dynamics (SGLD) yield valid PAC-Bayes
bounds provided the target distribution of SGLD is $\epsilon$-differentially
private. We observe that test error on MNIST and CIFAR10 falls within the
(empirically nonvacuous) risk bounds computed under the assumption that SGLD
reaches stationarity. In particular, Entropy-SGLD can be configured to yield
relatively tight generalization bounds and still fit real labels, although
these same settings do not obtain state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dziugaite_G/0/1/0/all/0/1&quot;&gt;Gintare Karolina Dziugaite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1&quot;&gt;Daniel M. Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01649">
<title>Gauged Mini-Bucket Elimination for Approximate Inference. (arXiv:1801.01649v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01649</link>
<description rdf:parseType="Literal">&lt;p&gt;Computing the partition function $Z$ of a discrete graphical model is a
fundamental inference challenge. Since this is computationally intractable,
variational approximations are often used in practice. Recently, so-called
gauge transformations were used to improve variational lower bounds on $Z$. In
this paper, we propose a new gauge-variational approach, termed WMBE-G, which
combines gauge transformations with the weighted mini-bucket elimination (WMBE)
method. WMBE-G can provide both upper and lower bounds on $Z$, and is easier to
optimize than the prior gauge-variational algorithm. We show that WMBE-G
strictly improves the earlier WMBE approximation for symmetric models including
Ising models with no magnetic field. Our experimental results demonstrate the
effectiveness of WMBE-G even for generic, nonsymmetric models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chertkov_M/0/1/0/all/0/1&quot;&gt;Michael Chertkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04153">
<title>Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then prove convergence rates for the method in the well-specified and
misspecified cases, and demonstrate its efficiency in the context of
multi-fidelity models for complex engineering systems and a problem of global
illumination in computer graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07575">
<title>Emulating dynamic non-linear simulators using Gaussian processes. (arXiv:1802.07575v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we examine the emulation of non-linear deterministic computer
codes where the output is a time series, possibly multivariate. Such computer
models simulate the evolution of some real-world phenomena over time, for
example models of the climate or the functioning of the human brain. The models
we are interested in are highly non-linear and exhibit tipping points,
bifurcations and chaotic behaviour. Each simulation run is too time-consuming
to perform naive uncertainty quantification. We therefore build emulators using
Gaussian processes to model the output of the code. We use the Gaussian process
to predict one-step ahead in an iterative way over the whole time series. We
consider a number of ways to propagate uncertainty through the time series
including both the uncertainty of inputs to the emulators at time t and the
correlation between them. The methodology is illustrated with a number of
examples. These include the highly non-linear dynamical systems described by
the Lorenz and Van der Pol equations. In both cases we will show that we not
only have very good predictive performance but also have measures of
uncertainty that reflect what is known about predictability in each system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohammadi_H/0/1/0/all/0/1&quot;&gt;Hossein Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Challenor_P/0/1/0/all/0/1&quot;&gt;Peter Challenor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_M/0/1/0/all/0/1&quot;&gt;Marc Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09963">
<title>Breaking the $1/\sqrt{n}$ Barrier: Faster Rates for Permutation-based Models in Polynomial Time. (arXiv:1802.09963v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09963</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications, including rank aggregation and crowd-labeling, can be
modeled in terms of a bivariate isotonic matrix with unknown permutations
acting on its rows and columns. We consider the problem of estimating such a
matrix based on noisy observations of a subset of its entries, and design and
analyze polynomial-time algorithms that improve upon the state of the art. In
particular, our results imply that any such $n \times n$ matrix can be
estimated efficiently in the normalized Frobenius norm at rate
$\widetilde{\mathcal O}(n^{-3/4})$, thus narrowing the gap between
$\widetilde{\mathcal O}(n^{-1})$ and $\widetilde{\mathcal O}(n^{-1/2})$, which
were hitherto the rates of the most statistically and computationally efficient
methods, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Cheng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pananjady_A/0/1/0/all/0/1&quot;&gt;Ashwin Pananjady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09129">
<title>Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning. (arXiv:1802.09129v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.09129</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised object detection and semantic segmentation require object or even
pixel level annotations. When there exist image level labels only, it is
challenging for weakly supervised algorithms to achieve accurate predictions.
The accuracy achieved by top weakly supervised algorithms is still
significantly lower than their fully supervised counterparts. In this paper, we
propose a novel weakly supervised curriculum learning pipeline for multi-label
object recognition, detection and semantic segmentation. In this pipeline, we
first obtain intermediate object localization and pixel labeling results for
the training images, and then use such results to train task-specific deep
networks in a fully supervised manner. The entire process consists of four
stages, including object localization in the training images, filtering and
fusing object instances, pixel labeling for the training images, and
task-specific network training. To obtain clean object instances in the
training images, we propose a novel algorithm for filtering, fusing and
classifying object instances collected from multiple solution mechanisms. In
this algorithm, we incorporate both metric learning and density-based
clustering to filter detected object instances. Experiments show that our
weakly supervised pipeline achieves state-of-the-art results in multi-label
image classification as well as weakly supervised object detection and very
competitive results in weakly supervised semantic segmentation on MS-COCO,
PASCAL VOC 2007 and PASCAL VOC 2012.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Weifeng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sibei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>