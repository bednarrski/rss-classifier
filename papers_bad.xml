<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08798"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08818"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00797"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08750"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08858"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08871"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08941"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.00459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04143"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00411"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09184"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08286"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08315"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08361"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08414"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08469"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08613"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08619"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08646"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08763"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08765"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1607.07607"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.02365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02963"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.04476"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07753"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08680"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00552"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05512"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08362"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04523"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05092"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.08347">
<title>A Comparison of the Taguchi Method and Evolutionary Optimization in Multivariate Testing. (arXiv:1808.08347v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.08347</link>
<description rdf:parseType="Literal">&lt;p&gt;Multivariate testing has recently emerged as a promising technique in web
interface design. In contrast to the standard A/B testing, multivariate
approach aims at evaluating a large number of values in a few key variables
systematically. The Taguchi method is a practical implementation of this idea,
focusing on orthogonal combinations of values. This paper evaluates an
alternative method: population-based search, i.e. evolutionary optimization.
Its performance is compared to that of the Taguchi method in several simulated
conditions, including an orthogonal one designed to favor the Taguchi method,
and two realistic conditions with dependences between variables. Evolutionary
optimization is found to perform significantly better especially in the
realistic conditions, suggesting that it forms a good approach for web
interface design in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jingbo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legrand_D/0/1/0/all/0/1&quot;&gt;Diego Legrand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severn_R/0/1/0/all/0/1&quot;&gt;Robert Severn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08798">
<title>Beyond expectation: Deep joint mean and quantile regression for spatio-temporal problems. (arXiv:1808.08798v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.08798</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatio-temporal problems are ubiquitous and of vital importance in many
research fields. Despite the potential already demonstrated by deep learning
methods in modeling spatio-temporal data, typical approaches tend to focus
solely on conditional expectations of the output variables being modeled. In
this paper, we propose a multi-output multi-quantile deep learning approach for
jointly modeling several conditional quantiles together with the conditional
expectation as a way to provide a more complete &quot;picture&quot; of the predictive
density in spatio-temporal problems. Using two large-scale datasets from the
transportation domain, we empirically demonstrate that, by approaching the
quantile regression problem from a multi-task learning perspective, it is
possible to solve the embarrassing quantile crossings problem, while
simultaneously significantly outperforming state-of-the-art quantile regression
methods. Moreover, we show that jointly modeling the mean and several
conditional quantiles not only provides a rich description about the predictive
density that can capture heteroscedastic properties at a neglectable
computational overhead, but also leads to improved predictions of the
conditional expectation due to the extra information and a regularization
effect induced by the added quantiles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodrigues_F/0/1/0/all/0/1&quot;&gt;Filipe Rodrigues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pereira_F/0/1/0/all/0/1&quot;&gt;Francisco C. Pereira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08818">
<title>A new Taxonomy of Continuous Global Optimization Algorithms. (arXiv:1808.08818v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.08818</link>
<description rdf:parseType="Literal">&lt;p&gt;Surrogate-based optimization and nature-inspired metaheuristics have become
the state-of-the-art in solving real-world optimization problems. Still, it is
difficult for beginners and even experts to get an overview that explains their
advantages in comparison to the large number of available methods in the scope
of continuous optimization. Available taxonomies lack the integration of
surrogate-based approaches and thus their embedding in the larger context of
this broad field. This article presents a taxonomy of the field, which further
matches the idea of nature-inspired algorithms, as it is based on the human
behavior in path finding. Intuitive analogies make it easy to conceive the most
basic principles of the search algorithms, even for beginners and non-experts
in this area of research. However, this scheme does not oversimplify the high
complexity of the different algorithms, as the class identifier only defines a
descriptive meta-level of the algorithm search strategies. The taxonomy was
established by exploring and matching algorithm schemes, extracting
similarities and differences, and creating a set of classification indicators
to distinguish between five distinct classes. In practice, this taxonomy allows
recommendations for the applicability of the corresponding algorithms and helps
developers trying to create or improve their own algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stork_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Stork&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiben_A/0/1/0/all/0/1&quot;&gt;A.E. Eiben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1&quot;&gt;Thomas Bartz-Beielstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00754">
<title>Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. (arXiv:1712.00754v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00754</link>
<description rdf:parseType="Literal">&lt;p&gt;A new class of non-homogeneous state-affine systems is introduced for use in
reservoir computing. Sufficient conditions are identified that guarantee first,
that the associated reservoir computers with linear readouts are causal,
time-invariant, and satisfy the fading memory property and second, that a
subset of this class is universal in the category of fading memory filters with
stochastic almost surely uniformly bounded inputs. This means that any
discrete-time filter that satisfies the fading memory property with random
inputs of that type can be uniformly approximated by elements in the
non-homogeneous state-affine family.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grigoryeva_L/0/1/0/all/0/1&quot;&gt;Lyudmila Grigoryeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1&quot;&gt;Juan-Pablo Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00797">
<title>Echo state networks are universal. (arXiv:1806.00797v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00797</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper shows that echo state networks are universal uniform approximants
in the context of discrete-time fading memory filters with uniformly bounded
inputs defined on negative infinite times. This result guarantees that any
fading memory input/output system in discrete time can be realized as a simple
finite-dimensional neural network-type state-space model with a static linear
readout map. This approximation is valid for infinite time intervals. The proof
of this statement is based on fundamental results, also presented in this work,
about the topological nature of the fading memory property and about reservoir
computing systems generated by continuous reservoir maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grigoryeva_L/0/1/0/all/0/1&quot;&gt;Lyudmila Grigoryeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1&quot;&gt;Juan-Pablo Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02502">
<title>GP-RVM: Genetic Programing-based Symbolic Regression Using Relevance Vector Machine. (arXiv:1806.02502v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02502</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a hybrid basis function construction method (GP-RVM) for
Symbolic Regression problem, which combines an extended version of Genetic
Programming called Kaizen Programming and Relevance Vector Machine to evolve an
optimal set of basis functions. Different from traditional evolutionary
algorithms where a single individual is a complete solution, our method
proposes a solution based on linear combination of basis functions built from
individuals during the evolving process. RVM which is a sparse Bayesian kernel
method selects suitable functions to constitute the basis. RVM determines the
posterior weight of a function by evaluating its quality and sparsity. The
solution produced by GP-RVM is a sparse Bayesian linear model of the
coefficients of many non-linear functions. Our hybrid approach is focused on
nonlinear white-box models selecting the right combination of functions to
build robust predictions without prior knowledge about data. Experimental
results show that GP-RVM outperforms conventional methods, which suggest that
it is an efficient and accurate technique for solving SR. The computational
complexity of GP-RVM scales in $O( M^{3})$, where $M$ is the number of
functions in the basis set and is typically much smaller than the number $N$ of
training patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rad_H/0/1/0/all/0/1&quot;&gt;Hossein Izadi Rad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Ji Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iba_H/0/1/0/all/0/1&quot;&gt;Hitoshi Iba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02957">
<title>A Deep Neural Network Surrogate for High-Dimensional Random Partial Differential Equations. (arXiv:1806.02957v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02957</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing efficient numerical algorithms for the solution of high
dimensional random Partial Differential Equations (PDEs) has been a challenging
task due to the well-known curse of dimensionality. We present a new solution
framework for these problems based on a deep learning approach. Specifically,
the random PDE is approximated by a feed-forward fully-connected deep residual
network, with either strong or weak enforcement of initial and boundary
constraints. The framework is mesh-free, and can handle irregular computational
domains. Parameters of the approximating deep neural network are determined
iteratively using variants of the Stochastic Gradient Descent (SGD) algorithm.
The satisfactory accuracy of the proposed frameworks is numerically
demonstrated on diffusion and heat conduction problems, in comparison with the
converged Monte Carlo-based finite element results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabian_M/0/1/0/all/0/1&quot;&gt;Mohammad Amin Nabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meidani_H/0/1/0/all/0/1&quot;&gt;Hadi Meidani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08716">
<title>NullaNet: Training Deep Neural Networks for Reduced-Memory-Access Inference. (arXiv:1807.08716v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08716</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have been successfully deployed in a wide variety of
applications including computer vision and speech recognition. However,
computational and storage complexity of these models has forced the majority of
computations to be performed on high-end computing platforms or on the cloud.
To cope with computational and storage complexity of these models, this paper
presents a training method that enables a radically different approach for
realization of deep neural networks through Boolean logic minimization. The
aforementioned realization completely removes the energy-hungry step of
accessing memory for obtaining model parameters, consumes about two orders of
magnitude fewer computing resources compared to realizations that use
floatingpoint operations, and has a substantially lower latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazemi_M/0/1/0/all/0/1&quot;&gt;Mahdi Nazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasandi_G/0/1/0/all/0/1&quot;&gt;Ghasem Pasandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1&quot;&gt;Massoud Pedram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08256">
<title>Adaptive Grey-Box Fuzz-Testing with Thompson Sampling. (arXiv:1808.08256v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08256</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzz testing, or &quot;fuzzing,&quot; refers to a widely deployed class of techniques
for testing programs by generating a set of inputs for the express purpose of
finding bugs and identifying security flaws. Grey-box fuzzing, the most popular
fuzzing strategy, combines light program instrumentation with a data driven
process to generate new program inputs. In this work, we present a machine
learning approach that builds on AFL, the preeminent grey-box fuzzer, by
adaptively learning a probability distribution over its mutation operators on a
program-specific basis. These operators, which are selected uniformly at random
in AFL and mutational fuzzers in general, dictate how new inputs are generated,
a core part of the fuzzer&apos;s efficacy. Our main contributions are two-fold:
First, we show that a sampling distribution over mutation operators estimated
from training programs can significantly improve performance of AFL. Second, we
introduce a Thompson Sampling, bandit-based optimization approach that
fine-tunes the mutator distribution adaptively, during the course of fuzzing an
individual program. A set of experiments across complex programs demonstrates
that tuning the mutational operator distribution generates sets of inputs that
yield significantly higher code coverage and finds more crashes faster and more
reliably than both baseline versions of AFL as well as other AFL-based learning
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1&quot;&gt;Siddharth Karamcheti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_G/0/1/0/all/0/1&quot;&gt;Gideon Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1&quot;&gt;David Rosenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08296">
<title>Brain Biomarker Interpretation in ASD Using Deep Learning and fMRI. (arXiv:1808.08296v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08296</link>
<description rdf:parseType="Literal">&lt;p&gt;Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder.
Finding the biomarkers associated with ASD is extremely helpful to understand
the underlying roots of the disorder and can lead to earlier diagnosis and more
targeted treatment. Although Deep Neural Networks (DNNs) have been applied in
functional magnetic resonance imaging (fMRI) to identify ASD, understanding the
data-driven computational decision making procedure has not been previously
explored. Therefore, in this work, we address the problem of interpreting
reliable biomarkers associated with identifying ASD; specifically, we propose a
2-stage method that classifies ASD and control subjects using fMRI images and
interprets the saliency features activated by the classifier. First, we trained
an accurate DNN classifier. Then, for detecting the biomarkers, different from
the DNN visualization works in computer vision, we take advantage of the
anatomical structure of brain fMRI and develop a frequency-normalized sampling
method to corrupt images. Furthermore, in the ASD vs. control subjects
classification scenario, we provide a new approach to detect and characterize
important brain features into three categories. The biomarkers we found by the
proposed method are robust and consistent with previous findings in the
literature. We also validate the detected biomarkers by neurological function
decoding and comparing with the DNN activation maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvornek_N/0/1/0/all/0/1&quot;&gt;Nicha C. Dvornek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Juntang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventola_P/0/1/0/all/0/1&quot;&gt;Pamela Ventola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08319">
<title>BOP: Benchmark for 6D Object Pose Estimation. (arXiv:1808.08319v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08319</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a benchmark for 6D pose estimation of a rigid object from a single
RGB-D input image. The training data consists of a texture-mapped 3D object
model or images of the object in known 6D poses. The benchmark comprises of: i)
eight datasets in a unified format that cover different practical scenarios,
including two new datasets focusing on varying lighting conditions, ii) an
evaluation methodology with a pose-error function that deals with pose
ambiguities, iii) a comprehensive evaluation of 15 diverse recent methods that
captures the status quo of the field, and iv) an online evaluation system that
is open for continuous submission of new results. The evaluation shows that
methods based on point-pair features currently perform best, outperforming
template matching methods, learning-based methods and methods based on 3D local
features. The project website is available at bop.felk.cvut.cz.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodan_T/0/1/0/all/0/1&quot;&gt;Tomas Hodan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michel_F/0/1/0/all/0/1&quot;&gt;Frank Michel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brachmann_E/0/1/0/all/0/1&quot;&gt;Eric Brachmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kehl_W/0/1/0/all/0/1&quot;&gt;Wadim Kehl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buch_A/0/1/0/all/0/1&quot;&gt;Anders Glent Buch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraft_D/0/1/0/all/0/1&quot;&gt;Dirk Kraft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drost_B/0/1/0/all/0/1&quot;&gt;Bertram Drost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1&quot;&gt;Joel Vidal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ihrke_S/0/1/0/all/0/1&quot;&gt;Stephan Ihrke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabulis_X/0/1/0/all/0/1&quot;&gt;Xenophon Zabulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_C/0/1/0/all/0/1&quot;&gt;Caner Sahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1&quot;&gt;Fabian Manhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tae-Kyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1&quot;&gt;Jiri Matas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1&quot;&gt;Carsten Rother&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08578">
<title>Automatic 3D bi-ventricular segmentation of cardiac images by a shape-constrained multi-task deep learning approach. (arXiv:1808.08578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08578</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning approaches have achieved state-of-the-art performance in
cardiac magnetic resonance (CMR) image segmentation. However, most approaches
have focused on learning image intensity features for segmentation, whereas the
incorporation of anatomical shape priors has received less attention. In this
paper, we combine a multi-task deep learning approach with atlas propagation to
develop a shape-constrained bi-ventricular segmentation pipeline for short-axis
CMR volumetric images. The pipeline first employs a fully convolutional network
(FCN) that learns segmentation and landmark localisation tasks simultaneously.
The architecture of the proposed FCN uses a 2.5D representation, thus combining
the computational advantage of 2D FCNs networks and the capability of
addressing 3D spatial consistency without compromising segmentation accuracy.
Moreover, the refinement step is designed to explicitly enforce a shape
constraint and improve segmentation quality. This step is effective for
overcoming image artefacts (e.g. due to different breath-hold positions and
large slice thickness), which preclude the creation of anatomically meaningful
3D cardiac shapes. The proposed pipeline is fully automated, due to network&apos;s
ability to infer landmarks, which are then used downstream in the pipeline to
initialise atlas propagation. We validate the pipeline on 1831 healthy subjects
and 649 subjects with pulmonary hypertension. Extensive numerical experiments
on the two datasets demonstrate that our proposed method is robust and capable
of producing accurate, high-resolution and anatomically smooth bi-ventricular
3D models, despite the artefacts in input CMR volumes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinming Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_G/0/1/0/all/0/1&quot;&gt;Ghalib Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlemper_J/0/1/0/all/0/1&quot;&gt;Jo Schlemper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Wenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawes_T/0/1/0/all/0/1&quot;&gt;Timothy J W Dawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biffi_C/0/1/0/all/0/1&quot;&gt;Carlo Biffi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvao_%5C/0/1/0/all/0/1&quot;&gt;\\Antonio de Marvao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doumou_G/0/1/0/all/0/1&quot;&gt;Georgia Doumou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ORegan_D/0/1/0/all/0/1&quot;&gt;Declan P O&amp;#x27;Regan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08617">
<title>Autonomous Driving without a Burden: View from Outside with Elevated LiDAR. (arXiv:1808.08617v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08617</link>
<description rdf:parseType="Literal">&lt;p&gt;The current autonomous driving architecture places a heavy burden in signal
processing for the graphics processing units (GPUs) in the car. This directly
transforms into battery drain and lower energy efficiency, crucial factors in
electric vehicles. This is due to the high bit rate of the captured video and
other sensing inputs, mainly due to Light Detection and Ranging (LiDAR) sensor
at the top of the car which is an essential feature in todays autonomous
vehicles. LiDAR is needed to obtain a high precision map for the vehicle AI to
make relevant decisions. However, this is still a quite restricted view from
the car. This is the same even in the case of cars without a LiDAR such as
Telsa. The existing LiDARs and the cameras have limited horizontal and vertical
fields of visions. In all cases it can be argued that precision is lower, given
the smaller map generated. This also results in the accumulation of a huge
amount of data in the order of several TBs in a day, the storage of which
becomes challenging. If we are to reduce the effort for the processing units
inside the car, we need to uplink the data to edge or an appropriately placed
cloud. However, the required data rates in the order of several Gbps are
difficult to be met even with the advent of 5G. Therefore, we propose to have a
coordinated set of LiDAR&apos;s outside at an elevation which can provide an
integrated view with a much larger field of vision (FoV) to a centralized
decision making body which then sends the required control actions to the
vehicles with a lower bit rate in the downlink and with the required latency.
The calculations we have based on industry standard equipment from several
manufacturers show that this is not just a concept but a feasible system which
can be implemented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaweera_N/0/1/0/all/0/1&quot;&gt;Nalin Jayaweera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajatheva_N/0/1/0/all/0/1&quot;&gt;Nandana Rajatheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latva_aho_M/0/1/0/all/0/1&quot;&gt;Matti Latva-aho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08702">
<title>Augmenting Bottleneck Features of Deep Neural Network Employing Motor State for Speech Recognition at Humanoid Robots. (arXiv:1808.08702v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1808.08702</link>
<description rdf:parseType="Literal">&lt;p&gt;As for the humanoid robots, the internal noise, which is generated by motors,
fans and mechanical components when the robot is moving or shaking its body,
severely degrades the performance of the speech recognition accuracy. In this
paper, a novel speech recognition system robust to ego-noise for humanoid
robots is proposed, in which on/off state of the motor is employed as auxiliary
information for finding the relevant input features. For this, we consider the
bottleneck features, which have been successfully applied to deep neural
network (DNN) based automatic speech recognition (ASR) system. When learning
the bottleneck features to catch, we first exploit the motor on/off state data
as supplementary information in addition to the acoustic features as the input
of the first deep neural network (DNN) for preliminary acoustic modeling. Then,
the second DNN for primary acoustic modeling employs both the bottleneck
features tossed from the first DNN and the acoustics features. When the
proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT
database, the experimental results show that achieve obvious improvement (11%
relative) is achieved by our algorithm over the conventional systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Moa Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Joon Hyuk Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08750">
<title>Generalisation in humans and deep neural networks. (arXiv:1808.08750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08750</link>
<description rdf:parseType="Literal">&lt;p&gt;We compare the robustness of humans and current convolutional deep neural
networks (DNNs) on object recognition under twelve different types of image
degradations. First, using three well known DNNs (ResNet-152, VGG-19,
GoogLeNet) we find the human visual system to be more robust to nearly all of
the tested image manipulations, and we observe progressively diverging
classification error-patterns between humans and DNNs when the signal gets
weaker. Secondly, we show that DNNs trained directly on distorted images
consistently surpass human performance on the exact distortion types they were
trained on, yet they display extremely poor generalisation abilities when
tested on other distortion types. For example, training on salt-and-pepper
noise does not imply robustness on uniform white noise and vice versa. Thus,
changes in the noise distribution between training and testing constitutes a
crucial challenge to deep learning vision systems that can be systematically
addressed in a lifelong machine learning approach. Our new dataset consisting
of 83K carefully measured human psychophysical trials provide a useful
reference for lifelong robustness against image degradations set by the human
visual system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Temme_C/0/1/0/all/0/1&quot;&gt;Carlos R. Medina Temme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rauber_J/0/1/0/all/0/1&quot;&gt;Jonas Rauber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuett_H/0/1/0/all/0/1&quot;&gt;Heiko H. Schuett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1&quot;&gt;Felix A. Wichmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08754">
<title>What Makes Natural Scene Memorable?. (arXiv:1808.08754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08754</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on image memorability have shed light on the visual features
that make generic images, object images or face photographs memorable. However,
a clear understanding and reliable estimation of natural scene memorability
remain elusive. In this paper, we provide an attempt to answer: &quot;what exactly
makes natural scene memorable&quot;. Specifically, we first build LNSIM, a
large-scale natural scene image memorability database (containing 2,632 images
and memorability annotations). Then, we mine our database to investigate how
low-, middle- and high-level handcrafted features affect the memorability of
natural scene. In particular, we find that high-level feature of scene category
is rather correlated with natural scene memorability. Thus, we propose a deep
neural network based natural scene memorability (DeepNSM) predictor, which
takes advantage of scene category. Finally, the experimental results validate
the effectiveness of DeepNSM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiaxin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ren Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zulin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08794">
<title>Theoretical Foundations of the A2RD Project: Part I. (arXiv:1808.08794v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08794</link>
<description rdf:parseType="Literal">&lt;p&gt;This article identifies and discusses the theoretical foundations that were
considered in the design of the A2RD model. In addition to the points
considered, references are made to the studies available and considered in the
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08858">
<title>Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised. (arXiv:1808.08858v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.08858</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural framework for opinion summarization from online product
reviews which is knowledge-lean and only requires light supervision (e.g., in
the form of product domain labels and user-provided ratings). Our method
combines two weakly supervised components to identify salient opinions and form
extractive summaries from multiple reviews: an aspect extractor trained under a
multi-task objective, and a sentiment predictor based on multiple instance
learning. We introduce an opinion summarization dataset that includes a
training set of product reviews from six diverse domains and human-annotated
development and test sets with gold standard aspect annotations, salience
labels, and opinion summaries. Automatic evaluation shows significant
improvements over baselines, and a large-scale study indicates that our opinion
summaries are preferred by human judges according to multiple criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1&quot;&gt;Stefanos Angelidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1&quot;&gt;Mirella Lapata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08871">
<title>B\&apos;ezierGAN: Automatic Generation of Smooth Curves from Interpretable Low-Dimensional Parameters. (arXiv:1808.08871v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08871</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world objects are designed by smooth curves, especially in the
domain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and
hydrodynamic shapes (e.g., hulls) are designed. To facilitate the design
process of those objects, we propose a deep learning based generative model
that can synthesize smooth curves. The model maps a low-dimensional latent
representation to a sequence of discrete points sampled from a rational
B\&apos;ezier curve. We demonstrate the performance of our method in completing both
synthetic and real-world generative tasks. Results show that our method can
generate diverse and realistic curves, while preserving consistent shape
variation in the latent space, which is favorable for latent space design
optimization or design space exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuge_M/0/1/0/all/0/1&quot;&gt;Mark Fuge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08941">
<title>Improving Information Extraction from Images with Learned Semantic Models. (arXiv:1808.08941v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08941</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications require an understanding of an image that goes beyond the
simple detection and classification of its objects. In particular, a great deal
of semantic information is carried in the relationships between objects. We
have previously shown that the combination of a visual model and a statistical
semantic prior model can improve on the task of mapping images to their
associated scene description. In this paper, we review the model and compare it
to a novel conditional multi-way model for visual relationship detection, which
does not include an explicitly trained visual prior model. We also discuss
potential relationships between the proposed methods and memory models of the
human brain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baier_S/0/1/0/all/0/1&quot;&gt;Stephan Baier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.00459">
<title>Deep Abstract Q-Networks. (arXiv:1710.00459v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.00459</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the problem of learning and planning on high-dimensional domains
with long horizons and sparse rewards. Recent approaches have shown great
successes in many Atari 2600 domains. However, domains with long horizons and
sparse rewards, such as Montezuma&apos;s Revenge and Venture, remain challenging for
existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup,
and Singh 1999) have shown to be useful in tackling long-horizon problems. We
combine recent techniques of deep reinforcement learning with existing
model-based approaches using an expert-provided state abstraction. We construct
toy domains that elucidate the problem of long horizons, sparse rewards and
high-dimensional inputs, and show that our algorithm significantly outperforms
previous methods on these domains. Our abstraction-based approach outperforms
Deep Q-Networks (Mnih et al. 2015) on Montezuma&apos;s Revenge and Venture, and
exhibits backtracking behavior that is absent from previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roderick_M/0/1/0/all/0/1&quot;&gt;Melrose Roderick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimm_C/0/1/0/all/0/1&quot;&gt;Christopher Grimm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1&quot;&gt;Stefanie Tellex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04143">
<title>Benchmarking Single Image Dehazing and Beyond. (arXiv:1712.04143v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04143</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a comprehensive study and evaluation of existing single image
dehazing algorithms, using a new large-scale benchmark consisting of both
synthetic and real-world hazy images, called REalistic Single Image DEhazing
(RESIDE). RESIDE highlights diverse data sources and image contents, and is
divided into five subsets, each serving different training or evaluation
purposes. We further provide a rich variety of criteria for dehazing algorithm
evaluation, ranging from full-reference metrics, to no-reference metrics, to
subjective evaluation and the novel task-driven evaluation. Experiments on
RESIDE shed light on the comparisons and limitations of state-of-the-art
dehazing algorithms, and suggest promising future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1&quot;&gt;Wenqi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Dengpan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00411">
<title>Dense 3D Object Reconstruction from a Single Depth View. (arXiv:1802.00411v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00411</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs
the complete 3D structure of a given object from a single arbitrary depth view
using generative adversarial networks. Unlike existing work which typically
requires multiple views of the same object or class labels to recover the full
3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation
of a depth view of the object as input, and is able to generate the complete 3D
occupancy grid with a high resolution of 256^3 by recovering the
occluded/missing regions. The key idea is to combine the generative
capabilities of autoencoders and the conditional Generative Adversarial
Networks (GAN) framework, to infer accurate and fine-grained 3D structures of
objects in high-dimensional voxel space. Extensive experiments on large
synthetic datasets and real-world Kinect datasets show that the proposed
3D-RecGAN++ significantly outperforms the state of the art in single view 3D
object reconstruction, and is able to reconstruct unseen types of objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_S/0/1/0/all/0/1&quot;&gt;Stefano Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1&quot;&gt;Andrew Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1&quot;&gt;Niki Trigoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongkai Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09184">
<title>Variance Reduction Methods for Sublinear Reinforcement Learning. (arXiv:1802.09184v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09184</link>
<description rdf:parseType="Literal">&lt;p&gt;This work considers the problem of provably optimal reinforcement learning
for episodic finite horizon MDPs, i.e. how an agent learns to maximize his/her
long term reward in an uncertain environment. The main contribution is in
providing a novel algorithm --- Variance-reduced Upper Confidence Q-learning
(vUCQ) --- which enjoys a regret bound of $\widetilde{O}(\sqrt{HSAT} + H^5SA)$,
where the $T$ is the number of time steps the agent acts in the MDP, $S$ is the
number of states, $A$ is the number of actions, and $H$ is the (episodic)
horizon time.
&lt;/p&gt;
&lt;p&gt;This is the first regret bound that is both sub-linear in the model size and
asymptotically optimal. The algorithm is sub-linear in that the time to achieve
$\epsilon$-average regret for any constant $\epsilon$ is $O(SA)$, which is a
number of samples that is far less than that required to learn any non-trivial
estimate of the transition model (the transition model is specified by
$O(S^2A)$ parameters). The importance of sub-linear algorithms is largely the
motivation for algorithms such as $Q$-learning and other &quot;model free&quot;
approaches. vUCQ algorithm also enjoys minimax optimal regret in the long run,
matching the $\Omega(\sqrt{HSAT})$ lower bound.
&lt;/p&gt;
&lt;p&gt;Variance-reduced Upper Confidence Q-learning (vUCQ) is a successive
refinement method in which the algorithm reduces the variance in $Q$-value
estimates and couples this estimation scheme with an upper confidence based
algorithm. Technically, the coupling of both of these techniques is what leads
to the algorithm enjoying both the sub-linear regret property and the
asymptotically optimal regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin F. Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08740">
<title>Speeding-up Object Detection Training for Robotics with FALKON. (arXiv:1803.08740v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08740</link>
<description rdf:parseType="Literal">&lt;p&gt;Latest deep learning methods for object detection provide remarkable
performance, but have limits when used in robotic applications. One of the most
relevant issues is the long training time, which is due to the large size and
imbalance of the associated training sets, characterized by few positive and a
large number of negative examples (i.e. background). Proposed approaches are
based on end-to-end learning by back-propagation [22] or kernel methods trained
with Hard Negatives Mining on top of deep features [8]. These solutions are
effective, but prohibitively slow for on-line applications. In this paper we
propose a novel pipeline for object detection that overcomes this problem and
provides comparable performance, with a 60x training speedup. Our pipeline
combines (i) the Region Proposal Network and the deep feature extractor from
[22] to efficiently select candidate RoIs and encode them into powerful
representations, with (ii) the FALKON [23] algorithm, a novel kernel-based
method that allows fast training on large scale problems (millions of points).
We address the size and imbalance of training data by exploiting the stochastic
subsampling intrinsic into the method and a novel, fast, bootstrapping
approach. We assess the effectiveness of the approach on a standard Computer
Vision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a
real robotic scenario with the iCubWorld Transformations [18] dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maiettini_E/0/1/0/all/0/1&quot;&gt;Elisa Maiettini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasquale_G/0/1/0/all/0/1&quot;&gt;Giulia Pasquale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1&quot;&gt;Lorenzo Natale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08230">
<title>Using Apple Machine Learning Algorithms to Detect and Subclassify Non-Small Cell Lung Cancer. (arXiv:1808.08230v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1808.08230</link>
<description rdf:parseType="Literal">&lt;p&gt;Lung cancer continues to be a major healthcare challenge with high morbidity
and mortality rates among both men and women worldwide. The majority of lung
cancer cases are of non-small cell lung cancer type. With the advent of
targeted cancer therapy, it is imperative not only to properly diagnose but
also sub-classify non-small cell lung cancer. In our study, we evaluated the
utility of using Apple Create ML module to detect and sub-classify non-small
cell carcinomas based on histopathological images. After module optimization,
the program detected 100% of non-small cell lung cancer images and successfully
subclassified the majority of the images. Trained modules, such as ours, can be
utilized in diagnostic smartphone-based applications, augmenting diagnostic
services in understaffed areas of the world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+MD_A/0/1/0/all/0/1&quot;&gt;Andrew A. Borkowski MD&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+MT_C/0/1/0/all/0/1&quot;&gt;Catherine P. Wilson MT&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Borkowski_S/0/1/0/all/0/1&quot;&gt;Steven A. Borkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+RN_L/0/1/0/all/0/1&quot;&gt;Lauren A. Deland RN&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+MD_S/0/1/0/all/0/1&quot;&gt;Stephen M. Mastorides MD&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08270">
<title>Building a Robust Text Classifier on a Test-Time Budget. (arXiv:1808.08270v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08270</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generic and interpretable learning framework for building robust
text classification model that achieves accuracy comparable to full models
under test-time budget constraints. Our approach learns a selector to identify
words that are relevant to the prediction tasks and passes them to the
classifier for processing. The selector is trained jointly with the classifier
and directly learns to incorporate with the classifier. We further propose a
data aggregation scheme to improve the robustness of the classifier. Our
learning framework is general and can be incorporated with any type of text
classification model. On real-world data, we show that the proposed approach
improves the performance of a given classifier and speeds up the model with a
mere loss in accuracy performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1&quot;&gt;Md Rizwan Parvez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balukbasi_T/0/1/0/all/0/1&quot;&gt;Tolga Balukbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_k/0/1/0/all/0/1&quot;&gt;kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarigrama_V/0/1/0/all/0/1&quot;&gt;Venkatesh Sarigrama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08271">
<title>An elementary introduction to information geometry. (arXiv:1808.08271v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08271</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe the fundamental differential-geometric structures of information
manifolds, state the fundamental theorem of information geometry, and
illustrate some uses of these information manifolds in information sciences.
The exposition is self-contained by concisely introducing the necessary
concepts of differential geometry with proofs omitted for brevity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1&quot;&gt;Frank Nielsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08280">
<title>Deep multiscale convolutional feature learning for weakly supervised localization of chest pathologies in X-ray images. (arXiv:1808.08280v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08280</link>
<description rdf:parseType="Literal">&lt;p&gt;Localization of chest pathologies in chest X-ray images is a challenging task
because of their varying sizes and appearances. We propose a novel weakly
supervised method to localize chest pathologies using class aware deep
multiscale feature learning. Our method leverages intermediate feature maps
from CNN layers at different stages of a deep network during the training of a
classification model using image level annotations of pathologies. During the
training phase, a set of \emph{layer relevance weights} are learned for each
pathology class and the CNN is optimized to perform pathology classification by
convex combination of feature maps from both shallow and deep layers using the
learned weights. During the test phase, to localize the predicted pathology,
the multiscale attention map is obtained by convex combination of class
activation maps from each stage using the \emph{layer relevance weights}
learned during the training phase. We have validated our method using 112000
X-ray images and compared with the state-of-the-art localization methods. We
experimentally demonstrate that the proposed weakly supervised method can
improve the localization performance of small pathologies such as nodule and
mass while giving comparable performance for bigger pathologies e.g.,
Cardiomegaly
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedai_S/0/1/0/all/0/1&quot;&gt;Suman Sedai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1&quot;&gt;Dwarikanath Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakravorty_R/0/1/0/all/0/1&quot;&gt;Rajib Chakravorty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garnavi_R/0/1/0/all/0/1&quot;&gt;Rahil Garnavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08286">
<title>Probabilistic Graphical Modeling approach to dynamic PET direct parametric map estimation and image reconstruction. (arXiv:1808.08286v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1808.08286</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of dynamic emission tomography, the conventional processing
pipeline consists of independent image reconstruction of single time frames,
followed by the application of a suitable kinetic model to time activity curves
(TACs) at the voxel or region-of-interest level. The relatively new field of 4D
PET direct reconstruction, by contrast, seeks to move beyond this scheme and
incorporate information from multiple time frames within the reconstruction
task. Existing 4D direct models are based on a deterministic description of
voxels&apos; TACs, captured by the chosen kinetic model, considering the photon
counting process the only source of uncertainty. In this work, we introduce a
new probabilistic modeling strategy based on the key assumption that activity
time course would be subject to uncertainty even if the parameters of the
underlying dynamic process were known. This leads to a hierarchical Bayesian
model, which we formulate using the formalism of Probabilistic Graphical
Modeling (PGM). The inference of the joint probability density function arising
from PGM is addressed using a new gradient-based iterative algorithm, which
presents several advantages compared to existing direct methods: it is flexible
to an arbitrary choice of linear and nonlinear kinetic model; it enables the
inclusion of arbitrary (sub)differentiable priors for parametric maps; it is
simpler to implement and suitable to integration in computing frameworks for
machine learning. Computer simulations and an application to real patient scan
showed how the proposed approach allows us to weight the importance of the
kinetic model, providing a bridge between indirect and deterministic direct
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scipioni_M/0/1/0/all/0/1&quot;&gt;Michele Scipioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pedemonte_S/0/1/0/all/0/1&quot;&gt;Stefano Pedemonte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Santarelli_M/0/1/0/all/0/1&quot;&gt;Maria Filomena Santarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Landini_L/0/1/0/all/0/1&quot;&gt;Luigi Landini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08315">
<title>A Deterministic Self-Organizing Map Approach and its Application on Satellite Data based Cloud Type Classification. (arXiv:1808.08315v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08315</link>
<description rdf:parseType="Literal">&lt;p&gt;A self-organizing map (SOM) is a type of competitive artificial neural
network, which projects the high-dimensional input space of the training
samples into a low-dimensional space with the topology relations preserved.
This makes SOMs supportive of organizing and visualizing complex data sets and
have been pervasively used among numerous disciplines with different
applications. Notwithstanding its wide applications, the self-organizing map is
perplexed by its inherent randomness, which produces dissimilar SOM patterns
even when being trained on identical training samples with the same parameters
every time, and thus causes usability concerns for other domain practitioners
and precludes more potential users from exploring SOM based applications in a
broader spectrum. Motivated by this practical concern, we propose a
deterministic approach as a supplement to the standard self-organizing map. In
accordance with the theoretical design, the experimental results with satellite
cloud data demonstrate the effective and efficient organization as well as
simplification capabilities of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianwu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;Daeho Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oreopoulos_L/0/1/0/all/0/1&quot;&gt;Lazaros Oreopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhibo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08317">
<title>To Cluster, or Not to Cluster: An Analysis of Clusterability Methods. (arXiv:1808.08317v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.08317</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is an essential data mining tool that aims to discover inherent
cluster structure in data. For most applications, applying clustering is only
appropriate when cluster structure is present. As such, the study of
clusterability, which evaluates whether data possesses such structure, is an
integral part of cluster analysis. However, methods for evaluating
clusterability vary radically, making it challenging to select a suitable
measure. In this paper, we perform an extensive comparison of measures of
clusterability and provide guidelines that clustering users can reference to
select suitable measures for their applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adolfsson_A/0/1/0/all/0/1&quot;&gt;A. Adolfsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ackerman_M/0/1/0/all/0/1&quot;&gt;M. Ackerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brownstein_N/0/1/0/all/0/1&quot;&gt;N. C. Brownstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08361">
<title>Data-dependent Learning of Symmetric/Antisymmetric Relations for Knowledge Base Completion. (arXiv:1808.08361v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08361</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding-based methods for knowledge base completion (KBC) learn
representations of entities and relations in a vector space, along with the
scoring function to estimate the likelihood of relations between entities. The
learnable class of scoring functions is designed to be expressive enough to
cover a variety of real-world relations, but this expressive comes at the cost
of an increased number of parameters. In particular, parameters in these
methods are superfluous for relations that are either symmetric or
antisymmetric. To mitigate this problem, we propose a new L1 regularizer for
Complex Embeddings, which is one of the state-of-the-art embedding-based
methods for KBC. This regularizer promotes symmetry or antisymmetry of the
scoring function on a relation-by-relation basis, in accordance with the
observed data. Our empirical evaluation shows that the proposed method
outperforms the original Complex Embeddings and other baseline methods on the
FB15k dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manabe_H/0/1/0/all/0/1&quot;&gt;Hitoshi Manabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1&quot;&gt;Katsuhiko Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimbo_M/0/1/0/all/0/1&quot;&gt;Masashi Shimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08414">
<title>Unsupervised Hypergraph Feature Selection via a Novel Point-Weighting Framework and Low-Rank Representation. (arXiv:1808.08414v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08414</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection methods are widely used in order to solve the &apos;curse of
dimensionality&apos; problem. Many proposed feature selection frameworks, treat all
data points equally; neglecting their different representation power and
importance. In this paper, we propose an unsupervised hypergraph feature
selection method via a novel point-weighting framework and low-rank
representation that captures the importance of different data points. We
introduce a novel soft hypergraph with low complexity to model data. Then, we
formulate the feature selection as an optimization problem to preserve local
relationships and also global structure of data. Our approach for global
structure preservation helps the framework overcome the problem of
unavailability of data labels in unsupervised learning. The proposed feature
selection method treats with different data points based on their importance in
defining data structure and representation power. Moreover, since the
robustness of feature selection methods against noise and outlier is of great
importance, we adopt low-rank representation in our model. Also, we provide an
efficient algorithm to solve the proposed optimization problem. The
computational cost of the proposed algorithm is lower than many
state-of-the-art methods which is of high importance in feature selection
tasks. We conducted comprehensive experiments with various evaluation methods
on different benchmark data sets. These experiments indicate significant
improvement, compared with state-of-the-art feature selection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilani_A/0/1/0/all/0/1&quot;&gt;Ammar Gilani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amirmazlaghani_M/0/1/0/all/0/1&quot;&gt;Maryam Amirmazlaghani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08416">
<title>Multiplayer bandits without observing collision information. (arXiv:1808.08416v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08416</link>
<description rdf:parseType="Literal">&lt;p&gt;We study multiplayer stochastic multi-armed bandit problems in which the
players cannot communicate, and if two or more players pull the same arm, a
collision occurs and the involved players receive zero reward. We consider two
feedback models: a model in which the players can observe whether a collision
has occurred, and a more difficult setup when no collision information is
available. We give the first theoretical guarantees for the second model: an
algorithm with a logarithmic regret, and an algorithm with a square-root regret
type that does not depend on the gaps between the means. For the first model,
we give the first square-root regret bounds that do not depend on the gaps.
Building on these ideas, we also give an algorithm for reaching approximate
Nash equilibria quickly in stochastic anti-coordination games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lugosi_G/0/1/0/all/0/1&quot;&gt;Gabor Lugosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrabian_A/0/1/0/all/0/1&quot;&gt;Abbas Mehrabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08460">
<title>The Social Cost of Strategic Classification. (arXiv:1808.08460v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08460</link>
<description rdf:parseType="Literal">&lt;p&gt;Consequential decision-making typically incentivizes individuals to behave
strategically, tailoring their behavior to the specifics of the decision rule.
A long line of work has therefore sought to counteract strategic behavior by
designing more conservative decision boundaries in an effort to increase
robustness to the effects of strategic covariate shift.
&lt;/p&gt;
&lt;p&gt;We show that these efforts benefit the institutional decision maker at the
expense of the individuals being classified. Introducing a notion of social
burden, we prove that any increase in institutional utility necessarily leads
to a corresponding increase in social burden. Moreover, we show that the
negative externalities of strategic classification can disproportionately harm
disadvantaged groups in the population.
&lt;/p&gt;
&lt;p&gt;Our results highlight that strategy-robustness must be weighed against
considerations of social welfare and fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milli_S/0/1/0/all/0/1&quot;&gt;Smitha Milli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1&quot;&gt;John Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca D. Dragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1&quot;&gt;Moritz Hardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08469">
<title>DNN: A Two-Scale Distributional Tale of Heterogeneous Treatment Effect Inference. (arXiv:1808.08469v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.08469</link>
<description rdf:parseType="Literal">&lt;p&gt;Heterogeneous treatment effects are the center of gravity in many modern
causal inference applications. In this paper, we investigate the estimation and
inference of heterogeneous treatment effects with precision in a general
nonparametric setting. To this end, we enhance the classical $k$-nearest
neighbor method with a simple algorithm, extend it to a distributional setting,
and suggest the two-scale distributional nearest neighbors (DNN) estimator with
reduced finite-sample bias. Our recipe is first to subsample the data and
average the 1-nearest neighbor estimators from each subsample. With
appropriately chosen subsampling scale, the resulting DNN estimator is proved
to be asymptotically unbiased and normal under mild regularity conditions. We
then proceed with combining DNN estimators with different subsampling scales to
further reduce bias. Our theoretical results on the advantages of the new
two-scale DNN framework are well supported by several Monte Carlo simulations.
The newly suggested method is also applied to a real-life data set to study the
heterogeneity of treatment effects of smoking on children&apos;s birth weights
across mothers&apos; ages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yingying Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jinchi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08558">
<title>Spectral-Pruning: Compressing deep neural network via spectral analysis. (arXiv:1808.08558v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.08558</link>
<description rdf:parseType="Literal">&lt;p&gt;The model size of deep neural network is getting larger and larger to realize
superior performance in complicated tasks. This makes it difficult to implement
deep neural network in small edge-computing devices. To overcome this problem,
model compression methods have been gathering much attention. However, there
have been only few theoretical back-grounds that explain what kind of quantity
determines the compression ability. To resolve this issue, we develop a new
theoretical frame-work for model compression, and propose a new method called
{\it Spectral-Pruning} based on the theory. Our theoretical analysis is based
on the observation such that the eigenvalues of the covariance matrix of the
output from nodes in the internal layers often shows rapid decay. We define
&quot;degree of freedom&quot; to quantify an intrinsic dimensionality of the model by
using the eigenvalue distribution and show that the compression ability is
essentially controlled by this quantity. Along with this, we give a
generalization error bound of the compressed model. Our proposed method is
applicable to wide range of models, unlike the existing methods, e.g., ones
possess complicated branches as implemented in SegNet and ResNet. Our method
makes use of both &quot;input&quot; and &quot;output&quot; in each layer and is easy to implement.
We apply our method to several datasets to justify our theoretical analyses and
show that the proposed method achieves the state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1&quot;&gt;Taiji Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abe_H/0/1/0/all/0/1&quot;&gt;Hiroshi Abe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murata_T/0/1/0/all/0/1&quot;&gt;Tomoya Murata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Horiuchi_S/0/1/0/all/0/1&quot;&gt;Shingo Horiuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ito_K/0/1/0/all/0/1&quot;&gt;Kotaro Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wachi_T/0/1/0/all/0/1&quot;&gt;Tokuma Wachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hirai_S/0/1/0/all/0/1&quot;&gt;So Hirai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yukishima_M/0/1/0/all/0/1&quot;&gt;Masatoshi Yukishima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nishimura_T/0/1/0/all/0/1&quot;&gt;Tomoaki Nishimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08613">
<title>Ensemble Learning Applied to Classify GPS Trajectories of Birds into Male or Female. (arXiv:1808.08613v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08613</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe our first-place solution to the Animal Behavior Challenge (ABC
2018) on predicting gender of bird from its GPS trajectory. The task consisted
in predicting the gender of shearwater based on how they navigate themselves
across a big ocean. The trajectories are collected from GPS loggers attached on
shearwaters&apos; body, and represented as a variable-length sequence of GPS points
(latitude and longitude), and associated meta-information, such as the sun
azimuth, the sun elevation, the daytime, the elapsed time on each GPS location
after starting the trip, the local time (date is trimmed), and the indicator of
the day starting the from the trip. We used ensemble of several variants of
Gradient Boosting Classifier along with Gaussian Process Classifier and Support
Vector Classifier after extensive feature engineering and we ranked first out
of 74 registered teams. The variants of Gradient Boosting Classifier we tried
are CatBoost (Developed by Yandex), LightGBM (Developed by Microsoft), XGBoost
(Developed by Distributed Machine Learning Community). Our approach could
easily be adapted to other applications in which the goal is to predict a
classification output from a variable-length sequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayzur_D/0/1/0/all/0/1&quot;&gt;Dewan Fayzur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08619">
<title>Discriminative but Not Discriminatory: A Comparison of Fairness Definitions under Different Worldviews. (arXiv:1808.08619v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08619</link>
<description rdf:parseType="Literal">&lt;p&gt;We mathematically compare three competing definitions of group-level
nondiscrimination: demographic parity, equalized odds, and calibration. Using
the theoretical framework of Friedler et al., we study the properties of each
definition under various worldviews, which are assumptions about how, if at
all, the observed data is biased. We prove that different worldviews call for
different definitions of fairness, and we specify when it is appropriate to use
demographic parity and equalized odds. In addition, we argue that calibration
is unsuitable for the purpose of ensuring nondiscrimination. Finally, we define
a worldview that is more realistic than the previously considered ones, and we
introduce a new notion of fairness that is suitable for this worldview.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_S/0/1/0/all/0/1&quot;&gt;Samuel Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschantz_M/0/1/0/all/0/1&quot;&gt;Michael Carl Tschantz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08646">
<title>The Disparate Effects of Strategic Classification. (arXiv:1808.08646v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08646</link>
<description rdf:parseType="Literal">&lt;p&gt;When consequential decisions are informed by algorithmic input, individuals
may feel compelled to alter their behavior in order to gain a system&apos;s
approval. Previous models of agent responsiveness, termed &quot;strategic
manipulation,&quot; have analyzed the interaction between a learner and agents in a
world where all agents are equally able to manipulate their features in an
attempt to &quot;trick&quot; a published classifier. In cases of real world
classification, however, an agent&apos;s ability to adapt to an algorithm, is not
simply a function of her personal interest in receiving a positive
classification, but is bound up in a complex web of social factors that affect
her ability to pursue certain action responses. In this paper, we adapt models
of strategic manipulation to better capture dynamics that may arise in a
setting of social inequality wherein candidate groups face different costs to
manipulation. We find that whenever one group&apos;s costs are higher than the
other&apos;s, the learner&apos;s equilibrium strategy exhibits an inequality-reinforcing
phenomenon wherein the learner erroneously admits some members of the
advantaged group, while erroneously excluding some members of the disadvantaged
group. We also consider the effects of potential interventions in which a
learner can subsidize members of the disadvantaged group, lowering their costs
in order to improve her own classification performance. Here we encounter a
paradoxical result: there exist cases in which providing a subsidy improves
only the learner&apos;s utility while actually making both candidate groups
worse-off--even the group receiving the subsidy. Our results reveal the
potentially adverse social ramifications of deploying tools that attempt to
evaluate an individual&apos;s &quot;quality&quot; when agents&apos; capacities to adaptively
respond differ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lily Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Immorlica_N/0/1/0/all/0/1&quot;&gt;Nicole Immorlica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1&quot;&gt;Jennifer Wortman Vaughan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08763">
<title>On the convergence of optimistic policy iteration for stochastic shortest path problem. (arXiv:1808.08763v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08763</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we prove some convergence results of a special case of
optimistic policy iteration algorithm for stochastic shortest path problem. We
consider the Monte Carlo and $TD(\lambda)$ methods for the policy evaluation
step under the condition that the termination state will be reached almost
surely.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanlong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08765">
<title>Identifiability of Low-Rank Sparse Component Analysis. (arXiv:1808.08765v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.08765</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse component analysis (SCA) is the following problem: Given an input
matrix $M$ and an integer $r$, find a dictionary $D$ with $r$ columns and a
sparse matrix $B$ with $r$ rows such that $M \approx DB$. A key issue in SCA is
identifiability, that is, characterizing the conditions under which $D$ and $B$
are essentially unique (that is, they are unique up to permutation and scaling
of the columns of $D$ and rows of $B$). Although SCA has been vastly
investigated in the last two decades, only a few works have tackled this issue
in the deterministic scenario, and no work provides reasonable bounds in the
minimum number of data points (that is, columns of $M$) that leads to
identifiability. In this work, we provide new results in the deterministic
scenario when the data has a low-rank structure, that is, when $D$ has rank
$r$, drastically improving with respect to previous results. In particular, we
show that if each column of $B$ contains at least $s$ zeros then
$\mathcal{O}(r^3/s^2)$ data points are sufficient to obtain an essentially
unique decomposition, as long as these data points are well spread among the
subspaces spanned by $r-1$ columns of $D$. This implies for example that for a
fixed proportion of zeros (constant and independent of $r$, e.g., 10\% of zero
entries in $B$), one only requires $O(r)$ data points to guarantee
identifiability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my E. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gillis_N/0/1/0/all/0/1&quot;&gt;Nicolas Gillis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08784">
<title>Sparsity in Deep Neural Networks - An Empirical Investigation with TensorQuant. (arXiv:1808.08784v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08784</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is finding its way into the embedded world with applications
such as autonomous driving, smart sensors and aug- mented reality. However, the
computation of deep neural networks is demanding in energy, compute power and
memory. Various approaches have been investigated to reduce the necessary
resources, one of which is to leverage the sparsity occurring in deep neural
networks due to the high levels of redundancy in the network parameters. It has
been shown that sparsity can be promoted specifically and the achieved sparsity
can be very high. But in many cases the methods are evaluated on rather small
topologies. It is not clear if the results transfer onto deeper topologies. In
this paper, the TensorQuant toolbox has been extended to offer a platform to
investigate sparsity, especially in deeper models. Several practical relevant
topologies for varying classification problem sizes are investigated to show
the differences in sparsity for activations, weights and gradients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loroch_D/0/1/0/all/0/1&quot;&gt;Dominik Marek Loroch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1&quot;&gt;Franz-Josef Pfreundt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wehn_N/0/1/0/all/0/1&quot;&gt;Norbert Wehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08811">
<title>Exponential inequalities for nonstationary Markov Chains. (arXiv:1808.08811v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.08811</link>
<description rdf:parseType="Literal">&lt;p&gt;Exponential inequalities are main tools in machine learning theory. To prove
exponential inequalities for non i.i.d random variables allows to extend many
learning techniques to these variables. Indeed, much work has been done both on
inequalities and learning theory for time series, in the past 15 years.
However, for the non independent case, almost all the results concern
stationary time series. This excludes many important applications: for example
any series with a periodic behaviour is non-stationary. In this paper, we
extend the basic tools of Dedecker and Fan (2015) to nonstationary Markov
chains. As an application, we provide a Bernstein-type inequality, and we
deduce risk bounds for the prediction of periodic autoregressive processes with
an unknown period.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1&quot;&gt;Pierre Alquier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doukhan_P/0/1/0/all/0/1&quot;&gt;Paul Doukhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiequan Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08833">
<title>Gradient-based Training of Slow Feature Analysis by Differentiable Approximate Whitening. (arXiv:1808.08833v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08833</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes Power Slow Feature Analysis, a gradient-based method to
extract temporally-slow features from a high-dimensional input stream that
varies on a faster time-scale, and a variant of Slow Feature Analysis (SFA).
While displaying performance comparable to hierarchical extensions to the SFA
algorithm, such as Hierarchical Slow Feature Analysis, for a small number of
output-features, our algorithm allows end-to-end training of arbitrary
differentiable approximators (e.g., deep neural networks). We provide
experimental evidence that PowerSFA is able to extract meaningful and
informative low-dimensional features in the case of a) synthetic
low-dimensional data, b) visual data, and also for c) a general dataset for
which symmetric non-temporal relations between points can be defined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuler_M/0/1/0/all/0/1&quot;&gt;Merlin Sch&amp;#xfc;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hlynsson_H/0/1/0/all/0/1&quot;&gt;Hlynur Dav&amp;#xed;&amp;#xf0; Hlynsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiskott_L/0/1/0/all/0/1&quot;&gt;Laurenz Wiskott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1607.07607">
<title>Adaptive Nonnegative Matrix Factorization and Measure Comparisons for Recommender Systems. (arXiv:1607.07607v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1607.07607</link>
<description rdf:parseType="Literal">&lt;p&gt;The Nonnegative Matrix Factorization (NMF) of the rating matrix has shown to
be an effective method to tackle the recommendation problem. In this paper we
propose new methods based on the NMF of the rating matrix and we compare them
with some classical algorithms such as the SVD and the regularized and
unregularized non-negative matrix factorization approach. In particular a new
algorithm is obtained changing adaptively the function to be minimized at each
step, realizing a sort of dynamic prior strategy. Another algorithm is obtained
modifying the function to be minimized in the NMF formulation by enforcing the
reconstruction of the unknown ratings toward a prior term. We then combine
different methods obtaining two mixed strategies which turn out to be very
effective in the reconstruction of missing observations. We perform a
thoughtful comparison of different methods on the basis of several evaluation
measures. We consider in particular rating, classification and ranking measures
showing that the algorithm obtaining the best score for a given measure is in
general the best also when different measures are considered, lowering the
interest in designing specific evaluation measures. The algorithms have been
tested on different datasets, in particular the 1M, and 10M MovieLens datasets
containing ratings on movies, the Jester dataset with ranting on jokes and
Amazon Fine Foods dataset with ratings on foods. The comparison of the
different algorithms, shows the good performance of methods employing both an
explicit and an implicit regularization scheme. Moreover we can get a boost by
mixed strategies combining a fast method with a more accurate one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_G/0/1/0/all/0/1&quot;&gt;Gianna M. Del Corso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romani_F/0/1/0/all/0/1&quot;&gt;Francesco Romani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.02365">
<title>NonSTOP: A NonSTationary Online Prediction Method for Time Series. (arXiv:1611.02365v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.02365</link>
<description rdf:parseType="Literal">&lt;p&gt;We present online prediction methods for time series that let us explicitly
handle nonstationary artifacts (e.g. trend and seasonality) present in most
real time series. Specifically, we show that applying appropriate
transformations to such time series before prediction can lead to improved
theoretical and empirical prediction performance. Moreover, since these
transformations are usually unknown, we employ the learning with experts
setting to develop a fully online method (NonSTOP-NonSTationary Online
Prediction) for predicting nonstationary time series. This framework allows for
seasonality and/or other trends in univariate time series and cointegration in
multivariate time series. Our algorithms and regret analysis subsume recent
related work while significantly expanding the applicability of such methods.
For all the methods, we provide sub-linear regret bounds using relaxed
assumptions. The theoretical guarantees do not fully capture the benefits of
the transformations, thus we provide a data-dependent analysis of the
follow-the-leader algorithm that provides insight into the success of using
such transformations. We support all of our results with experiments on
simulated and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Christopher Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bijral_A/0/1/0/all/0/1&quot;&gt;Avleen Bijral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ferres_J/0/1/0/all/0/1&quot;&gt;Juan Lavista Ferres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02963">
<title>An Interactive Greedy Approach to Group Sparsity in High Dimensions. (arXiv:1707.02963v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02963</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparsity learning with known grouping structure has received considerable
attention due to wide modern applications in high-dimensional data analysis.
Although advantages of using group information have been well-studied by
shrinkage-based approaches, benefits of group sparsity have not been
well-documented for greedy-type methods, which much limits our understanding
and use of this important class of methods. In this paper, generalizing from a
popular forward-backward greedy approach, we propose a new interactive greedy
algorithm for group sparsity learning and prove that the proposed greedy-type
algorithm attains the desired benefits of group sparsity under high dimensional
settings. An estimation error bound refining other existing methods and a
guarantee for group support recovery are also established simultaneously. In
addition, we incorporate a general M-estimation framework and introduce an
interactive feature to allow extra algorithm flexibility without compromise in
theoretical properties. The promising use of our proposal is demonstrated
through numerical evaluations including a real industrial application in human
activity recognition at home. Supplementary materials for this article are
available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qian_W/0/1/0/all/0/1&quot;&gt;Wei Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wending Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sogawa_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Sogawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fujimaki_R/0/1/0/all/0/1&quot;&gt;Ryohei Fujimaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xitong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.04476">
<title>Collaborative Nested Sampling: Big Data vs. complex physical models. (arXiv:1707.04476v4 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1707.04476</link>
<description rdf:parseType="Literal">&lt;p&gt;The data torrent unleashed by current and upcoming instruments requires
scalable analysis methods. Machine Learning approaches scale well. However,
separating the instrument measurement from the physical effects of interest,
dealing with variable errors, and deriving parameter uncertainties is usually
an afterthought. Classic forward-folding analyses with Markov Chain Monte Carlo
or Nested Sampling enable parameter estimation and model comparison, even for
complex and slow-to-evaluate physical models. However, these approaches require
independent runs for each data set, implying an unfeasible number of model
evaluations in the Big Data regime. Here we present a new algorithm,
collaborative nested sampling, for deriving parameter probability distributions
for each observation. Importantly, in our method the number of physical model
evaluations scales sub-linearly with the number of data sets, and we make no
assumptions about homogeneous errors, Gaussianity, the form of the model or
heterogeneity/completeness of the observations. Collaborative nested sampling
has immediate application in speeding up analyses of large surveys,
integral-field-unit observations, and Monte Carlo simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buchner_J/0/1/0/all/0/1&quot;&gt;Johannes Buchner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07954">
<title>Bayesian Cluster Enumeration Criterion for Unsupervised Learning. (arXiv:1710.07954v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07954</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive a new Bayesian Information Criterion (BIC) by formulating the
problem of estimating the number of clusters in an observed data set as
maximization of the posterior probability of the candidate models. Given that
some mild assumptions are satisfied, we provide a general BIC expression for a
broad class of data distributions. This serves as a starting point when
deriving the BIC for specific distributions. Along this line, we provide a
closed-form BIC expression for multivariate Gaussian distributed variables. We
show that incorporating the data structure of the clustering problem into the
derivation of the BIC results in an expression whose penalty term is different
from that of the original BIC. We propose a two-step cluster enumeration
algorithm. First, a model-based unsupervised learning algorithm partitions the
data according to a given set of candidate models. Subsequently, the number of
clusters is determined as the one associated with the model for which the
proposed BIC is maximal. The performance of the proposed two-step algorithm is
tested using synthetic and real data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Teklehaymanot_F/0/1/0/all/0/1&quot;&gt;Freweyni K. Teklehaymanot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Muma_M/0/1/0/all/0/1&quot;&gt;Michael Muma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zoubir_A/0/1/0/all/0/1&quot;&gt;Abdelhak M. Zoubir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08473">
<title>A Unified Framework for Long Range and Cold Start Forecasting of Seasonal Profiles in Time Series. (arXiv:1710.08473v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08473</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing long-range forecasts is a fundamental challenge in time series
modeling, which is only compounded by the challenge of having to form such
forecasts when a time series has never previously been observed. The latter
challenge is the time series version of the cold-start problem seen in
recommender systems which, to our knowledge, has not been addressed in previous
work. A similar problem occurs when a long range forecast is required after
only observing a small number of time points --- a warm start forecast. With
these aims in mind, we focus on forecasting seasonal profiles---or baseline
demand---for periods on the order of a year in three cases: the long range case
with multiple previously observed seasonal profiles, the cold start case with
no previous observed seasonal profiles, and the warm start case with only a
single partially observed profile. Classical time series approaches that
perform iterated step-ahead forecasts based on previous observations struggle
to provide accurate long range predictions; in settings with little to no
observed data, such approaches are simply not applicable. Instead, we present a
straightforward framework which combines ideas from high-dimensional regression
and matrix factorization on a carefully constructed data matrix. Key to our
formulation and resulting performance is leveraging (1) repeated patterns over
fixed periods of time and across series, and (2) metadata associated with the
individual series; without this additional data, the cold-start/warm-start
problems are nearly impossible to solve. We demonstrate that our framework can
accurately forecast an array of seasonal profiles on multiple large scale
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Christopher Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tank_A/0/1/0/all/0/1&quot;&gt;Alex Tank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Greaves_Tunnell_A/0/1/0/all/0/1&quot;&gt;Alec Greaves-Tunnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07318">
<title>Variable Prioritization in Nonlinear Black Box Methods: A Genetic Association Case Study. (arXiv:1801.07318v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07318</link>
<description rdf:parseType="Literal">&lt;p&gt;The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the &quot;RelATive cEntrality&quot; (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other &quot;black box&quot; methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Crawford_L/0/1/0/all/0/1&quot;&gt;Lorin Crawford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flaxman_S/0/1/0/all/0/1&quot;&gt;Seth R. Flaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Runcie_D/0/1/0/all/0/1&quot;&gt;Daniel E. Runcie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+West_M/0/1/0/all/0/1&quot;&gt;Mike West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07753">
<title>Sample Complexity of Sparse System Identification Problem. (arXiv:1803.07753v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07753</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the system identification problem for sparse linear
time-invariant systems. We propose a sparsity promoting block-regularized
estimator to identify the dynamics of the system with only a limited number of
input-state data samples. We characterize the properties of this estimator
under high-dimensional scaling, where the growth rate of the system dimension
is comparable to or even faster than that of the number of available sample
trajectories. In particular, using contemporary results on high-dimensional
statistics, we show that the proposed estimator results in a small element-wise
error, provided that the number of sample trajectories is above a threshold.
This threshold depends polynomially on the size of each block and the number of
nonzero elements at different rows of input and state matrices, but only
logarithmically on the system dimension. A by-product of this result is that
the number of sample trajectories required for sparse system identification is
significantly smaller than the dimension of the system. Furthermore, we show
that, unlike the recently celebrated least-squares estimators for system
identification problems, the method developed in this work is capable of
\textit{exact recovery} of the underlying sparsity structure of the system with
the aforementioned number of data samples. Extensive case studies on
synthetically generated systems, physical mass-spring networks, and multi-agent
systems are offered to demonstrate the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fattahi_S/0/1/0/all/0/1&quot;&gt;Salar Fattahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1&quot;&gt;Somayeh Sojoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08680">
<title>Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization. (arXiv:1803.08680v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08680</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have lately shown tremendous performance in various
applications including vision and speech processing tasks. However, alongside
their ability to perform these tasks with such high accuracy, it has been shown
that they are highly susceptible to adversarial attacks: a small change in the
input would cause the network to err with high confidence. This phenomenon
exposes an inherent fault in these networks and their ability to generalize
well. For this reason, providing robustness to adversarial attacks is an
important challenge in networks training, which has led to extensive research.
In this work, we suggest a theoretically inspired novel approach to improve the
networks&apos; robustness. Our method applies regularization using the Frobenius
norm of the Jacobian of the network, which is applied as post-processing, after
regular training has finished. We demonstrate empirically that it leads to
enhanced robustness results with a minimal change in the original network&apos;s
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakubovitz_D/0/1/0/all/0/1&quot;&gt;Daniel Jakubovitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12547">
<title>Long-time predictive modeling of nonlinear dynamical systems using neural networks. (arXiv:1805.12547v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12547</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the use of feedforward neural networks (FNN) to develop models of
nonlinear dynamical systems from data. Emphasis is placed on predictions at
long times, with limited data availability. Inspired by global stability
analysis, and the observation of the strong correlation between the local error
and the maximum singular value of the Jacobian of the ANN, we introduce
Jacobian regularization in the loss function. This regularization suppresses
the sensitivity of the prediction to the local error and is shown to improve
accuracy and robustness. Comparison between the proposed approach and sparse
polynomial regression is presented in numerical examples ranging from simple
ODE systems to nonlinear PDE systems including vortex shedding behind a
cylinder, and instability-driven buoyant mixing flow. Furthermore, limitations
of feedforward neural networks are highlighted, especially when the training
data does not include a low dimensional attractor. Strategies of data
augmentation are presented as remedies to address these issues to a certain
extent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shaowu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duraisamy_K/0/1/0/all/0/1&quot;&gt;Karthik Duraisamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00552">
<title>Bayesian approach to model-based extrapolation of nuclear observables. (arXiv:1806.00552v3 [nucl-th] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00552</link>
<description rdf:parseType="Literal">&lt;p&gt;The mass, or binding energy, is the basis property of the atomic nucleus. It
determines its stability, and reaction and decay rates. Quantifying the nuclear
binding is important for understanding the origin of elements in the universe.
The astrophysical processes responsible for the nucleosynthesis in stars often
take place far from the valley of stability, where experimental masses are not
known. In such cases, missing nuclear information must be provided by
theoretical predictions using extreme extrapolations. Bayesian machine learning
techniques can be applied to improve predictions by taking full advantage of
the information contained in the deviations between experimental and calculated
masses. We consider 10 global models based on nuclear Density Functional Theory
as well as two more phenomenological mass models. The emulators of S2n
residuals and credibility intervals defining theoretical error bars are
constructed using Bayesian Gaussian processes and Bayesian neural networks. We
consider a large training dataset pertaining to nuclei whose masses were
measured before 2003. For the testing datasets, we considered those exotic
nuclei whose masses have been determined after 2003. We then carried out
extrapolations towards the 2n dripline. While both Gaussian processes and
Bayesian neural networks reduce the rms deviation from experiment
significantly, GP offers a better and much more stable performance. The
increase in the predictive power is quite astonishing: the resulting rms
deviations from experiment on the testing dataset are similar to those of more
phenomenological models. The empirical coverage probability curves we obtain
match very well the reference values which is highly desirable to ensure
honesty of uncertainty quantification, and the estimated credibility intervals
on predictions make it possible to evaluate predictive power of individual
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Neufcourt_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;o Neufcourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuchen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Nazarewicz_W/0/1/0/all/0/1&quot;&gt;Witold Nazarewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-th/1/au:+Viens_F/0/1/0/all/0/1&quot;&gt;Frederi Viens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03432">
<title>Hierarchical Clustering with Prior Knowledge. (arXiv:1806.03432v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03432</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical clustering is a class of algorithms that seeks to build a
hierarchy of clusters. It has been the dominant approach to constructing
embedded classification schemes since it outputs dendrograms, which capture the
hierarchical relationship among members at all levels of granularity,
simultaneously. Being greedy in the algorithmic sense, a hierarchical
clustering partitions data at every step solely based on a similarity /
dissimilarity measure. The clustering results oftentimes depend on not only the
distribution of the underlying data, but also the choice of dissimilarity
measure and the clustering algorithm. In this paper, we propose a method to
incorporate prior domain knowledge about entity relationship into the
hierarchical clustering. Specifically, we use a distance function in
ultrametric space to encode the external ontological information. We show that
popular linkage-based algorithms can faithfully recover the encoded structure.
Similar to some regularized machine learning techniques, we add this distance
as a penalty term to the original pairwise distance to regulate the final
structure of the dendrogram. As a case study, we applied this method on real
data in the building of a customer behavior based product taxonomy for an
Amazon service, leveraging the information from a larger Amazon-wide browse
structure. The method is useful when one wants to leverage the relational
information from external sources, or the data used to generate the distance
matrix is noisy and sparse. Our work falls in the category of semi-supervised
or constrained clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaofei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhavala_S/0/1/0/all/0/1&quot;&gt;Satya Dhavala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05512">
<title>NetScore: Towards Universal Metrics for Large-scale Performance Analysis of Deep Neural Networks for Practical On-Device Edge Usage. (arXiv:1806.05512v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05512</link>
<description rdf:parseType="Literal">&lt;p&gt;Much of the focus in the design of deep neural networks has been on improving
accuracy, leading to more powerful yet highly complex network architectures
that are difficult to deploy in practical scenarios, particularly on edge
devices such as mobile and other consumer devices given their high
computational and memory requirements. As a result, there has been a recent
interest in the design of quantitative metrics for evaluating deep neural
networks that accounts for more than just model accuracy as the sole indicator
of network performance. In this study, we continue the conversation towards
universal metrics for evaluating the performance of deep neural networks for
practical on-device edge usage. In particular, we propose a new balanced metric
called NetScore, which is designed specifically to provide a quantitative
assessment of the balance between accuracy, computational complexity, and
network architecture complexity of a deep neural network, which is important
for on-device edge operation. In what is one of the largest comparative
analysis between deep neural networks in literature, the NetScore metric, the
top-1 accuracy metric, and the popular information density metric were compared
across a diverse set of 60 different deep convolutional neural networks for
image classification on the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC 2012) dataset. The evaluation results across these three metrics for
this diverse set of networks are presented in this study to act as a reference
guide for practitioners in the field. The proposed NetScore metric, along with
the other tested metrics, are by no means perfect, but the hope is to push the
conversation towards better universal metrics for evaluating deep neural
networks for use in practical on-device edge scenarios to help guide
practitioners in model design for such scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08362">
<title>An Intersectional Definition of Fairness. (arXiv:1807.08362v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08362</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a measure of fairness for algorithms and data with regard to
multiple protected attributes. Our proposed definition, differential fairness,
is informed by the framework of intersectionality, which analyzes how
interlocking systems of power and oppression affect individuals along
overlapping dimensions including race, gender, sexual orientation, class, and
disability. We show that our criterion behaves sensibly for any subset of the
set of protected attributes, and we illustrate links to differential privacy. A
case study on census data demonstrates the utility of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1&quot;&gt;James Foulds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shimei Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00892">
<title>Semi-blind source separation with multichannel variational autoencoder. (arXiv:1808.00892v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00892</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a multichannel source separation technique called the
multichannel variational autoencoder (MVAE) method, which uses a conditional
VAE (CVAE) to model and estimate the power spectrograms of the sources in a
mixture. By training the CVAE using the spectrograms of training examples with
source-class labels, we can use the trained decoder distribution as a universal
generative model capable of generating spectrograms conditioned on a specified
class label. By treating the latent space variables and the class label as the
unknown parameters of this generative model, we can develop a
convergence-guaranteed semi-blind source separation algorithm that consists of
iteratively estimating the power spectrograms of the underlying sources as well
as the separation matrices. In experimental evaluations, our MVAE produced
better separation performance than a baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Inoue_S/0/1/0/all/0/1&quot;&gt;Shota Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Makino_S/0/1/0/all/0/1&quot;&gt;Shoji Makino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04523">
<title>Adaptive Sampling for Convex Regression. (arXiv:1808.04523v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04523</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the first principled adaptive-sampling procedure
for learning a convex function in the $L_\infty$ norm, a problem that arises
often in the behavioral and social sciences. We present a function-specific
measure of complexity and use it to prove that, for each convex function
$f_{\star}$, our algorithm nearly attains the information-theoretically
optimal, function-specific error rate. We also corroborate our theoretical
contributions with numerical experiments, finding that our method substantially
outperforms passive, uniform sampling for favorable synthetic and data-derived
functions in low-noise settings with large sampling budgets. Our results also
suggest an idealized &quot;oracle strategy&quot;, which we use to gauge the potential
advance of any adaptive-sampling strategy over passive sampling, for any given
convex function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchow_J/0/1/0/all/0/1&quot;&gt;Jordan W. Suchow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05092">
<title>ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder. (arXiv:1808.05092v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05092</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaneko_T/0/1/0/all/0/1&quot;&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kou Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hojo_N/0/1/0/all/0/1&quot;&gt;Nobukatsu Hojo&lt;/a&gt;</dc:creator>
</item></rdf:RDF>