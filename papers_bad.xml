<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02637"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02879"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02886"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02963"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03039"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03083"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03210"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03223"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.08028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04854"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00643"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05626"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02629"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02682"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02701"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02816"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02872"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02873"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02901"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02910"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03026"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03091"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03135"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03139"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03148"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03149"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03155"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03173"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03205"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.06404"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.06655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07708"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06061"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05787"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05187"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08680"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01808"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.02581">
<title>The Goldilocks zone: Towards better understanding of neural network loss landscapes. (arXiv:1807.02581v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02581</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the loss landscape of fully-connected neural networks using
random, low-dimensional hyperplanes and hyperspheres. Evaluating the Hessian,
$H$, of the loss function on these hypersurfaces, we observe 1) an unusual
excess of the number of positive eigenvalues of $H$, and 2) a large value of
$\mathrm{Tr}(H) / |H|$ at a well defined range of configuration space radii,
corresponding to a thick, hollow, spherical shell we refer to as the
\textit{Goldilocks zone}. We observe this effect for fully-connected neural
networks over a range of network widths and depths on MNIST and CIFAR-10 with
the $\mathrm{ReLU}$ non-linearity. The effect is not observed for the $\tanh$
non-linearity. Using our observations, we demonstrate a close connection
between the Goldilocks zone, measures of local convexity/prevalence of positive
curvature, and the suitability of a network initialization. We show that the
high and stable accuracy reached when optimizing on random, low-dimensional
hypersurfaces is directly related to the overlap between the hypersurface and
the Goldilocks zone. We note that common initialization techniques initialize
neural networks in this particular region of unusually high convexity, and
offer a geometric intuition for their success. We take steps towards an
analytic description of the general features of the loss function geometry,
exploring its anisotropy and strong radial dependence. We support our
theoretical results with experiments. Furthermore, we demonstrate that
initializing a neural network at a number of points and selecting for high
measures of local convexity such as $\mathrm{Tr}(H) / |H|$, number of positive
eigenvalues of $H$, or low initial loss, leads to statistically significantly
faster training on MNIST. Based on our observations, we hypothesize that the
Goldilocks zone contains a high density of suitable initialization
configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1&quot;&gt;Stanislav Fort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherlis_A/0/1/0/all/0/1&quot;&gt;Adam Scherlis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02621">
<title>Reservoir Computing Universality With Stochastic Inputs. (arXiv:1807.02621v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1807.02621</link>
<description rdf:parseType="Literal">&lt;p&gt;The universal approximation properties with respect to $L ^p $-type criteria
of three important families of reservoir computers with stochastic
discrete-time semi-infinite inputs is shown. First, it is proved that linear
reservoir systems with either polynomial or neural network readout maps are
universal. More importantly, it is proved that the same property holds for two
families with linear readouts, namely, trigonometric state-affine systems and
echo state networks, which are the most widely used reservoir systems in
applications. The linearity in the readouts is a key feature in supervised
machine learning applications. It guarantees that these systems can be used in
high-dimensional situations and in the presence of large datasets. The $L ^p $
criteria used in this paper allow the formulation of universality results that
do not necessarily impose almost sure uniform boundedness in the inputs or the
fading memory property in the filter that needs to be approximated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonon_L/0/1/0/all/0/1&quot;&gt;Lukas Gonon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1&quot;&gt;Juan-Pablo Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02754">
<title>Model-Free Optimization Using Eagle Perching Optimizer. (arXiv:1807.02754v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.02754</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper proposes a novel nature-inspired technique of optimization. It
mimics the perching nature of eagles and uses mathematical formulations to
introduce a new addition to metaheuristic algorithms. The nature of the
proposed algorithm is based on exploration and exploitation. The proposed
algorithm is developed into two versions with some modifications. In the first
phase, it undergoes a rigorous analysis to find out their performance. In the
second phase it is benchmarked using ten functions of two categories; uni-modal
functions and multi-modal functions. In the third phase, we conducted a
detailed analysis of the algorithm by exploiting its controlling units or
variables. In the fourth and last phase, we consider real world optimization
problems with constraints. Both versions of the algorithm show an appreciable
performance, but analysis puts more weight to the modified version. The
competitive analysis shows that the proposed algorithm outperforms the other
tested metaheuristic algorithms. The proposed method has better robustness and
computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Ameer Tamoor Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senior_S/0/1/0/all/0/1&quot;&gt;Shuai Li Senior&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanimirovic_P/0/1/0/all/0/1&quot;&gt;Predrag S. Stanimirovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinyan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03010">
<title>XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary Neural Network Inference. (arXiv:1807.03010v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03010</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary Neural Networks (BNNs) are promising to deliver accuracy comparable to
conventional deep neural networks at a fraction of the cost in terms of memory
and energy. In this paper, we introduce the XNOR Neural Engine (XNE), a fully
digital configurable hardware accelerator IP for BNNs, integrated within a
microcontroller unit (MCU) equipped with an autonomous I/O subsystem and hybrid
SRAM / standard cell memory. The XNE is able to fully compute convolutional and
dense layers in autonomy or in cooperation with the core in the MCU to realize
more complex behaviors. We show post-synthesis results in 65nm and 22nm
technology for the XNE IP and post-layout results in 22nm for the full MCU
indicating that this system can drop the energy cost per binary operation to
21.6fJ per operation at 0.4V, and at the same time is flexible and performant
enough to execute state-of-the-art BNN topologies such as ResNet-34 in less
than 2.2mJ per frame at 8.9 fps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1&quot;&gt;Francesco Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiavone_P/0/1/0/all/0/1&quot;&gt;Pasquale Davide Schiavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00254">
<title>Joint Bootstrapping Machines for High Confidence Relation Extraction. (arXiv:1805.00254v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.00254</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised bootstrapping techniques for relationship extraction from
text iteratively expand a set of initial seed instances. Due to the lack of
labeled data, a key challenge in bootstrapping is semantic drift: if a false
positive instance is added during an iteration, then all following iterations
are contaminated. We introduce BREX, a new bootstrapping method that protects
against such contamination by highly effective confidence assessment. This is
achieved by using entity and template seeds jointly (as opposed to just one as
in previous work), by expanding entities and templates in parallel and in a
mutually constraining fashion in each iteration and by introducing
higherquality similarity measures for templates. Experimental results show that
BREX achieves an F1 that is 0.13 (0.87 vs. 0.74) better than the state of the
art for four relationships.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Pankaj Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1&quot;&gt;Benjamin Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1&quot;&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02612">
<title>Gradient Hyperalignment for multi-subject fMRI data alignment. (arXiv:1807.02612v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02612</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-subject fMRI data analysis is an interesting and challenging problem in
human brain decoding studies. The inherent anatomical and functional
variability across subjects make it necessary to do both anatomical and
functional alignment before classification analysis. Besides, when it comes to
big data, time complexity becomes a problem that cannot be ignored. This paper
proposes Gradient Hyperalignment (Gradient-HA) as a gradient-based functional
alignment method that is suitable for multi-subject fMRI datasets with large
amounts of samples and voxels. The advantage of Gradient-HA is that it can
solve independence and high dimension problems by using Independent Component
Analysis (ICA) and Stochastic Gradient Ascent (SGA). Validation using
multi-classification tasks on big data demonstrates that Gradient-HA method has
less time complexity and better or comparable performance compared with other
state-of-the-art functional alignment methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tonglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yousefnezhad_M/0/1/0/all/0/1&quot;&gt;Muhammad Yousefnezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daoqiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02637">
<title>Recommender system for learning SQL using hints. (arXiv:1807.02637v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.02637</link>
<description rdf:parseType="Literal">&lt;p&gt;Today&apos;s software industry requires individuals who are proficient in as many
programming languages as possible. Structured query language (SQL), as an
adopted standard, is no exception, as it is the most widely used query language
to retrieve and manipulate data. However, the process of learning SQL turns out
to be challenging. The need for a computer-aided solution to help users learn
SQL and improve their proficiency is vital. In this study, we present a new
approach to help users conceptualize basic building blocks of the language
faster and more efficiently. The adaptive design of the proposed approach aids
users in learning SQL by supporting their own path to the solution and
employing successful previous attempts, while not enforcing the ideal solution
provided by the instructor. Furthermore, we perform an empirical evaluation
with 93 participants and demonstrate that the employment of hints is
successful, being especially beneficial for users with lower prior knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavbic_D/0/1/0/all/0/1&quot;&gt;Dejan Lavbi&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matek_T/0/1/0/all/0/1&quot;&gt;Tadej Matek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zrnec_A/0/1/0/all/0/1&quot;&gt;Alja&amp;#x17e; Zrnec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02879">
<title>Reasoning about exceptions in ontologies: from the lexicographic closure to the skeptical closure. (arXiv:1807.02879v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.02879</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning about exceptions in ontologies is nowadays one of the challenges
the description logics community is facing. The paper describes a preferential
approach for dealing with exceptions in Description Logics, based on the
rational closure. The rational closure has the merit of providing a simple and
efficient approach for reasoning with exceptions, but it does not allow
independent handling of the inheritance of different defeasible properties of
concepts. In this work we outline a possible solution to this problem by
introducing a variant of the lexicographical closure, that we call skeptical
closure, which requires to construct a single base. We develop a bi-preference
semantics semantics for defining a characterization of the skeptical closure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giordano_L/0/1/0/all/0/1&quot;&gt;Laura Giordano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gliozzi_V/0/1/0/all/0/1&quot;&gt;Valentina Gliozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02886">
<title>Auto Deep Compression by Reinforcement Learning Based Actor-Critic Structure. (arXiv:1807.02886v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02886</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based compression is an effective, facilitating, and expanded model of
neural network models with limited computing and low power. However,
conventional models of compression techniques utilize crafted features [2,3,12]
and explore specialized areas for exploration and design of large spaces in
terms of size, speed, and accuracy, which usually have returns Less and time is
up. This paper will effectively analyze deep auto compression (ADC) and
reinforcement learning strength in an effective sample and space design, and
improve the compression quality of the model. The results of compression of the
advanced model are obtained without any human effort and in a completely
automated way. With a 4- fold reduction in FLOP, the accuracy of 2.8% is higher
than the manual compression model for VGG-16 in ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hakkak_H/0/1/0/all/0/1&quot;&gt;Hamed Hakkak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02963">
<title>Jointly learning relevant subgraph patterns and nonlinear models of their indicators. (arXiv:1807.02963v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02963</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification and regression in which the inputs are graphs of arbitrary
size and shape have been paid attention in various fields such as computational
chemistry and bioinformatics. Subgraph indicators are often used as the most
fundamental features, but the number of possible subgraph patterns are
intractably large due to the combinatorial explosion. We propose a novel
efficient algorithm to jointly learn relevant subgraph patterns and nonlinear
models of their indicators. Previous methods for such joint learning of
subgraph features and models are based on search for single best subgraph
features with specific pruning and boosting procedures of adding their
indicators one by one, which result in linear models of subgraph indicators. In
contrast, the proposed approach is based on directly learning regression trees
for graph inputs using a newly derived bound of the total sum of squares for
data partitions by a given subgraph feature, and thus can learn nonlinear
models through standard gradient boosting. An illustrative example we call the
Graph-XOR problem to consider nonlinearity, numerical experiments with real
datasets, and scalability comparisons to naive approaches using explicit
pattern enumeration are also presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirakawa_R/0/1/0/all/0/1&quot;&gt;Ryo Shirakawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokoyama_Y/0/1/0/all/0/1&quot;&gt;Yusei Yokoyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okazaki_F/0/1/0/all/0/1&quot;&gt;Fumiya Okazaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takigawa_I/0/1/0/all/0/1&quot;&gt;Ichigaku Takigawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03024">
<title>Constraint-based Causal Discovery for Non-Linear Structural Causal Models with Cycles and Latent Confounders. (arXiv:1807.03024v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03024</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of causal discovery from data, making use of the
recently proposed causal modeling framework of modular structural causal models
(mSCM) to handle cycles, latent confounders and non-linearities. We introduce
{\sigma}-connection graphs ({\sigma}-CG), a new class of mixed graphs
(containing undirected, bidirected and directed edges) with additional
structure, and extend the concept of {\sigma}-separation, the appropriate
generalization of the well-known notion of d-separation in this setting, to
apply to {\sigma}-CGs. We prove the closedness of {\sigma}-separation under
marginalisation and conditioning and exploit this to implement a test of
{\sigma}-separation on a {\sigma}-CG. This then leads us to the first causal
discovery algorithm that can handle non-linear functional relations, latent
confounders, cyclic causal relationships, and data from different (stochastic)
perfect interventions. As a proof of concept, we show on synthetic data how
well the algorithm recovers features of the causal graph of modular structural
causal models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Forre_P/0/1/0/all/0/1&quot;&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mooij_J/0/1/0/all/0/1&quot;&gt;Joris M. Mooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03039">
<title>Glow: Generative Flow with Invertible 1x1 Convolutions. (arXiv:1807.03039v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03039</link>
<description rdf:parseType="Literal">&lt;p&gt;Flow-based generative models (Dinh et al., 2014) are conceptually attractive
due to tractability of the exact log-likelihood, tractability of exact
latent-variable inference, and parallelizability of both training and
synthesis. In this paper we propose &amp;lt;i&amp;gt;Glow&amp;lt;/i&amp;gt;, a simple type of generative
flow using an invertible 1x1 convolution. Using our method we demonstrate a
significant improvement in log-likelihood on standard benchmarks. Perhaps most
strikingly, we demonstrate that a generative model optimized towards the plain
log-likelihood objective is capable of efficient realistic-looking synthesis
and manipulation of large images. The code for our model is available at &amp;lt;a
href=&quot;https://github.com/openai/glow&quot;&amp;gt;https://github.com/openai/glow&amp;lt;/a&amp;gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kingma_D/0/1/0/all/0/1&quot;&gt;Diederik P. Kingma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhariwal_P/0/1/0/all/0/1&quot;&gt;Prafulla Dhariwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03083">
<title>Evaluating Active Learning Heuristics for Sequential Diagnosis. (arXiv:1807.03083v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.03083</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a malfunctioning system, sequential diagnosis aims at identifying the
root cause of the failure in terms of abnormally behaving system components. As
initial system observations usually do not suffice to deterministically pin
down just one explanation of the system&apos;s misbehavior, additional system
measurements can help to differentiate between possible explanations. The goal
is to restrict the space of explanations until there is only one (highly
probable) explanation left. To achieve this with a minimal-cost set of
measurements, various (active learning) heuristics for selecting the best next
measurement have been proposed.
&lt;/p&gt;
&lt;p&gt;We report preliminary results of extensive ongoing experiments with a set of
selection heuristics on real-world diagnosis cases. In particular, we try to
answer questions such as &quot;Is some heuristic always superior to all others?&quot;,
&quot;On which factors does the (relative) performance of the particular heuristics
depend?&quot; or &quot;Under which circumstances should I use which heuristic?&quot;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodler_P/0/1/0/all/0/1&quot;&gt;Patrick Rodler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_W/0/1/0/all/0/1&quot;&gt;Wolfgang Schmid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03210">
<title>Efficient Decentralized Deep Learning by Dynamic Model Averaging. (arXiv:1807.03210v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03210</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an efficient protocol for decentralized training of deep neural
networks from distributed data sources. The proposed protocol allows to handle
different phases of model training equally well and to quickly adapt to concept
drifts. This leads to a reduction of communication by an order of magnitude
compared to periodically communicating state-of-the-art approaches. Moreover,
we derive a communication bound that scales well with the hardness of the
serialized learning problem. The reduction in communication comes at almost no
cost, as the predictive performance remains virtually unchanged. Indeed, the
proposed protocol retains loss bounds of periodically averaging schemes. An
extensive empirical evaluation validates major improvement of the trade-off
between model performance and communication which could be beneficial for
numerous decentralized learning applications, such as autonomous driving, or
voice recognition and image classification on mobile phones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_M/0/1/0/all/0/1&quot;&gt;Michael Kamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adilova_L/0/1/0/all/0/1&quot;&gt;Linara Adilova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sicking_J/0/1/0/all/0/1&quot;&gt;Joachim Sicking&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huger_F/0/1/0/all/0/1&quot;&gt;Fabian H&amp;#xfc;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlicht_P/0/1/0/all/0/1&quot;&gt;Peter Schlicht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirtz_T/0/1/0/all/0/1&quot;&gt;Tim Wirtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wrobel_S/0/1/0/all/0/1&quot;&gt;Stefan Wrobel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03223">
<title>Entropy Maximization for Markov Decision Processes Under Temporal Logic Constraints. (arXiv:1807.03223v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.03223</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of synthesizing a policy that maximizes the entropy of a
Markov decision process (MDP) subject to a temporal logic constraint. Such a
policy minimizes the predictability of the paths it generates, or dually,
maximizes the continual exploration of different paths in an MDP while ensuring
the satisfaction of a temporal logic specification. We first show that the
maximum entropy of an MDP can be finite, infinite or unbounded. We provide
necessary and sufficient conditions under which the maximum entropy of an MDP
is finite, infinite or unbounded. We then present an algorithm to synthesize a
policy that maximizes the entropy of an MDP. The proposed algorithm is based on
a convex optimization problem and runs in time polynomial in the size of the
MDP. We also show that maximizing the entropy of an MDP is equivalent to
maximizing the entropy of the paths that reach a certain set of states in the
MDP. Finally, we extend the algorithm to an MDP subject to a temporal logic
specification. In numerical examples, we demonstrate the proposed method on
different motion planning scenarios and illustrate that as the restrictions
imposed on the paths by a specification increase, the maximum entropy
decreases, which in turn, increases the predictability of paths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Savas_Y/0/1/0/all/0/1&quot;&gt;Yagiz Savas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ornik_M/0/1/0/all/0/1&quot;&gt;Melkior Ornik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cubuktepe_M/0/1/0/all/0/1&quot;&gt;Murat Cubuktepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.08028">
<title>From Deterministic ODEs to Dynamic Structural Causal Models. (arXiv:1608.08028v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1608.08028</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural Causal Models are widely used in causal modelling, but how they
relate to other modelling tools is poorly understood. In this paper we provide
a novel perspective on the relationship between Ordinary Differential Equations
and Structural Causal Models. We show how, under certain conditions, the
asymptotic behaviour of an Ordinary Differential Equation under non-constant
interventions can be modelled using Dynamic Structural Causal Models. In
contrast to earlier work, we study not only the effect of interventions on
equilibrium states; rather, we model asymptotic behaviour that is dynamic under
interventions that vary in time, and include as a special case the study of
static equilibria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubenstein_P/0/1/0/all/0/1&quot;&gt;Paul K. Rubenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongers_S/0/1/0/all/0/1&quot;&gt;Stephan Bongers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Schoelkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mooij_J/0/1/0/all/0/1&quot;&gt;Joris M. Mooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06620">
<title>Visualizing the Feature Importance for Black Box Models. (arXiv:1804.06620v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06620</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, a large amount of model-agnostic methods to improve the
transparency, trustability and interpretability of machine learning models have
been developed. We introduce local feature importance as a local version of a
recent model-agnostic global feature importance method. Based on local feature
importance, we propose two visual tools: partial importance (PI) and individual
conditional importance (ICI) plots which visualize how changes in a feature
affect the model performance on average, as well as for individual
observations. Our proposed methods are related to partial dependence (PD) and
individual conditional expectation (ICE) plots, but visualize the expected
(conditional) feature importance instead of the expected (conditional)
prediction. Furthermore, we show that averaging ICI curves across observations
yields a PI curve, and integrating the PI curve with respect to the
distribution of the considered feature results in the global feature
importance. Another contribution of our paper is the Shapley feature
importance, which fairly distributes the overall performance of a model among
the features according to the marginal contributions and which can be used to
compare the feature importance across different models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1&quot;&gt;Giuseppe Casalicchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molnar_C/0/1/0/all/0/1&quot;&gt;Christoph Molnar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09160">
<title>No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling. (arXiv:1804.09160v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09160</link>
<description rdf:parseType="Literal">&lt;p&gt;Though impressive results have been achieved in visual captioning, the task
of generating abstract stories from photo streams is still a little-tapped
problem. Different from captions, stories have more expressive language styles
and contain many imaginary concepts that do not appear in the images. Thus it
poses challenges to behavioral cloning algorithms. Furthermore, due to the
limitations of automatic metrics on evaluating story quality, reinforcement
learning methods with hand-crafted rewards also face difficulties in gaining an
overall performance boost. Therefore, we propose an Adversarial REward Learning
(AREL) framework to learn an implicit reward function from human
demonstrations, and then optimize policy search with the learned reward
function. Though automatic eval- uation indicates slight performance boost over
state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation
shows that our approach achieves significant improvement in generating more
human-like stories than SOTA systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05287">
<title>A Cost-Effective Framework for Preference Elicitation and Aggregation. (arXiv:1805.05287v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05287</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a cost-effective framework for preference elicitation and
aggregation under the Plackett-Luce model with features. Given a budget, our
framework iteratively computes the most cost-effective elicitation questions in
order to help the agents make a better group decision.
&lt;/p&gt;
&lt;p&gt;We illustrate the viability of the framework with experiments on Amazon
Mechanical Turk, which we use to estimate the cost of answering different types
of elicitation questions. We compare the prediction accuracy of our framework
when adopting various information criteria that evaluate the expected
information gain from a question. Our experiments show carefully designed
information criteria are much more efficient, i.e., they arrive at the correct
answer using fewer queries, than randomly asking questions given the budget
constraint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhibing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kephart_J/0/1/0/all/0/1&quot;&gt;Jeffrey Kephart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattei_N/0/1/0/all/0/1&quot;&gt;Nicholas Mattei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hui Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lirong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02664">
<title>Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning. (arXiv:1806.02664v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02664</link>
<description rdf:parseType="Literal">&lt;p&gt;In zero-shot learning (ZSL), a classifier is trained to recognize visual
classes without any image samples. Instead, it is given semantic information
about the class, like a textual description or a set of attributes. Learning
from attributes could benefit from explicitly modeling structure of the
attribute space. Unfortunately, learning of general structure from empirical
samples is hard with typical dataset sizes.
&lt;/p&gt;
&lt;p&gt;Here we describe LAGO, a probabilistic model designed to capture natural soft
and-or relations across groups of attributes. We show how this model can be
learned end-to-end with a deep attribute-detection model. The soft group
structure can be learned from data jointly as part of the model, and can also
readily incorporate prior knowledge about groups if available. The soft and-or
structure succeeds to capture meaningful and predictive structures, improving
the accuracy of zero-shot learning on two of three benchmarks.
&lt;/p&gt;
&lt;p&gt;Finally, LAGO reveals a unified formulation over two ZSL approaches: DAP
(Lampert et al., 2009) and ESZSL (Romera-Paredes &amp;amp; Torr, 2015). Interestingly,
taking only one singleton group for each attribute, introduces a new
soft-relaxation of DAP, that outperforms DAP by ~40.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atzmon_Y/0/1/0/all/0/1&quot;&gt;Yuval Atzmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04854">
<title>Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam. (arXiv:1806.04854v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04854</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nielsen_D/0/1/0/all/0/1&quot;&gt;Didrik Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tangkaratt_V/0/1/0/all/0/1&quot;&gt;Voot Tangkaratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yarin Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06514">
<title>The Information Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Models. (arXiv:1806.06514v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06514</link>
<description rdf:parseType="Literal">&lt;p&gt;A large number of objectives have been proposed to train latent variable
generative models. We show that many of them are Lagrangian dual functions of
the same primal optimization problem. The primal problem optimizes the mutual
information between latent and visible variables, subject to the constraints of
accurately modeling the data distribution and performing correct amortized
inference. By choosing to maximize or minimize mutual information, and choosing
different Lagrange multipliers, we obtain different objectives including
InfoGAN, ALI/BiGAN, ALICE, CycleGAN, beta-VAE, adversarial autoencoders, AVB,
AS-VAE and InfoVAE. Based on this observation, we provide an exhaustive
characterization of the statistical and computational trade-offs made by all
the training objectives in this class of Lagrangian duals. Next, we propose a
dual optimization method where we optimize model parameters as well as the
Lagrange multipliers. This method achieves Pareto optimal solutions in terms of
optimizing information and satisfying the constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shengjia Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06953">
<title>Qualitative Measurements of Policy Discrepancy for Return-based Deep Q-Network. (arXiv:1806.06953v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06953</link>
<description rdf:parseType="Literal">&lt;p&gt;The deep Q-network (DQN) and return-based reinforcement learning are two
promising algorithms proposed in recent years. DQN brings advances to complex
sequential decision problems, while return-based algorithms have advantages in
making use of sample trajectories. In this paper, we propose a general
framework to combine DQN and most of the return-based reinforcement learning
algorithms, named R-DQN. We show the performance of traditional DQN can be
improved effectively by introducing return-based reinforcement learning. In
order to further improve the R-DQN, we design a strategy with two measurements
which can qualitatively measure the policy discrepancy. Moreover, we give the
two measurements&apos; bounds in the proposed R-DQN framework. We show that
algorithms with our strategy can accurately express the trace coefficient and
achieve a better approximation to return. The experiments, conducted on several
representative tasks from the OpenAI Gym library, validate the effectiveness of
the proposed measurements. The results also show that the algorithms with our
strategy outperform the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1&quot;&gt;Wenjia Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qian Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Long Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pengfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1&quot;&gt;Gang Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00589">
<title>Lifted Marginal MAP Inference. (arXiv:1807.00589v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00589</link>
<description rdf:parseType="Literal">&lt;p&gt;Lifted inference reduces the complexity of inference in relational
probabilistic models by identifying groups of constants (or atoms) which behave
symmetric to each other. A number of techniques have been proposed in the
literature for lifting marginal as well MAP inference. We present the first
application of lifting rules for marginal-MAP (MMAP), an important inference
problem in models having latent (random) variables. Our main contribution is
two fold: (1) we define a new equivalence class of (logical) variables, called
Single Occurrence for MAX (SOM), and show that solution lies at extreme with
respect to the SOM variables, i.e., predicate groundings differing only in the
instantiation of the SOM variables take the same truth value (2) we define a
sub-class {\em SOM-R} (SOM Reduce) and exploit properties of extreme
assignments to show that MMAP inference can be performed by reducing the domain
of SOM-R variables to a single constant.We refer to our lifting technique as
the {\em SOM-R} rule for lifted MMAP. Combined with existing rules such as
decomposer and binomial, this results in a powerful framework for lifted MMAP.
Experiments on three benchmark domains show significant gains in both time and
memory compared to ground inference as well as lifted approaches not using
SOM-R.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vishal Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikh_N/0/1/0/all/0/1&quot;&gt;Noman Ahmed Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_H/0/1/0/all/0/1&quot;&gt;Happy Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gogate_V/0/1/0/all/0/1&quot;&gt;Vibhav Gogate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1&quot;&gt;Parag Singla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00643">
<title>Block-Value Symmetries in Probabilistic Graphical Models. (arXiv:1807.00643v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00643</link>
<description rdf:parseType="Literal">&lt;p&gt;One popular way for lifted inference in probabilistic graphical models is to
first merge symmetric states into a single cluster (orbit) and then use these
for downstream inference, via variations of orbital MCMC [Niepert, 2012]. These
orbits are represented compactly using permutations over variables, and
variable-value (VV) pairs, but they can miss several state symmetries in a
domain.
&lt;/p&gt;
&lt;p&gt;We define the notion of permutations over block-value (BV) pairs, where a
block is a set of variables. BV strictly generalizes VV symmetries, and can
compute many more symmetries for increasing block sizes. To operationalize use
of BV permutations in lifted inference, we describe 1) an algorithm to compute
BV permutations given a block partition of the variables, 2) BV-MCMC, an
extension of orbital MCMC that can sample from BV orbits, and 3) a heuristic to
suggest good block partitions. Our experiments show that BV-MCMC can mix much
faster compared to vanilla MCMC and orbital MCMC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_G/0/1/0/all/0/1&quot;&gt;Gagan Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1&quot;&gt;Ankit Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1&quot;&gt;Mausam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1&quot;&gt;Parag Singla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02322">
<title>Memory Augmented Policy Optimization for Program Synthesis with Generalization. (arXiv:1807.02322v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02322</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Memory Augmented Policy Optimization (MAPO): a novel
policy optimization formulation that incorporates a memory buffer of promising
trajectories to reduce the variance of policy gradient estimates for
deterministic environments with discrete actions. The formulation expresses the
expected return objective as a weighted sum of two terms: an expectation over a
memory of trajectories with high rewards, and a separate expectation over the
trajectories outside the memory. We propose 3 techniques to make an efficient
training algorithm for MAPO: (1) distributed sampling from inside and outside
memory with an actor-learner architecture; (2) a marginal likelihood constraint
over the memory to accelerate training; (3) systematic exploration to discover
high reward trajectories. MAPO improves the sample efficiency and robustness of
policy gradient, especially on tasks with a sparse reward. We evaluate MAPO on
weakly supervised program synthesis from natural language with an emphasis on
generalization. On the WikiTableQuestions benchmark we improve the
state-of-the-art by 2.5%, achieving an accuracy of 46.2%, and on the WikiSQL
benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision,
outperforming several strong baselines with full supervision. Our code is open
sourced at https://github.com/crazydonkey200/neural-symbolic-machines
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1&quot;&gt;Ni Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05626">
<title>Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time. (arXiv:1711.05626v2 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1711.05626</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic topic modeling facilitates the identification of topical trends over
time in temporal collections of unstructured documents. We introduce a novel
unsupervised neural dynamic topic model named as Recurrent Neural
Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each
time influence the topic discovery in the subsequent time steps. We account for
the temporal ordering of documents by explicitly modeling a joint distribution
of latent topical dependencies over time, using distributional estimators with
temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP
research, we demonstrate that compared to state-of-the art topic models, RNNRSM
shows better generalization, topic interpretation, evolution and trends. We
also introduce a metric (named as SPAN) to quantify the capability of dynamic
topic model to capture word evolution in topics over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Pankaj Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajaram_S/0/1/0/all/0/1&quot;&gt;Subburam Rajaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1&quot;&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrassy_B/0/1/0/all/0/1&quot;&gt;Bernt Andrassy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02537">
<title>Fully Scalable Gaussian Processes using Subspace Inducing Inputs. (arXiv:1807.02537v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02537</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce fully scalable Gaussian processes, an implementation scheme that
tackles the problem of treating a high number of training instances together
with high dimensional input data. Our key idea is a representation trick over
the inducing variables called subspace inducing inputs. This is combined with
certain matrix-preconditioning based parametrizations of the variational
distributions that lead to simplified and numerically stable variational lower
bounds. Our illustrative applications are based on challenging extreme
multi-label classification problems with the extra burden of the very large
number of class labels. We demonstrate the usefulness of our approach by
presenting predictive performances together with low computational times in
datasets with extremely large number of instances and input dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panos_A/0/1/0/all/0/1&quot;&gt;Aristeidis Panos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dellaportas_P/0/1/0/all/0/1&quot;&gt;Petros Dellaportas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Titsias_M/0/1/0/all/0/1&quot;&gt;Michalis K. Titsias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02582">
<title>Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences. (arXiv:1807.02582v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02582</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is an attempt to bridge the conceptual gaps between researchers
working on the two widely used approaches based on positive definite kernels:
Bayesian learning or inference using Gaussian processes on the one side, and
frequentist kernel methods based on reproducing kernel Hilbert spaces on the
other. It is widely known in machine learning that these two formalisms are
closely related; for instance, the estimator of kernel ridge regression is
identical to the posterior mean of Gaussian process regression. However, they
have been studied and developed almost independently by two essentially
separate communities, and this makes it difficult to seamlessly transfer
results between them. Our aim is to overcome this potential difficulty. To this
end, we review several old and new results and concepts from either side, and
juxtapose algorithmic quantities from each framework to highlight close
similarities. We also provide discussions on subtle philosophical and
theoretical differences between the two approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kanagawa_M/0/1/0/all/0/1&quot;&gt;Motonobu Kanagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hennig_P/0/1/0/all/0/1&quot;&gt;Philipp Hennig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1&quot;&gt;Dino Sejdinovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sriperumbudur_B/0/1/0/all/0/1&quot;&gt;Bharath K Sriperumbudur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02608">
<title>Synthetic Sampling for Multi-Class Malignancy Prediction. (arXiv:1807.02608v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02608</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore several oversampling techniques for an imbalanced multi-label
classification problem, a setting often encountered when developing models for
Computer-Aided Diagnosis (CADx) systems. While most CADx systems aim to
optimize classifiers for overall accuracy without considering the relative
distribution of each class, we look into using synthetic sampling to increase
per-class performance when predicting the degree of malignancy. Using low-level
image features and a random forest classifier, we show that using synthetic
oversampling techniques increases the sensitivity of the minority classes by an
average of 7.22% points, with as much as a 19.88% point increase in sensitivity
for a particular minority class. Furthermore, the analysis of low-level image
feature distributions for the synthetic nodules reveals that these nodules can
provide insights on how to preprocess image data for better classification
performance or how to supplement the original datasets when more data
acquisition is feasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yung_M/0/1/0/all/0/1&quot;&gt;Matthew Yung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_E/0/1/0/all/0/1&quot;&gt;Eli T. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasin_A/0/1/0/all/0/1&quot;&gt;Alexander Rasin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furst_J/0/1/0/all/0/1&quot;&gt;Jacob D. Furst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raicu_D/0/1/0/all/0/1&quot;&gt;Daniela S. Raicu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02617">
<title>Predicting Infant Motor Development Status using Day Long Movement Data from Wearable Sensors. (arXiv:1807.02617v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02617</link>
<description rdf:parseType="Literal">&lt;p&gt;Infants with a variety of complications at or before birth are classified as
being at risk for developmental delays (AR). As they grow older, they are
followed by healthcare providers in an effort to discern whether they are on a
typical or impaired developmental trajectory. Often, it is difficult to make an
accurate determination early in infancy as infants with typical development
(TD) display high variability in their developmental trajectories both in
content and timing. Studies have shown that spontaneous movements have the
potential to differentiate typical and atypical trajectories early in life
using sensors and kinematic analysis systems. In this study, machine learning
classification algorithms are used to take inertial movement from wearable
sensors placed on an infant for a day and predict if the infant is AR or TD,
thus further establishing the connection between early spontaneous movement and
developmental trajectory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_D/0/1/0/all/0/1&quot;&gt;David Goodfellow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_R/0/1/0/all/0/1&quot;&gt;Ruoyu Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funke_R/0/1/0/all/0/1&quot;&gt;Rebecca Funke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pulido_J/0/1/0/all/0/1&quot;&gt;Jose Carlos Pulido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1&quot;&gt;Maja Mataric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1&quot;&gt;Beth A. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02629">
<title>Mirror descent in saddle-point problems: Going the extra (gradient) mile. (arXiv:1807.02629v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02629</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to their connection with generative adversarial networks (GANs),
saddle-point problems have recently attracted considerable interest in machine
learning and beyond. By necessity, most theoretical guarantees revolve around
convex-concave problems; however, making theoretical inroads towards efficient
GAN training crucially depends on moving beyond this classic framework. To make
piecemeal progress along these lines, we analyze the widely used mirror descent
(MD) method in a class of non-monotone problems - called coherent - whose
solutions coincide with those of a naturally associated variational inequality.
Our first result is that, under strict coherence (a condition satisfied by all
strictly convex-concave problems), MD methods converge globally; however, they
may fail to converge even in simple, bilinear models. To mitigate this
deficiency, we add on an &quot;extra-gradient&quot; step which we show stabilizes MD
methods by looking ahead and using a &quot;future gradient&quot;. These theoretical
results are subsequently validated by numerical experiments in GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mertikopoulos_P/0/1/0/all/0/1&quot;&gt;Panayotis Mertikopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenati_H/0/1/0/all/0/1&quot;&gt;Houssam Zenati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecouat_B/0/1/0/all/0/1&quot;&gt;Bruno Lecouat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1&quot;&gt;Chuan-Sheng Foo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekhar_V/0/1/0/all/0/1&quot;&gt;Vijay Chandrasekhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1&quot;&gt;Georgios Piliouras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02682">
<title>A Supervised Geometry-Aware Mapping Approach for Classification of Hyperspectral Images. (arXiv:1807.02682v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02682</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of proper class discrimination among the Hyperspectral (HS) data
points poses a potential challenge in HS classification. To address this issue,
this paper proposes an optimal geometry-aware transformation for enhancing the
classification accuracy. The underlying idea of this method is to obtain a
linear projection matrix by solving a nonlinear objective function based on the
intrinsic geometrical structure of the data. The objective function is
constructed to quantify the discrimination between the points from dissimilar
classes on the projected data space. Then the obtained projection matrix is
used to linearly map the data to more discriminative space. The effectiveness
of the proposed transformation is illustrated with three benchmark real-world
HS data sets. The experiments reveal that the classification and dimensionality
reduction methods on the projected discriminative space outperform their
counterpart in the original space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanty_R/0/1/0/all/0/1&quot;&gt;Ramanarayan Mohanty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Happy_S/0/1/0/all/0/1&quot;&gt;S L Happy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Routray_A/0/1/0/all/0/1&quot;&gt;Aurobinda Routray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02684">
<title>VFPred: A Fusion of Signal Processing and Machine Learning techniques in Detecting Ventricular Fibrillation from ECG Signals. (arXiv:1807.02684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02684</link>
<description rdf:parseType="Literal">&lt;p&gt;Ventricular Fibrillation (VF), one of the most dangerous arrhythmias, is
responsible for sudden cardiac arrests. Thus, various algorithms have been
developed to predict VF from Electrocardiogram (ECG), which is a binary
classification problem. In the literature, we find a number of algorithms based
on signal processing, where, after some robust mathematical operations the
decision is given based on a predefined threshold over a single value. On the
other hand, some machine learning based algorithms are also reported in the
literature; however, these algorithms merely combine some parameters and make a
prediction using those as features. Both the approaches have their perks and
pitfalls; thus our motivation was to coalesce them to get the best out of the
both worlds. Thus we have developed, VFPred that, in addition to employing a
signal processing pipeline, namely, Empirical Mode Decomposition and Discrete
Time Fourier Transform for useful feature extraction, uses a Support Vector
Machine for efficient classification. VFPred turns out to be a robust algorithm
as it is able to successfully segregate the two classes with equal confidence
(Sensitivity = 99.99%, Specificity = 98.40%) even from a short signal of 5
seconds long, whereas existing works though requires longer signals, flourishes
in one but fails in the other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibtehaz_N/0/1/0/all/0/1&quot;&gt;Nabil Ibtehaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;M. Saifur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;M. Sohel Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02694">
<title>Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions. (arXiv:1807.02694v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02694</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider the following class of learning schemes: $$\hat{\boldsymbol{\beta}}
:= \arg\min_{\boldsymbol{\beta}}\;\sum_{j=1}^n
\ell(\boldsymbol{x}_j^\top\boldsymbol{\beta}; y_j) + \lambda
R(\boldsymbol{\beta}),\qquad\qquad (1) $$ where $\boldsymbol{x}_i \in
\mathbb{R}^p$ and $y_i \in \mathbb{R}$ denote the $i^{\text{th}}$ feature and
response variable respectively. Let $\ell$ and $R$ be the loss function and
regularizer, $\boldsymbol{\beta}$ denote the unknown weights, and $\lambda$ be
a regularization parameter. Finding the optimal choice of $\lambda$ is a
challenging problem in high-dimensional regimes where both $n$ and $p$ are
large. We propose two frameworks to obtain a computationally efficient
approximation ALO of the leave-one-out cross validation (LOOCV) risk for
nonsmooth losses and regularizers. Our two frameworks are based on the primal
and dual formulations of (1). We prove the equivalence of the two approaches
under smoothness conditions. This equivalence enables us to justify the
accuracy of both methods under such conditions. We use our approaches to obtain
a risk estimate for several standard problems, including generalized LASSO,
nuclear norm regularization, and support vector machines. We empirically
demonstrate the effectiveness of our results for non-differentiable cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenda Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haihao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maleki_A/0/1/0/all/0/1&quot;&gt;Arian Maleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mirrokni_V/0/1/0/all/0/1&quot;&gt;Vahab Mirrokni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02701">
<title>DeepSource: Point Source Detection using Deep Learning. (arXiv:1807.02701v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/1807.02701</link>
<description rdf:parseType="Literal">&lt;p&gt;Point source detection at low signal-to-noise is challenging for astronomical
surveys, particularly in radio interferometry images where the noise is
correlated. Machine learning is a promising solution, allowing the development
of algorithms tailored to specific telescope arrays and science cases. We
present DeepSource - a deep learning solution - that uses convolutional neural
networks to achieve these goals. DeepSource enhances the Signal-to-Noise Ratio
(SNR) of the original map and then uses dynamic blob detection to detect
sources. Trained and tested on two sets of 500 simulated 1 deg x 1 deg MeerKAT
images with a total of 300,000 sources, DeepSource is essentially perfect in
both purity and completeness down to SNR = 4 and outperforms PyBDSF in all
metrics. For uniformly-weighted images it achieves a Purity x Completeness (PC)
score at SNR = 3 of 0.73, compared to 0.31 for the best PyBDSF model. For
natural-weighting we find a smaller improvement of ~40% in the PC score at SNR
= 3. If instead we ask where either of the purity or completeness first drop to
90%, we find that DeepSource reaches this value at SNR = 3.6 compared to the
4.3 of PyBDSF (natural-weighting). A key advantage of DeepSource is that it can
learn to optimally trade off purity and completeness for any science case under
consideration. Our results show that deep learning is a promising approach to
point source detection in astronomical images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Sadr_A/0/1/0/all/0/1&quot;&gt;A. Vafaei Sadr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Vos_E/0/1/0/all/0/1&quot;&gt;Etienne. E. Vos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bassett_B/0/1/0/all/0/1&quot;&gt;Bruce A. Bassett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hosenie_Z/0/1/0/all/0/1&quot;&gt;Zafiirah Hosenie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Oozeer_N/0/1/0/all/0/1&quot;&gt;N. Oozeer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lochner_M/0/1/0/all/0/1&quot;&gt;Michelle Lochner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02716">
<title>A Deep-Learning-Based Geological Parameterization for History Matching Complex Models. (arXiv:1807.02716v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02716</link>
<description rdf:parseType="Literal">&lt;p&gt;A new low-dimensional parameterization based on principal component analysis
(PCA) and convolutional neural networks (CNN) is developed to represent complex
geological models. The CNN-PCA method is inspired by recent developments in
computer vision using deep learning. CNN-PCA can be viewed as a generalization
of an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA
entail post-processing a PCA model to better honor complex geological features.
In CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new
regularization involving a set of metrics for multipoint statistics is
introduced. The metrics are based on summary statistics of the nonlinear filter
responses of geological models to a pre-trained deep CNN. In addition, in the
CNN-PCA formulation presented here, a convolutional neural network is trained
as an explicit transform function that can post-process PCA models quickly.
CNN-PCA is shown to provide both unconditional and conditional realizations
that honor the geological features present in reference SGeMS geostatistical
realizations for a binary channelized system. Flow statistics obtained through
simulation of random CNN-PCA models closely match results for random SGeMS
models for a demanding case in which O-PCA models lead to significant
discrepancies. Results for history matching are also presented. In this
assessment CNN-PCA is applied with derivative-free optimization, and a subspace
randomized maximum likelihood method is used to provide multiple posterior
models. Data assimilation and significant uncertainty reduction are achieved
for existing wells, and physically reasonable predictions are also obtained for
new wells. Finally, the CNN-PCA method is extended to a more complex
non-stationary bimodal deltaic fan system, and is shown to provide high-quality
realizations for this challenging example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yimin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenyue Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Durlofsky_L/0/1/0/all/0/1&quot;&gt;Louis J. Durlofsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02789">
<title>The modal age of Statistics. (arXiv:1807.02789v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1807.02789</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a number of statistical problems have found an unexpected solution
by inspecting them through a &quot;modal point of view&quot;. These include classical
tasks such as clustering or regression. This has led to a renewed interest in
estimation and inference for the mode. This paper offers an extensive survey of
the traditional approaches to mode estimation and explores the consequences of
applying this modern modal methodology to other, seemingly unrelated, fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chacon_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; E. Chac&amp;#xf3;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02795">
<title>BALSON: Bayesian Least Squares Optimization with Nonnegative L1-Norm Constraint. (arXiv:1807.02795v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02795</link>
<description rdf:parseType="Literal">&lt;p&gt;A Bayesian approach termed BAyesian Least Squares Optimization with
Nonnegative L1-norm constraint (BALSON) is proposed. The error distribution of
data fitting is described by Gaussian likelihood. The parameter distribution is
assumed to be a Dirichlet distribution. With the Bayes rule, searching for the
optimal parameters is equivalent to finding the mode of the posterior
distribution. In order to explicitly characterize the nonnegative L1-norm
constraint of the parameters, we further approximate the true posterior
distribution by a Dirichlet distribution. We estimate the statistics of the
approximating Dirichlet posterior distribution by sampling methods. Four
sampling methods have been introduced. With the estimated posterior
distributions, the original parameters can be effectively reconstructed in
polynomial fitting problems, and the BALSON framework is found to perform
better than conventional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_J/0/1/0/all/0/1&quot;&gt;Jen-Tzung Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiqing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jun Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02811">
<title>A Tutorial on Bayesian Optimization. (arXiv:1807.02811v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02811</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization is an approach to optimizing objective functions that
take a long time (minutes or hours) to evaluate. It is best-suited for
optimization over continuous domains of less than 20 dimensions, and tolerates
stochastic noise in function evaluations. It builds a surrogate for the
objective and quantifies the uncertainty in that surrogate using a Bayesian
machine learning technique, Gaussian process regression, and then uses an
acquisition function defined from this surrogate to decide where to sample. In
this tutorial, we describe how Bayesian optimization works, including Gaussian
process regression and three common acquisition functions: expected
improvement, entropy search, and knowledge gradient. We then discuss more
advanced techniques, including running multiple function evaluations in
parallel, multi-fidelity and multi-information source optimization,
expensive-to-evaluate constraints, random environmental conditions, multi-task
Bayesian optimization, and the inclusion of derivative information. We conclude
with a discussion of Bayesian optimization software and future research
directions in the field. Within our tutorial material we provide a
generalization of expected improvement to noisy evaluations, beyond the
noise-free setting where it is more commonly applied. This generalization is
justified by a formal decision-theoretic argument, standing in contrast to
previous ad hoc modifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frazier_P/0/1/0/all/0/1&quot;&gt;Peter I. Frazier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02816">
<title>Improving Deep Learning through Automatic Programming. (arXiv:1807.02816v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02816</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning and deep architectures are emerging as the best machine
learning methods so far in many practical applications such as reducing the
dimensionality of data, image classification, speech recognition or object
segmentation. In fact, many leading technology companies such as Google,
Microsoft or IBM are researching and using deep architectures in their systems
to replace other traditional models. Therefore, improving the performance of
these models could make a strong impact in the area of machine learning.
However, deep learning is a very fast-growing research domain with many core
methodologies and paradigms just discovered over the last few years. This
thesis will first serve as a short summary of deep learning, which tries to
include all of the most important ideas in this research area. Based on this
knowledge, we suggested, and conducted some experiments to investigate the
possibility of improving the deep learning based on automatic programming
(ADATE). Although our experiments did produce good results, there are still
many more possibilities that we could not try due to limited time as well as
some limitations of the current ADATE version. I hope that this thesis can
promote future work on this topic, especially when the next version of ADATE
comes out. This thesis also includes a short analysis of the power of ADATE
system, which could be useful for other researchers who want to know what it is
capable of.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Ha_T/0/1/0/all/0/1&quot;&gt;The-Hien Dang-Ha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02872">
<title>Large Margin Few-Shot Learning. (arXiv:1807.02872v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02872</link>
<description rdf:parseType="Literal">&lt;p&gt;The key issue of few-shot learning is learning to generalize. In this paper,
we propose a large margin principle to improve the generalization capacity of
metric based methods for few-shot learning. To realize it, we develop a unified
framework to learn a more discriminative metric space by augmenting the softmax
classification loss function with a large margin distance loss function for
training. Extensive experiments on two state-of-the-art few-shot learning
models, graph neural networks and prototypical networks, show that our method
can improve the performance of existing models substantially with very little
computational overhead, demonstrating the effectiveness of the large margin
principle and the potential of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiao-Ming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qimai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wangmeng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_V/0/1/0/all/0/1&quot;&gt;Victor O.K. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02873">
<title>Separability is not the best goal for machine learning. (arXiv:1807.02873v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02873</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks use their hidden layers to transform input data into linearly
separable data clusters, with a linear or a perceptron type output layer making
the final projection on the line perpendicular to the discriminating
hyperplane. For complex data with multimodal distributions this transformation
is difficult to learn. Projection on $k\geq 2$ line segments is the simplest
extension of linear separability, defining much easier goal for the learning
process. Simple problems are 2-separable, but problems with inherent complex
logic may be solved in a simple way by $k$-separable projections. The
difficulty of learning non-linear data distributions is shifted to separation
of line intervals, simplifying the transformation of data by hidden network
layers. For classification of difficult Boolean problems, such as the parity
problem, linear projection combined with \ksep is sufficient and provides a
powerful new target for learning. More complex targets may also be defined,
changing the goal of learning from linear discrimination to creation of data
distributions that can easily be handled by specialized models selected to
analyze output distributions. This approach can replace many layers of
transformation required by deep learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duch_W/0/1/0/all/0/1&quot;&gt;Wlodzislaw Duch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02876">
<title>Machine Learning in High Energy Physics Community White Paper. (arXiv:1807.02876v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/1807.02876</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is an important research area in particle physics, beginning
with applications to high-level physics analysis in the 1990s and 2000s,
followed by an explosion of applications in particle and event identification
and reconstruction in the 2010s. In this document we discuss promising future
research and development areas in machine learning in particle physics with a
roadmap for their implementation, software and hardware resource requirements,
collaborative initiatives with the data science community, academia and
industry, and training the particle physics community in data science. The main
objective of the document is to connect and motivate these areas of research
and development with the physics drivers of the High-Luminosity Large Hadron
Collider and future neutrino experiments and identify the resource needs for
their implementation. Additionally we identify areas where collaboration with
external communities will be of great benefit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Albertsson_K/0/1/0/all/0/1&quot;&gt;Kim Albertsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Altoe_P/0/1/0/all/0/1&quot;&gt;Piero Altoe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Dustin Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1&quot;&gt;Michael Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Espinosa_J/0/1/0/all/0/1&quot;&gt;Juan Pedro Araque Espinosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Aurisano_A/0/1/0/all/0/1&quot;&gt;Adam Aurisano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Basara_L/0/1/0/all/0/1&quot;&gt;Laurent Basara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bevan_A/0/1/0/all/0/1&quot;&gt;Adrian Bevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bhimji_W/0/1/0/all/0/1&quot;&gt;Wahid Bhimji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bonacorsi_D/0/1/0/all/0/1&quot;&gt;Daniele Bonacorsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Calafiura_P/0/1/0/all/0/1&quot;&gt;Paolo Calafiura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Campanelli_M/0/1/0/all/0/1&quot;&gt;Mario Campanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Capps_L/0/1/0/all/0/1&quot;&gt;Louis Capps&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Carminati_F/0/1/0/all/0/1&quot;&gt;Federico Carminati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Carrazza_S/0/1/0/all/0/1&quot;&gt;Stefano Carrazza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Childers_T/0/1/0/all/0/1&quot;&gt;Taylor Childers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Coniavitis_E/0/1/0/all/0/1&quot;&gt;Elias Coniavitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+David_C/0/1/0/all/0/1&quot;&gt;Claire David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Davis_D/0/1/0/all/0/1&quot;&gt;Douglas Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Duarte_J/0/1/0/all/0/1&quot;&gt;Javier Duarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Erdmann_M/0/1/0/all/0/1&quot;&gt;Martin Erdmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Eschle_J/0/1/0/all/0/1&quot;&gt;Jonas Eschle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Farbin_A/0/1/0/all/0/1&quot;&gt;Amir Farbin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Feickert_M/0/1/0/all/0/1&quot;&gt;Matthew Feickert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Castro_N/0/1/0/all/0/1&quot;&gt;Nuno Filipe Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fitzpatrick_C/0/1/0/all/0/1&quot;&gt;Conor Fitzpatrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Floris_M/0/1/0/all/0/1&quot;&gt;Michele Floris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Forti_A/0/1/0/all/0/1&quot;&gt;Alessandra Forti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Garra_Tico_J/0/1/0/all/0/1&quot;&gt;Jordi Garra-Tico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gemmler_J/0/1/0/all/0/1&quot;&gt;Jochen Gemmler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Girone_M/0/1/0/all/0/1&quot;&gt;Maria Girone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Glaysher_P/0/1/0/all/0/1&quot;&gt;Paul Glaysher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1&quot;&gt;Sergei Gleyzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gligorov_V/0/1/0/all/0/1&quot;&gt;Vladimir Gligorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Golling_T/0/1/0/all/0/1&quot;&gt;Tobias Golling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Graw_J/0/1/0/all/0/1&quot;&gt;Jonas Graw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gray_L/0/1/0/all/0/1&quot;&gt;Lindsey Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Greenwood_D/0/1/0/all/0/1&quot;&gt;Dick Greenwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hacker_T/0/1/0/all/0/1&quot;&gt;Thomas Hacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Harvey_J/0/1/0/all/0/1&quot;&gt;John Harvey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hegner_B/0/1/0/all/0/1&quot;&gt;Benedikt Hegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Heinrich_L/0/1/0/all/0/1&quot;&gt;Lukas Heinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hooberman_B/0/1/0/all/0/1&quot;&gt;Ben Hooberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Junggeburth_J/0/1/0/all/0/1&quot;&gt;Johannes Junggeburth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kagan_M/0/1/0/all/0/1&quot;&gt;Michael Kagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kane_M/0/1/0/all/0/1&quot;&gt;Meghan Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kanishchev_K/0/1/0/all/0/1&quot;&gt;Konstantin Kanishchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Karpinski_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Karpi&amp;#x144;ski&lt;/a&gt;, et al. (69 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02892">
<title>Automated labeling of bugs and tickets using attention-based mechanisms in recurrent neural networks. (arXiv:1807.02892v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.02892</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore solutions for automated labeling of content in bug trackers and
customer support systems. In order to do that, we classify content in terms of
several criteria, such as priority or product area. In the first part of the
paper, we provide an overview of existing methods used for text classification.
These methods fall into two categories - the ones that rely on neural networks
and the ones that don&apos;t. We evaluate results of several solutions of both
kinds. In the second part of the paper we present our own recurrent neural
network solution based on hierarchical attention paradigm. It consists of
several Hierarchical Attention network blocks with varying Gated Recurrent Unit
cell sizes and a complementary shallow network that goes alongside. Lastly, we
evaluate above-mentioned methods when predicting fields from two datasets -
Arch Linux bug tracker and Chromium bug tracker. Our contributions include a
comprehensive benchmark between a variety of methods on relevant datasets; a
novel solution that outperforms previous generation methods; and two new
datasets that are made public for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyubinets_V/0/1/0/all/0/1&quot;&gt;Volodymyr Lyubinets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boiko_T/0/1/0/all/0/1&quot;&gt;Taras Boiko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicholas_D/0/1/0/all/0/1&quot;&gt;Deon Nicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02901">
<title>Quantifying model form uncertainty in Reynolds-averaged turbulence models with Bayesian deep neural networks. (arXiv:1807.02901v1 [physics.flu-dyn])</title>
<link>http://arxiv.org/abs/1807.02901</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven methods for improving turbulence modeling in Reynolds-Averaged
Navier-Stokes (RANS) simulations have gained significant interest in the
computational fluid dynamics community. Modern machine learning models have
opened up a new area of black-box turbulence models allowing for the tuning of
RANS simulations to increase their predictive accuracy. While several
data-driven turbulence models have been reported, the quantification of the
uncertainties introduced has mostly been neglected. Uncertainty quantification
for such data-driven models is essential since their predictive capability
rapidly declines as they are tested for flow physics that deviate from that in
the training data. In this work, we propose a novel data-driven framework that
not only improves RANS predictions but also provides probabilistic bounds for
fluid quantities such as velocity and pressure. The uncertainties capture
include both model form uncertainty as well as epistemic uncertainty induced by
the limited training data. An invariant Bayesian deep neural network is used to
predict the anisotropic tensor component of the Reynolds stress. This model is
trained using Stein&apos;s variational gradient decent algorithm. The computed
uncertainty on the Reynolds stress is propagated to the quantities of interest
by vanilla Monte Carlo simulation. Results are presented for two test cases
that differ geometrically from the training flows at several different Reynolds
numbers. The prediction enhancement of the data-driven model is discussed as
well as the associated probabilistic bounds for flow properties of interest.
Ultimately this framework allows for a quantitative measurement of model
confidence and uncertainty quantification for flows in which no high-fidelity
observations or prior knowledge is available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Geneva_N/0/1/0/all/0/1&quot;&gt;Nicholas Geneva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zabaras_N/0/1/0/all/0/1&quot;&gt;Nicholas Zabaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02910">
<title>Supervised Local Modeling for Interpretability. (arXiv:1807.02910v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02910</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability is an increasingly important component of practical
machine learning. Some of the most common forms of interpretability systems are
example-based, local, and global explanations. One of the main challenges in
interpretability is designing explanation systems that can capture aspects of
each of these explanation types, in order to develop a more thorough
understanding of the model. We address this challenge in a novel model called
SLIM that uses local linear modeling techniques along with a dual
interpretation of random forests (both as a supervised neighborhood approach
and as a feature selection method). SLIM has two fundamental advantages over
existing interpretability systems. First, while it is effective as a black-box
explanation system, SLIM itself is a highly accurate predictive model that
provides faithful self explanations, and thus sidesteps the typical
accuracy-interpretability trade-off. Second, SLIM provides both example- based
and local explanations and can detect global patterns, which allows it to
diagnose limitations in its local explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1&quot;&gt;Gregory Plumb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molitor_D/0/1/0/all/0/1&quot;&gt;Denali Molitor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1&quot;&gt;Ameet Talwalkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02927">
<title>Zero-shot Domain Adaptation without Domain Semantic Descriptors. (arXiv:1807.02927v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02927</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to infer domain-specific models such as classifiers for
unseen domains, from which no data are given in the training phase, without
domain semantic descriptors. When training and test distributions are
different, standard supervised learning methods perform poorly. Zero-shot
domain adaptation attempts to alleviate this problem by inferring models that
generalize well to unseen domains by using training data in multiple source
domains. Existing methods use observed semantic descriptors characterizing
domains such as time information to infer the domain-specific models for the
unseen domains. However, it cannot always be assumed that such metadata can be
used in real-world applications. The proposed method can infer appropriate
domain-specific models without any semantic descriptors by introducing the
concept of latent domain vectors, which are latent representations for the
domains and are used for inferring the models. The latent domain vector for the
unseen domain is inferred from the set of the feature vectors in the
corresponding domain, which is given in the testing phase. The domain-specific
models consist of two components: the first is for extracting a representation
of a feature vector to be predicted, and the second is for inferring model
parameters given the latent domain vector. The posterior distributions of the
latent domain vectors and the domain-specific models are parametrized by neural
networks, and are optimized by maximizing the variational lower bound using
stochastic gradient descent. The effectiveness of the proposed method was
demonstrated through experiments using one regression and two classification
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumagai_A/0/1/0/all/0/1&quot;&gt;Atsutoshi Kumagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1&quot;&gt;Tomoharu Iwata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03026">
<title>Pioneer Networks: Progressively Growing Generative Autoencoder. (arXiv:1807.03026v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03026</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel generative autoencoder network model that learns to
encode and reconstruct images with high quality and resolution, and supports
smooth random sampling from the latent space of the encoder. Generative
adversarial networks (GANs) are known for their ability to simulate random
high-quality images, but they cannot reconstruct existing images. Previous
works have attempted to extend GANs to support such inference but, so far, have
not delivered satisfactory high-quality results. Instead, we propose the
Progressively Growing Generative Autoencoder (PIONEER) network which achieves
high-quality reconstruction with $128{\times}128$ images without requiring a
GAN discriminator. We merge recent techniques for progressively building up the
parts of the network with the recently introduced adversarial encoder-generator
network. The ability to reconstruct input images is crucial in many real-world
applications, and allows for precise intelligent manipulation of existing
images. We show promising results in image synthesis and inference, with
state-of-the-art results in CelebA inference tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heljakka_A/0/1/0/all/0/1&quot;&gt;Ari Heljakka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1&quot;&gt;Juho Kannala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03064">
<title>Temporal Difference Learning with Neural Networks - Study of the Leakage Propagation Problem. (arXiv:1807.03064v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03064</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal-Difference learning (TD) [Sutton, 1988] with function approximation
can converge to solutions that are worse than those obtained by Monte-Carlo
regression, even in the simple case of on-policy evaluation. To increase our
understanding of the problem, we investigate the issue of approximation errors
in areas of sharp discontinuities of the value function being further
propagated by bootstrap updates. We show empirical evidence of this leakage
propagation, and show analytically that it must occur, in a simple Markov
chain, when function approximation errors are present. For reversible policies,
the result can be interpreted as the tension between two terms of the loss
function that TD minimises, as recently described by [Ollivier, 2018]. We show
that the upper bounds from [Tsitsiklis and Van Roy, 1997] hold, but they do not
imply that leakage propagation occurs and under what conditions. Finally, we
test whether the problem could be mitigated with a better state representation,
and whether it can be learned in an unsupervised manner, without rewards or
privileged information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penedones_H/0/1/0/all/0/1&quot;&gt;Hugo Penedones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_D/0/1/0/all/0/1&quot;&gt;Damien Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maennel_H/0/1/0/all/0/1&quot;&gt;Hartmut Maennel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelly_S/0/1/0/all/0/1&quot;&gt;Sylvain Gelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_T/0/1/0/all/0/1&quot;&gt;Timothy Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1&quot;&gt;Andre Barreto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03091">
<title>Computer Assisted Localization of a Heart Arrhythmia. (arXiv:1807.03091v1 [q-bio.TO])</title>
<link>http://arxiv.org/abs/1807.03091</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of locating a point-source heart arrhythmia using
data from a standard diagnostic procedure, where a reference catheter is placed
in the heart, and arrival times from a second diagnostic catheter are recorded
as the diagnostic catheter moves around within the heart. We model this
situation as a nonconvex feasibility problem, where given a set of arrival
times, we look for a source location that is consistent with the available
data. We develop a new optimization approach and fast algorithm to obtain
online proposals for the next location to suggest to the operator as she
collects data. We validate the procedure using a Monte Carlo simulation based
on patients&apos; electrophysiological data. The proposed procedure robustly and
quickly locates the source of arrhythmias without any prior knowledge of heart
anatomy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vogl_C/0/1/0/all/0/1&quot;&gt;Chris Vogl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zheng_P/0/1/0/all/0/1&quot;&gt;Peng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Seslar_S/0/1/0/all/0/1&quot;&gt;Stephen P. Seslar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03095">
<title>Mammography Assessment using Multi-Scale Deep Classifiers. (arXiv:1807.03095v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03095</link>
<description rdf:parseType="Literal">&lt;p&gt;Applying deep learning methods to mammography assessment has remained a
challenging topic. Dense noise with sparse expressions, mega-pixel raw data
resolution, lack of diverse examples have all been factors affecting
performance. The lack of pixel-level ground truths have especially limited
segmentation methods in pushing beyond approximately bounding regions. We
propose a classification approach grounded in high performance tissue
assessment as an alternative to all-in-one localization and assessment models
that is also capable of pinpointing the causal pixels. First, the objective of
the mammography assessment task is formalized in the context of local tissue
classifiers. Then, the accuracy of a convolutional neural net is evaluated on
classifying patches of tissue with suspicious findings at varying scales, where
highest obtained AUC is above $0.9$. The local evaluations of one such expert
tissue classifier is used to augment the results of a heatmap regression model
and additionally recover the exact causal regions at high resolution as a
saliency image suitable for clinical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_U/0/1/0/all/0/1&quot;&gt;Ulzee An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shameer_K/0/1/0/all/0/1&quot;&gt;Khader Shameer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_L/0/1/0/all/0/1&quot;&gt;Lakshmi Subramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03113">
<title>Sampling and Inference for Beta Neutral-to-the-Left Models of Sparse Networks. (arXiv:1807.03113v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03113</link>
<description rdf:parseType="Literal">&lt;p&gt;Empirical evidence suggests that heavy-tailed degree distributions occurring
in many real networks are well-approximated by power laws with exponents $\eta$
that may take values either less than and greater than two. Models based on
various forms of exchangeability are able to capture power laws with $\eta &amp;lt;
2$, and admit tractable inference algorithms; we draw on previous results to
show that $\eta &amp;gt; 2$ cannot be generated by the forms of exchangeability used
in existing random graph models. Preferential attachment models generate power
law exponents greater than two, but have been of limited use as statistical
models due to the inherent difficulty of performing inference in
non-exchangeable models. Motivated by this gap, we design and implement
inference algorithms for a recently proposed class of models that generates
$\eta$ of all possible values. We show that although they are not exchangeable,
these models have probabilistic structure amenable to inference. Our methods
make a large class of previously intractable models useful for statistical
inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bloem_Reddy_B/0/1/0/all/0/1&quot;&gt;Benjamin Bloem-Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1&quot;&gt;Adam Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1&quot;&gt;Emile Mathieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03126">
<title>Estimating Bicycle Route Attractivity from Image Data. (arXiv:1807.03126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03126</link>
<description rdf:parseType="Literal">&lt;p&gt;This master thesis focuses on practical application of Convolutional Neural
Network models on the task of road labeling with bike attractivity score. We
start with an abstraction of real world locations into nodes and scored edges
in partially annotated dataset. We enhance information available about each
edge with photographic data from Google Street View service and with additional
neighborhood information from Open Street Map database. We teach a model on
this enhanced dataset and experiment with ImageNet Large Scale Visual
Recognition Competition. We try different dataset enhancing techniques as well
as various model architectures to improve road scoring. We also make use of
transfer learning to use features from a task with rich dataset of ImageNet
into our task with smaller number of images, to prevent model overfitting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R%5Cr%7Bu%7Dzicka_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;t R&amp;#x16f;&amp;#x17e;i&amp;#x10d;ka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03135">
<title>Deep Networks with Shape Priors for Nucleus Detection. (arXiv:1807.03135v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03135</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of cell nuclei in microscopic images is a challenging research
topic, because of limitations in cellular image quality and diversity of
nuclear morphology, i.e. varying nuclei shapes, sizes, and overlaps between
multiple cell nuclei. This has been a topic of enduring interest with promising
recent success shown by deep learning methods. These methods train for example
convolutional neural networks (CNNs) with a training set of input images and
known, labeled nuclei locations. Many of these methods are supplemented by
spatial or morphological processing. We develop a new approach that we call
Shape Priors with Convolutional Neural Networks (SP-CNN) to perform
significantly enhanced nuclei detection. A set of canonical shapes is prepared
with the help of a domain expert. Subsequently, we present a new network
structure that can incorporate `expected behavior&apos; of nucleus shapes via two
components: {\em learnable} layers that perform the nucleus detection and a
{\em fixed} processing part that guides the learning with prior information.
Analytically, we formulate a new regularization term that is targeted at
penalizing false positives while simultaneously encouraging detection inside
cell nucleus boundary. Experimental results on a challenging dataset reveal
that SP-CNN is competitive with or outperforms several state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tofighi_M/0/1/0/all/0/1&quot;&gt;Mohammad Tofighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tiantong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanamala_J/0/1/0/all/0/1&quot;&gt;Jairam K.P. Vanamala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1&quot;&gt;Vishal Monga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03139">
<title>Utility in Fashion with implicit feedback. (arXiv:1807.03139v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03139</link>
<description rdf:parseType="Literal">&lt;p&gt;Fashion preference is a fuzzy concept that depends on customer taste,
prevailing norms in fashion product/style, henceforth used interchangeably, and
a customer&apos;s perception of utility or fashionability, yet fashion e-retail
relies on algorithmically generated search and recommendation systems that
process structured data and images to best match customer preference. Retailers
study tastes solely as a function of what sold vs what did not, and take it to
represent customer preference. Such explicit modeling, however, belies the
underlying user preference, which is a complicated interplay of preference and
commercials such as brand, price point, promotions, other sale events, and
competitor push/marketing. It is hard to infer a notion of utility or even
customer preference by looking at sales data.
&lt;/p&gt;
&lt;p&gt;In search and recommendation systems for fashion e-retail, customer
preference is implicitly derived by user-user similarity or item-item
similarity. In this work, we aim to derive a metric that separates the buying
preferences of users from the commercials of the merchandise (price,
promotions, etc). We extend our earlier work on explicit signals to gauge
sellability or preference with implicit signals from user behaviour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_V/0/1/0/all/0/1&quot;&gt;Vikram Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sathyanarayana_G/0/1/0/all/0/1&quot;&gt;Girish Sathyanarayana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borar_S/0/1/0/all/0/1&quot;&gt;Sumit Borar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Aruna Rajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03146">
<title>Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning. (arXiv:1807.03146v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03146</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents KeypointNet, an end-to-end geometric reasoning framework
to learn an optimal set of category-specific 3D keypoints, along with their
detectors. Given a single image, KeypointNet extracts 3D keypoints that are
optimized for a downstream task. We demonstrate this framework on 3D pose
estimation by proposing a differentiable objective that seeks the optimal set
of keypoints for recovering the relative pose between two views of an object.
Our model discovers geometrically and semantically consistent keypoints across
viewing angles and instances of an object category. Importantly, we find that
our end-to-end framework using no ground-truth keypoint annotations outperforms
a fully supervised baseline using the same neural network architecture on the
task of pose estimation. The discovered 3D keypoints on the car, chair, and
plane categories of ShapeNet are visualized at &lt;a href=&quot;http://keypointnet.github.io/.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1&quot;&gt;Supasorn Suwajanakorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1&quot;&gt;Noah Snavely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1&quot;&gt;Jonathan Tompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Norouzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03148">
<title>Deep Spatio-Temporal Random Fields for Efficient Video Segmentation. (arXiv:1807.03148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03148</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we introduce a time- and memory-efficient method for structured
prediction that couples neuron decisions across both space at time. We show
that we are able to perform exact and efficient inference on a densely
connected spatio-temporal graph by capitalizing on recent advances on deep
Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)
efficient, (b) has a unique global minimum, and (c) can be trained end-to-end
alongside contemporary deep networks for video understanding. We experiment
with multiple connectivity patterns in the temporal domain, and present
empirical improvements over strong baselines on the tasks of both semantic and
instance segmentation of videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1&quot;&gt;Siddhartha Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokkinos_I/0/1/0/all/0/1&quot;&gt;Iasonas Kokkinos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03149">
<title>Learning models for visual 3D localization with implicit mapping. (arXiv:1807.03149v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03149</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a formulation of visual localization that does not require
construction of explicit maps in the form of point clouds or voxels. The goal
is to learn an implicit representation of the environment at a higher, more
abstract level, for instance that of objects. To study this approach we
consider procedurally generated Minecraft worlds, for which we can generate
visually rich images along with camera pose coordinates. We first show that
Generative Query Networks (GQNs) enhanced with a novel attention mechanism can
capture the visual structure of 3D scenes in Minecraft, as evidenced by their
samples. We then apply the models to the localization problem, investigating
both generative and discriminative approaches, and compare the different ways
in which they each capture task uncertainty. Our results show that models with
implicit mapping are able to capture the underlying 3D structure of visually
complex scenes, and use this to accurately localize new observations, paving
the way towards future applications in sequential localization. Supplementary
video available at https://youtu.be/iHEXX5wXbCI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenbaum_D/0/1/0/all/0/1&quot;&gt;Dan Rosenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besse_F/0/1/0/all/0/1&quot;&gt;Frederic Besse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viola_F/0/1/0/all/0/1&quot;&gt;Fabio Viola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezende_D/0/1/0/all/0/1&quot;&gt;Danilo J. Rezende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1&quot;&gt;S. M. Ali Eslami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03155">
<title>Jigsaw Puzzle Solving Using Local Feature Co-Occurrences in Deep Neural Networks. (arXiv:1807.03155v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03155</link>
<description rdf:parseType="Literal">&lt;p&gt;Archaeologists are in dire need of automated object reconstruction methods.
Fragments reassembly is close to puzzle problems, which may be solved by
computer vision algorithms. As they are often beaten on most image related
tasks by deep learning algorithms, we study a classification method that can
solve jigsaw puzzles. In this paper, we focus on classifying the relative
position: given a couple of fragments, we compute their local relation (e.g. on
top). We propose several enhancements over the state of the art in this domain,
which is outperformed by our method by 25\%. We propose an original dataset
composed of pictures from the Metropolitan Museum of Art. We propose a greedy
reconstruction method based on the predicted relative positions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paumard_M/0/1/0/all/0/1&quot;&gt;Marie-Morgane Paumard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1&quot;&gt;David Picard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabia_H/0/1/0/all/0/1&quot;&gt;Hedi Tabia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03159">
<title>Forecasting Disease Trajectories in Alzheimer&apos;s Disease Using Deep Learning. (arXiv:1807.03159v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03159</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint models for longitudinal and time-to-event data are commonly used in
longitudinal studies to forecast disease trajectories over time. Despite the
many advantages of joint modeling, the standard forms suffer from limitations
that arise from a fixed model specification and computational difficulties when
applied to large datasets. We adopt a deep learning approach to address these
limitations, enhancing existing methods with the flexibility and scalability of
deep neural networks while retaining the benefits of joint modeling. Using data
from the Alzheimer&apos;s Disease Neuroimaging Institute, we show improvements in
performance and scalability compared to traditional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Bryan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03173">
<title>Graph of brain structures grading for early detection of Alzheimer&apos;s disease. (arXiv:1807.03173v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03173</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s disease is the most common dementia leading to an irreversible
neurodegenerative process. To date, subject revealed advanced brain structural
alterations when the diagnosis is established. Therefore, an earlier diagnosis
of this dementia is crucial although it is a challenging task. Recently, many
studies have proposed biomarkers to perform early detection of Alzheimer&apos;s
disease. Some of them have proposed methods based on inter-subject similarity
while other approaches have investigated framework using intra-subject
variability. In this work, we propose a novel framework combining both
approaches within an efficient graph of brain structures grading. Subsequently,
we demonstrate the competitive performance of the proposed method compared to
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hett_K/0/1/0/all/0/1&quot;&gt;Kilian Hett&lt;/a&gt; (LaBRI, CNRS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ta_V/0/1/0/all/0/1&quot;&gt;Vinh-Thong Ta&lt;/a&gt; (Bordeaux INP), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manjon_J/0/1/0/all/0/1&quot;&gt;Jose Vicente Manjon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coupe_P/0/1/0/all/0/1&quot;&gt;Pierrick Coup&amp;#xe9;&lt;/a&gt; (LaBRI, CNRS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03205">
<title>Delayed Bandit Online Learning with Unknown Delays. (arXiv:1807.03205v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03205</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies bandit learning problems with delayed feedback, which
included multi-armed bandit (MAB) and bandit convex optimization (BCO). Given
only function value information (a.k.a. bandit feedback), algorithms for both
MAB and BCO typically rely on (possibly randomized) gradient estimators based
on function values, and then feed them into well-studied gradient-based
algorithms. Different from existing works however, the setting considered here
is more challenging, where the bandit feedback is not only delayed but also the
presence of its delay is not revealed to the learner. Existing algorithms for
delayed MAB and BCO become intractable in this setting. To tackle such
challenging settings, DEXP3 and DBGD have been developed for MAB and BCO,
respectively. Leveraging a unified analysis framework, it is established that
both DEXP3 and DBGD guarantee an ${\cal O}\big( \sqrt{T+D} \big)$ regret over
$T$ time slots with $D$ being the overall delay accumulated over slots. The new
regret bounds match those in full information settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingcong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03233">
<title>A New ECOC Algorithm for Multiclass Microarray Data Classification. (arXiv:1807.03233v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03233</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of multi-class microarray datasets is a hard task because
of the small samples size in each class and the heavy overlaps among classes.
To effectively solve these problems, we propose novel Error Correcting Output
Code (ECOC) algorithm by Enhance Class Separability related Data Complexity
measures during encoding process, named as ECOCECS. In this algorithm, two
nearest neighbor related DC measures are deployed to extract the intrinsic
overlapping information from microarray data. Our ECOC algorithm aims to search
an optimal class split scheme by minimizing these measures. The class splitting
process ends when each class is separated from others, and then the class
assignment scheme is mapped as a coding matrix. Experiments are carried out on
five microarray datasets, and results demonstrate the effectiveness and
robustness of our method in comparison with six state-of-art ECOC methods. In
short, our work confirm the probability of applying DC to ECOC framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mengxin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kunhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1&quot;&gt;Qingqi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Beizhan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03288">
<title>Dynamic Pricing with Finitely Many Unknown Valuations. (arXiv:1807.03288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03288</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by posted price auctions where buyers are grouped in an unknown
number of latent types characterized by their private values for the good on
sale, we investigate revenue maximization in stochastic dynamic pricing when
the distribution of buyers&apos; private values is supported on an unknown set of
points in [0,1] of unknown cardinality K. This setting can be viewed as an
instance of a stochastic K-armed bandit problem where the location of the arms
(the K unknown valuations) must be learned as well. In the distribution-free
case, we show that our setting is just as hard as K-armed stochastic bandits:
we prove that no algorithm can achieve a regret significantly better than
$\sqrt{KT}$, (where T is the time horizon) and present an efficient algorithm
matching this lower bound up to logarithmic factors. In the
distribution-dependent case, we show that for all K&amp;gt;2 our setting is strictly
harder than K-armed stochastic bandits by proving that it is impossible to
obtain regret bounds that grow logarithmically in time or slower. On the other
hand, when a lower bound $\gamma&amp;gt;0$ on the smallest drop in the demand curve is
known, we prove an upper bound on the regret of order $(1/\Delta+(\log \log
T)/\gamma^2)(K\log T)$. This is a significant improvement on previously known
regret bounds for discontinuous demand curves, that are at best of order
$(K^{12}/\gamma^8)\sqrt{T}$. When K=2 in the distribution-dependent case, the
hardness of our setting reduces to that of a stochastic 2-armed bandit: we
prove that an upper bound of order $(\log T)/\Delta$ (up to $\log\log$ factors)
on the regret can be achieved with no information on the demand curve. Finally,
we show a $O(\sqrt{T})$ upper bound on the regret for the setting in which the
buyers&apos; decisions are nonstochastic, and the regret is measured with respect to
the best between two fixed valuations one of which is known to the seller.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1&quot;&gt;Tommaso Cesari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1&quot;&gt;Vianney Perchet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.06404">
<title>Random Walk Models of Network Formation and Sequential Monte Carlo Methods for Graphs. (arXiv:1612.06404v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1612.06404</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a class of generative network models that insert edges by
connecting the starting and terminal vertices of a random walk on the network
graph. Within the taxonomy of statistical network models, this class is
distinguished by permitting the location of a new edge to explicitly depend on
the structure of the graph, but being nonetheless statistically and
computationally tractable. In the limit of infinite walk length, the model
converges to an extension of the preferential attachment model---in this sense,
it can be motivated alternatively by asking what preferential attachment is an
approximation to. Theoretical properties, including the limiting degree
sequence, are studied analytically. If the entire history of the graph is
observed, parameters can be estimated by maximum likelihood. If only the final
graph is available, its history can be imputed using MCMC. We develop a class
of sequential Monte Carlo algorithms that are more generally applicable to
sequential network models, and may be of interest in their own right. The model
parameters can be recovered from a single graph generated by the model.
Applications to data clarify the role of the random walk length as a length
scale of interactions within the graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bloem_Reddy_B/0/1/0/all/0/1&quot;&gt;Benjamin Bloem-Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Orbanz_P/0/1/0/all/0/1&quot;&gt;Peter Orbanz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.06655">
<title>Patchwork Kriging for Large-scale Gaussian Process Regression. (arXiv:1701.06655v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1701.06655</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new approach for Gaussian process (GP) regression for
large datasets. The approach involves partitioning the regression input domain
into multiple local regions with a different local GP model fitted in each
region. Unlike existing local partitioned GP approaches, we introduce a
technique for patching together the local GP models nearly seamlessly to ensure
that the local GP models for two neighboring regions produce nearly the same
response prediction and prediction error variance on the boundary between the
two regions. This largely mitigates the well-known discontinuity problem that
degrades the boundary accuracy of existing local partitioned GP methods. Our
main innovation is to represent the continuity conditions as additional
pseudo-observations that the differences between neighboring GP responses are
identically zero at an appropriately chosen set of boundary input locations. To
predict the response at any input location, we simply augment the actual
response observations with the pseudo-observations and apply standard GP
prediction methods to the augmented data. In contrast to heuristic continuity
adjustments, this has an advantage of working within a formal GP framework, so
that the GP-based predictive uncertainty quantification remains valid. Our
approach also inherits a sparse block-like structure for the sample covariance
matrix, which results in computationally efficient closed-form expressions for
the predictive mean and variance. In addition, we provide a new spatial
partitioning scheme based on a recursive space partitioning along local
principal component directions, which makes the proposed approach applicable
for regression domains having more than two dimensions. Using three spatial
datasets and three higher dimensional datasets, we investigate the numerical
performance of the approach and compare it to several state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chiwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apley_D/0/1/0/all/0/1&quot;&gt;Daniel Apley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.06065">
<title>Block CUR: Decomposing Matrices using Groups of Columns. (arXiv:1703.06065v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06065</link>
<description rdf:parseType="Literal">&lt;p&gt;A common problem in large-scale data analysis is to approximate a matrix
using a combination of specifically sampled rows and columns, known as CUR
decomposition. Unfortunately, in many real-world environments, the ability to
sample specific individual rows or columns of the matrix is limited by either
system constraints or cost. In this paper, we consider matrix approximation by
sampling predefined \emph{blocks} of columns (or rows) from the matrix. We
present an algorithm for sampling useful column blocks and provide novel
guarantees for the quality of the approximation. This algorithm has application
in problems as diverse as biometric data analysis to distributed computing. We
demonstrate the effectiveness of the proposed algorithms for computing the
Block CUR decomposition of large matrices in a distributed setting with
multiple nodes in a compute cluster, where such blocks correspond to columns
(or rows) of the matrix stored on the same node, which can be retrieved with
much less overhead than retrieving individual columns stored across different
nodes. In the biometric setting, the rows correspond to different users and
columns correspond to users&apos; biometric reaction to external stimuli, {\em
e.g.,}~watching video content, at a particular time instant. There is
significant cost in acquiring each user&apos;s reaction to lengthy content so we
sample a few important scenes to approximate the biometric response. An
individual time sample in this use case cannot be queried in isolation due to
the lack of context that caused that biometric reaction. Instead, collections
of time segments ({\em i.e.,} blocks) must be presented to the user. The
practical application of these algorithms is shown via experimental results
using real-world user biometric data from a content testing environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oswal_U/0/1/0/all/0/1&quot;&gt;Urvashi Oswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Swayambhoo Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kevin S. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eriksson_B/0/1/0/all/0/1&quot;&gt;Brian Eriksson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00119">
<title>Bayesian fairness. (arXiv:1706.00119v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00119</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of how decision making can be fair when the
underlying probabilistic model of the world is not known with certainty. We
argue that recent notions of fairness in machine learning need to explicitly
incorporate parameter uncertainty, hence we introduce the notion of {\em
Bayesian fairness} as a suitable candidate for fair decision rules. Using
balance, a definition of fairness introduced by Kleinberg et al (2016), we show
how a Bayesian perspective can lead to well-performing, fair decision rules
even under high uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrakakis_C/0/1/0/all/0/1&quot;&gt;Christos Dimitrakakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1&quot;&gt;David Parkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1&quot;&gt;Goran Radanovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07708">
<title>Per-instance Differential Privacy. (arXiv:1707.07708v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07708</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a refinement of differential privacy --- per instance
differential privacy (pDP), which captures the privacy of a specific individual
with respect to a fixed data set. We show that this is a strict generalization
of the standard DP and inherits all its desirable properties, e.g.,
composition, invariance to side information and closedness to postprocessing,
except that they all hold for every instance separately. When the data is drawn
from a distribution, we show that per-instance DP implies generalization.
Moreover, we provide explicit calculations of the per-instance DP for the
output perturbation on a class of smooth learning problems. The result reveals
an interesting and intuitive fact that an individual has stronger privacy if
he/she has small &quot;leverage score&quot; with respect to the data set and if he/she
can be predicted more accurately using the leave-one-out data set. Our
simulation shows several orders-of-magnitude more favorable privacy and utility
trade-off when we consider the privacy of only the users in the data set. In a
case study on differentially private linear regression, provide a novel
analysis of the One-Posterior-Sample (OPS) estimator and show that when the
data set is well-conditioned it provides $(\epsilon,\delta)$-pDP for any target
individuals and matches the exact lower bound up to a
$1+\tilde{O}(n^{-1}\epsilon^{-2})$ multiplicative factor. We also demonstrate
how we can use a &quot;pDP to DP conversion&quot; step to design AdaOPS which uses
adaptive regularization to achieve the same results with
$(\epsilon,\delta)$-DP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08165">
<title>Fast MCMC sampling algorithms on polytopes. (arXiv:1710.08165v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08165</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and
the John walk, for generating samples from the uniform distribution over a
polytope. Both random walks are sampling algorithms derived from interior point
methods. The former is based on volumetric-logarithmic barrier introduced by
Vaidya whereas the latter uses John&apos;s ellipsoids. We show that the Vaidya walk
mixes in significantly fewer steps than the logarithmic-barrier based Dikin
walk studied in past work. For a polytope in $\mathbb{R}^d$ defined by $n &amp;gt;d$
linear constraints, we show that the mixing time from a warm start is bounded
as $\mathcal{O}(n^{0.5}d^{1.5})$, compared to the $\mathcal{O}(nd)$ mixing time
bound for the Dikin walk. The cost of each step of the Vaidya walk is of the
same order as the Dikin walk, and at most twice as large in terms of constant
pre-factors. For the John walk, we prove an
$\mathcal{O}(d^{2.5}\cdot\log^4(n/d))$ bound on its mixing time and conjecture
that an improved variant of it could achieve a mixing time of
$\mathcal{O}(d^2\cdot\text{polylog}(n/d))$. Additionally, we propose variants
of the Vaidya and John walks that mix in polynomial time from a deterministic
starting point. We illustrate the speed-up of the Vaidya walk over the Dikin
walk via several numerical examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuansi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1&quot;&gt;Raaz Dwivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06061">
<title>Nearly Optimal Robust Subspace Tracking. (arXiv:1712.06061v4 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06061</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the robust subspace tracking (RST) problem and obtain
one of the first two provable guarantees for it. The goal of RST is to track
sequentially arriving data vectors that lie in a slowly changing
low-dimensional subspace, while being robust to corruption by additive sparse
outliers. It can also be interpreted as a dynamic (time-varying) extension of
robust PCA (RPCA), with the minor difference that RST also requires a short
tracking delay. We develop a recursive projected compressive sensing algorithm
that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its
tracking delay is nearly optimal. We prove that NORST solves both the RST and
the dynamic RPCA problems under weakened standard RPCA assumptions, two simple
extra assumptions (slow subspace change and most outlier magnitudes lower
bounded), and a few minor assumptions.
&lt;/p&gt;
&lt;p&gt;Our guarantee shows that NORST enjoys a near optimal tracking delay of $O(r
\log n \log(1/\epsilon))$. Its required delay between subspace change times is
the same, and its memory complexity is $n$ times this value. Thus both these
are also nearly optimal. Here $n$ is the ambient space dimension, $r$ is the
subspaces&apos; dimension, and $\epsilon$ is the tracking accuracy. NORST also has
the best outlier tolerance compared with all previous RPCA or RST methods, both
theoretically and empirically (including for real videos), without requiring
any model on how the outlier support is generated. This is possible because of
the extra assumptions it uses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1&quot;&gt;Praneeth Narayanamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1&quot;&gt;Namrata Vaswani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02309">
<title>Log-concave sampling: Metropolis-Hastings algorithms are fast!. (arXiv:1801.02309v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02309</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of sampling from a strongly log-concave density in
$\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of
the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by
running a Markov chain obtained from the discretization of an appropriate
Langevin diffusion, combined with an accept-reject step to ensure the correct
stationary distribution. Relative to known guarantees for the unadjusted
Langevin algorithm (ULA), our bounds show that the use of an accept-reject step
in MALA leads to an exponentially improved dependence on the error-tolerance.
Concretely, in order to obtain samples with TV error at most $\delta$ for a
density with condition number $\kappa$, we show that MALA requires $\mathcal{O}
\big(\kappa d \log(1/\delta) \big)$ steps, as compared to the $\mathcal{O}
\big(\kappa^2 d/\delta^2 \big)$ steps established in past work on ULA. We also
demonstrate the gains of MALA over ULA for weakly log-concave densities.
Furthermore, we derive mixing time bounds for a zeroth-order method
Metropolized random walk (MRW) and show that it mixes $\mathcal{O}(\kappa d)$
slower than MALA. We provide numerical examples that support our theoretical
findings, and demonstrate the potential gains of Metropolis-Hastings adjustment
for Langevin-type algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1&quot;&gt;Raaz Dwivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuansi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05787">
<title>Faster gaze prediction with dense networks and Fisher pruning. (arXiv:1801.05787v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05787</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting human fixations from images has recently seen large improvements
by leveraging deep representations which were pretrained for object
recognition. However, as we show in this paper, these networks are highly
overparameterized for the task of fixation prediction. We first present a
simple yet principled greedy pruning method which we call Fisher pruning.
Through a combination of knowledge distillation and Fisher pruning, we obtain
much more runtime-efficient architectures for saliency prediction, achieving a
10x speedup for the same AUC performance as a state of the art network on the
CAT2000 dataset. Speeding up single-image gaze prediction is important for many
real-world applications, but it is also a crucial step in the development of
video saliency models, where the amount of data to be processed is
substantially larger.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theis_L/0/1/0/all/0/1&quot;&gt;Lucas Theis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korshunova_I/0/1/0/all/0/1&quot;&gt;Iryna Korshunova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tejani_A/0/1/0/all/0/1&quot;&gt;Alykhan Tejani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huszar_F/0/1/0/all/0/1&quot;&gt;Ferenc Husz&amp;#xe1;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02538">
<title>Yes, but Did It Work?: Evaluating Variational Inference. (arXiv:1802.02538v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02538</link>
<description rdf:parseType="Literal">&lt;p&gt;While it&apos;s always possible to compute a variational approximation to a
posterior distribution, it can be difficult to discover problems with this
approximation. We propose two diagnostic algorithms to alleviate this problem.
The Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of
fit measurement for joint distributions, while simultaneously improving the
error in the estimate. The variational simulation-based calibration (VSBC)
assesses the average performance of point estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuling Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1&quot;&gt;Aki Vehtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simpson_D/0/1/0/all/0/1&quot;&gt;Daniel Simpson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1&quot;&gt;Andrew Gelman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04474">
<title>Deep Neural Networks Learn Non-Smooth Functions Effectively. (arXiv:1802.04474v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04474</link>
<description rdf:parseType="Literal">&lt;p&gt;We theoretically discuss why deep neural networks (DNNs) performs better than
other models in some cases by investigating statistical properties of DNNs for
non-smooth functions. While DNNs have empirically shown higher performance than
other standard methods, understanding its mechanism is still a challenging
problem. From an aspect of the statistical theory, it is known many standard
methods attain the optimal rate of generalization errors for smooth functions
in large sample asymptotics, and thus it has not been straightforward to find
theoretical advantages of DNNs. This paper fills this gap by considering
learning of a certain class of non-smooth functions, which was not covered by
the previous theory. We derive the generalization error of estimators by DNNs
with a ReLU activation, and show that convergence rates of the generalization
by DNNs are almost optimal to estimate the non-smooth functions, while some of
the popular models do not attain the optimal rate. In addition, our theoretical
result provides guidelines for selecting an appropriate number of layers and
edges of DNNs. We provide numerical experiments to support the theoretical
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1&quot;&gt;Masaaki Imaizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fukumizu_K/0/1/0/all/0/1&quot;&gt;Kenji Fukumizu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05187">
<title>On the Blindspots of Convolutional Networks. (arXiv:1802.05187v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05187</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional network has been the state-of-the-art approach for a wide
variety of tasks over the last few years. Its successes have, in many cases,
turned it into the default model in quite a few domains. In this work, we will
demonstrate that convolutional networks have limitations that may, in some
cases, hinder it from learning properties of the data, which are easily
recognizable by traditional, less demanding, models. To this end, we present a
series of competitive analysis studies on image recognition and text analysis
tasks, for which convolutional networks are known to provide state-of-the-art
results. In our studies, we inject a truth-revealing signal, indiscernible for
the network, thus hitting time and again the network&apos;s blind spots. The signal
does not impair the network&apos;s existing performances, but it does provide an
opportunity for a significant performance boost by models that can capture it.
The various forms of the carefully designed signals shed a light on the
strengths and weaknesses of convolutional network, which may provide insights
for both theoreticians that study the power of deep architectures, and for
practitioners that consider applying convolutional networks to the task at
hand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoffer_E/0/1/0/all/0/1&quot;&gt;Elad Hoffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fine_S/0/1/0/all/0/1&quot;&gt;Shai Fine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07928">
<title>Asynchronous Byzantine Machine Learning (the case of SGD). (arXiv:1802.07928v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07928</link>
<description rdf:parseType="Literal">&lt;p&gt;Asynchronous distributed machine learning solutions have proven very
effective so far, but always assuming perfectly functioning workers. In
practice, some of the workers can however exhibit Byzantine behavior, caused by
hardware failures, software bugs, corrupt data, or even malicious attacks. We
introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient
descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of
two complementary components: a filtering and a dampening component. The first
is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers.
Essentially, this filter leverages the Lipschitzness of cost functions and acts
as a self-stabilizer against Byzantine workers that would attempt to corrupt
the progress of SGD. The dampening component bounds the convergence rate by
adjusting to stale information through a generic gradient weighting scheme. We
prove that Kardam guarantees almost sure convergence in the presence of
asynchrony and Byzantine behavior, and we derive its convergence rate. We
evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead
with respect to non Byzantine-resilient solutions. We empirically show that
Kardam does not introduce additional noise to the learning procedure but does
induce a slowdown (the cost of Byzantine resilience) that we both theoretically
and empirically show to be less than $f/n$, where $f$ is the number of
Byzantine failures tolerated and $n$ the total number of workers.
Interestingly, we also empirically observe that the dampening component is
interesting in its own right for it enables to build an SGD algorithm that
outperforms alternative staleness-aware asynchronous competitors in
environments with honest workers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Damaskinos_G/0/1/0/all/0/1&quot;&gt;Georgios Damaskinos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mhamdi_E/0/1/0/all/0/1&quot;&gt;El Mahdi El Mhamdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guerraoui_R/0/1/0/all/0/1&quot;&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Patra_R/0/1/0/all/0/1&quot;&gt;Rhicheek Patra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taziki_M/0/1/0/all/0/1&quot;&gt;Mahsa Taziki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02596">
<title>Revisiting differentially private linear regression: optimal and adaptive prediction &amp; estimation in unbounded domain. (arXiv:1803.02596v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02596</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit the problem of linear regression under a differential privacy
constraint. By consolidating existing pieces in the literature, we clarify the
correct dependence of the feature, label and coefficient domains in the
optimization error and estimation error, hence revealing the delicate price of
differential privacy in statistical estimation and statistical learning.
Moreover, we propose simple modifications of two existing DP algorithms: (a)
posterior sampling, (b) sufficient statistics perturbation, and show that they
can be upgraded into **adaptive** algorithms that are able to exploit
data-dependent quantities and behave nearly optimally **for every instance**.
Extensive experiments are conducted on both simulated data and real data, which
conclude that both AdaOPS and AdaSSP outperform the existing techniques on
nearly all 36 data sets that we test on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08680">
<title>Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization. (arXiv:1803.08680v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08680</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have lately shown tremendous performance in various
applications including vision and speech processing tasks. However, alongside
their ability to perform these tasks with such high accuracy, it has been shown
that they are highly susceptible to adversarial attacks: a small change in the
input would cause the network to err with high confidence. This phenomenon
exposes an inherent fault in these networks and their ability to generalize
well. For this reason, providing robustness to adversarial attacks is an
important challenge in networks training, which has led to extensive research.
In this work, we suggest a theoretically inspired novel approach to improve the
networks&apos; robustness. Our method applies regularization using the Frobenius
norm of the Jacobian of the network, which is applied as post-processing, after
regular training has finished. We demonstrate empirically that it leads to
enhanced robustness results with a minimal change in the original network&apos;s
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakubovitz_D/0/1/0/all/0/1&quot;&gt;Daniel Jakubovitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10366">
<title>Smoothed Online Convex Optimization in High Dimensions via Online Balanced Descent. (arXiv:1803.10366v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10366</link>
<description rdf:parseType="Literal">&lt;p&gt;We study Smoothed Online Convex Optimization, a version of online convex
optimization where the learner incurs a penalty for changing her actions
between rounds. Given a $\Omega(\sqrt{d})$ lower bound on the competitive ratio
of any online algorithm, where $d$ is the dimension of the action space, we ask
under what conditions this bound can be beaten. We introduce a novel
algorithmic framework for this problem, Online Balanced Descent (OBD), which
works by iteratively projecting the previous point onto a carefully chosen
level set of the current cost function so as to balance the switching costs and
hitting costs. We demonstrate the generality of the OBD framework by showing
how, with different choices of &quot;balance,&quot; OBD can improve upon state-of-the-art
performance guarantees for both competitive ratio and regret, in particular,
OBD is the first algorithm to achieve a dimension-free competitive ratio, $3 +
O(1/\alpha)$, for locally polyhedral costs, where $\alpha$ measures the
&quot;steepness&quot; of the costs. We also prove bounds on the dynamic regret of OBD
when the balance is performed in the dual space that are dimension-free and
imply that OBD has sublinear static regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Niangjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_G/0/1/0/all/0/1&quot;&gt;Gautam Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1&quot;&gt;Adam Wierman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04756">
<title>Predictive Uncertainty in Large Scale Classification using Dropout - Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:1805.04756v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04756</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive uncertainty is crucial for many computer vision tasks, from image
classification to autonomous driving systems. Hamiltonian Monte Carlo (HMC) is
an inference method for sampling complex posterior distributions. On the other
hand, Dropout regularization has been proposed as an approximate model
averaging technique that tends to improve generalization in large scale models
such as deep neural networks. Although, HMC provides convergence guarantees for
most standard Bayesian models, it does not handle discrete parameters arising
from Dropout regularization. In this paper, we present a robust methodology for
predictive uncertainty in large scale classification problems, based on Dropout
and Stochastic Gradient Hamiltonian Monte Carlo. Even though Dropout induces a
non-smooth energy function with no such convergence guarantees, the resulting
discretization of the Hamiltonian proves empirical success. The proposed method
allows to effectively estimate predictive accuracy and to provide better
generalization for difficult test examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vergara_D/0/1/0/all/0/1&quot;&gt;Diego Vergara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_S/0/1/0/all/0/1&quot;&gt;Sergio Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdenegro_M/0/1/0/all/0/1&quot;&gt;Mat&amp;#xed;as Valdenegro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jorquera_F/0/1/0/all/0/1&quot;&gt;Felipe Jorquera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09917">
<title>Machine-learning inference of fluid variables from data using reservoir computing. (arXiv:1805.09917v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09917</link>
<description rdf:parseType="Literal">&lt;p&gt;We infer both microscopic and macroscopic behaviors of a chaotic fluid flow
using reservoir computing. In our procedure of the inference, we assume no
prior knowledge of a physical process of a fluid flow except that its behavior
is complex but deterministic. We present two ways of inference of the complex
behavior, the first called partial-inference requires continued knowledge of
partial time-series data during the inference as well as past time-series data,
while the second called full-inference requires only past time-series data as
training data. For the first case, we are able to infer long-time motion of
microscopic fluid variables. For the second case, we show that the reservoir
dynamics constructed from only past data of energy functions can infer the
future behavior of energy functions and reproduce the energy spectrum. This
implies that the obtained reservoir system constructed without the knowledge of
microscopic data is equivalent to the dynamical system describing macroscopic
behavior of energy functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nakai_K/0/1/0/all/0/1&quot;&gt;Kengo Nakai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Saiki_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Saiki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10694">
<title>Towards a Theoretical Understanding of Batch Normalization. (arXiv:1805.10694v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10694</link>
<description rdf:parseType="Literal">&lt;p&gt;Normalization techniques such as Batch Normalization have been applied very
successfully for training deep neural networks. Yet, despite its apparent
empirical benefits, the reasons behind the success of Batch Normalization are
mostly hypothetical. We thus aim to provide a more thorough theoretical
understanding from an optimization perspective. Our main contribution towards
this goal is the identification of various problem instances in the realm of
machine learning where, under certain assumptions, Batch Normalization can
provably accelerate optimization with gradient-based methods. We thereby turn
Batch Normalization from an effective practical heuristic into a provably
converging algorithm for these settings. Furthermore, we substantiate our
analysis with empirical evidence that suggests the validity of our theoretical
results in a broader context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jonas Kohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daneshmand_H/0/1/0/all/0/1&quot;&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neymeyr_K/0/1/0/all/0/1&quot;&gt;Klaus Neymeyr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11429">
<title>Certifying Global Optimality of Graph Cuts via Semidefinite Relaxation: A Performance Guarantee for Spectral Clustering. (arXiv:1806.11429v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.11429</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering has become one of the most widely used clustering
techniques when the structure of the individual clusters is non-convex or
highly anisotropic. Yet, despite its immense popularity, there exists fairly
little theory about performance guarantees for spectral clustering. This issue
is partly due to the fact that spectral clustering typically involves two steps
which complicated its theoretical analysis: first, the eigenvectors of the
associated graph Laplacian are used to embed the dataset, and second, k-means
clustering algorithm is applied to the embedded dataset to get the labels. This
paper is devoted to the theoretical foundations of spectral clustering and
graph cuts. We consider a convex relaxation of graph cuts, namely ratio cuts
and normalized cuts, that makes the usual two-step approach of spectral
clustering obsolete and at the same time gives rise to a rigorous theoretical
analysis of graph cuts and spectral clustering. We derive deterministic bounds
for successful spectral clustering via a spectral proximity condition that
naturally depends on the algebraic connectivity of each cluster and the
inter-cluster connectivity. Moreover, we demonstrate by means of some popular
examples that our bounds can achieve near-optimality. Our findings are also
fundamental for the theoretical understanding of kernel k-means. Numerical
simulations confirm and complement our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ling_S/0/1/0/all/0/1&quot;&gt;Shuyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strohmer_T/0/1/0/all/0/1&quot;&gt;Thomas Strohmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01808">
<title>Discrete Sampling using Semigradient-based Product Mixtures. (arXiv:1807.01808v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01808</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of inference in discrete probabilistic models, that
is, distributions over subsets of a finite ground set. These encompass a range
of well-known models in machine learning, such as determinantal point processes
and Ising models. Locally-moving Markov chain Monte Carlo algorithms, such as
the Gibbs sampler, are commonly used for inference in such models, but their
convergence is, at times, prohibitively slow. This is often caused by
state-space bottlenecks that greatly hinder the movement of such samplers. We
propose a novel sampling strategy that uses a specific mixture of product
distributions to propose global moves and, thus, accelerate convergence.
Furthermore, we show how to construct such a mixture using semigradient
information. We illustrate the effectiveness of combining our sampler with
existing ones, both theoretically on an example model, as well as practically
on three models learned from real-world data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotovos_A/0/1/0/all/0/1&quot;&gt;Alkis Gotovos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item></rdf:RDF>