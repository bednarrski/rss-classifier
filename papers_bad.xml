<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10394"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10105"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10108"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02908"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09964"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10026"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10038"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10261"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10356"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10430"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.01118"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07821"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05484"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02129"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02933"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08763"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09526"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.10394">
<title>Symbolic regression based genetic approximations of the Colebrook equation for flow friction. (arXiv:1808.10394v1 [cs.CE])</title>
<link>http://arxiv.org/abs/1808.10394</link>
<description rdf:parseType="Literal">&lt;p&gt;Widely used in hydraulics, the Colebrook equation for flow friction relates
implicitly to the input parameters; the Reynolds number, and the relative
roughness of inner pipe surface, with the output unknown parameter; the flow
friction factor. In this paper, a few explicit approximations to the Colebrook
equation are generated using the ability of artificial intelligence to make
inner patterns to connect input and output parameters in explicit way not
knowing their nature or the physical law that connects them, but only knowing
raw numbers. The fact that the used genetic programming tool does not know the
structure of the Colebrook equation which is based on computationally expensive
logarithmic law, is used to obtain better structure of the approximations which
is less demanding for calculation but also enough accurate. All generated
approximations are with low computational cost because they contain a limited
number of logarithmic forms used although for normalization of input parameters
or for acceleration, but they are also sufficiently accurate. The relative
error regarding the friction factor in best case is up to 0.13% with only two
logarithmic forms used. As the second logarithm can be accurately approximated
by the Pade approximation, practically the same error is obtained also using
only one logarithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Praks_P/0/1/0/all/0/1&quot;&gt;Pavel Praks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brkic_D/0/1/0/all/0/1&quot;&gt;Dejan Brkic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10012">
<title>Reasoning about Actions and State Changes by Injecting Commonsense Knowledge. (arXiv:1808.10012v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10012</link>
<description rdf:parseType="Literal">&lt;p&gt;Comprehending procedural text, e.g., a paragraph describing photosynthesis,
requires modeling actions and the state changes they produce, so that questions
about entities at different timepoints can be answered. Although several recent
systems have shown impressive progress in this task, their predictions can be
globally inconsistent or highly improbable. In this paper, we show how the
predicted effects of actions in the context of a paragraph can be improved in
two ways: (1) by incorporating global, commonsense constraints (e.g., a
non-existent entity cannot be destroyed), and (2) by biasing reading with
preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier
methods, we treat the problem as a neural structured prediction task, allowing
hard and soft constraints to steer the model away from unlikely predictions. We
show that the new model significantly outperforms earlier systems on a
benchmark dataset for procedural text comprehension (+8% relative gain), and
that it also avoids some of the nonsensical predictions that earlier systems
make.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1&quot;&gt;Niket Tandon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bhavana Dalvi Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grus_J/0/1/0/all/0/1&quot;&gt;Joel Grus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1&quot;&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1&quot;&gt;Antoine Bosselut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10104">
<title>Modeling OWL with Rules: The ROWL Protege Plugin. (arXiv:1808.10104v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10104</link>
<description rdf:parseType="Literal">&lt;p&gt;In our experience, some ontology users find it much easier to convey logical
statements using rules rather than OWL (or description logic) axioms. Based on
recent theoretical developments on transformations between rules and
description logics, we develop ROWL, a Protege plugin that allows users to
enter OWL axioms by way of rules; the plugin then automatically converts these
rules into OWL DL axioms if possible, and prompts the user in case such a
conversion is not possible without weakening the semantics of the rule.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md. Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carral_D/0/1/0/all/0/1&quot;&gt;David Carral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1&quot;&gt;Adila A. Krisnadhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10105">
<title>OWLAx: A Protege Plugin to Support Ontology Axiomatization through Diagramming. (arXiv:1808.10105v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10105</link>
<description rdf:parseType="Literal">&lt;p&gt;Once the conceptual overview, in terms of a somewhat informal class diagram,
has been designed in the course of engineering an ontology, the process of
adding many of the appropriate logical axioms is mostly a routine task. We
provide a Protege plugin which supports this task, together with a visual user
interface, based on established methods for ontology design pattern modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md. Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1&quot;&gt;Adila A. Krisnadhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10108">
<title>Rule-based OWL Modeling with ROWLTab Protege Plugin. (arXiv:1808.10108v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10108</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been argued that it is much easier to convey logical statements using
rules rather than OWL (or description logic (DL)) axioms. Based on recent
theoretical developments on transformations between rules and DLs, we have
developed ROWLTab, a Protege plugin that allows users to enter OWL axioms by
way of rules; the plugin then automatically converts these rules into OWL 2 DL
axioms if possible, and prompts the user in case such a conversion is not
possible without weakening the semantics of the rule. In this paper, we present
ROWLTab, together with a user evaluation of its effectiveness compared to
entering axioms using the standard Protege interface. Our evaluation shows that
modeling with ROWLTab is much quicker than the standard interface, while at the
same time, also less prone to errors for hard modeling tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md. Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1&quot;&gt;Adila Krisnadhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carral_D/0/1/0/all/0/1&quot;&gt;David Carral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10369">
<title>Robot_gym: accelerated robot training through simulation in the cloud with ROS and Gazebo. (arXiv:1808.10369v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.10369</link>
<description rdf:parseType="Literal">&lt;p&gt;Rather than programming, training allows robots to achieve behaviors that
generalize better and are capable to respond to real-world needs. However, such
training requires a big amount of experimentation which is not always feasible
for a physical robot. In this work, we present robot_gym, a framework to
accelerate robot training through simulation in the cloud that makes use of
roboticists&apos; tools, simplifying the development and deployment processes on
real robots. We unveil that, for simple tasks, simple 3DoF robots require more
than 140 attempts to learn. For more complex, 6DoF robots, the number of
attempts increases to more than 900 for the same task. We demonstrate that our
framework, for simple tasks, accelerates the robot training time by more than
33% while maintaining similar levels of accuracy and repeatability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilches_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Mayoral Vilches&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordero_A/0/1/0/all/0/1&quot;&gt;Alejandro Hern&amp;#xe1;ndez Cordero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvo_A/0/1/0/all/0/1&quot;&gt;Asier Bilbao Calvo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugarte_I/0/1/0/all/0/1&quot;&gt;Irati Zamalloa Ugarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kojcev_R/0/1/0/all/0/1&quot;&gt;Risto Kojcev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10238">
<title>DeepSOFA: A Continuous Acuity Score for Critically Ill Patients using Clinically Interpretable Deep Learning. (arXiv:1802.10238v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.10238</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional methods for assessing illness severity and predicting in-hospital
mortality among critically ill patients require time-consuming, error-prone
calculations using static variable thresholds. These methods do not capitalize
on the emerging availability of streaming electronic health record data or
capture time-sensitive individual physiological patterns, a critical task in
the intensive care unit. We propose a novel acuity score framework (DeepSOFA)
that leverages temporal measurements and interpretable deep learning models to
assess illness severity at any point during an ICU stay. We compare DeepSOFA
with SOFA (Sequential Organ Failure Assessment) baseline models using the same
model inputs and find that at any point during an ICU admission, DeepSOFA
yields significantly more accurate predictions of in-hospital mortality. A
DeepSOFA model developed in a public database and validated in a single
institutional cohort had a mean AUC for the entire ICU stay of 0.90 (95% CI
0.90-0.91) compared with baseline SOFA models with mean AUC 0.79 (95% CI
0.79-0.80) and 0.85 (95% CI 0.85-0.86). Deep models are well-suited to identify
ICU patients in need of life-saving interventions prior to the occurrence of an
unexpected adverse event and inform shared decision-making processes among
patients, providers, and families regarding goals of care and optimal resource
utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftus_T/0/1/0/all/0/1&quot;&gt;Tyler J. Loftus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhikari_L/0/1/0/all/0/1&quot;&gt;Lasith Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozrazgat_Baslanti_T/0/1/0/all/0/1&quot;&gt;Tezcan Ozrazgat-Baslanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02711">
<title>POTs: Protective Optimization Technologies. (arXiv:1806.02711v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02711</link>
<description rdf:parseType="Literal">&lt;p&gt;In spite of their many advantages, optimization systems often neglect the
economic, ethical, moral, social, and political impact they have on populations
and their environments. In this paper we argue that the frameworks through
which the discontents of optimization systems have been approached so far cover
a narrow subset of these problems by (i) assuming that the system provider has
the incentives and means to mitigate the imbalances optimization causes, (ii)
disregarding problems that go beyond discrimination due to disparate treatment
or impact in algorithmic decision making, and (iii) developing solutions
focused on removing algorithmic biases related to discrimination.
&lt;/p&gt;
&lt;p&gt;In response we introduce Protective Optimization Technologies: solutions that
enable optimization subjects to defend from unwanted consequences. We provide a
framework that formalizes the design space of POTs and show how it differs from
other design paradigms in the literature. We show how the framework can capture
strategies developed in the wild against real optimization systems, and how it
can be used to design, implement, and evaluate a POT that enables individuals
and collectives to protect themselves from unbalances in a credit scoring
application related to loan allocation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Overdorf_R/0/1/0/all/0/1&quot;&gt;Rebekah Overdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulynych_B/0/1/0/all/0/1&quot;&gt;Bogdan Kulynych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balsa_E/0/1/0/all/0/1&quot;&gt;Ero Balsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troncoso_C/0/1/0/all/0/1&quot;&gt;Carmela Troncoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurses_S/0/1/0/all/0/1&quot;&gt;Seda G&amp;#xfc;rses&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02908">
<title>Is preprocessing of text really worth your time for online comment classification?. (arXiv:1806.02908v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02908</link>
<description rdf:parseType="Literal">&lt;p&gt;A large proportion of online comments present on public domains are
constructive, however a significant proportion are toxic in nature. The
comments contain lot of typos which increases the number of features manifold,
making the ML model difficult to train. Considering the fact that the data
scientists spend approximately 80% of their time in collecting, cleaning and
organizing their data [1], we explored how much effort should we invest in the
preprocessing (transformation) of raw comments before feeding it to the
state-of-the-art classification models. With the help of four models on Jigsaw
toxic comment classification data, we demonstrated that the training of model
without any transformation produce relatively decent model. Applying even basic
transformations, in some cases, lead to worse performance and should be applied
with caution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammad_F/0/1/0/all/0/1&quot;&gt;Fahim Mohammad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03737">
<title>Learning Multi-touch Conversion Attribution with Dual-attention Mechanisms for Online Advertising. (arXiv:1808.03737v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03737</link>
<description rdf:parseType="Literal">&lt;p&gt;In online advertising, the Internet users may be exposed to a sequence of
different ad campaigns, i.e., display ads, search, or referrals from multiple
channels, before led up to any final sales conversion and transaction. For both
campaigners and publishers, it is fundamentally critical to estimate the
contribution from ad campaign touch-points during the customer journey
(conversion funnel) and assign the right credit to the right ad exposure
accordingly. However, the existing research on the multi-touch attribution
problem lacks a principled way of utilizing the users&apos; pre-conversion actions
(i.e., clicks), and quite often fails to model the sequential patterns among
the touch points from a user&apos;s behavior data. To make it worse, the current
industry practice is merely employing a set of arbitrary rules as the
attribution model, e.g., the popular last-touch model assigns 100% credit to
the final touch-point regardless of actual attributions. In this paper, we
propose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch
attribution problem. It learns the attribution values through an attention
mechanism directly from the conversion estimation objective. To achieve this,
we utilize sequence-to-sequence prediction for user clicks, and combine both
post-view and post-click attribution patterns together for the final conversion
estimation. To quantitatively benchmark attribution models, we also propose a
novel yet practical attribution evaluation scheme through the proxy of budget
allocation (under the estimated attributions) over ad channels. The
experimental results on two real datasets demonstrate the significant
performance gains of our attribution model against the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08794">
<title>Theoretical Foundations of the A2RD Project: Part I. (arXiv:1808.08794v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08794</link>
<description rdf:parseType="Literal">&lt;p&gt;This article identifies and discusses the theoretical foundations that were
considered in the design of the A2RD model. In addition to the points
considered, references are made to the studies available and considered in the
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09964">
<title>Semi-Metrification of the Dynamic Time Warping Distance. (arXiv:1808.09964v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09964</link>
<description rdf:parseType="Literal">&lt;p&gt;The dynamic time warping (dtw) distance fails to satisfy the triangle
inequality and the identity of indiscernibles. As a consequence, the
dtw-distance is not warping-invariant, which in turn results in peculiarities
in data mining applications. This article converts the dtw-distance to a
semi-metric and shows that its canonical extension is warping-invariant.
Empirical results indicate that the nearest-neighbor classifier in the proposed
semi-metric space performs comparable to the same classifier in the standard
dtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result
suggest to further explore the semi-metric space for data mining applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Brijnesh J. Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10013">
<title>Group calibration is a byproduct of unconstrained learning. (arXiv:1808.10013v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10013</link>
<description rdf:parseType="Literal">&lt;p&gt;Much recent work on fairness in machine learning has focused on how well a
score function is calibrated in different groups within a given population,
where each group is defined by restricting one or more sensitive attributes.
&lt;/p&gt;
&lt;p&gt;We investigate to which extent group calibration follows from unconstrained
empirical risk minimization on its own, without the need for any explicit
intervention. We show that under reasonable conditions, the deviation from
satisfying group calibration is bounded by the excess loss of the empirical
risk minimizer relative to the Bayes optimal score function. As a corollary, it
follows that empirical risk minimization can simultaneously achieve calibration
for many groups, a task that prior work deferred to highly complex algorithms.
We complement our results with a lower bound, and a range of experimental
findings.
&lt;/p&gt;
&lt;p&gt;Our results challenge the view that group calibration necessitates an active
intervention, suggesting that often we ought to think of it as a byproduct of
unconstrained machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lydia T. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1&quot;&gt;Moritz Hardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10026">
<title>Physically-inspired Gaussian processes for transcriptional regulation in Drosophila melanogaster. (arXiv:1808.10026v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.10026</link>
<description rdf:parseType="Literal">&lt;p&gt;The regulatory process in Drosophila melanogaster is thoroughly studied for
understanding several principles in systems biology. Since transcriptional
regulation of the Drosophila depends on spatiotemporal interactions between
mRNA expressions and gap-gene proteins, proper physically-inspired stochastic
models are required to describe the existing link between both biological
quantities. Many studies have shown that the use of Gaussian processes (GPs)
and differential equations yields promising inference results when modelling
regulatory processes. In order to exploit the benefits of GPs, two types of
physically-inspired GPs based on the reaction-diffusion equation are further
investigated in this paper. The main difference between both approaches lies on
whether the GP prior is placed: either over mRNA expressions or protein
concentrations. Contrarily to other stochastic frameworks, discretising the
spatial space is not required here. Both GP models are tested under different
conditions depending on the availability of biological data. Finally, their
performances are assessed using a high-resolution dataset describing the
blastoderm stage of the early embryo of Drosophila.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopez_Lopera_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s F. L&amp;#xf3;pez-Lopera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Durrande_N/0/1/0/all/0/1&quot;&gt;Nicolas Durrande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alvarez_M/0/1/0/all/0/1&quot;&gt;Mauricio A. Alvarez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10038">
<title>Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds. (arXiv:1808.10038v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10038</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, unfolding iterative algorithms as neural networks has become
an empirical success in solving sparse recovery problems. However, its
theoretical understanding is still immature, which prevents us from fully
utilizing the power of neural networks. In this work, we study unfolded ISTA
(Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We
introduce a weight structure that is necessary for asymptotic convergence to
the true sparse signal. With this structure, unfolded ISTA can attain a linear
convergence, which is better than the sublinear convergence of ISTA/FISTA in
general cases. Furthermore, we propose to incorporate thresholding in the
network to perform support selection, which is easy to implement and able to
boost the convergence rate both theoretically and empirically. Extensive
simulations, including sparse vector recovery and a compressive sensing
experiment on real image data, corroborate our theoretical results and
demonstrate their practical usefulness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jialin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10073">
<title>Rational Neural Networks for Approximating Jump Discontinuities of Graph Convolution Operator. (arXiv:1808.10073v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10073</link>
<description rdf:parseType="Literal">&lt;p&gt;For node level graph encoding, a recent important state-of-art method is the
graph convolutional networks (GCN), which nicely integrate local vertex
features and graph topology in the spectral domain. However, current studies
suffer from several drawbacks: (1) graph CNNs relies on Chebyshev polynomial
approximation which results in oscillatory approximation at jump
discontinuities; (2) Increasing the order of Chebyshev polynomial can reduce
the oscillations issue, but also incurs unaffordable computational cost; (3)
Chebyshev polynomials require degree $\Omega$(poly(1/$\epsilon$)) to
approximate a jump signal such as $|x|$, while rational function only needs
$\mathcal{O}$(poly log(1/$\epsilon$))\cite{liang2016deep,telgarsky2017neural}.
However, it&apos;s non-trivial to apply rational approximation without increasing
computational complexity due to the denominator. In this paper, the superiority
of rational approximation is exploited for graph signal recovering. RatioanlNet
is proposed to integrate rational function and neural networks. We show that
rational function of eigenvalues can be rewritten as a function of graph
Laplacian, which can avoid multiplication by the eigenvector matrix. Focusing
on the analysis of approximation on graph convolution operation, a graph signal
regression task is formulated. Under graph signal regression task, its time
complexity can be significantly reduced by graph Fourier transform. To overcome
the local minimum problem of neural networks model, a relaxed Remez algorithm
is utilized to initialize the weight parameters. Convergence rate of
RatioanlNet and polynomial based methods on jump signal is analyzed for a
theoretical guarantee. The extensive experimental results demonstrated that our
approach could effectively characterize the jump discontinuities, outperforming
competing methods by a substantial margin on both synthetic and real-world
graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1&quot;&gt;Rongjie Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chang-Tien Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10078">
<title>Discriminative Learning of Similarity and Group Equivariant Representations. (arXiv:1808.10078v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.10078</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most fundamental problems in machine learning is to compare
examples: Given a pair of objects we want to return a value which indicates
degree of (dis)similarity. Similarity is often task specific, and pre-defined
distances can perform poorly, leading to work in metric learning. However,
being able to learn a similarity-sensitive distance function also presupposes
access to a rich, discriminative representation for the objects at hand. In
this dissertation we present contributions towards both ends. In the first part
of the thesis, assuming good representations for the data, we present a
formulation for metric learning that makes a more direct attempt to optimize
for the k-NN accuracy as compared to prior work. We also present extensions of
this formulation to metric learning for kNN regression, asymmetric similarity
learning and discriminative learning of Hamming distance. In the second part,
we consider a situation where we are on a limited computational budget i.e.
optimizing over a space of possible metrics would be infeasible, but access to
a label aware distance metric is still desirable. We present a simple, and
computationally inexpensive approach for estimating a well motivated metric
that relies only on gradient estimates, discussing theoretical and experimental
results. In the final part, we address representational issues, considering
group equivariant convolutional neural networks (GCNNs). Equivariance to
symmetry transformations is explicitly encoded in GCNNs; a classical CNN being
the simplest example. In particular, we present a SO(3)-equivariant neural
network architecture for spherical data, that operates entirely in Fourier
space, while also providing a formalism for the design of fully Fourier neural
networks that are equivariant to the action of any continuous compact group.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trivedi_S/0/1/0/all/0/1&quot;&gt;Shubhendu Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10101">
<title>DP-ADMM: ADMM-based Distributed Learning with Differential Privacy. (arXiv:1808.10101v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10101</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed machine learning is making great changes in a wide variety of
domains but also brings privacy risk from the exchanged information during the
learning process. This paper focuses on a class of regularized empirical risk
minimization problems, and develops a privacy-preserving distributed learning
algorithm. We use Alternating Direction Method of Multipliers (ADMM) to
decentralize the learning algorithm, and apply Gaussian mechanisms locally to
guarantee differential privacy. However, simply combining ADMM and local
randomization mechanisms would result in an unconvergent algorithm with bad
performance, especially when the introduced noise is large to guarantee a low
total privacy loss. Besides, this approach cannot be applied to the learning
problems with non-smooth objective functions. To figure out these concerns, we
propose an improved ADMM-based differentially private distributed learning
algorithm: DP-ADMM, where an approximate augmented Lagrangian function and
Gaussian mechanisms with time-varying variance are utilized. We also apply the
moment accountant method to bound the total privacy loss. Our theoretical
analysis proves that DP-ADMM can be applied to a general class of convex
learning problems, provides differential privacy guarantee, and achieves an
$O(1/\sqrt{t})$ rate of convergence, where $t$ is the number of iterations. Our
evaluations demonstrate that our approach can achieve good accuracy and
effectiveness even with a low total privacy leakage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zonghao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yanmin Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_Tin_E/0/1/0/all/0/1&quot;&gt;Eric Chan-Tin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10261">
<title>Centroid estimation based on symmetric KL divergence for Multinomial text classification problem. (arXiv:1808.10261v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.10261</link>
<description rdf:parseType="Literal">&lt;p&gt;We define a new method to estimate centroid for text classification based on
the symmetric KL-divergence between the distribution of words in training
documents and their class centroids. Experiments on several standard data sets
indicate that the new method achieves substantial improvements over the
traditional classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiangning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matzinger_H/0/1/0/all/0/1&quot;&gt;Heinrich Matzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_H/0/1/0/all/0/1&quot;&gt;Haoyan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10340">
<title>A Coordinate-Free Construction of Scalable Natural Gradient. (arXiv:1808.10340v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10340</link>
<description rdf:parseType="Literal">&lt;p&gt;Most neural networks are trained using first-order optimization methods,
which are sensitive to the parameterization of the model. Natural gradient
descent is invariant to smooth reparameterizations because it is defined in a
coordinate-free way, but tractable approximations are typically defined in
terms of coordinate systems, and hence may lose the invariance properties. We
analyze the invariance properties of the Kronecker-Factored Approximate
Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free
way. We explicitly construct a Riemannian metric under which the natural
gradient matches the K-FAC update; invariance to affine transformations of the
activations follows immediately. We extend our framework to analyze the
invariance properties of K-FAC applied to convolutional networks and recurrent
neural networks, as well as metrics other than the usual Fisher metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luk_K/0/1/0/all/0/1&quot;&gt;Kevin Luk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10356">
<title>Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images. (arXiv:1808.10356v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10356</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have been shown to produce
realistically looking synthetic images with remarkable success, yet their
performance seems less impressive when the training set is highly diverse. In
order to provide a better fit to the target data distribution when the dataset
includes many different classes, we propose a variant of the basic GAN model,
called Gaussian Mixture GAN (GM-GAN), where the probability distribution over
the latent space is a mixture of Gaussians. We also propose a supervised
variant which is capable of conditional sample synthesis. In order to evaluate
the model&apos;s performance, we propose a new scoring method which separately takes
into account two (typically conflicting) measures - diversity vs. quality of
the generated data. Through a series of empirical experiments, using both
synthetic and real-world datasets, we quantitatively show that GM-GANs
outperform baselines, both when evaluated using the commonly used Inception
Score, and when evaluated using our own alternative scoring method. In
addition, we qualitatively demonstrate how the \textit{unsupervised} variant of
GM-GAN tends to map latent vectors sampled from different Gaussians in the
latent space to samples of different classes in the data space. We show how
this phenomenon can be exploited for the task of unsupervised clustering, and
provide quantitative evaluation showing the superiority of our method for the
unsupervised clustering of image datasets. Finally, we demonstrate a feature
which further sets our model apart from other GAN models: the option to control
the quality-diversity trade-off by altering, post-training, the probability
distribution of the latent space. This allows one to sample higher quality and
lower diversity samples, or vice versa, according to one&apos;s needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Yosef_M/0/1/0/all/0/1&quot;&gt;Matan Ben-Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1&quot;&gt;Daphna Weinshall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10396">
<title>A Unified Analysis of Stochastic Momentum Methods for Deep Learning. (arXiv:1808.10396v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10396</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic momentum methods have been widely adopted in training deep neural
networks. However, their theoretical analysis of convergence of the training
objective and the generalization error for prediction is still under-explored.
This paper aims to bridge the gap between practice and theory by analyzing the
stochastic gradient (SG) method, and the stochastic momentum methods including
two famous variants, i.e., the stochastic heavy-ball (SHB) method and the
stochastic variant of Nesterov&apos;s accelerated gradient (SNAG) method. We propose
a framework that unifies the three variants. We then derive the convergence
rates of the norm of gradient for the non-convex optimization problem, and
analyze the generalization performance through the uniform stability approach.
Particularly, the convergence analysis of the training objective exhibits that
SHB and SNAG have no advantage over SG. However, the stability analysis shows
that the momentum term can improve the stability of the learned model and hence
improve the generalization performance. These theoretical insights verify the
common wisdom and are also corroborated by our empirical analysis on deep
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qihang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10406">
<title>Towards Reproducible Empirical Research in Meta-Learning. (arXiv:1808.10406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-learning is increasingly used to support the recommendation of machine
learning algorithms and their configurations. Such recommendations are made
based on meta-data, consisting of performance evaluations of algorithms on
prior datasets, as well as characterizations of these datasets. These
characterizations, also called meta-features, describe properties of the data
which are predictive for the performance of machine learning algorithms trained
on them. Unfortunately, despite being used in a large number of studies,
meta-features are not uniformly described and computed, making many empirical
studies irreproducible and hard to compare. This paper aims to remedy this by
systematizing and standardizing data characterization measures used in
meta-learning, and performing an in-depth analysis of their utility. Moreover,
it presents MFE, a new tool for extracting meta-features from datasets and
identify more subtle reproducibility issues in the literature, proposing
guidelines for data characterization that strengthen reproducible empirical
research in meta-learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivolli_A/0/1/0/all/0/1&quot;&gt;Adriano Rivolli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s P. F. Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1&quot;&gt;Carlos Soares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1&quot;&gt;Joaquin Vanschoren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; C. P. L. F. de Carvalho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10430">
<title>Nested multi-instance classification. (arXiv:1808.10430v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.10430</link>
<description rdf:parseType="Literal">&lt;p&gt;There are classification tasks that take as inputs groups of images rather
than single images. In order to address such situations, we introduce a nested
multi-instance deep network. The approach is generic in that it is applicable
to general data instances, not just images. The network has several
convolutional neural networks grouped together at different stages. This
primarily differs from other previous works in that we organize instances into
relevant groups that are treated differently. We also introduce a method to
replace instances that are missing which successfully creates neutral input
instances and consistently outperforms standard fill-in methods in real world
use cases. In addition, we propose a method for manual dropout when a whole
group of instances is missing that allows us to use richer training data and
obtain higher accuracy at the end of training. With specific pretraining, we
find that the model works to great effect on our real world and pub-lic
datasets in comparison to baseline methods, justifying the different treatment
among groups of instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stec_A/0/1/0/all/0/1&quot;&gt;Alexander Stec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Utke_J/0/1/0/all/0/1&quot;&gt;Jean Utke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.01118">
<title>A supermartingale approach to Gaussian process based sequential design of experiments. (arXiv:1608.01118v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1608.01118</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian process (GP) models have become a well-established frameworkfor the
adaptive design of costly experiments, and notably of computerexperiments.
GP-based sequential designs have been found practicallyefficient for various
objectives, such as global optimization(estimating the global maximum or
maximizer(s) of a function),reliability analysis (estimating a probability of
failure) or theestimation of level sets and excursion sets. In this paper, we
studythe consistency of an important class of sequential designs, known
asstepwise uncertainty reduction (SUR) strategies. Our approach relieson the
key observation that the sequence of residual uncertaintymeasures, in SUR
strategies, is generally a supermartingale withrespect to the filtration
generated by the observations. Thisobservation enables us to establish generic
consistency results for abroad class of SUR strategies. The consistency of
several popularsequential design strategies is then obtained by means of this
generalresult. Notably, we establish the consistency of two SUR
strategiesproposed by Bect, Ginsbourger, Li, Picheny and Vazquez (Stat.
Comp.,2012)---to the best of our knowledge, these are the first proofs
ofconsistency for GP-based sequential design algorithms dedicated to
theestimation of excursion sets and their measure. We also establish anew, more
general proof of consistency for the expected improvementalgorithm for global
optimization which, unlike previous results inthe literature, applies to any GP
with continuous sample paths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bect_J/0/1/0/all/0/1&quot;&gt;Julien Bect&lt;/a&gt; (L2S, GdR MASCOT-NUM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bachoc_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Bachoc&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ginsbourger_D/0/1/0/all/0/1&quot;&gt;David Ginsbourger&lt;/a&gt; (IMSV)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02586">
<title>DSOS and SDSOS Optimization: More Tractable Alternatives to Sum of Squares and Semidefinite Optimization. (arXiv:1706.02586v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02586</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, optimization theory has been greatly impacted by the advent
of sum of squares (SOS) optimization. The reliance of this technique on
large-scale semidefinite programs however, has limited the scale of problems to
which it can be applied. In this paper, we introduce DSOS and SDSOS
optimization as linear programming and second-order cone programming-based
alternatives to sum of squares optimization that allow one to trade off
computation time with solution quality. These are optimization problems over
certain subsets of sum of squares polynomials (or equivalently subsets of
positive semidefinite matrices), which can be of interest in general
applications of semidefinite programming where scalability is a limitation. We
show that some basic theorems from SOS optimization which rely on results from
real algebraic geometry are still valid for DSOS and SDSOS optimization.
Furthermore, we show with numerical experiments from diverse application
areas---polynomial optimization, statistics and machine learning, derivative
pricing, and control theory---that with reasonable tradeoffs in accuracy, we
can handle problems at scales that are currently significantly beyond the reach
of traditional sum of squares approaches. Finally, we provide a review of
recent techniques that bridge the gap between our DSOS/SDSOS approach and the
SOS approach at the expense of additional running time. The Supplementary
Material of the paper introduces an accompanying MATLAB package for DSOS and
SDSOS optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1&quot;&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Anirudha Majumdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07821">
<title>Concept Drift Detection and Adaptation with Hierarchical Hypothesis Testing. (arXiv:1707.07821v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07821</link>
<description rdf:parseType="Literal">&lt;p&gt;When using statistical models (such as a classifier) in a streaming
environment, there is often a need to detect and adapt to concept drifts to
mitigate any deterioration in the model&apos;s predictive performance over time.
Unfortunately, the ability of popular concept drift approaches in detecting
these drifts in the relationship of the response and predictor variable is
often dependent on the distribution characteristics of the data streams, as
well as its sensitivity on parameter tuning. This paper presents Hierarchical
Linear Four Rates (HLFR), a framework that detects concept drifts for different
data stream distributions (including imbalanced data) by leveraging a
hierarchical set of hypothesis tests in an online setting. The performance of
HLFR is compared to benchmark approaches using both simulated and real-world
datasets spanning the breadth of concept drift types. HLFR significantly
outperforms benchmark approaches in terms of accuracy, G-mean, recall, delay in
detection and adaptability across the various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abraham_Z/0/1/0/all/0/1&quot;&gt;Zubin Abraham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mohak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+You_X/0/1/0/all/0/1&quot;&gt;Xinge You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; C. Pr&amp;#xed;ncipe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05484">
<title>Block Mean Approximation for Efficient Second Order Optimization. (arXiv:1804.05484v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05484</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced optimization algorithms such as Newton method and AdaGrad benefit
from second order derivative or second order statistics to achieve better
descent directions and faster convergence rates. At their heart, such
algorithms need to compute the inverse or inverse square root of a matrix whose
size is quadratic of the dimensionality of the search space. For high
dimensional search spaces, the matrix inversion or inversion of square root
becomes overwhelming which in turn demands for approximate methods. In this
work, we propose a new matrix approximation method which divides a matrix into
blocks and represents each block by one or two numbers. The method allows
efficient computation of matrix inverse and inverse square root. We apply our
method to AdaGrad in training deep neural networks. Experiments show
encouraging results compared to the diagonal approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06827">
<title>PAC-Bayes bounds for stable algorithms with instance-dependent priors. (arXiv:1806.06827v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06827</link>
<description rdf:parseType="Literal">&lt;p&gt;PAC-Bayes bounds have been proposed to get risk estimates based on a training
sample. In this paper the PAC-Bayes approach is combined with stability of the
hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting
is used with a Gaussian prior centered at the expected output. Thus a novelty
of our paper is using priors defined in terms of the data-generating
distribution. Our main result estimates the risk of the randomized algorithm in
terms of the hypothesis stability coefficients. We also provide a new bound for
the SVM classifier, which is compared to other known bounds experimentally.
Ours appears to be the first stability-based bound that evaluates to
non-trivial values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rivasplata_O/0/1/0/all/0/1&quot;&gt;Omar Rivasplata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parrado_Hernandez_E/0/1/0/all/0/1&quot;&gt;Emilio Parrado-Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shawe_Taylor_J/0/1/0/all/0/1&quot;&gt;John Shawe-Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shiliang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesvari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02129">
<title>Probabilistic Causal Analysis of Social Influence. (arXiv:1808.02129v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.02129</link>
<description rdf:parseType="Literal">&lt;p&gt;Mastering the dynamics of social influence requires separating, in a database
of information propagation traces, the genuine causal processes from temporal
correlation, i.e., homophily and other spurious causes. However, most studies
to characterize social influence, and, in general, most data-science analyses
focus on correlations, statistical independence, or conditional independence.
Only recently, there has been a resurgence of interest in &quot;causal data
science&quot;, e.g., grounded on causality theories. In this paper we adopt a
principled causal approach to the analysis of social influence from
information-propagation data, rooted in the theory of probabilistic causation.
&lt;/p&gt;
&lt;p&gt;Our approach consists of two phases. In the first one, in order to avoid the
pitfalls of misinterpreting causation when the data spans a mixture of several
subtypes (&quot;Simpson&apos;s paradox&quot;), we partition the set of propagation traces into
groups, in such a way that each group is as less contradictory as possible in
terms of the hierarchical structure of information propagation. To achieve this
goal, we borrow the notion of &quot;agony&quot; and define the Agony-bounded Partitioning
problem, which we prove being hard, and for which we develop two efficient
algorithms with approximation guarantees. In the second phase, for each group
from the first phase, we apply a constrained MLE approach to ultimately learn a
minimal causal topology. Experiments on synthetic data show that our method is
able to retrieve the genuine causal arcs w.r.t. a ground-truth generative
model. Experiments on real data show that, by focusing only on the extracted
causal structures instead of the whole social graph, the effectiveness of
predicting influence spread is significantly improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gullo_F/0/1/0/all/0/1&quot;&gt;Francesco Gullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bud Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02933">
<title>(Sequential) Importance Sampling Bandits. (arXiv:1808.02933v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.02933</link>
<description rdf:parseType="Literal">&lt;p&gt;The multi-armed bandit (MAB) problem is a sequential allocation task where
the goal is to learn a policy that maximizes long term payoff, where only the
reward of the executed action is observed; i.e., sequential optimal decisions
are made, while simultaneously learning how the world operates. In the
stochastic setting, the reward for each action is generated from an unknown
distribution. To decide the next optimal action to take, one must compute
sufficient statistics of this unknown reward distribution, e.g.
upper-confidence bounds (UCB), or expectations in Thompson sampling.
Closed-form expressions for these statistics of interest are analytically
intractable except for simple cases. We here propose to leverage Monte Carlo
estimation and, in particular, the flexibility of (sequential) importance
sampling (IS) to allow for accurate estimation of the statistics of interest
within the MAB problem. IS methods estimate posterior densities or expectations
in probabilistic models that are analytically intractable. We first show how IS
can be combined with state-of-the-art MAB algorithms (Thompson sampling and
Bayes-UCB) for classic (Bernoulli and contextual linear-Gaussian) bandit
problems. Furthermore, we leverage the power of sequential IS to extend the
applicability of these algorithms beyond the classic settings, and tackle
additional useful cases. Specifically, we study the dynamic linear-Gaussian
bandit, and both the static and dynamic logistic cases too. The flexibility of
(sequential) importance sampling is shown to be fundamental for obtaining
efficient estimates of the key sufficient statistics in these challenging
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Urteaga_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;igo Urteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiggins_C/0/1/0/all/0/1&quot;&gt;Chris H. Wiggins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07380">
<title>On the Predictability of non-CGM Diabetes Data for Personalized Recommendation. (arXiv:1808.07380v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07380</link>
<description rdf:parseType="Literal">&lt;p&gt;With continuous glucose monitoring (CGM), data-driven models on blood glucose
prediction have been shown to be effective in related work. However, such (CGM)
systems are not always available, e.g., for a patient at home. In this work, we
conduct a study on 9 patients and examine the predictability of data-driven
(aka. machine learning) based models on patient-level blood glucose prediction;
with measurements are taken only periodically (i.e., after several hours). To
this end, we propose several post-prediction methods to account for the noise
nature of these data, that marginally improves the performance of the end
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokicki_M/0/1/0/all/0/1&quot;&gt;Markus Rokicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08763">
<title>On the convergence of optimistic policy iteration for stochastic shortest path problem. (arXiv:1808.08763v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08763</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we prove some convergence results of a special case of
optimistic policy iteration algorithm for stochastic shortest path problem. We
consider both Monte Carlo and $TD(\lambda)$ methods for the policy evaluation
step under the condition that the termination state will eventually be reached
almost surely.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanlong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09034">
<title>Importance Weighting and Variational Inference. (arXiv:1808.09034v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09034</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work used importance sampling ideas for better variational bounds on
likelihoods. We clarify the applicability of these ideas to pure probabilistic
inference, by showing the resulting Importance Weighted Variational Inference
(IWVI) technique is an instance of augmented variational inference, thus
identifying the looseness in previous work. Experiments confirm IWVI&apos;s
practicality for probabilistic inference. As a second contribution, we
investigate inference with elliptical distributions, which improves accuracy in
low dimensions, and convergence in high dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domke_J/0/1/0/all/0/1&quot;&gt;Justin Domke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1&quot;&gt;Daniel Sheldon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09526">
<title>Deep Lidar CNN to Understand the Dynamics of Moving Vehicles. (arXiv:1808.09526v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09526</link>
<description rdf:parseType="Literal">&lt;p&gt;Perception technologies in Autonomous Driving are experiencing their golden
age due to the advances in Deep Learning. Yet, most of these systems rely on
the semantically rich information of RGB images. Deep Learning solutions
applied to the data of other sensors typically mounted on autonomous cars (e.g.
lidars or radars) are not explored much. In this paper we propose a novel
solution to understand the dynamics of moving vehicles of the scene from only
lidar information. The main challenge of this problem stems from the fact that
we need to disambiguate the proprio-motion of the &apos;observer&apos; vehicle from that
of the external &apos;observed&apos; vehicles. For this purpose, we devise a CNN
architecture which at testing time is fed with pairs of consecutive lidar
scans. However, in order to properly learn the parameters of this network,
during training we introduce a series of so-called pretext tasks which also
leverage on image data. These tasks include semantic information about
vehicleness and a novel lidar-flow feature which combines standard image-based
optical flow with lidar scans. We obtain very promising results and show that
including distilled image information only during training, allows improving
the inference results of the network at test time, even when image data is no
longer used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaquero_V/0/1/0/all/0/1&quot;&gt;Victor Vaquero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1&quot;&gt;Alberto Sanfeliu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;</dc:creator>
</item></rdf:RDF>