<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02148"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.06960"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09709"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01937"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02209"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.04970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02125"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02171"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02261"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02472"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02476"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02477"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1209.0521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1310.1147"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.04981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.04208"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.02110"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05462"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.01973"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02111"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05259"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03269"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03985"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01447"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.02037">
<title>Complexity Theory for Discrete Black-Box Optimization Heuristics. (arXiv:1801.02037v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.02037</link>
<description rdf:parseType="Literal">&lt;p&gt;A predominant topic in the theory of evolutionary algorithms and, more
generally, theory of randomized black-box optimization techniques is running
time analysis. Running time analysis aims at understanding the performance of a
given heuristic on a given problem by bounding the number of function
evaluations that are needed by the heuristic to identify a solution of a
desired quality. As in general algorithms theory, this running time perspective
is most useful when it is complemented by a meaningful complexity theory that
studies the limits of algorithmic solutions.
&lt;/p&gt;
&lt;p&gt;In the context of discrete black-box optimization, several black-box
complexity models have been developed to analyze the best possible performance
that a black-box optimization algorithm can achieve on a given problem. The
models differ in the classes of algorithms to which these lower bounds apply.
This way, black-box complexity contributes to a better understanding of how
certain algorithmic choices (such as the amount of memory used by a heuristic,
its selective pressure, or properties of the strategies that it uses to create
new solution candidates) influences performance.
&lt;/p&gt;
&lt;p&gt;In this chapter we review the different black-box complexity models that have
been proposed in the literature, survey the bounds that have been obtained for
these models, and discuss how the interplay of running time analysis and
black-box complexity can inspire new algorithmic solutions to well-researched
problems in evolutionary computation. We also discuss in this chapter several
interesting open questions for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1&quot;&gt;Carola Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02148">
<title>Australia&apos;s long-term electricity demand forecasting using deep neural networks. (arXiv:1801.02148v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.02148</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate prediction of long-term electricity demand has a significant role in
demand side management and electricity network planning and operation. Demand
over-estimation results in over-investment in network assets, driving up the
electricity prices, while demand under-estimation may lead to under-investment
resulting in unreliable and insecure electricity. In this manuscript, we apply
deep neural networks to predict Australia&apos;s long-term electricity demand. A
stacked autoencoder is used in combination with multilayer perceptrons or
cascade-forward multilayer perceptrons to predict the nation-wide electricity
consumption rates for 1-24 months ahead of time. The experimental results show
that the deep structures have better performance than classical neural
networks, especially for 12-month to 24-month prediction horizon
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamedmoghadam_H/0/1/0/all/0/1&quot;&gt;Homayoun Hamedmoghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joorabloo_N/0/1/0/all/0/1&quot;&gt;Nima Joorabloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalili_M/0/1/0/all/0/1&quot;&gt;Mahdi Jalili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02335">
<title>On Enhancing Genetic Algorithms Using New Crossovers. (arXiv:1801.02335v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.02335</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the use of more than one crossover operator to
enhance the performance of genetic algorithms. Novel crossover operators are
proposed such as the Collision crossover, which is based on the physical rules
of elastic collision, in addition to proposing two selection strategies for the
crossover operators, one of which is based on selecting the best crossover
operator and the other randomly selects any operator. Several experiments on
some Travelling Salesman Problems (TSP) have been conducted to evaluate the
proposed methods, which are compared to the well-known Modified crossover
operator and partially mapped Crossover (PMX) crossover. The results show the
importance of some of the proposed methods, such as the collision crossover, in
addition to the significant enhancement of the genetic algorithms performance,
particularly when using more than one crossover operator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanat_A/0/1/0/all/0/1&quot;&gt;Ahmad B. A. Hassanat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkafaween_E/0/1/0/all/0/1&quot;&gt;Esra&amp;#x27;a Alkafaween&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02508">
<title>Spiking memristor logic gates are a type of time-variant perceptron. (arXiv:1801.02508v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1801.02508</link>
<description rdf:parseType="Literal">&lt;p&gt;Memristors are low-power memory-holding resistors thought to be useful for
neuromophic computing, which can compute via spike-interactions mediated
through the device&apos;s short-term memory. Using interacting spikes, it is
possible to build an AND gate that computes OR at the same time, similarly a
full adder can be built that computes the arithmetical sum of its inputs. Here
we show how these gates can be understood by modelling the memristors as a
novel type of perceptron: one which is sensitive to input order. The
memristor&apos;s memory can change the input weights for later inputs, and thus the
memristor gates cannot be accurately described by a single perceptron,
requiring either a network of time-invarient perceptrons or a complex
time-varying self-reprogrammable perceptron. This work demonstrates the high
functionality of memristor logic gates, and also that the addition of
theasholding could enable the creation of a standard perceptron in hardware,
which may have use in building neural net chips.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gale_E/0/1/0/all/0/1&quot;&gt;Ella M. Gale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02567">
<title>Weighted Contrastive Divergence. (arXiv:1801.02567v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02567</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning algorithms for energy based Boltzmann architectures that rely on
gradient descent are in general computationally prohibitive, typically due to
the exponential number of terms involved in computing the partition function.
In this way one has to resort to approximation schemes for the evaluation of
the gradient. This is the case of Restricted Boltzmann Machines (RBM) and its
learning algorithm Contrastive Divergence (CD). It is well-known that CD has a
number of shortcomings, and its approximation to the gradient has several
drawbacks. Overcoming these defects has been the basis of much research and new
algorithms have been devised, such as persistent CD. In this manuscript we
propose a new algorithm that we call Weighted CD (WCD), built from small
modifications of the negative phase in standard CD. However small these
modifications may be, experimental work reported in this paper suggest that WCD
provides a significant improvement over standard CD and persistent CD at a
small additional computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merino_E/0/1/0/all/0/1&quot;&gt;Enrique Romero Merino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castrillejo_F/0/1/0/all/0/1&quot;&gt;Ferran Mazzanti Castrillejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pin_J/0/1/0/all/0/1&quot;&gt;Jordi Delgado Pin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prats_D/0/1/0/all/0/1&quot;&gt;David Buchaca Prats&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.06960">
<title>Translating Neuralese. (arXiv:1704.06960v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1704.06960</link>
<description rdf:parseType="Literal">&lt;p&gt;Several approaches have recently been proposed for learning decentralized
deep multiagent policies that coordinate via a differentiable communication
channel. While these policies are effective for many tasks, interpretation of
their induced communication strategies has remained a challenge. Here we
propose to interpret agents&apos; messages by translating them. Unlike in typical
machine translation problems, we have no parallel data to learn from. Instead
we develop a translation model based on the insight that agent messages and
natural language strings mean the same thing if they induce the same belief
about the world in a listener. We present theoretical guarantees and empirical
evidence that our approach preserves both the semantics and pragmatics of
messages by ensuring that players communicating through a translation layer do
not suffer a substantial loss in reward relative to players with a common
language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca Dragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1&quot;&gt;Dan Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10460">
<title>Toward predictive machine learning for active vision. (arXiv:1710.10460v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10460</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a comprehensive description of the active inference framework, as
proposed by Friston (2010), under a machine-learning compliant perspective.
Stemming from a biological inspiration and the auto-encoding principles, the
sketch of a cognitive architecture is proposed that should provide ways to
implement estimation-oriented control policies. Computer simulations illustrate
the effectiveness of the approach through a foveated inspection of the input
data. The pros and cons of the control policy are analyzed in detail, showing
interesting promises in terms of processing compression. Though optimizing
future posterior entropy over the actions set is shown enough to attain locally
optimal action selection, offline calculation using class-specific saliency
maps is shown better for it saves processing costs through saccades pathways
pre-processing, with a negligible effect on the recognition/compression rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauce_E/0/1/0/all/0/1&quot;&gt;Emmanuel Dauc&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02114">
<title>Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02114</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the representational power of deep neural networks
(DNN) that belong to the family of piecewise-linear (PWL) functions, based on
PWL activation units such as rectifier or maxout. We investigate the complexity
of such networks by studying the number of linear regions of the PWL function.
Typically, a PWL function from a DNN can be seen as a large family of linear
functions acting on millions of such regions. We directly build upon the work
of Montufar et al. (2014), Montufar (2017) and Raghu et al. (2017) by refining
the upper and lower bounds on the number of linear regions for rectified and
maxout networks. In addition to achieving tighter bounds, we also develop a
novel method to perform exact enumeration or counting of the number of linear
regions with a mixed-integer linear formulation that maps the input space to
output. We use this new capability to visualize how the number of linear
regions change while training DNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_T/0/1/0/all/0/1&quot;&gt;Thiago Serra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjandraatmadja_C/0/1/0/all/0/1&quot;&gt;Christian Tjandraatmadja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1&quot;&gt;Srikumar Ramalingam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09709">
<title>Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro Gesture. (arXiv:1712.09709v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09709</link>
<description rdf:parseType="Literal">&lt;p&gt;In the research of the impact of gestures using by a lecturer, one
challenging task is to infer the attention of a group of audiences. Two
important measurements that can help infer the level of attention are eye
movement data and Electroencephalography (EEG) data. Under the fundamental
assumption that a group of people would look at the same place if they all pay
attention at the same time, we apply a method, &quot;Time Warp Edit Distance&quot;, to
calculate the similarity of their eye movement trajectories. Moreover, we also
cluster eye movement pattern of audiences based on these pair-wised similarity
metrics. Besides, since we don&apos;t have a direct metric for the &quot;attention&quot;
ground truth, a visual assessment would be beneficial to evaluate the
gesture-attention relationship. Thus we also implement a visualization tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kender_J/0/1/0/all/0/1&quot;&gt;John Kender&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01937">
<title>A Comprehensive Survey of Ontology Summarization: Measures and Methods. (arXiv:1801.01937v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1801.01937</link>
<description rdf:parseType="Literal">&lt;p&gt;The Semantic Web is becoming a large scale framework that enables data to be
published, shared, and reused in the form of ontologies. The ontology which is
considered as basic building block of semantic web consists of two layers
including data and schema layer. With the current exponential development of
ontologies in both data size and complexity of schemas, ontology understanding
which is playing an important role in different tasks such as ontology
engineering, ontology learning, etc., is becoming more difficult. Ontology
summarization as a way to distill knowledge from an ontology and generate an
abridge version to facilitate a better understanding is getting more attention
recently. There are various approaches available for ontology summarization
which are focusing on different measures in order to produce a proper summary
for a given ontology. In this paper, we mainly focus on the common metrics
which are using for ontology summarization and meet the state-of-the-art in
ontology summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouriyeh_S/0/1/0/all/0/1&quot;&gt;Seyedamin Pouriyeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allahyari_M/0/1/0/all/0/1&quot;&gt;Mehdi Allahyari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochut_K/0/1/0/all/0/1&quot;&gt;Krys Kochut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1&quot;&gt;Hamid Reza Arabnia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02068">
<title>On the inherent competition between valid and spurious inductive inferences in Boolean data. (arXiv:1801.02068v1 [physics.data-an])</title>
<link>http://arxiv.org/abs/1801.02068</link>
<description rdf:parseType="Literal">&lt;p&gt;Inductive inference is the process of extracting general rules from specific
observations. This problem also arises in the analysis of biological networks,
such as genetic regulatory networks, where the interactions are complex and the
observations are incomplete. A typical task in these problems is to extract
general interaction rules as combinations of Boolean covariates, that explain a
measured response variable. The inductive inference process can be considered
as an incompletely specified Boolean function synthesis problem. This
incompleteness of the problem will also generate spurious inferences, which are
a serious threat to valid inductive inference rules. Using random Boolean data
as a null model, here we attempt to measure the competition between valid and
spurious inductive inference rules from a given data set. We formulate two
greedy search algorithms, which synthesize a given Boolean response variable in
a sparse disjunct normal form, and respectively a sparse generalized algebraic
normal form of the variables from the observation data, and we evaluate
numerically their performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Andrecut_M/0/1/0/all/0/1&quot;&gt;M. Andrecut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02193">
<title>Multi-platform Version of StarCraft: Brood War in a Docker Container: Technical Report. (arXiv:1801.02193v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.02193</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a dockerized version of a real-time strategy game StarCraft: Brood
War, commonly used as a domain for AI research, with a pre-installed collection
of AI developement tools supporting all the major types of StarCraft bots. This
provides a convenient way to deploy StarCraft AIs on numerous hosts at once and
across multiple platforms despite limited OS support of StarCraft. In this
technical report, we describe the design of our Docker images and present a few
use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sustr_M/0/1/0/all/0/1&quot;&gt;Michal &amp;#x160;ustr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maly_J/0/1/0/all/0/1&quot;&gt;Jan Mal&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Certicky_M/0/1/0/all/0/1&quot;&gt;Michal &amp;#x10c;ertick&amp;#xfd;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02209">
<title>Building Generalizable Agents with a Realistic and Rich 3D Environment. (arXiv:1801.02209v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02209</link>
<description rdf:parseType="Literal">&lt;p&gt;Towards bridging the gap between machine and human intelligence, it is of
utmost importance to introduce environments that are visually realistic and
rich in content. In such environments, one can evaluate and improve a crucial
property of practical intelligent systems, namely \emph{generalization}. In
this work, we build \emph{House3D}, a rich, extensible and efficient
environment that contains 45,622 human-designed 3D scenes of houses, ranging
from single-room studios to multi-storeyed houses, equipped with a diverse set
of fully labeled 3D objects, textures and scene layouts, based on the SUNCG
dataset (Song et al., 2017). With an emphasis on semantic-level generalization,
we study the task of concept-driven navigation, \emph{RoomNav}, using a subset
of houses in House3D. In RoomNav, an agent navigates towards a target specified
by a semantic concept. To succeed, the agent learns to comprehend the scene it
lives in by developing perception, understand the concept by mapping it to the
correct semantics, and navigate to the target by obeying the underlying
physical rules. We train RL agents with both continuous and discrete action
spaces and show their ability to generalize in new unseen environments. In
particular, we observe that (1) training is substantially harder on large house
sets but results in better generalization, (2) using semantic signals (e.g.,
segmentation mask) boosts the generalization performance, and (3) gated
networks on semantic input signal lead to improved training performance and
generalization. We hope House3D, including the analysis of the RoomNav task,
serves as a building block towards designing practical intelligent systems and
we wish it to be broadly adopted by the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gkioxari_G/0/1/0/all/0/1&quot;&gt;Georgia Gkioxari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02243">
<title>Trading the Twitter Sentiment with Reinforcement Learning. (arXiv:1801.02243v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.02243</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is to explore the possibility to use alternative data and
artificial intelligence techniques to trade stocks. The efficacy of the daily
Twitter sentiment on predicting the stock return is examined using machine
learning methods. Reinforcement learning(Q-learning) is applied to generate the
optimal trading policy based on the sentiment signal. The predicting power of
the sentiment signal is more significant if the stock price is driven by the
expectation of the company growth and when the company has a major event that
draws the public attention. The optimal trading strategy based on reinforcement
learning outperforms the trading strategy based on the machine learning
prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Catherine Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wanfeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02270">
<title>Perceptual Context in Cognitive Hierarchies. (arXiv:1801.02270v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.02270</link>
<description rdf:parseType="Literal">&lt;p&gt;Cognition does not only depend on bottom-up sensor feature abstraction, but
also relies on contextual information being passed top-down. Context is higher
level information that helps to predict belief states at lower levels. The main
contribution of this paper is to provide a formalisation of perceptual context
and its integration into a new process model for cognitive hierarchies. Several
simple instantiations of a cognitive hierarchy are used to illustrate the role
of context. Notably, we demonstrate the use context in a novel approach to
visually track the pose of rigid objects with just a 2D camera.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hengst_B/0/1/0/all/0/1&quot;&gt;Bernhard Hengst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1&quot;&gt;Maurice Pagnucco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajaratnam_D/0/1/0/all/0/1&quot;&gt;David Rajaratnam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sammut_C/0/1/0/all/0/1&quot;&gt;Claude Sammut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thielscher_M/0/1/0/all/0/1&quot;&gt;Michael Thielscher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.04970">
<title>Relaxation heuristics for the set multicover problem with generalized upper bound constraints. (arXiv:1705.04970v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1705.04970</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider an extension of the set covering problem (SCP) introducing
(i)~multicover and (ii)~generalized upper bound (GUB)~constraints. For the
conventional SCP, the pricing method has been introduced to reduce the size of
instances, and several efficient heuristic algorithms based on such reduction
techniques have been developed to solve large-scale instances. However, GUB
constraints often make the pricing method less effective, because they often
prevent solutions from containing highly evaluated variables together. To
overcome this problem, we develop heuristic algorithms to reduce the size of
instances, in which new evaluation schemes of variables are introduced taking
account of GUB constraints. We also develop an efficient implementation of a
2-flip neighborhood local search algorithm that reduces the number of
candidates in the neighborhood without sacrificing the solution quality. In
order to guide the search to visit a wide variety of good solutions, we also
introduce a path relinking method that generates new solutions by combining two
or more solutions obtained so far. According to computational comparison on
benchmark instances, the proposed method succeeds in selecting a small number
of promising variables properly and performs quite effectively even for
large-scale instances having hard GUB constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umetani_S/0/1/0/all/0/1&quot;&gt;Shunji Umetani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arakawa_M/0/1/0/all/0/1&quot;&gt;Masanao Arakawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yagiura_M/0/1/0/all/0/1&quot;&gt;Mutsunori Yagiura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05401">
<title>Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs. (arXiv:1711.05401v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05401</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of learning vector representations for entities and
relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This
problem has received significant attention in the past few years and multiple
methods have been proposed. Most of the existing methods in the literature use
a predefined characteristic scoring function for evaluating the correctness of
KG triples. These scoring functions distinguish correct triples (high score)
from incorrect ones (low score). However, their performance vary across
different datasets. In this work, we demonstrate that a simple neural network
based score function can consistently achieve near start-of-the-art performance
on multiple datasets. We also quantitatively demonstrate biases in standard
benchmark datasets, and highlight the need to perform evaluation spanning
various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1&quot;&gt;Srinivas Ravishankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrahas/0/1/0/all/0/1&quot;&gt;Chandrahas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1&quot;&gt;Partha Pratim Talukdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01899">
<title>Clustering with Outlier Removal. (arXiv:1801.01899v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.01899</link>
<description rdf:parseType="Literal">&lt;p&gt;Cluster analysis and outlier detection are strongly coupled tasks in data
mining area. Cluster structure can be easily destroyed by few outliers; on the
contrary, the outliers are defined by the concept of cluster, which are
recognized as the points belonging to none of the clusters. However, most
existing studies handle them separately. In light of this, we consider the
joint cluster analysis and outlier detection problem, and propose the
Clustering with Outlier Removal (COR) algorithm. Generally speaking, the
original space is transformed into the binary space via generating basic
partitions in order to define clusters. Then an objective function based
Holoentropy is designed to enhance the compactness of each cluster with a few
outliers removed. With further analyses on the objective function, only partial
of the problem can be handled by K-means optimization. To provide an integrated
solution, an auxiliary binary matrix is nontrivally introduced so that COR
completely and efficiently solves the challenging problem via a unified
K-means- - with theoretical supports. Extensive experimental results on
numerous data sets in various domains demonstrate the effectiveness and
efficiency of COR significantly over the rivals including K-means- - and other
state-of-the-art outlier detection methods in terms of cluster validity and
outlier detection. Some key factors in COR are further analyzed for practical
use. Finally, an application on flight trajectory is provided to demonstrate
the effectiveness of COR in the real-world scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongfu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yun Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01953">
<title>Adversarial Perturbation Intensity Achieving Chosen Intra-Technique Transferability Level for Logistic Regression. (arXiv:1801.01953v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.01953</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning models have been shown to be vulnerable to adversarial
examples, ie. the manipulation of data by a attacker to defeat a defender&apos;s
classifier at test time. We present a novel probabilistic definition of
adversarial examples in perfect or limited knowledge setting using prior
probability distributions on the defender&apos;s classifier. Using the asymptotic
properties of the logistic regression, we derive a closed-form expression of
the intensity of any adversarial perturbation, in order to achieve a given
expected misclassification rate. This technique is relevant in a threat model
of known model specifications and unknown training data. To our knowledge, this
is the first method that allows an attacker to directly choose the probability
of attack success. We evaluate our approach on two real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gubri_M/0/1/0/all/0/1&quot;&gt;Martin Gubri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01961">
<title>Compressive sensing adaptation for polynomial chaos expansions. (arXiv:1801.01961v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.01961</link>
<description rdf:parseType="Literal">&lt;p&gt;Basis adaptation in Homogeneous Chaos spaces rely on a suitable rotation of
the underlying Gaussian germ. Several rotations have been proposed in the
literature resulting in adaptations with different convergence properties. In
this paper we present a new adaptation mechanism that builds on compressive
sensing algorithms, resulting in a reduced polynomial chaos approximation with
optimal sparsity. The developed adaptation algorithm consists of a two-step
optimization procedure that computes the optimal coefficients and the input
projection matrix of a low dimensional chaos expansion with respect to an
optimally rotated basis. We demonstrate the attractive features of our
algorithm through several numerical examples including the application on
Large-Eddy Simulation (LES) calculations of turbulent combustion in a HIFiRE
scramjet engine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsilifis_P/0/1/0/all/0/1&quot;&gt;Panagiotis Tsilifis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huan_X/0/1/0/all/0/1&quot;&gt;Xun Huan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Safta_C/0/1/0/all/0/1&quot;&gt;Cosmin Safta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sargsyan_K/0/1/0/all/0/1&quot;&gt;Khachik Sargsyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacaze_G/0/1/0/all/0/1&quot;&gt;Guilhem Lacaze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oefelein_J/0/1/0/all/0/1&quot;&gt;Joseph C. Oefelein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Najm_H/0/1/0/all/0/1&quot;&gt;Habib N. Najm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghanem_R/0/1/0/all/0/1&quot;&gt;Roger G. Ghanem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02013">
<title>Multiscale Sparse Microcanonical Models. (arXiv:1801.02013v1 [math-ph])</title>
<link>http://arxiv.org/abs/1801.02013</link>
<description rdf:parseType="Literal">&lt;p&gt;We study density estimation of stationary processes defined over an infinite
grid from a single, finite realization. Gaussian Processes and Markov Random
Fields avoid the curse of dimensionality by focusing on low-order and localized
potentials respectively, but its application to complex datasets is limited by
their inability to capture singularities and long-range interactions, and their
expensive inference and learning respectively. These are instances of Gibbs
models, defined as maximum entropy distributions under moment constraints
determined by an energy vector. The Boltzmann equivalence principle states that
under appropriate ergodicity, such \emph{macrocanonical} models are
approximated by their \emph{microcanonical} counterparts, which replace the
expectation by the sample average. Microcanonical models are appealing since
they avoid computing expensive Lagrange multipliers to meet the constraints.
This paper introduces microcanonical measures whose energy vector is given by a
wavelet scattering transform, built by cascading wavelet decompositions and
point-wise nonlinearities. We study asymptotic properties of generic
microcanonical measures, which reveal the fundamental role of the differential
structure of the energy vector in controlling e.g. the entropy rate. Gradient
information is also used to define a microcanonical sampling algorithm, for
which we provide convergence analysis to the microcanonical measure. Whereas
wavelet transforms capture local regularity at different scales, scattering
transforms provide scale interaction information, critical to restore the
geometry of many physical phenomena. We demonstrate the efficiency of sparse
multiscale microcanonical measures on several processes and real data
exhibiting long-range interactions, such as Ising, Cox Processes and image and
audio textures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math-ph/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math-ph/1/au:+Mallat_S/0/1/0/all/0/1&quot;&gt;Stephane Mallat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02124">
<title>Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:1801.02124v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02124</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the problem of inverse reinforcement learning in
zero-sum stochastic games when expert demonstrations are known to be not
optimal. Compared to previous works that decouple agents in the game by
assuming optimality in expert strategies, we introduce a new objective function
that directly pits experts against Nash Equilibrium strategies, and we design
an algorithm to solve for the reward function in the context of inverse
reinforcement learning with deep neural networks as model approximations. In
our setting the model and algorithm do not decouple by agent. In order to find
Nash Equilibrium in large-scale games, we also propose an adversarial training
algorithm for zero-sum stochastic games, and show the theoretical appeal of
non-existence of local optima in its objective function. In our numerical
experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement
learning algorithms address games that are not amenable to previous approaches
using tabular representations. Moreover, with sub-optimal expert demonstrations
our algorithms recover both reward functions and strategies with good quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02125">
<title>Threshold Auto-Tuning Metric Learning. (arXiv:1801.02125v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02125</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been reported repeatedly that discriminative learning of distance
metric boosts the pattern recognition performance. A weak point of ITML-based
methods is that the distance threshold for similarity/dissimilarity constraints
must be determined manually and it is sensitive to generalization performance,
although the ITML-based methods enjoy an advantage that the Bregman projection
framework can be applied for optimization of distance metric. In this paper, we
present a new formulation of metric learning algorithm in which the distance
threshold is optimized together. Since the optimization is still in the Bregman
projection framework, the Dykstra algorithm can be applied for optimization. A
non-linear equation has to be solved to project the solution onto a half-space
in each iteration. Na\&quot;{i}ve method takes $O(LMn^{3})$ computational time to
solve the nonlinear equation. In this study, an efficient technique that can
solve the nonlinear equation in $O(Mn^{3})$ has been discovered. We have proved
that the root exists and is unique. We empirically show that the accuracy of
pattern recognition for the proposed metric learning algorithm is comparable to
the existing metric learning methods, yet the distance threshold is
automatically tuned for the proposed metric learning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onuma_Y/0/1/0/all/0/1&quot;&gt;Yuya Onuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivero_R/0/1/0/all/0/1&quot;&gt;Rachelle Rivero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_T/0/1/0/all/0/1&quot;&gt;Tsuyoshi Kato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02171">
<title>Detection and segmentation of the Left Ventricle in Cardiac MRI using Deep Learning. (arXiv:1801.02171v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.02171</link>
<description rdf:parseType="Literal">&lt;p&gt;Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous
task that can vary depending on the patient, the Magnetic Resonance Images
(MRI) cuts and the experts. Still today, we consider manual delineation done by
experts as being the ground truth for cardiac diagnosticians. Thus, we are
reviewing the paper - written by Avendi and al. - who presents a combined
approach with Convolutional Neural Networks, Stacked Auto-Encoders and
Deformable Models, to try and automate the segmentation while performing more
accurately. Furthermore, we have implemented parts of the paper (around three
quarts) and experimented both the original method and slightly modified
versions when changing the architecture and the parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attia_A/0/1/0/all/0/1&quot;&gt;Alexandre Attia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayan_S/0/1/0/all/0/1&quot;&gt;Sharone Dayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02257">
<title>Denoising Dictionary Learning Against Adversarial Perturbations. (arXiv:1801.02257v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02257</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose denoising dictionary learning (DDL), a simple yet effective
technique as a protection measure against adversarial perturbations. We
examined denoising dictionary learning on MNIST and CIFAR10 perturbed under two
different perturbation techniques, fast gradient sign (FGSM) and jacobian
saliency maps (JSMA). We evaluated it against five different deep neural
networks (DNN) representing the building blocks of most recent architectures
indicating a successive progression of model complexity of each other. We show
that each model tends to capture different representations based on their
architecture. For each model we recorded its accuracy both on the perturbed
test data previously misclassified with high confidence and on the denoised one
after the reconstruction using dictionary learning. The reconstruction quality
of each data point is assessed by means of PSNR (Peak Signal to Noise Ratio)
and Structure Similarity Index (SSI). We show that after applying (DDL) the
reconstruction of the original data point from a noisy
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mitro_J/0/1/0/all/0/1&quot;&gt;John Mitro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bridge_D/0/1/0/all/0/1&quot;&gt;Derek Bridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prestwich_S/0/1/0/all/0/1&quot;&gt;Steven Prestwich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02261">
<title>Anatomical Data Augmentation For CNN based Pixel-wise Classification. (arXiv:1801.02261v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.02261</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose a method for anatomical data augmentation that is
based on using slices of computed tomography (CT) examinations that are
adjacent to labeled slices as another resource of labeled data for training the
network. The extended labeled data is used to train a U-net network for a
pixel-wise classification into different hepatic lesions and normal liver
tissues. Our dataset contains CT examinations from 140 patients with 333 CT
images annotated by an expert radiologist. We tested our approach and compared
it to the conventional training process. Results indicate superiority of our
method. Using the anatomical data augmentation we achieved an improvement of 3%
in the success rate, 5% in the classification accuracy, and 4% in Dice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Cohen_A/0/1/0/all/0/1&quot;&gt;Avi Ben-Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klang_E/0/1/0/all/0/1&quot;&gt;Eyal Klang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amitai_M/0/1/0/all/0/1&quot;&gt;Michal Marianne Amitai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1&quot;&gt;Jacob Goldberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_H/0/1/0/all/0/1&quot;&gt;Hayit Greenspan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02309">
<title>Log-concave sampling: Metropolis-Hastings algorithms are fast!. (arXiv:1801.02309v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02309</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of sampling from a strongly log-concave density in
$\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of
the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by
running a Markov chain obtained from the discretization of an appropriate
Langevin diffusion, combined with an accept-reject step to ensure the correct
stationary distribution. Relative to known guarantees for the unadjusted
Langevin algorithm (ULA), our bounds show that the use of an accept-reject step
in MALA leads to an exponentially improved dependence on the error-tolerance.
Concretely, in order to obtain samples with TV error at most $\delta$ for a
density with condition number $\kappa$, we show that MALA requires $\mathcal{O}
\big(\kappa d \log(1/\delta) \big)$ steps, as compared to the $\mathcal{O}
\big(\kappa^2 d/\delta^2 \big)$ steps established in past work on ULA. We also
demonstrate the gains of MALA over ULA for weakly log-concave densities.
Furthermore, we derive mixing time bounds for a zeroth-order method
Metropolized random walk (MRW) and show that it mixes $\mathcal{O}(\kappa d)$
slower than MALA. We provide numerical examples that support our theoretical
findings, and demonstrate the potential gains of Metropolis-Hastings adjustment
for Langevin-type algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1&quot;&gt;Raaz Dwivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuansi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02472">
<title>Optimizing Channel Selection for Seizure Detection. (arXiv:1801.02472v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1801.02472</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretation of electroencephalogram (EEG) signals can be complicated by
obfuscating artifacts. Artifact detection plays an important role in the
observation and analysis of EEG signals. Spatial information contained in the
placement of the electrodes can be exploited to accurately detect artifacts.
However, when fewer electrodes are used, less spatial information is available,
making it harder to detect artifacts. In this study, we investigate the
performance of a deep learning algorithm, CNN-LSTM, on several channel
configurations. Each configuration was designed to minimize the amount of
spatial information lost compared to a standard 22-channel EEG. Systems using a
reduced number of channels ranging from 8 to 20 achieved sensitivities between
33% and 37% with false alarms in the range of [38, 50] per 24 hours. False
alarms increased dramatically (e.g., over 300 per 24 hours) when the number of
channels was further reduced. Baseline performance of a system that used all 22
channels was 39% sensitivity with 23 false alarms. Since the 22-channel system
was the only system that included referential channels, the rapid increase in
the false alarm rate as the number of channels was reduced underscores the
importance of retaining referential channels for artifact reduction. This
cautionary result is important because one of the biggest differences between
various types of EEGs administered is the type of referential channel used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Vinit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ziyabari_S/0/1/0/all/0/1&quot;&gt;Saeedeh Ziyabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weltin_E/0/1/0/all/0/1&quot;&gt;Eva Von Weltin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02474">
<title>An Analysis of Two Common Reference Points for EEGs. (arXiv:1801.02474v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1801.02474</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical electroencephalographic (EEG) data varies significantly depending on
a number of operational conditions (e.g., the type and placement of electrodes,
the type of electrical grounding used). This investigation explores the
statistical differences present in two different referential montages: Linked
Ear (LE) and Averaged Reference (AR). Each of these accounts for approximately
45% of the data in the TUH EEG Corpus. In this study, we explore the impact
this variability has on machine learning performance. We compare the
statistical properties of features generated using these two montages, and
explore the impact of performance on our standard Hidden Markov Model (HMM)
based classification system. We show that a system trained on LE data
significantly outperforms one trained only on AR data (77.2% vs. 61.4%). We
also demonstrate that performance of a system trained on both data sets is
somewhat compromised (71.4% vs. 77.2%). A statistical analysis of the data
suggests that mean, variance and channel normalization should be considered.
However, cepstral mean subtraction failed to produce an improvement in
performance, suggesting that the impact of these statistical differences is
subtler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lopez_S/0/1/0/all/0/1&quot;&gt;Silvia Lopez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gross_A/0/1/0/all/0/1&quot;&gt;Aaron Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Scott Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02476">
<title>Semi-automated Annotation of Signal Events in Clinical EEG Data. (arXiv:1801.02476v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1801.02476</link>
<description rdf:parseType="Literal">&lt;p&gt;To be effective, state of the art machine learning technology needs large
amounts of annotated data. There are numerous compelling applications in
healthcare that can benefit from high performance automated decision support
systems provided by deep learning technology, but they lack the comprehensive
data resources required to apply sophisticated machine learning models.
Further, for economic reasons, it is very difficult to justify the creation of
large annotated corpora for these applications. Hence, automated annotation
techniques become increasingly important. In this study, we investigated the
effectiveness of using an active learning algorithm to automatically annotate a
large EEG corpus. The algorithm is designed to annotate six types of EEG
events. Two model training schemes, namely threshold-based and volume-based,
are evaluated. In the threshold-based scheme the threshold of confidence scores
is optimized in the initial training iteration, whereas for the volume-based
scheme only a certain amount of data is preserved after each iteration.
Recognition performance is improved 2% absolute and the system is capable of
automatically annotating previously unlabeled data. Given that the
interpretation of clinical EEG data is an exceedingly difficult task, this
study provides some evidence that the proposed method is a viable alternative
to expensive manual annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Scott Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lopez_S/0/1/0/all/0/1&quot;&gt;Silvia Lopez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02477">
<title>Improved EEG Event Classification Using Differential Energy. (arXiv:1801.02477v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1801.02477</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature extraction for automatic classification of EEG signals typically
relies on time frequency representations of the signal. Techniques such as
cepstral-based filter banks or wavelets are popular analysis techniques in many
signal processing applications including EEG classification. In this paper, we
present a comparison of a variety of approaches to estimating and
postprocessing features. To further aid in discrimination of periodic signals
from aperiodic signals, we add a differential energy term. We evaluate our
approaches on the TUH EEG Corpus, which is the largest publicly available EEG
corpus and an exceedingly challenging task due to the clinical nature of the
data. We demonstrate that a variant of a standard filter bank-based approach,
coupled with first and second derivatives, provides a substantial reduction in
the overall error rate. The combination of differential energy and derivatives
produces a 24% absolute reduction in the error rate and improves our ability to
discriminate between signal events and background noise. This relatively simple
approach proves to be comparable to other popular feature extraction approaches
such as wavelets, but is much more computationally efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harati_A/0/1/0/all/0/1&quot;&gt;Amir Harati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lopez_S/0/1/0/all/0/1&quot;&gt;Silvia Lopez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02612">
<title>Spatially transformed adversarial examples. (arXiv:1801.02612v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1801.02612</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies show that widely used deep neural networks (DNNs) are
vulnerable to carefully crafted adversarial examples. Many advanced algorithms
have been proposed to generate adversarial examples by leveraging the
$\mathcal{L}_p$ distance for penalizing perturbations. Researchers have
explored different defense methods to defend against such adversarial attacks.
While the effectiveness of $\mathcal{L}_p$ distance as a metric of perceptual
quality remains an active research area, in this paper we will instead focus on
a different type of perturbation, namely spatial transformation, as opposed to
manipulating the pixel values directly as in prior works. Perturbations
generated through spatial transformation could result in large $\mathcal{L}_p$
distance measures, but our extensive experiments show that such spatially
transformed adversarial examples are perceptually realistic and more difficult
to defend against with existing defense systems. This potentially provides a
new direction in adversarial example generation and the design of corresponding
defenses. We visualize the spatial transformation based perturbation for
different examples and show that our technique can produce realistic
adversarial examples with smooth image deformation. Finally, we visualize the
attention of deep networks with different types of adversarial examples to
better understand how these examples are interpreted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Warren He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1209.0521">
<title>Efficient EM Training of Gaussian Mixtures with Missing Data. (arXiv:1209.0521v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1209.0521</link>
<description rdf:parseType="Literal">&lt;p&gt;In data-mining applications, we are frequently faced with a large fraction of
missing entries in the data matrix, which is problematic for most discriminant
machine learning algorithms. A solution that we explore in this paper is the
use of a generative model (a mixture of Gaussians) to compute the conditional
expectation of the missing variables given the observed variables. Since
training a Gaussian mixture with many different patterns of missing values can
be computationally very expensive, we introduce a spanning-tree based algorithm
that significantly speeds up training in these conditions. We also observe that
good results can be obtained by using the generative model to fill-in the
missing values for a separate discriminant learning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delalleau_O/0/1/0/all/0/1&quot;&gt;Olivier Delalleau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1310.1147">
<title>A Unified Primal Dual Active Set Algorithm for Nonconvex Sparse Recovery. (arXiv:1310.1147v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1310.1147</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of recovering a sparse signal based on
penalized least squares. We develop an algorithm of primal-dual active set type
for a class of nonconvex sparsity-promoting penalties, which includes $\ell^0$,
bridge, smoothly clipped absolute deviation, capped $\ell^1$ and minimax
concavity penalty. First we establish the existence of a global minimizer for
the class of optimization problems. Then we derive a novel necessary optimality
condition for the global minimizer using the associated thresholding operator.
The solutions to the optimality system are coordinate-wise minimizers, and
under minor conditions, they are also local minimizers. Upon introducing the
dual variable, the active set can be determined from the primal and dual
variables. This relation lends itself to an iterative algorithm of active set
type which at each step involves updating the primal variable only on the
active set and then updating the dual variable explicitly. When combined with a
continuation strategy on the regularization parameter, the primal dual active
set method converges globally to the underlying regression target under some
regularity conditions. Extensive numerical experiments demonstrate its superior
performance in efficiency and accuracy compared with the existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yuling Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bangti Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiliang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Can Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.04981">
<title>Repeated Games with Vector Losses: A Set-valued Dynamic Programming Approach. (arXiv:1603.04981v4 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1603.04981</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider infinitely repeated games with vector losses discounted over
time. We characterize the set of minimal upper bounds on expected losses that a
player can simultaneously guarantee across the different dimensions.
Specifically, we show that this set is the fixed point of a set-valued dynamic
programming operator. This approach also characterizes the strategies that
achieve these bounds. These optimal strategies are shown to be independent of
the player&apos;s own past actions and stationary relative to a compact state space
obtained by parameterizing the set of the minimal bounds. We also present a
computational procedure to approximate this set and the optimal strategies.
&lt;/p&gt;
&lt;p&gt;We discuss two applications of our results: 1) characterization of the
optimal strategy of the uninformed player in zero-sum discounted repeated games
with incomplete information on one side; 2) characterization of the minmax
optimal regret and the regret-optimal strategy in repeated games with
discounted losses. Our approximation procedure can be used to compute
approximately optimal strategies in both these applications. We illustrate this
procedure by computing approximately regret-optimal strategies for the problem
of prediction using expert advice from two and three experts under
$\{0,1\}-$losses. Our numerical evaluations demonstrate improved performance
over existing algorithms for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamble_V/0/1/0/all/0/1&quot;&gt;Vijay Kamble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loiseau_P/0/1/0/all/0/1&quot;&gt;Patrick Loiseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walrand_J/0/1/0/all/0/1&quot;&gt;Jean Walrand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.04208">
<title>Joint mean and covariance estimation with unreplicated matrix-variate data. (arXiv:1611.04208v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.04208</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been proposed that complex populations, such as those that arise in
genomics studies, may exhibit dependencies among observations as well as among
variables. This gives rise to the challenging problem of analyzing unreplicated
high-dimensional data with unknown mean and dependence structures.
Matrix-variate approaches that impose various forms of (inverse) covariance
sparsity allow flexible dependence structures to be estimated, but cannot
directly be applied when the mean and covariance matrices are estimated
jointly. We present a practical method utilizing generalized least squares and
penalized (inverse) covariance estimation to address this challenge. We
establish consistency and obtain rates of convergence for estimating the mean
parameters and covariance matrices. The advantages of our approaches are: (i)
dependence graphs and covariance structures can be estimated in the presence of
unknown mean structure, (ii) the mean structure becomes more efficiently
estimated when accounting for the dependence structure among observations; and
(iii) inferences about the mean parameters become correctly calibrated. We use
simulation studies and analysis of genomic data from a twin study of ulcerative
colitis to illustrate the statistical convergence and the performance of our
methods in practical settings. Several lines of evidence show that the test
statistics for differential gene expression produced by our methods are
correctly calibrated and improve power over conventional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hornstein_M/0/1/0/all/0/1&quot;&gt;Michael Hornstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Roger Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shedden_K/0/1/0/all/0/1&quot;&gt;Kerby Shedden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuheng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.02110">
<title>Transformation Forests. (arXiv:1701.02110v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1701.02110</link>
<description rdf:parseType="Literal">&lt;p&gt;Regression models for supervised learning problems with a continuous target
are commonly understood as models for the conditional mean of the target given
predictors. This notion is simple and therefore appealing for interpretation
and visualisation. Information about the whole underlying conditional
distribution is, however, not available from these models. A more general
understanding of regression models as models for conditional distributions
allows much broader inference from such models, for example the computation of
prediction intervals. Several random forest-type algorithms aim at estimating
conditional distributions, most prominently quantile regression forests
(Meinshausen, 2006, JMLR). We propose a novel approach based on a parametric
family of distributions characterised by their transformation function. A
dedicated novel &quot;transformation tree&quot; algorithm able to detect distributional
changes is developed. Based on these transformation trees, we introduce
&quot;transformation forests&quot; as an adaptive local likelihood estimator of
conditional distribution functions. The resulting models are fully parametric
yet very general and allow broad inference procedures, such as the model-based
bootstrap, to be applied in a straightforward way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hothorn_T/0/1/0/all/0/1&quot;&gt;Torsten Hothorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeileis_A/0/1/0/all/0/1&quot;&gt;Achim Zeileis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05462">
<title>Objective Bayesian Analysis for Change Point Problems. (arXiv:1702.05462v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05462</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a loss-based approach to change point analysis. In
particular, we look at the problem from two perspectives. The first focuses on
the definition of a prior when the number of change points is known a priori.
The second contribution aims to estimate the number of change points by using a
loss-based approach recently introduced in the literature. The latter considers
change point estimation as a model selection exercise. We show the performance
of the proposed approach on simulated data and real data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hinoveanu_L/0/1/0/all/0/1&quot;&gt;Laurentiu Hinoveanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leisen_F/0/1/0/all/0/1&quot;&gt;Fabrizio Leisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villa_C/0/1/0/all/0/1&quot;&gt;Cristiano Villa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05536">
<title>Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits. (arXiv:1702.05536v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05536</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on follow the perturbed leader (FTPL) algorithms for the
adversarial multi-armed bandit problem has highlighted the role of the hazard
rate of the distribution generating the perturbations. Assuming that the hazard
rate is bounded, it is possible to provide regret analyses for a variety of
FTPL algorithms for the multi-armed bandit problem. This paper pushes the
inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate
condition. There are good reasons to do so: natural distributions such as the
uniform and Gaussian violate the condition. We give regret bounds for both
bounded support and unbounded support distributions without assuming the hazard
rate condition. We also disprove a conjecture that the Gaussian distribution
cannot lead to a low-regret algorithm. In fact, it turns out that it leads to
near optimal regret, up to logarithmic factors. A key ingredient in our
approach is the introduction of a new notion called the generalized hazard
rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zifan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ambuj Tewari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.01973">
<title>Batched High-dimensional Bayesian Optimization via Structural Kernel Learning. (arXiv:1703.01973v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.01973</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization of high-dimensional black-box functions is an extremely
challenging problem. While Bayesian optimization has emerged as a popular
approach for optimizing black-box functions, its applicability has been limited
to low-dimensional problems due to its computational and statistical challenges
arising from high-dimensional settings. In this paper, we propose to tackle
these challenges by (1) assuming a latent additive structure in the function
and inferring it properly for more efficient and effective BO, and (2)
performing multiple evaluations in parallel to reduce the number of iterations
required by the method. Our novel approach learns the latent structure with
Gibbs sampling and constructs batched queries using determinantal point
processes. Experimental validations on both synthetic and real-world functions
demonstrate that the proposed method outperforms the existing state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengtao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02111">
<title>Classification and clustering for samples of event time data using non-homogeneous Poisson process models. (arXiv:1703.02111v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02111</link>
<description rdf:parseType="Literal">&lt;p&gt;Data of the form of event times arise in various applications. A simple model
for such data is a non-homogeneous Poisson process (NHPP) which is specified by
a rate function that depends on time. We consider the problem of having access
to multiple independent samples of event time data, observed on a common
interval, from which we wish to classify or cluster the samples according to
their rate functions. Each rate function is unknown but assumed to belong to a
finite number of rate functions each defining a distinct class. We model the
rate functions using a spline basis expansion, the coefficients of which need
to be estimated from data. The classification approach consists of using
training data for which the class membership is known, to calculate maximum
likelihood estimates of the coefficients for each group, then assigning test
samples to a class by a maximum likelihood criterion. For clustering, by
analogy to the Gaussian mixture model approach for Euclidean data, we consider
a mixture of NHPP models and use the expectation-maximisation algorithm to
estimate the coefficients of the rate functions for the component models and
cluster membership probabilities for each sample. The classification and
clustering approaches perform well on both synthetic and real-world data sets.
Code associated with this paper is available at
https://github.com/duncan-barrack/NHPP .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrack_D/0/1/0/all/0/1&quot;&gt;Duncan Barrack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preston_S/0/1/0/all/0/1&quot;&gt;Simon Preston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01445">
<title>Batched Large-scale Bayesian Optimization in High-dimensional Spaces. (arXiv:1706.01445v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01445</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) has become an effective approach for black-box
function optimization problems when function evaluations are expensive and the
optimum can be achieved within a relatively small number of queries. However,
many cases, such as the ones with high-dimensional inputs, may require a much
larger number of observations for optimization. Despite an abundance of
observations thanks to parallel experiments, current BO techniques have been
limited to merely a few thousand observations. In this paper, we propose
ensemble Bayesian optimization (EBO) to address three current challenges in BO
simultaneously: (1) large-scale observations; (2) high dimensional input
spaces; and (3) selections of batch queries that balance quality and diversity.
The key idea of EBO is to operate on an ensemble of additive Gaussian process
models, each of which possesses a randomized strategy to divide and conquer. We
show unprecedented, previously impossible results of scaling up BO to tens of
thousands of observations within minutes of computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gehring_C/0/1/0/all/0/1&quot;&gt;Clement Gehring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05259">
<title>Learning with Feature Evolvable Streams. (arXiv:1706.05259v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05259</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning with streaming data has attracted much attention during the past few
years. Though most studies consider data stream with fixed features, in real
practice the features may be evolvable. For example, features of data gathered
by limited-lifespan sensors will change when these sensors are substituted by
new ones. In this paper, we propose a novel learning paradigm: \emph{Feature
Evolvable Streaming Learning} where old features would vanish and new features
would occur. Rather than relying on only the current features, we attempt to
recover the vanished features and exploit it to improve performance.
Specifically, we learn two models from the recovered features and the current
features, respectively. To benefit from the recovered features, we develop two
ensemble methods. In the first method, we combine the predictions from two
models and theoretically show that with the assistance of old features, the
performance on new features can be improved. In the second approach, we
dynamically select the best single prediction and establish a better
performance guarantee when the best model switches. Experiments on both
synthetic and real data validate the effectiveness of our proposal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Bo-Jian Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi-Hua Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03269">
<title>Improving Downlink Coordinated Multipoint Performance in Heterogeneous Networks. (arXiv:1707.03269v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03269</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method for practical Joint Processing downlink coordinated
multipoint (DL CoMP) implementation in LTE/LTE-A systems using supervised
machine learning. DL CoMP has not been thoroughly studied in previous work
although cluster formation and interference mitigation have been studied
extensively. In this paper, we attempt to improve the cell edge data rate
served by a heterogeneous network cluster by means of dynamically changing the
DL SINR threshold at which the DL CoMP feature is triggered. We do so by using
a support vector machine (SVM) classifier. The simulation results show a cell
edge user throughput improvement of 33.3\% for pico cells and more than
four-fold improvement in user throughput in the cluster. This has resulted from
a reduction in the downlink block error rate (DL BLER) and an improvement in
the spectral efficiency due to the informed triggering of the multiple radio
streams as part of DL CoMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mismar_F/0/1/0/all/0/1&quot;&gt;Faris B. Mismar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Brian L. Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05070">
<title>Data-driven Advice for Applying Machine Learning to Bioinformatics Problems. (arXiv:1708.05070v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05070</link>
<description rdf:parseType="Literal">&lt;p&gt;As the bioinformatics field grows, it must keep pace not only with new data
but with new algorithms. Here we contribute a thorough analysis of 13
state-of-the-art, commonly used machine learning algorithms on a set of 165
publicly available classification problems in order to provide data-driven
algorithm recommendations to current researchers. We present a number of
statistical and visual comparisons of algorithm performance and quantify the
effect of model selection and algorithm tuning for each algorithm and dataset.
The analysis culminates in the recommendation of five algorithms with
hyperparameters that maximize classifier performance across the tested
problems, as well as general guidelines for applying machine learning to
supervised classification problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Olson_R/0/1/0/all/0/1&quot;&gt;Randal S. Olson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cava_W/0/1/0/all/0/1&quot;&gt;William La Cava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mustahsan_Z/0/1/0/all/0/1&quot;&gt;Zairah Mustahsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Varik_A/0/1/0/all/0/1&quot;&gt;Akshay Varik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jason H. Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07433">
<title>Perturbative Black Box Variational Inference. (arXiv:1709.07433v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07433</link>
<description rdf:parseType="Literal">&lt;p&gt;Black box variational inference (BBVI) with reparameterization gradients
triggered the exploration of divergence measures other than the
Kullback-Leibler (KL) divergence, such as alpha divergences. In this paper, we
view BBVI with generalized divergences as a form of estimating the marginal
likelihood via biased importance sampling. The choice of divergence determines
a bias-variance trade-off between the tightness of a bound on the marginal
likelihood (low bias) and the variance of its gradient estimators. Drawing on
variational perturbation theory of statistical physics, we use these insights
to construct a family of new variational bounds. Enumerated by an odd integer
order $K$, this family captures the standard KL bound for $K=1$, and converges
to the exact marginal likelihood as $K\to\infty$. Compared to
alpha-divergences, our reparameterization gradients have a lower variance. We
show in experiments on Gaussian Processes and Variational Autoencoders that the
new bounds are more mass covering, and that the resulting posterior covariances
are closer to the true posterior and lead to higher likelihoods on held-out
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bamler_R/0/1/0/all/0/1&quot;&gt;Robert Bamler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1&quot;&gt;Manfred Opper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09363">
<title>GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks. (arXiv:1710.09363v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09363</link>
<description rdf:parseType="Literal">&lt;p&gt;The Fisher information metric is an important foundation of information
geometry, wherein it allows us to approximate the local geometry of a
probability distribution. Recurrent neural networks such as the
Sequence-to-Sequence (Seq2Seq) networks that have lately been used to yield
state-of-the-art performance on speech translation or image captioning have so
far ignored the geometry of the latent embedding, that they iteratively learn.
We propose the information geometric Seq2Seq (GeoSeq2Seq) network which
abridges the gap between deep recurrent neural networks and information
geometry. Specifically, the latent embedding offered by a recurrent network is
encoded as a Fisher kernel of a parametric Gaussian Mixture Model, a formalism
common in computer vision. We utilise such a network to predict the shortest
routes between two nodes of a graph by learning the adjacency matrix using the
GeoSeq2Seq formalism; our results show that for such a problem the
probabilistic representation of the latent embedding supersedes the
non-probabilistic embedding by 10-15\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bay_A/0/1/0/all/0/1&quot;&gt;Alessandro Bay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sengupta_B/0/1/0/all/0/1&quot;&gt;Biswa Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11383">
<title>Flexible Prior Distributions for Deep Generative Models. (arXiv:1710.11383v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11383</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of training generative models with deep neural
networks as generators, i.e. to map latent codes to data points. Whereas the
dominant paradigm combines simple priors over codes with complex deterministic
models, we argue that it might be advantageous to use more flexible code
distributions. We demonstrate how these distributions can be induced directly
from the data. The benefits include: more powerful generative models, better
modeling of latent structure and explicit control of the degree of
generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1&quot;&gt;Yannic Kilcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00464">
<title>An Information-Theoretic Analysis of Deep Latent-Variable Models. (arXiv:1711.00464v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00464</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an information-theoretic framework for understanding trade-offs in
unsupervised learning of deep latent-variables models using variational
inference. This framework emphasizes the need to consider latent-variable
models along two dimensions: the ability to reconstruct inputs (distortion) and
the communication cost (rate). We derive the optimal frontier of generative
models in the two-dimensional rate-distortion plane, and show how the standard
evidence lower bound objective is insufficient to select between points along
this frontier. However, by performing targeted optimization to learn generative
models with different rates, we are able to learn many models that can achieve
similar generative performance but make vastly different trade-offs in terms of
the usage of the latent variable. Through experiments on MNIST and Omniglot
with a variety of architectures, we show how our framework sheds light on many
recent proposed extensions to the variational autoencoder family.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1&quot;&gt;Alexander A. Alemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1&quot;&gt;Ben Poole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1&quot;&gt;Ian Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dillon_J/0/1/0/all/0/1&quot;&gt;Joshua V. Dillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1&quot;&gt;Rif A. Saurous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kevin Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03985">
<title>Applications of Deep Learning and Reinforcement Learning to Biological Data. (arXiv:1711.03985v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03985</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapid advances of hardware-based technologies during the past decades have
opened up new possibilities for Life scientists to gather multimodal data in
various application domains (e.g., Omics, Bioimaging, Medical Imaging, and
[Brain/Body]-Machine Interfaces), thus generating novel opportunities for
development of dedicated data intensive machine learning techniques. Overall,
recent research in Deep learning (DL), Reinforcement learning (RL), and their
combination (Deep RL) promise to revolutionize Artificial Intelligence. The
growth in computational power accompanied by faster and increased data storage
and declining computing costs have already allowed scientists in various fields
to apply these techniques on datasets that were previously intractable for
their size and complexity. This review article provides a comprehensive survey
on the application of DL, RL, and Deep RL techniques in mining Biological data.
In addition, we compare performances of DL techniques when applied to different
datasets across various application domains. Finally, we outline open issues in
this challenging research area and discuss future development perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmud_M/0/1/0/all/0/1&quot;&gt;Mufti Mahmud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1&quot;&gt;M. Shamim Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1&quot;&gt;Amir Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassanelli_S/0/1/0/all/0/1&quot;&gt;Stefano Vassanelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01447">
<title>Gaussian Process bandits with adaptive discretization. (arXiv:1712.01447v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01447</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, the problem of maximizing a black-box function $f:\mathcal{X}
\to \mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process
(GP) prior. In particular, a new algorithm for this problem is proposed, and
high probability bounds on its simple and cumulative regret are established.
The query point selection rule in most existing methods involves an exhaustive
search over an increasingly fine sequence of uniform discretizations of
$\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines
$\mathcal{X}$ which leads to a lower computational complexity, particularly
when $\mathcal{X}$ is a subset of a high dimensional Euclidean space. In
addition to the computational gains, sufficient conditions are identified under
which the regret bounds of the new algorithm improve upon the known results.
Finally an extension of the algorithm to the case of contextual bandits is
proposed, and high probability bounds on the contextual regret are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shekhar_S/0/1/0/all/0/1&quot;&gt;Shubhanshu Shekhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Javidi_T/0/1/0/all/0/1&quot;&gt;Tara Javidi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>