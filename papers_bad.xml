<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01766"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01358"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01743"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01843"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.06288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.06221"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08676"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09787"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06919"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01359"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01410"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01517"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01524"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01614"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01842"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01951"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01960"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01990"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.04706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.09499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.01701"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06823"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03154"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00089"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00701"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00892"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.01342">
<title>A Cooperative Group Optimization System. (arXiv:1808.01342v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.01342</link>
<description rdf:parseType="Literal">&lt;p&gt;A cooperative group optimization (CGO) system is presented to implement CGO
cases by integrating the advantages of the cooperative group and low-level
algorithm portfolio design. Following the nature-inspired paradigm of a
cooperative group, the agents not only explore in a parallel way with their
individual memory, but also cooperate with their peers through the group
memory. Each agent holds a portfolio of (heterogeneous) embedded search
heuristics (ESHs), in which each ESH can drive the group into a stand-alone CGO
case, and hybrid CGO cases in an algorithmic space can be defined by low-level
cooperative search among a portfolio of ESHs through customized memory sharing.
The optimization process might also be facilitated by a passive group leader
through encoding knowledge in the search landscape. Based on a concrete
framework, CGO cases are defined by a script assembling over instances of
algorithmic components in a toolbox. A multilayer design of the script, with
the support of the inherent updatable graph in the memory protocol, enables a
simple way to address the challenge of accumulating heterogeneous ESHs and
defining customized portfolios without any additional code. The CGO system is
implemented for solving the constrained optimization problem with some generic
components and only a few domain-specific components. Guided by the insights
from algorithm portfolio design, customized CGO cases based on basic search
operators can achieve competitive performance over existing algorithms as
compared on a set of commonly-used benchmark instances. This work might provide
a basic step toward a user-oriented development framework, since the
algorithmic space might be easily evolved by accumulating competent ESHs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiao-Feng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zun-Jing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01766">
<title>On Optimizing Deep Convolutional Neural Networks by Evolutionary Computing. (arXiv:1808.01766v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.01766</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization for deep networks is currently a very active area of research.
As neural networks become deeper, the ability in manually optimizing the
network becomes harder. Mini-batch normalization, identification of effective
respective fields, momentum updates, introduction of residual blocks, learning
rate adoption, etc. have been proposed to speed up the rate of convergent in
manual training process while keeping the higher accuracy level. However, the
problem of finding optimal topological structure for a given problem is
becoming a challenging task need to be addressed immediately. Few researchers
have attempted to optimize the network structure using evolutionary computing
approaches. Among them, few have successfully evolved networks with
reinforcement learning and long-short-term memory. A very few has applied
evolutionary programming into deep convolution neural networks. These attempts
are mainly evolved the network structure and then subsequently optimized the
hyper-parameters of the network. However, a mechanism to evolve the deep
network structure under the techniques currently being practiced in manual
process is still absent. Incorporation of such techniques into chromosomes
level of evolutionary computing, certainly can take us to better topological
deep structures. The paper concludes by identifying the gap between
evolutionary based deep neural networks and deep neural networks. Further, it
proposes some insights for optimizing deep neural networks using evolutionary
computing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dias_M/0/1/0/all/0/1&quot;&gt;M. U. B. Dias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1&quot;&gt;D. D. N. De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_S/0/1/0/all/0/1&quot;&gt;S. Fernando&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04924">
<title>Emergence and Evolution of Hierarchical Structure in Complex Systems. (arXiv:1805.04924v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04924</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well known that many complex systems, both in technology and nature,
exhibit hierarchical modularity: smaller modules, each of them providing a
certain function, are used within larger modules that perform more complex
functions. What is not well understood however is how this hierarchical
structure (which is fundamentally a network property) emerges, and how it
evolves over time. We propose a modeling framework, referred to as Evo-Lexis,
that provides insight to some fundamental questions about evolving hierarchical
systems. Evo-Lexis models the most elementary modules of the system as symbols
(&quot;sources&quot;) and the modules at the highest level of the hierarchy as sequences
of those symbols (&quot;targets&quot;). Evo-Lexis computes the optimized adjustment of a
given hierarchy when the set of targets changes over time by additions and
removals (a process referred to as &quot;incremental design&quot;). In this paper we use
computation modeling to show that:
&lt;/p&gt;
&lt;p&gt;- Low-cost and deep hierarchies emerge when the population of target
sequences evolves through tinkering and mutation. - Strong selection on the
cost of new candidate targets results in reuse of more complex (longer) nodes
in an optimized hierarchy. - The bias towards reuse of complex nodes results in
an &quot;hourglass architecture&quot; (i.e., few intermediate nodes that cover almost all
source-target paths). - With such bias, the core nodes are conserved for
relatively long time periods although still being vulnerable to major
transitions and punctuated equilibria. - Finally, we analyze the differences in
terms of cost and structure between incrementally designed hierarchies and the
corresponding &quot;clean-slate&quot; hierarchies which result when the system is
designed from scratch after a change.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siyari_P/0/1/0/all/0/1&quot;&gt;Payam Siyari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dovrolis_C/0/1/0/all/0/1&quot;&gt;Constantine Dovrolis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01358">
<title>Attributes&apos; Importance for Zero-Shot Pose-Classification Based on Wearable Sensors. (arXiv:1808.01358v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.01358</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a simple yet effective method for improving the
performance of zero-shot learning (ZSL). ZSL classifies instances of unseen
classes, from which no training data is available, by utilizing the attributes
of the classes. Conventional ZSL methods have equally dealt with all the
available attributes, but this sometimes causes misclassification. This is
because an attribute that is effective for classifying instances of one class
is not always effective for another class. In this case, a metric of
classifying the latter class can be undesirably influenced by the irrelevant
attribute. This paper solves this problem by taking the importance of each
attribute for each class into account when calculating the metric. In addition
to the proposal of this new method, this paper also contributes by providing a
dataset for pose classification based on wearable sensors, named HDPoseDS. It
contains 22 classes of poses performed by 10 subjects with 31 IMU sensors
across full body. To the best of our knowledge, it is the richest
wearable-sensor dataset especially in terms of sensor density, and thus it is
suitable for studying zero-shot pose/action recognition. The presented method
was evaluated on HDPoseDS and outperformed relative improvement of 5.9% in
comparison to the best baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohashi_H/0/1/0/all/0/1&quot;&gt;Hiroki Ohashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Naser_M/0/1/0/all/0/1&quot;&gt;Mohammad Al-Naser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sheraz Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1&quot;&gt;Katsuyuki Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1&quot;&gt;Takuto Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01412">
<title>Active Learning for Wireless IoT Intrusion Detection. (arXiv:1808.01412v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1808.01412</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet of Things (IoT) is becoming truly ubiquitous in our everyday life,
but it also faces unique security challenges. Intrusion detection is critical
for the security and safety of a wireless IoT network. This paper discusses the
human-in-the-loop active learning approach for wireless intrusion detection. We
first present the fundamental challenges against the design of a successful
Intrusion Detection System (IDS) for wireless IoT network. We then briefly
review the rudimentary concepts of active learning and propose its employment
in the diverse applications of wireless intrusion detection. Experimental
example is also presented to show the significant performance improvement of
the active learning method over traditional supervised learning approach. While
machine learning techniques have been widely employed for intrusion detection,
the application of human-in-the-loop machine learning that leverages both
machine and human intelligence to intrusion detection of IoT is still in its
infancy. We hope this article can assist the readers in understanding the key
concepts of active learning and spur further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiyi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01743">
<title>NIMFA: A Python Library for Nonnegative Matrix Factorization. (arXiv:1808.01743v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01743</link>
<description rdf:parseType="Literal">&lt;p&gt;NIMFA is an open-source Python library that provides a unified interface to
nonnegative matrix factorization algorithms. It includes implementations of
state-of-the-art factorization methods, initialization approaches, and quality
scoring. It supports both dense and sparse matrix representation. NIMFA&apos;s
component-based implementation and hierarchical design should help the users to
employ already implemented techniques or design and code new strategies for
matrix factorization tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zupan_B/0/1/0/all/0/1&quot;&gt;Blaz Zupan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01785">
<title>Defense Against Adversarial Attacks with Saak Transform. (arXiv:1808.01785v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.01785</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are known to be vulnerable to adversarial
perturbations, which imposes a serious threat to DNN-based decision systems. In
this paper, we propose to apply the lossy Saak transform to adversarially
perturbed images as a preprocessing tool to defend against adversarial attacks.
Saak transform is a recently-proposed state-of-the-art for computing the
spatial-spectral representations of input images. Empirically, we observe that
outputs of the Saak transform are very discriminative in differentiating
adversarial examples from clean ones. Therefore, we propose a Saak transform
based preprocessing method with three steps: 1) transforming an input image to
a joint spatial-spectral representation via the forward Saak transform, 2)
apply filtering to its high-frequency components, and, 3) reconstructing the
image via the inverse Saak transform. The processed image is found to be robust
against adversarial perturbations. We conduct extensive experiments to
investigate various settings of the Saak transform and filtering functions.
Without harming the decision performance on clean images, our method
outperforms state-of-the-art adversarial defense methods by a substantial
margin on both the CIFAR-10 and ImageNet datasets. Importantly, our results
suggest that adversarial perturbations can be effectively and efficiently
defended using state-of-the-art frequency analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sibo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yueru Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1&quot;&gt;C.-C. Jay Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01843">
<title>An Efficient Approach to Learning Chinese Judgment Document Similarity Based on Knowledge Summarization. (arXiv:1808.01843v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.01843</link>
<description rdf:parseType="Literal">&lt;p&gt;A previous similar case in common law systems can be used as a reference with
respect to the current case such that identical situations can be treated
similarly in every case. However, current approaches for judgment document
similarity computation failed to capture the core semantics of judgment
documents and therefore suffer from lower accuracy and higher computation
complexity. In this paper, a knowledge block summarization based machine
learning approach is proposed to compute the semantic similarity of Chinese
judgment documents. By utilizing domain ontologies for judgment documents, the
core semantics of Chinese judgment documents is summarized based on knowledge
blocks. Then the WMD algorithm is used to calculate the similarity between
knowledge blocks. At last, the related experiments were made to illustrate that
our approach is very effective and efficient in achieving higher accuracy and
faster computation speed in comparison with the traditional approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yinglong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiangang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01876">
<title>An Efficient Deep Reinforcement Learning Model for Urban Traffic Control. (arXiv:1808.01876v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.01876</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban Traffic Control (UTC) plays an essential role in Intelligent
Transportation System (ITS) but remains difficult. Since model-based UTC
methods may not accurately describe the complex nature of traffic dynamics in
all situations, model-free data-driven UTC methods, especially reinforcement
learning (RL) based UTC methods, received increasing interests in the last
decade. However, existing DL approaches did not propose an efficient algorithm
to solve the complicated multiple intersections control problems whose
state-action spaces are vast. To solve this problem, we propose a Deep
Reinforcement Learning (DRL) algorithm that combines several tricks to master
an appropriate control strategy within an acceptable time. This new algorithm
relaxes the fixed traffic demand pattern assumption and reduces human invention
in parameter tuning. Simulation experiments have shown that our method
outperforms traditional rule-based approaches and has the potential to handle
more complex traffic problems in the real world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yilun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xingyuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei-Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01968">
<title>Discovering Latent Information By Spreading Activation Algorithm For Document Retrieval. (arXiv:1808.01968v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.01968</link>
<description rdf:parseType="Literal">&lt;p&gt;Syntactic search relies on keywords contained in a query to find suitable
documents. So, documents that do not contain the keywords but contain
information related to the query are not retrieved. Spreading activation is an
algorithm for finding latent information in a query by exploiting relations
between nodes in an associative network or semantic network. However, the
classical spreading activation algorithm uses all relations of a node in the
network that will add unsuitable information into the query. In this paper, we
propose a novel approach for semantic text search, called
query-oriented-constrained spreading activation that only uses relations
relating to the content of the query to find really related information.
Experiments on a benchmark dataset show that, in terms of the MAP measure, our
search engine is 18.9% and 43.8% respectively better than the syntactic search
and the search using the classical constrained spreading activation.
&lt;/p&gt;
&lt;p&gt;KEYWORDS: Information Retrieval, Ontology, Semantic Search, Spreading
Activation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_V/0/1/0/all/0/1&quot;&gt;Vuong M. Ngo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.06288">
<title>Multi-fidelity Gaussian Process Bandit Optimisation. (arXiv:1603.06288v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1603.06288</link>
<description rdf:parseType="Literal">&lt;p&gt;In many scientific and engineering applications, we are tasked with the
maximisation of an expensive to evaluate black box function $f$. Traditional
settings for this problem assume just the availability of this single function.
However, in many cases, cheap approximations to $f$ may be obtainable. For
example, the expensive real world behaviour of a robot can be approximated by a
cheap computer simulation. We can use these approximations to eliminate low
function value regions cheaply and use the expensive evaluations of $f$ in a
small but promising region and speedily identify the optimum. We formalise this
task as a \emph{multi-fidelity} bandit problem where the target function and
its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a
novel method based on upper confidence bound techniques. In our theoretical
analysis we demonstrate that it exhibits precisely the above behaviour, and
achieves better regret than strategies which ignore multi-fidelity information.
Empirically, MF-GP-UCB outperforms such naive strategies and other
multi-fidelity methods on several synthetic and real experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kandasamy_K/0/1/0/all/0/1&quot;&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dasarathy_G/0/1/0/all/0/1&quot;&gt;Gautam Dasarathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliva_J/0/1/0/all/0/1&quot;&gt;Junier B. Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnabas Poczos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.06221">
<title>Theoretical Aspects of Cyclic Structural Causal Models. (arXiv:1611.06221v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1611.06221</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural causal models (SCMs), also known as (non-parametric) structural
equation models (SEMs), are widely used for causal modeling purposes. A large
body of theoretical results is available for the special case in which cycles
are absent (i.e., acyclic SCMs, also known as recursive SEMs). However, in many
application domains cycles are abundantly present, for example in the form of
feedback loops. In this paper, we provide a general and rigorous theory of
cyclic SCMs. The paper consists of two parts: the first part gives a rigorous
treatment of structural causal models, dealing with measure-theoretic and other
complications that arise in the presence of cycles. In contrast with the
acyclic case, in cyclic SCMs solutions may no longer exist, or if they exist,
they may no longer be unique, or even measurable in general. We give several
sufficient and necessary conditions for the existence of (unique) measurable
solutions. We show how causal reasoning proceeds in these models and how this
differs from the acyclic case. Moreover, we give an overview of the Markov
properties that hold for cyclic SCMs. In the second part, we address the
question of how one can marginalize an SCM (possibly with cycles) to a subset
of the endogenous variables. We show that under a certain condition, one can
effectively remove a subset of the endogenous variables from the model, leading
to a more parsimonious marginal SCM that preserves the causal and
counterfactual semantics of the original SCM on the remaining variables.
Moreover, we show how the marginalization relates to the latent projection and
to latent confounders, i.e. latent common causes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bongers_S/0/1/0/all/0/1&quot;&gt;Stephan Bongers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jonas Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mooij_J/0/1/0/all/0/1&quot;&gt;Joris M. Mooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08676">
<title>Learning the structure of Bayesian Networks: A quantitative assessment of the effect of different algorithmic schemes. (arXiv:1704.08676v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1704.08676</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most challenging tasks when adopting Bayesian Networks (BNs) is
the one of learning their structure from data. This task is complicated by the
huge search space of possible solutions, and by the fact that the problem is
NP-hard. Hence, full enumeration of all the possible solutions is not always
feasible and approximations are often required. However, to the best of our
knowledge, a quantitative analysis of the performance and characteristics of
the different heuristics to solve this problem has never been done before.
&lt;/p&gt;
&lt;p&gt;For this reason, in this work, we provide a detailed comparison of many
different state-of-the-arts methods for structural learning on simulated data
considering both BNs with discrete and continuous variables, and with different
rates of noise in the data. In particular, we investigate the performance of
different widespread scores and algorithmic approaches proposed for the
inference and the statistical pitfalls within them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beretta_S/0/1/0/all/0/1&quot;&gt;Stefano Beretta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castelli_M/0/1/0/all/0/1&quot;&gt;Mauro Castelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_I/0/1/0/all/0/1&quot;&gt;Ivo Goncalves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_R/0/1/0/all/0/1&quot;&gt;Roberto Henriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07387">
<title>How morphological development can guide evolution. (arXiv:1711.07387v4 [q-bio.PE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07387</link>
<description rdf:parseType="Literal">&lt;p&gt;Organisms result from adaptive processes interacting across different time
scales. One such interaction is that between development and evolution. Models
have shown that development sweeps over several traits in a single agent,
sometimes exposing promising static traits. Subsequent evolution can then
canalize these rare traits. Thus, development can, under the right conditions,
increase evolvability. Here, we report on a previously unknown phenomenon when
embodied agents are allowed to develop and evolve: Evolution discovers body
plans robust to control changes, these body plans become genetically
assimilated, yet controllers for these agents are not assimilated. This allows
evolution to continue climbing fitness gradients by tinkering with the
developmental programs for controllers within these permissive body plans. This
exposes a previously unknown detail about the Baldwin effect: instead of all
useful traits becoming genetically assimilated, only traits that render the
agent robust to changes in other traits become assimilated. We refer to this as
differential canalization. This finding also has implications for the
evolutionary design of artificial and embodied agents such as robots: robots
robust to internal changes in their controllers may also be robust to external
changes in their environment, such as transferal from simulation to reality or
deployment in novel environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kriegman_S/0/1/0/all/0/1&quot;&gt;Sam Kriegman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09045">
<title>When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms. (arXiv:1805.09045v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09045</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient exploration is one of the key challenges for reinforcement learning
(RL) algorithms. Most traditional sample efficiency bounds require strategic
exploration. Recently many deep RL algorithms with simple heuristic exploration
strategies that have few formal guarantees, achieve surprising success in many
domains. These results pose an important question about understanding these
exploration strategies such as $e$-greedy, as well as understanding what
characterize the difficulty of exploration in MDPs. In this work we propose
problem specific sample complexity bounds of $Q$ learning with random walk
exploration that rely on several structural properties. We also link our
theoretical results to some empirical benchmark domains, to illustrate if our
bound gives polynomial sample complexity in these domains and how that is
related with the empirical performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma Brunskill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09787">
<title>Causal Inference for Early Detection of Pathogenic Social Media Accounts. (arXiv:1806.09787v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09787</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathogenic social media accounts such as terrorist supporters exploit
communities of supporters for conducting attacks on social media. Early
detection of PSM accounts is crucial as they are likely to be key users in
making a harmful message &quot;viral&quot;. This paper overviews my recent doctoral work
on utilizing causal inference to identify PSM accounts within a short time
frame around their activity. The proposed scheme (1) assigns time-decay
causality scores to users, (2) applies a community detection-based algorithm to
group of users sharing similar causality scores and finally (3) deploys a
classification algorithm to classify accounts. Unlike existing techniques that
require network structure, cascade path, or content, our scheme relies solely
on action log of users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvari_H/0/1/0/all/0/1&quot;&gt;Hamidreza Alvari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakarian_P/0/1/0/all/0/1&quot;&gt;Paulo Shakarian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06919">
<title>Backplay: &quot;Man muss immer umkehren&quot;. (arXiv:1807.06919v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06919</link>
<description rdf:parseType="Literal">&lt;p&gt;A long-standing problem in model free reinforcement learning (RL) is that it
requires a large number of trials to learn a good policy, especially in
environments with sparse rewards. We explore a method to increase the sample
efficiency of RL when we have access to demonstrations. Our approach, which we
call Backplay, uses a single demonstration to construct a curriculum for a
given task. Rather than starting each training episode in the environment&apos;s
fixed initial state, we start the agent near the end of the demonstration and
move the starting point backwards during the course of training until we reach
the initial state. We perform experiments in a competitive four player game
(Pommerman) and a path-finding maze game. We find that this weak form of
guidance provides significant gains in sample complexity with a stark advantage
in sparse reward environments. In some cases, standard RL did not yield any
improvement while Backplay reached success rates greater than 50% and
generalized to unseen initial conditions in the same amount of training time.
Additionally, we see that agents trained via Backplay can learn policies
superior to those of the original demonstration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resnick_C/0/1/0/all/0/1&quot;&gt;Cinjon Resnick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1&quot;&gt;Roberta Raileanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1&quot;&gt;Sanyam Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peysakhovich_A/0/1/0/all/0/1&quot;&gt;Alex Peysakhovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01345">
<title>Multi-objective optimization to explicitly account for model complexity when learning Bayesian Networks. (arXiv:1808.01345v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01345</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Networks have been widely used in the last decades in many fields,
to describe statistical dependencies among random variables. In general,
learning the structure of such models is a problem with considerable
theoretical interest that still poses many challenges. On the one hand, this is
a well-known NP-complete problem, which is practically hardened by the huge
search space of possible solutions. On the other hand, the phenomenon of
I-equivalence, i.e., different graphical structures underpinning the same set
of statistical dependencies, may lead to multimodal fitness landscapes further
hindering maximum likelihood approaches to solve the task. Despite all these
difficulties, greedy search methods based on a likelihood score coupled with a
regularization term to account for model complexity, have been shown to be
surprisingly effective in practice. In this paper, we consider the formulation
of the task of learning the structure of Bayesian Networks as an optimization
problem based on a likelihood score. Nevertheless, our approach do not adjust
this score by means of any of the complexity terms proposed in the literature;
instead, it accounts directly for the complexity of the discovered solutions by
exploiting a multi-objective optimization procedure. To this extent, we adopt
NSGA-II and define the first objective function to be the likelihood of a
solution and the second to be the number of selected arcs. We thoroughly
analyze the behavior of our method on a wide set of simulated data, and we
discuss the performance considering the goodness of the inferred solutions both
in terms of their objective functions and with respect to the retrieved
structure. Our results show that NSGA-II can converge to solutions
characterized by better likelihood and less arcs than classic approaches,
although paradoxically frequently characterized by a lower similarity to the
target network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cazzaniga_P/0/1/0/all/0/1&quot;&gt;Paolo Cazzaniga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nobile_M/0/1/0/all/0/1&quot;&gt;Marco S. Nobile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01357">
<title>Recurrent Convolutional Fusion for RGB-D Object Recognition. (arXiv:1808.01357v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.01357</link>
<description rdf:parseType="Literal">&lt;p&gt;Technological development aims to produce generations of increasingly
efficient robots able to perform complex tasks. This requires considerable
efforts, from the scientific community, to find new algorithms that solve
computer vision problems, such as object recognition. The diffusion of RGB-D
cameras directed the study towards the research of new architectures able to
exploit the RGB and Depth information. The project that is developed in this
thesis concerns the realization of a new end-to-end architecture for the
recognition of RGB-D objects called RCFusion. Our method generates compact and
highly discriminative multi-modal features by combining complementary RGB and
depth information representing different levels of abstraction. We evaluate our
method on standard object recognition datasets, RGB-D Object Dataset and
JHUIT-50. The experiments performed show that our method outperforms the
existing approaches and establishes new state-of-the-art results for both
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1&quot;&gt;Mirco Planamente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loghmani_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Loghmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1&quot;&gt;Barbara Caputo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01359">
<title>Deep Neural Network for Analysis of DNA Methylation Data. (arXiv:1808.01359v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/1808.01359</link>
<description rdf:parseType="Literal">&lt;p&gt;Many researches demonstrated that the DNA methylation, which occurs in the
context of a CpG, has strong correlation with diseases, including cancer. There
is a strong interest in analyzing the DNA methylation data to find how to
distinguish different subtypes of the tumor. However, the conventional
statistical methods are not suitable for analyzing the highly dimensional DNA
methylation data with bounded support. In order to explicitly capture the
properties of the data, we design a deep neural network, which composes of
several stacked binary restricted Boltzmann machines, to learn the low
dimensional deep features of the DNA methylation data. Experiments show these
features perform best in breast cancer DNA methylation data cluster analysis,
comparing with some state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01410">
<title>Predicting Expressive Speaking Style From Text In End-To-End Speech Synthesis. (arXiv:1808.01410v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.01410</link>
<description rdf:parseType="Literal">&lt;p&gt;Global Style Tokens (GSTs) are a recently-proposed method to learn latent
disentangled representations of high-dimensional data. GSTs can be used within
Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to
uncover expressive factors of variation in speaking style. In this work, we
introduce the Text-Predicted Global Style Token (TP-GST) architecture, which
treats GST combination weights or style embeddings as &quot;virtual&quot; speaking style
labels within Tacotron. TP-GST learns to predict stylistic renderings from text
alone, requiring neither explicit labels during training nor auxiliary inputs
for inference. We show that, when trained on a dataset of expressive speech,
our system generates audio with more pitch and energy variation than two
state-of-the-art baseline models. We further demonstrate that TP-GSTs can
synthesize speech with background noise removed, and corroborate these analyses
with positive results on human-rated listener preference audiobook tasks.
Finally, we demonstrate that multi-speaker TP-GST models successfully factorize
speaker identity and speaking style. We provide a website with audio samples
for each of our findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanton_D/0/1/0/all/0/1&quot;&gt;Daisy Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skerry_Ryan_R/0/1/0/all/0/1&quot;&gt;RJ Skerry-Ryan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01517">
<title>DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging. (arXiv:1808.01517v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01517</link>
<description rdf:parseType="Literal">&lt;p&gt;DELIMIT is a framework extension for deep learning in diffusion imaging,
which extends the basic framework PyTorch towards spherical signals. Based on
several novel layers, deep learning can be applied to spherical diffusion
imaging data in a very convenient way. First, two spherical harmonic
interpolation layers are added to the extension, which allow to transform the
signal from spherical surface space into the spherical harmonic space, and vice
versa. In addition, a local spherical convolution layer is introduced that adds
the possibility to include gradient neighborhood information within the
network. Furthermore, these extensions can also be utilized for the
preprocessing of diffusion signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koppers_S/0/1/0/all/0/1&quot;&gt;Simon Koppers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1&quot;&gt;Dorit Merhof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01524">
<title>Learning disentangled representation from 12-lead electrograms: application in localizing the origin of Ventricular Tachycardia. (arXiv:1808.01524v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01524</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing availability of electrocardiogram (ECG) data has motivated the
use of data-driven models for automating various clinical tasks based on ECG
data. The development of subject-specific models are limited by the cost and
difficulty of obtaining sufficient training data for each individual. The
alternative of population model, however, faces challenges caused by the
significant inter-subject variations within the ECG data. We address this
challenge by investigating for the first time the problem of learning
representations for clinically-informative variables while disentangling other
factors of variations within the ECG data. In this work, we present a
conditional variational autoencoder (VAE) to extract the subject-specific
adjustment to the ECG data, conditioned on task-specific representations
learned from a deterministic encoder. To encourage the representation for
inter-subject variations to be independent from the task-specific
representation, maximum mean discrepancy is used to match all the moments
between the distributions learned by the VAE conditioning on the code from the
deterministic encoder. The learning of the task-specific representation is
regularized by a weak supervision in the form of contrastive regularization. We
apply the proposed method to a novel yet important clinical task of classifying
the origin of ventricular tachycardia (VT) into pre-defined segments,
demonstrating the efficacy of the proposed method against the standard VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gyawali_P/0/1/0/all/0/1&quot;&gt;Prashnna K Gyawali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horacek_B/0/1/0/all/0/1&quot;&gt;B. Milan Horacek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapp_J/0/1/0/all/0/1&quot;&gt;John L. Sapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01535">
<title>Triplet Network with Attention for Speaker Diarization. (arXiv:1808.01535v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1808.01535</link>
<description rdf:parseType="Literal">&lt;p&gt;In automatic speech processing systems, speaker diarization is a crucial
front-end component to separate segments from different speakers. Inspired by
the recent success of deep neural networks (DNNs) in semantic inferencing,
triplet loss-based architectures have been successfully used for this problem.
However, existing work utilizes conventional i-vectors as the input
representation and builds simple fully connected networks for metric learning,
thus not fully leveraging the modeling power of DNN architectures. This paper
investigates the importance of learning effective representations from the
sequences directly in metric learning pipelines for speaker diarization. More
specifically, we propose to employ attention models to learn embeddings and the
metric jointly in an end-to-end fashion. Experiments are conducted on the
CALLHOME conversational speech corpus. The diarization results demonstrate
that, besides providing a unified model, the proposed approach achieves
improved performance when compared against existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Huan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Willi_M/0/1/0/all/0/1&quot;&gt;Megan Willi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Berisha_V/0/1/0/all/0/1&quot;&gt;Visar Berisha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Spanias_A/0/1/0/all/0/1&quot;&gt;Andreas Spanias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01550">
<title>Designing Adaptive Neural Networks for Energy-Constrained Image Classification. (arXiv:1808.01550v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01550</link>
<description rdf:parseType="Literal">&lt;p&gt;As convolutional neural networks (CNNs) enable state-of-the-art computer
vision applications, their high energy consumption has emerged as a key
impediment to their deployment on embedded and mobile devices. Towards
efficient image classification under hardware constraints, prior work has
proposed adaptive CNNs, i.e., systems of networks with different accuracy and
computation characteristics, where a selection scheme adaptively selects the
network to be evaluated for each input image. While previous efforts have
investigated different network selection schemes, we find that they do not
necessarily result in energy savings when deployed on mobile systems. The key
limitation of existing methods is that they learn only how data should be
processed among the CNNs and not the network architectures, with each network
being treated as a blackbox.
&lt;/p&gt;
&lt;p&gt;To address this limitation, we pursue a more powerful design paradigm where
the architecture settings of the CNNs are treated as hyper-parameters to be
globally optimized. We cast the design of adaptive CNNs as a hyper-parameter
optimization problem with respect to energy, accuracy, and communication
constraints imposed by the mobile device. To efficiently solve this problem, we
adapt Bayesian optimization to the properties of the design space, reaching
near-optimal configurations in few tens of function evaluations. Our method
reduces the energy consumed for image classification on a mobile device by up
to 6x, compared to the best previously published work that uses CNNs as
blackboxes. Finally, we evaluate two image classification practices, i.e.,
classifying all images locally versus over the cloud under energy and
communication constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamoulis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Stamoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting-Wu/0/1/0/all/0/1&quot;&gt;Ting-Wu&lt;/a&gt; (Rudy) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin/0/1/0/all/0/1&quot;&gt;Chin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1&quot;&gt;Anand Krishnan Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Haocheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajja_S/0/1/0/all/0/1&quot;&gt;Sribhuvan Sajja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bognar_M/0/1/0/all/0/1&quot;&gt;Mitchell Bognar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1&quot;&gt;Diana Marculescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01614">
<title>Using Machine Learning Safely in Automotive Software: An Assessment and Adaption of Software Process Requirements in ISO 26262. (arXiv:1808.01614v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01614</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of machine learning (ML) is on the rise in many sectors of software
development, and automotive software development is no different. In
particular, Advanced Driver Assistance Systems (ADAS) and Automated Driving
Systems (ADS) are two areas where ML plays a significant role. In automotive
development, safety is a critical objective, and the emergence of standards
such as ISO 26262 has helped focus industry practices to address safety in a
systematic and consistent way. Unfortunately, these standards were not designed
to accommodate technologies such as ML or the type of functionality that is
provided by an ADS and this has created a conflict between the need to innovate
and the need to improve safety. In this report, we take steps to address this
conflict by doing a detailed assessment and adaption of ISO 26262 for ML,
specifically in the context of supervised learning. First we analyze the key
factors that are the source of the conflict. Then we assess each software
development process requirement (Part 6 of ISO 26262) for applicability to ML.
Where there are gaps, we propose new requirements to address the gaps. Finally
we discuss the application of this adapted and extended variant of Part 6 to ML
development scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salay_R/0/1/0/all/0/1&quot;&gt;Rick Salay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1&quot;&gt;Krzysztof Czarnecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01642">
<title>Multi-Objective Cognitive Model: a supervised approach for multi-subject fMRI analysis. (arXiv:1808.01642v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.01642</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to decode the human brain, Multivariate Pattern (MVP) classification
generates cognitive models by using functional Magnetic Resonance Imaging
(fMRI) datasets. As a standard pipeline in the MVP analysis, brain patterns in
multi-subject fMRI dataset must be mapped to a shared space and then a
classification model is generated by employing the mapped patterns. However,
the MVP models may not provide stable performance on a new fMRI dataset because
the standard pipeline uses disjoint steps for generating these models. Indeed,
each step in the pipeline includes an objective function with independent
optimization approach, where the best solution of each step may not be optimum
for the next steps. For tackling the mentioned issue, this paper introduces the
Multi-Objective Cognitive Model (MOCM) that utilizes an integrated objective
function for MVP analysis rather than just using those disjoint steps. For
solving the integrated problem, we proposed a customized multi-objective
optimization approach, where all possible solutions are firstly generated, and
then our method ranks and selects the robust solutions as the final results.
Empirical studies confirm that the proposed method can generate superior
performance in comparison with other techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yousefnezhad_M/0/1/0/all/0/1&quot;&gt;Muhammad Yousefnezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daoqiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01684">
<title>Missing Value Imputation Based on Deep Generative Models. (arXiv:1808.01684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01684</link>
<description rdf:parseType="Literal">&lt;p&gt;Missing values widely exist in many real-world datasets, which hinders the
performing of advanced data analytics. Properly filling these missing values is
crucial but challenging, especially when the missing rate is high. Many
approaches have been proposed for missing value imputation (MVI), but they are
mostly heuristics-based, lacking a principled foundation and do not perform
satisfactorily in practice. In this paper, we propose a probabilistic framework
based on deep generative models for MVI. Under this framework, imputing the
missing entries amounts to seeking a fixed-point solution between two
conditional distributions defined on the missing entries and latent variables
respectively. These distributions are parameterized by deep neural networks
(DNNs) which possess high approximation power and can capture the nonlinear
relationships between missing entries and the observed values. The learning of
weight parameters of DNNs is performed by maximizing an approximation of the
log-likelihood of observed values. We conducted extensive evaluation on 13
datasets and compared with 11 baselines methods, where our methods largely
outperforms the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongbao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengtao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01687">
<title>Hybrid Subspace Learning for High-Dimensional Data. (arXiv:1808.01687v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01687</link>
<description rdf:parseType="Literal">&lt;p&gt;The high-dimensional data setting, in which p &amp;gt;&amp;gt; n, is a challenging
statistical paradigm that appears in many real-world problems. In this setting,
learning a compact, low-dimensional representation of the data can
substantially help distinguish signal from noise. One way to achieve this goal
is to perform subspace learning to estimate a small set of latent features that
capture the majority of the variance in the original data. Most existing
subspace learning models, such as PCA, assume that the data can be fully
represented by its embedding in one or more latent subspaces. However, in this
work, we argue that this assumption is not suitable for many high-dimensional
datasets; often only some variables can easily be projected to a
low-dimensional space. We propose a hybrid dimensionality reduction technique
in which some features are mapped to a low-dimensional subspace while others
remain in the original space. Our model leads to more accurate estimation of
the latent space and lower reconstruction error. We present a simple
optimization procedure for the resulting biconvex problem and show synthetic
data results that demonstrate the advantages of our approach over existing
methods. Finally, we demonstrate the effectiveness of this method for
extracting meaningful features from both gene expression and video background
subtraction datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchetti_Bowick_M/0/1/0/all/0/1&quot;&gt;Micol Marchetti-Bowick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1&quot;&gt;Benjamin J. Lengerich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1&quot;&gt;Ankur P. Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01813">
<title>Regret Bounds for Reinforcement Learning via Markov Chain Concentration. (arXiv:1808.01813v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01813</link>
<description rdf:parseType="Literal">&lt;p&gt;We give a simple optimistic algorithm for which it is easy to derive regret
bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ after $T$ steps in uniformly
ergodic MDPs with $S$ states, $A$ actions, and mixing time parameter $t_{\rm
mix}$. These bounds are the first regret bounds in the general, non-episodic
setting with an optimal dependence on all given parameters. They could only be
improved by using an alternative mixing time parameter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortner_R/0/1/0/all/0/1&quot;&gt;Ronald Ortner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01842">
<title>Beyond $1/2$-Approximation for Submodular Maximization on Massive Data Streams. (arXiv:1808.01842v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01842</link>
<description rdf:parseType="Literal">&lt;p&gt;Many tasks in machine learning and data mining, such as data diversification,
non-parametric learning, kernel machines, clustering etc., require extracting a
small but representative summary from a massive dataset. Often, such problems
can be posed as maximizing a submodular set function subject to a cardinality
constraint. We consider this question in the streaming setting, where elements
arrive over time at a fast pace and thus we need to design an efficient,
low-memory algorithm. One such method, proposed by Badanidiyuru et al. (2014),
always finds a $0.5$-approximate solution. Can this approximation factor be
improved? We answer this question affirmatively by designing a new algorithm
SALSA for streaming submodular maximization. It is the first low-memory,
single-pass algorithm that improves the factor $0.5$, under the natural
assumption that elements arrive in a random order. We also show that this
assumption is necessary, i.e., that there is no such algorithm with better than
$0.5$-approximation when elements arrive in arbitrary order. Our experiments
demonstrate that SALSA significantly outperforms the state of the art in
applications related to exemplar-based clustering, social graph analysis, and
recommender systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_Fard_A/0/1/0/all/0/1&quot;&gt;Ashkan Norouzi-Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarnawski_J/0/1/0/all/0/1&quot;&gt;Jakub Tarnawski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1&quot;&gt;Slobodan Mitrovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1&quot;&gt;Amir Zandieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavifar_A/0/1/0/all/0/1&quot;&gt;Aida Mousavifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svensson_O/0/1/0/all/0/1&quot;&gt;Ola Svensson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01944">
<title>V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial Segmentation. (arXiv:1808.01944v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.01944</link>
<description rdf:parseType="Literal">&lt;p&gt;Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder
that causes changes in the anatomy of the atria. A better characterization of
these changes is desirable for the definition of clinical biomarkers, and thus
there is a need of its fully automatic segmentation from clinical images. In
this work we present an architecture based in 3D-convolution kernels, a
Volumetric Fully Convolution Neural Network (V-FCNN), able to segment the
entire volume in one-shot, and consequently integrate the implicit spatial
redundancy present in high resolution images. A loss function based on the
mixture of both Mean Square Error (MSE) and Dice Loss (DL) is used, in an
attempt to combine the ability to capture the bulk shape and the reduction of
local errors products by over segmentation. Results demonstrate a reasonable
performance in the middle region of the atria, and the impact of the challenges
of capturing the variability of the pulmonary veins or the identification of
the valve plane that separates the atria to the ventricle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Savioli_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xf3; Savioli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montana_G/0/1/0/all/0/1&quot;&gt;Giovanni Montana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lamata_P/0/1/0/all/0/1&quot;&gt;Pablo Lamata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01951">
<title>A Review on Image- and Network-based Brain Data Analysis Techniques for Alzheimer&apos;s Disease Diagnosis Reveals a Gap in Developing Predictive Methods for Prognosis. (arXiv:1808.01951v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01951</link>
<description rdf:parseType="Literal">&lt;p&gt;Unveiling pathological brain changes associated with Alzheimer&apos;s disease (AD)
is a challenging task especially that people do not show symptoms of dementia
until it is late. Over the past years, neuroimaging techniques paved the way
for computer-based diagnosis and prognosis to facilitate the automation of
medical decision support and help clinicians identify cognitively intact
subjects that are at high-risk of developing AD. As a progressive
neurodegenerative disorder, researchers investigated how AD affects the brain
using different approaches: 1) image-based methods where mainly neuroimaging
modalities are used to provide early AD biomarkers, and 2) network-based
methods which focus on functional and structural brain connectivities to give
insights into how AD alters brain wiring. In this study, we reviewed
neuroimaging-based technical methods developed for AD and mild-cognitive
impairment (MCI) classification and prediction tasks, selected by screening all
MICCAI proceedings published between 2010 and 2016. We included papers that fit
into image-based or network-based categories. The majority of papers focused on
classifying MCI vs. AD brain states, which has enabled the discovery of
discriminative or altered brain regions and connections. However, very few
works aimed to predict MCI progression based on early neuroimaging-based
observations. Despite the high importance of reliably identifying which early
MCI patient will convert to AD, remain stable or reverse to normal over
months/years, predictive models are still lagging behind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soussia_M/0/1/0/all/0/1&quot;&gt;Mayssa Soussia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1&quot;&gt;Islem Rekik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01960">
<title>Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN. (arXiv:1808.01960v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01960</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently proposed distributional approach to reinforcement learning
(DiRL) is centered on learning the distribution of the reward-to-go, often
referred to as the value distribution. In this work, we show that the
distributional Bellman equation, which drives DiRL methods, is equivalent to a
generative adversarial network (GAN) model. In this formulation, DiRL can be
seen as learning a deep generative model of the value distribution, driven by
the discrepancy between the distribution of the current value, and the
distribution of the sum of current reward and next value. We use this insight
to propose a GAN-based approach to DiRL, which leverages the strengths of GANs
in learning distributions of high-dimensional data. In particular, we show that
our GAN approach can be used for DiRL with multivariate rewards, an important
setting which cannot be tackled with prior methods. The multivariate setting
also allows us to unify learning the distribution of values and state
transitions, and we exploit this idea to devise a novel exploration method that
is driven by the discrepancy in estimating both values and states.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freirich_D/0/1/0/all/0/1&quot;&gt;Dror Freirich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meir_R/0/1/0/all/0/1&quot;&gt;Ron Meir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1&quot;&gt;Aviv Tamar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01975">
<title>A Survey on Surrogate Approaches to Non-negative Matrix Factorization. (arXiv:1808.01975v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01975</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by applications in hyperspectral imaging we investigate methods for
approximating a high-dimensional non-negative matrix $\mathbf{\mathit{Y}}$ by a
product of two lower-dimensional, non-negative matrices $\mathbf{\mathit{K}}$
and $\mathbf{\mathit{X}}.$ This so-called non-negative matrix factorization is
based on defining suitable Tikhonov functionals, which combine a discrepancy
measure for $\mathbf{\mathit{Y}}\approx\mathbf{\mathit{KX}}$ with penalty terms
for enforcing additional properties of $\mathbf{\mathit{K}}$ and
$\mathbf{\mathit{X}}$. The minimization is based on alternating minimization
with respect to $\mathbf{\mathit{K}}$ or $\mathbf{\mathit{X}}$, where in each
iteration step one replaces the original Tikhonov functional by a locally
defined surrogate functional. The choice of surrogate functionals is crucial:
It should allow a comparatively simple minimization and simultaneously its
first order optimality condition should lead to multiplicative update rules,
which automatically preserve non-negativity of the iterates. We review the most
standard construction principles for surrogate functionals for Frobenius-norm
and Kullback-Leibler discrepancy measures. We extend the known surrogate
constructions by a general framework, which allows to add a large variety of
penalty terms. The paper finishes by deriving the corresponding alternating
minimization schemes explicitely and by applying these methods to MALDI imaging
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernsel_P/0/1/0/all/0/1&quot;&gt;Pascal Fernsel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_P/0/1/0/all/0/1&quot;&gt;Peter Maa&amp;#xdf;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01990">
<title>Hashing with Binary Matrix Pursuit. (arXiv:1808.01990v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01990</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose theoretical and empirical improvements for two-stage hashing
methods. We first provide a theoretical analysis on the quality of the binary
codes and show that, under mild assumptions, a residual learning scheme can
construct binary codes that fit any neighborhood structure with arbitrary
accuracy. Secondly, we show that with high-capacity hash functions such as
CNNs, binary code inference can be greatly simplified for many standard
neighborhood definitions, yielding smaller optimization problems and more
robust codes. Incorporating our findings, we propose a novel two-stage hashing
method that significantly outperforms previous hashing studies on widely used
image retrieval benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cakir_F/0/1/0/all/0/1&quot;&gt;Fatih Cakir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1&quot;&gt;Stan Sclaroff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.04706">
<title>DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression. (arXiv:1604.04706v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1604.04706</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling multinomial logistic regression to datasets with very large number of
data points and classes is challenging. This is primarily because one needs to
compute the log-partition function on every data point. This makes distributing
the computation hard. In this paper, we present a distributed stochastic
gradient descent based optimization method (DS-MLR) for scaling up multinomial
logistic regression problems to massive scale datasets without hitting any
storage constraints on the data and model parameters. Our algorithm exploits
double-separability, an attractive property that allows us to achieve both data
as well as model parallelism simultaneously. In addition, we introduce a
non-blocking and asynchronous variant of our algorithm that avoids
bulk-synchronization. We demonstrate the versatility of DS-MLR to various
scenarios in data and model parallelism, through an extensive empirical study
using several real-world datasets. In particular, we demonstrate the
scalability of DS-MLR by solving an extreme multi-class classification problem
on the Reddit dataset (159 GB data, 358 GB parameters) where, to the best of
our knowledge, no other existing methods apply.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Sriram Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsushima_S/0/1/0/all/0/1&quot;&gt;Shin Matsushima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Hyokun Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwanathan_S/0/1/0/all/0/1&quot;&gt;S.V.N. Vishwanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.09499">
<title>Extreme Stochastic Variational Inference: Distributed and Asynchronous. (arXiv:1605.09499v9 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.09499</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic variational inference (SVI), the state-of-the-art algorithm for
scaling variational inference to large-datasets, is inherently serial.
Moreover, it requires the parameters to fit in the memory of a single
processor; this is problematic when the number of parameters is in billions. In
this paper, we propose extreme stochastic variational inference (ESVI), an
asynchronous and lock-free algorithm to perform variational inference for
mixture models on massive real world datasets. ESVI overcomes the limitations
of SVI by requiring that each processor only access a subset of the data and a
subset of the parameters, thus providing data and model parallelism
simultaneously. We demonstrate the effectiveness of ESVI by running Latent
Dirichlet Allocation (LDA) on UMBC-3B, a dataset that has a vocabulary of 3
million and a token size of 3 billion. In our experiments, we found that ESVI
not only outperforms VI and SVI in wallclock-time, but also achieves a better
quality solution. In addition, we propose a strategy to speed up computation
and save memory when fitting large number of topics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shihao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishwanathan_S/0/1/0/all/0/1&quot;&gt;S.V.N. Vishwanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.01701">
<title>Learning Certifiably Optimal Rule Lists for Categorical Data. (arXiv:1704.01701v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.01701</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the design and implementation of a custom discrete optimization
technique for building rule lists over a categorical feature space. Our
algorithm produces rule lists with optimal training performance, according to
the regularized empirical risk, with a certificate of optimality. By leveraging
algorithmic bounds, efficient data structures, and computational reuse, we
achieve several orders of magnitude speedup in time and a massive reduction of
memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is
possible to construct optimal sparse rule lists that are approximately as
accurate as the COMPAS proprietary risk prediction tool on data from Broward
County, Florida, but that are completely interpretable. This framework is a
novel alternative to CART and other decision tree methods for interpretable
modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Angelino_E/0/1/0/all/0/1&quot;&gt;Elaine Angelino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Larus_Stone_N/0/1/0/all/0/1&quot;&gt;Nicholas Larus-Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alabi_D/0/1/0/all/0/1&quot;&gt;Daniel Alabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seltzer_M/0/1/0/all/0/1&quot;&gt;Margo Seltzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02771">
<title>Group Importance Sampling for Particle Filtering and MCMC. (arXiv:1704.02771v4 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02771</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian methods and their implementations by means of sophisticated Monte
Carlo techniques have become very popular in signal processing over the last
years. Importance Sampling (IS) is a well-known Monte Carlo technique that
approximates integrals involving a posterior distribution by means of weighted
samples. In this work, we study the assignation of a single weighted sample
which compresses the information contained in a population of weighted samples.
Part of the theory that we present as Group Importance Sampling (GIS) has been
employed implicitly in different works in the literature. The provided analysis
yields several theoretical and practical consequences. For instance, we discuss
the application of GIS into the Sequential Importance Resampling framework and
show that Independent Multiple Try Metropolis schemes can be interpreted as a
standard Metropolis-Hastings algorithm, following the GIS approach. We also
introduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.
The first one, named Group Metropolis Sampling method, produces a Markov chain
of sets of weighted samples. All these sets are then employed for obtaining a
unique global estimator. The second one is the Distributed Particle
Metropolis-Hastings technique, where different parallel particle filters are
jointly used to drive an MCMC algorithm. Different resampled trajectories are
compared and then tested with a proper acceptance probability. The novel
schemes are tested in different numerical experiments such as learning the
hyperparameters of Gaussian Processes, two localization problems in a wireless
sensor network (with synthetic and real data) and the tracking of vegetation
parameters given satellite observations, where they are compared with several
benchmark Monte Carlo techniques. Three illustrative Matlab demos are also
provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1&quot;&gt;L. Martino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1&quot;&gt;V. Elvira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Camps_Valls_G/0/1/0/all/0/1&quot;&gt;G. Camps-Valls&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07888">
<title>Stochastic Optimization from Distributed, Streaming Data in Rate-limited Networks. (arXiv:1704.07888v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07888</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by machine learning applications in networks of sensors,
internet-of-things (IoT) devices, and autonomous agents, we propose techniques
for distributed stochastic convex learning from high-rate data streams. The
setup involves a network of nodes---each one of which has a stream of data
arriving at a constant rate---that solve a stochastic convex optimization
problem by collaborating with each other over rate-limited communication links.
To this end, we present and analyze two algorithms---termed distributed
stochastic approximation mirror descent (D-SAMD) and accelerated distributed
stochastic approximation mirror descent (AD-SAMD)---that are based on two
stochastic variants of mirror descent and in which nodes collaborate via
approximate averaging of the local, noisy subgradients using distributed
consensus. Our main contributions are (i) bounds on the convergence rates of
D-SAMD and AD-SAMD in terms of the number of nodes, network topology, and ratio
of the data streaming and communication rates, and (ii) sufficient conditions
for order-optimum convergence of these algorithms. In particular, we show that
for sufficiently well-connected networks, distributed learning schemes can
obtain order-optimum convergence even if the communications rate is small.
Further we find that the use of accelerated methods significantly enlarges the
regime in which order-optimum convergence is achieved; this is in contrast to
the centralized setting, where accelerated methods usually offer only a modest
improvement. Finally, we demonstrate the effectiveness of the proposed
algorithms using numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nokleby_M/0/1/0/all/0/1&quot;&gt;Matthew Nokleby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bajwa_W/0/1/0/all/0/1&quot;&gt;Waheed U. Bajwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02893">
<title>Convolutional Dictionary Learning: A Comparative Review and New Algorithms. (arXiv:1709.02893v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02893</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional sparse representations are a form of sparse representation with
a dictionary that has a structure that is equivalent to convolution with a set
of linear filters. While effective algorithms have recently been developed for
the convolutional sparse coding problem, the corresponding dictionary learning
problem is substantially more challenging. Furthermore, although a number of
different approaches have been proposed, the absence of thorough comparisons
between them makes it difficult to determine which of them represents the
current state of the art. The present work both addresses this deficiency and
proposes some new approaches that outperform existing ones in certain contexts.
A thorough set of performance comparisons indicates a very wide range of
performance differences among the existing and proposed methods, and clearly
identifies those that are the most effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Cardona_C/0/1/0/all/0/1&quot;&gt;Cristina Garcia-Cardona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1&quot;&gt;Brendt Wohlberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04876">
<title>Uncertainty Quantification for Online Learning and Stochastic Approximation via Hierarchical Incremental Gradient Descent. (arXiv:1802.04876v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04876</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient descent (SGD) is an immensely popular approach for online
learning in settings where data arrives in a stream or data sizes are very
large. However, despite an ever- increasing volume of work on SGD, much less is
known about the statistical inferential properties of SGD-based predictions.
Taking a fully inferential viewpoint, this paper introduces a novel procedure
termed HiGrad to conduct statistical inference for online learning, without
incurring additional computational cost compared with SGD. The HiGrad procedure
begins by performing SGD updates for a while and then splits the single thread
into several threads, and this procedure hierarchically operates in this
fashion along each thread. With predictions provided by multiple threads in
place, a t-based confidence interval is constructed by decorrelating
predictions using covariance structures given by a Donsker-style extension of
the Ruppert--Polyak averaging scheme, which is a technical contribution of
independent interest. Under certain regularity conditions, the HiGrad
confidence interval is shown to attain asymptotically exact coverage
probability. Finally, the performance of HiGrad is evaluated through extensive
simulation studies and a real data example. An R package higrad has been
developed to implement the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie J. Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06823">
<title>Entropy-Isomap: Manifold Learning for High-dimensional Dynamic Processes. (arXiv:1802.06823v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06823</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific and engineering processes deliver massive high-dimensional data
sets that are generated as non-linear transformations of an initial state and
few process parameters. Mapping such data to a low-dimensional manifold
facilitates better understanding of the underlying processes, and enables their
optimization. In this paper, we first show that off-the-shelf non-linear
spectral dimensionality reduction methods, e.g., Isomap, fail for such data,
primarily due to the presence of strong temporal correlations. Then, we propose
a novel method, Entropy-Isomap, to address the issue. The proposed method is
successfully applied to large data describing a fabrication process of organic
materials. The resulting low-dimensional representation correctly captures
process control variables, allows for low-dimensional visualization of the
material morphology evolution, and provides key insights to improve the
process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoeneman_F/0/1/0/all/0/1&quot;&gt;Frank Schoeneman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chandola_V/0/1/0/all/0/1&quot;&gt;Varun Chandola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Napp_N/0/1/0/all/0/1&quot;&gt;Nils Napp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wodo_O/0/1/0/all/0/1&quot;&gt;Olga Wodo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zola_J/0/1/0/all/0/1&quot;&gt;Jaroslaw Zola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07756">
<title>Determining the best classifier for predicting the value of a boolean field on a blood donor database using genetic algorithms. (arXiv:1802.07756v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07756</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: Thanks to digitization, we often have access to large databases,
consisting of various fields of information, ranging from numbers to texts and
even boolean values. Such databases lend themselves especially well to machine
learning, classification and big data analysis tasks. We are able to train
classifiers, using already existing data and use them for predicting the values
of a certain field, given that we have information regarding the other fields.
Most specifically, in this study, we look at the Electronic Health Records
(EHRs) that are compiled by hospitals. These EHRs are convenient means of
accessing data of individual patients, but there processing as a whole still
remains a task. However, EHRs that are composed of coherent, well-tabulated
structures lend themselves quite well to the application to machine language,
via the usage of classifiers. In this study, we look at a Blood Transfusion
Service Center Data Set (Data taken from the Blood Transfusion Service Center
in Hsin-Chu City in Taiwan). We used scikit-learn machine learning in python.
From Support Vector Machines(SVM), we use Support Vector Classification(SVC),
from the linear model we import Perceptron. We also used the
K.neighborsclassifier and the decision tree classifiers. Furthermore, we use
the TPOT library to find an optimized pipeline using genetic algorithms. Using
the above classifiers, we score each one of them using k fold cross-validation.
&lt;/p&gt;
&lt;p&gt;Contact: ritabratamaiti@hiretrex.com GitHub Repository:
https://github.com/ritabratamaiti/Blooddonorprediction
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maiti_R/0/1/0/all/0/1&quot;&gt;Ritabrata Maiti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03154">
<title>Cauchy noise loss for stochastic optimization of random matrix models via free deterministic equivalents. (arXiv:1804.03154v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03154</link>
<description rdf:parseType="Literal">&lt;p&gt;For random matrix models, the parameter estimation based on the traditional
likelihood is not straightforward in particular when there is only one sample
matrix. We introduce a new parameter optimization method of random matrix
models which works even in such a case not based on the traditional likelihood,
instead based on the spectral distribution. We use the spectral distribution
perturbed by Cauchy noises because the free deterministic equivalent, which is
a tool in free probability theory, allows us to approximate it by a smooth and
accessible density function.
&lt;/p&gt;
&lt;p&gt;Moreover, we study an asymptotic property of a determination gap, which has a
similar role as the generalization gap. In addition, we propose a new
dimensionality recovery method for the signal-plus-noise model, and
experimentally demonstrate that it recovers the rank of the signal part even if
the rank is not low. It is a simultaneous rank selection and parameter
estimation procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hayase_T/0/1/0/all/0/1&quot;&gt;Tomohiro Hayase&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00089">
<title>Concolic Testing for Deep Neural Networks. (arXiv:1805.00089v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00089</link>
<description rdf:parseType="Literal">&lt;p&gt;Concolic testing combines program execution and symbolic analysis to explore
the execution paths of a software program. This paper presents the first
concolic testing approach for Deep Neural Networks (DNNs). More specifically,
we formalise coverage criteria for DNNs that have been studied in the
literature, and then develop a coherent method for performing concolic testing
to increase test coverage. Our experimental results show the effectiveness of
the concolic testing approach in both achieving high coverage and finding
adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Youcheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1&quot;&gt;Marta Kwiatkowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroening_D/0/1/0/all/0/1&quot;&gt;Daniel Kroening&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00534">
<title>Run Procrustes, Run! On the convergence of accelerated Procrustes Flow. (arXiv:1806.00534v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00534</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present theoretical results on the convergence of non-convex
accelerated gradient descent in matrix factorization models. The technique is
applied to matrix sensing problems with squared loss, for the estimation of a
rank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show that
the acceleration leads to linear convergence rate, even under non-convex
settings where the variable $X$ is represented as $U U^\top$ for $U \in
\mathbb{R}^{n \times r}$. Our result has the same dependence on the condition
number of the objective --and the optimal solution-- as that of the recent
results on non-accelerated algorithms. However, acceleration is observed in
practice, both in synthetic examples and in two real applications: neuronal
multi-unit activities recovery from single electrode recordings, and quantum
state tomography on quantum computing simulators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ubaru_S/0/1/0/all/0/1&quot;&gt;Shashanka Ubaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_G/0/1/0/all/0/1&quot;&gt;Georgios Kollias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1&quot;&gt;Kristofer Bouchard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00701">
<title>On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional Neural Networks. (arXiv:1806.00701v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00701</link>
<description rdf:parseType="Literal">&lt;p&gt;Parsimonious representations in data modeling are ubiquitous and central for
processing information. Motivated by the recent Multi-Layer Convolutional
Sparse Coding (ML-CSC) model, we herein generalize the traditional Basis
Pursuit regression problem to a multi-layer setting, introducing similar sparse
enforcing penalties at different representation layers in a symbiotic relation
between synthesis and analysis sparse priors. We propose and analyze different
iterative algorithms to solve this new problem in practice. We prove that the
presented multi-layer Iterative Soft Thresholding (ML-ISTA) and multi-layer
Fast ISTA (ML-FISTA) converge to the global optimum of our multi-layer
formulation at a rate of $\mathcal{O}(1/k)$ and $\mathcal{O}(1/k^2)$,
respectively and independently of the number of layers. We further show how
these algorithms effectively implement particular recurrent neural networks
that generalize feed-forward architectures without any increase in the number
of parameters. We present different architectures that result from unfolding
the iterations of the proposed multi-layer pursuit algorithms, providing a
principled way to construct deep recurrent CNNs from feed-forward ones. We
demonstrate the emerging constructions by training them in an end-to-end
manner, consistently improving the performance of classical networks without
introducing extra filters or parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1&quot;&gt;Jeremias Sulam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberdam_A/0/1/0/all/0/1&quot;&gt;Aviad Aberdam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10206">
<title>Deep Feature Factorization For Concept Discovery. (arXiv:1806.10206v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10206</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Deep Feature Factorization (DFF), a method capable of localizing
similar semantic concepts within an image or a set of images. We use DFF to
gain insight into a deep convolutional neural network&apos;s learned features, where
we detect hierarchical cluster structures in feature space. This is visualized
as heat maps, which highlight semantically matching regions across a set of
images, revealing what the network `perceives&apos; as similar. DFF can also be used
to perform co-segmentation and co-localization, and we report state-of-the-art
results on these tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_E/0/1/0/all/0/1&quot;&gt;Edo Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achanta_R/0/1/0/all/0/1&quot;&gt;Radhakrishna Achanta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1&quot;&gt;Sabine S&amp;#xfc;sstrunk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00892">
<title>Semi-blind source separation with multichannel variational autoencoder. (arXiv:1808.00892v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00892</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a multichannel source separation method called the
multichannel variational autoencoder (MVAE), which uses a conditional VAE
(CVAE) to model and estimate the power spectrograms of the sources in a
mixture. By training the CVAE using the spectrograms of training examples with
source-class labels, we can use the trained decoder distribution as a universal
generative model that is able to generate spectrograms conditioned on a
specified class label. By treating the latent space variables and the class
label as the unknown parameters of this generative model, we can develop a
convergence-guaranteed semi-blind source separation algorithm that consists of
iteratively estimating the power spectrograms of the underlying sources as well
as the separation matrices. Through experimental evaluations, our MVAE showed
higher separation performance than a baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Inoue_S/0/1/0/all/0/1&quot;&gt;Shota Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Makino_S/0/1/0/all/0/1&quot;&gt;Shoji Makino&lt;/a&gt;</dc:creator>
</item></rdf:RDF>