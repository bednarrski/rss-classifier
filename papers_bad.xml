<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00838"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00906"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00986"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00590"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00653"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.05681"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.02255"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02669"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00116"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00606"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00641"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00650"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00810"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00860"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.04460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08999"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.02436"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00183"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/0808.3416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03824"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.00684">
<title>Autostacker: A Compositional Evolutionary Learning System. (arXiv:1803.00684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00684</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an automatic machine learning (AutoML) modeling architecture
called Autostacker, which combines an innovative hierarchical stacking
architecture and an Evolutionary Algorithm (EA) to perform efficient parameter
search. Neither prior domain knowledge about the data nor feature preprocessing
is needed. Using EA, Autostacker quickly evolves candidate pipelines with high
predictive accuracy. These pipelines can be used as is or as a starting point
for human experts to build on. Autostacker finds innovative combinations and
structures of machine learning models, rather than selecting a single model and
optimizing its hyperparameters. Compared with other AutoML systems on fifteen
datasets, Autostacker achieves state-of-art or competitive performance both in
terms of test accuracy and time cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Boyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Harvey Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_W/0/1/0/all/0/1&quot;&gt;Warren Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_I/0/1/0/all/0/1&quot;&gt;Ishanu Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00838">
<title>A multi-instance deep neural network classifier: application to Higgs boson CP measurement. (arXiv:1803.00838v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00838</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate properties of a classifier applied to the measurements of the
CP state of the Higgs boson in $H\rightarrow\tau\tau$ decays. The problem is
framed as binary classifier applied to individual instances. Then the prior
knowledge that the instances belong to the same class is used to define the
multi-instance classifier. Its final score is calculated as multiplication of
single instance scores for a given series of instances. In the paper we discuss
properties of such classifier, notably its dependence on the number of
instances in the series. This classifier exhibits very strong random dependence
on the number of epochs used for training and requires careful tuning of the
classification threshold. We derive formula for this optimal threshold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bialas_P/0/1/0/all/0/1&quot;&gt;P. Bialas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nemeth_D/0/1/0/all/0/1&quot;&gt;D. Nemeth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_Was_E/0/1/0/all/0/1&quot;&gt;E. Richter-W&amp;#x105;s&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00906">
<title>Enhancing Cooperative Coevolution for Large Scale Optimization by Adaptively Constructing Surrogate Models. (arXiv:1803.00906v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.00906</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been shown that cooperative coevolution (CC) can effectively deal with
large scale optimization problems (LSOPs) through a divide-and-conquer
strategy. However, its performance is severely restricted by the current
context-vector-based sub-solution evaluation method since this method needs to
access the original high dimensional simulation model when evaluating each
sub-solution and thus requires many computation resources. To alleviate this
issue, this study proposes an adaptive surrogate model assisted CC framework.
This framework adaptively constructs surrogate models for different
sub-problems by fully considering their characteristics. For the single
dimensional sub-problems obtained through decomposition, accurate enough
surrogate models can be obtained and used to find out the optimal solutions of
the corresponding sub-problems directly. As for the nonseparable sub-problems,
the surrogate models are employed to evaluate the corresponding sub-solutions,
and the original simulation model is only adopted to reevaluate some good
sub-solutions selected by surrogate models. By these means, the computation
cost could be greatly reduced without significantly sacrificing evaluation
quality. Empirical studies on IEEE CEC 2010 benchmark functions show that the
concrete algorithm based on this framework is able to find much better
solutions than the conventional CC algorithms and a non-CC algorithm even with
much fewer computation resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1&quot;&gt;Bei Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhigang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;An Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00986">
<title>Niching an Archive-based Gaussian Estimation of Distribution Algorithm via Adaptive Clustering. (arXiv:1803.00986v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.00986</link>
<description rdf:parseType="Literal">&lt;p&gt;As a model-based evolutionary algorithm, estimation of distribution algorithm
(EDA) possesses unique characteristics and has been widely applied to global
optimization. However, traditional Gaussian EDA (GEDA) may suffer from
premature convergence and has a high risk of falling into local optimum when
dealing with multimodal problem. In this paper, we first attempts to improve
the performance of GEDA by utilizing historical solutions and develops a novel
archive-based EDA variant. The use of historical solutions not only enhances
the search efficiency of EDA to a large extent, but also significantly reduces
the population size so that a faster convergence could be achieved. Then, the
archive-based EDA is further integrated with a novel adaptive clustering
strategy for solving multimodal optimization problems. Taking the advantage of
the clustering strategy in locating different promising areas and the powerful
exploitation ability of the archive-based EDA, the resultant algorithm is
endowed with strong capability in finding multiple optima. To verify the
efficiency of the proposed algorithm, we tested it on a set of well-known
niching benchmark problems and compared it with several state-of-the-art
niching algorithms. The experimental results indicate that the proposed
algorithm is competitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhigang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1&quot;&gt;Bei Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;An Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00997">
<title>A computational perspective of the role of Thalamus in cognition. (arXiv:1803.00997v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1803.00997</link>
<description rdf:parseType="Literal">&lt;p&gt;Thalamus has traditionally been considered as only a relay source of cortical
inputs, with hierarchically organized cortical circuits serially transforming
thalamic signals to cognitively-relevant representations. Given the absence of
local excitatory connections within the thalamus, the notion of thalamic
`relay&apos; seemed like a reasonable description over the last several decades.
Recent advances in experimental approaches and theory provide a broader
perspective on the role of the thalamus in cognitively-relevant cortical
computations, and suggest that only a subset of thalamic circuit motifs fit the
relay description. Here, we discuss this perspective and highlight the
potential role for the thalamus in dynamic selection of cortical
representations through a combination of intrinsic thalamic computations and
output signals that change cortical network functional parameters. We suggest
that through the contextual modulation of cortical computation, thalamus and
cortex jointly optimize the information/cost tradeoff in an emergent fashion.
We emphasize that coordinated experimental and theoretical efforts will provide
a path to understanding the role of the thalamus in cognition, along with an
understanding to augment cognitive capacity in health and disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dehghani_N/0/1/0/all/0/1&quot;&gt;Nima Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wimmer_R/0/1/0/all/0/1&quot;&gt;Ralf D. Wimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00754">
<title>Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. (arXiv:1712.00754v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00754</link>
<description rdf:parseType="Literal">&lt;p&gt;A new class of non-homogeneous state-affine systems is introduced for use in
reservoir computing. Sufficient conditions are identified that guarantee first,
that the associated reservoir computers with linear readouts are causal,
time-invariant, and satisfy the fading memory property and second, that a
subset of this class is universal in the category of fading memory filters with
stochastic almost surely uniformly bounded inputs. This means that any
discrete-time filter that satisfies the fading memory property with random
inputs of that type can be uniformly approximated by elements in the
non-homogeneous state-affine family.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grigoryeva_L/0/1/0/all/0/1&quot;&gt;Lyudmila Grigoryeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1&quot;&gt;Juan-Pablo Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00590">
<title>Hierarchical Imitation and Reinforcement Learning. (arXiv:1803.00590v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00590</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning policies over long time horizons. We present
a framework that leverages and integrates two key concepts. First, we utilize
hierarchical policy classes that enable planning over different time scales,
i.e., the high level planner proposes a sequence of subgoals for the low level
planner to achieve. Second, we utilize expert demonstrations within the
hierarchical action space to dramatically reduce cost of exploration. Our
framework is flexible and can incorporate different combinations of imitation
learning (IL) and reinforcement learning (RL) at different levels of the
hierarchy. Using long-horizon benchmarks, including Montezuma&apos;s Revenge, we
empirically demonstrate that our approach can learn significantly faster
compared to hierarchical RL, and can be significantly more label- and
sample-efficient compared to flat IL. We also provide theoretical analysis of
the labeling cost for certain instantiations of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hoang M. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1&quot;&gt;Miroslav Dud&amp;#xed;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1&quot;&gt;Hal Daum&amp;#xe9; III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00653">
<title>Semi-parametric Topological Memory for Navigation. (arXiv:1803.00653v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00653</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new memory architecture for navigation in previously unseen
environments, inspired by landmark-based navigation in animals. The proposed
semi-parametric topological memory (SPTM) consists of a (non-parametric) graph
with nodes corresponding to locations in the environment and a (parametric)
deep network capable of retrieving nodes from the graph based on observations.
The graph stores no metric information, only connectivity of locations
corresponding to the nodes. We use SPTM as a planning module in a navigation
system. Given only 5 minutes of footage of a previously unseen maze, an
SPTM-based navigation agent can build a topological map of the environment and
use it to confidently navigate towards goals. The average success rate of the
SPTM agent in goal-directed navigation across test environments is higher than
the best-performing baseline by a factor of three. A video of the agent is
available at https://youtu.be/vRF7f4lhswo
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1&quot;&gt;Nikolay Savinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1&quot;&gt;Alexey Dosovitskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1&quot;&gt;Vladlen Koltun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00781">
<title>Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration. (arXiv:1803.00781v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00781</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsically motivated goal exploration algorithms enable machines to
discover repertoires of policies that produce a diversity of effects in complex
environments. These exploration algorithms have been shown to allow real world
robots to acquire skills such as tool use in high-dimensional continuous state
and action spaces. However, they have so far assumed that self-generated goals
are sampled in a specifically engineered feature space, limiting their
autonomy. In this work, we propose to use deep representation learning
algorithms to learn an adequate goal space. This is a developmental 2-stage
approach: first, in a perceptual learning stage, deep learning algorithms use
passive raw sensor observations of world changes to learn a corresponding
latent space; then goal exploration happens in a second stage by sampling goals
in this latent space. We present experiments where a simulated robot arm
interacts with an object, and we show that exploration algorithms using such
learned representations can match the performance obtained using engineered
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pere_A/0/1/0/all/0/1&quot;&gt;Alexandre P&amp;#xe9;r&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forestier_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Forestier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1&quot;&gt;Olivier Sigaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00907">
<title>Multi-Instance Dynamic Ordinal Random Fields for Weakly-supervised Facial Behavior Analysis. (arXiv:1803.00907v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.00907</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Multi-Instance-Learning (MIL) approach for weakly-supervised
learning problems, where a training set is formed by bags (sets of feature
vectors or instances) and only labels at bag-level are provided. Specifically,
we consider the Multi-Instance Dynamic-Ordinal-Regression (MI-DOR) setting,
where the instance labels are naturally represented as ordinal variables and
bags are structured as temporal sequences. To this end, we propose
Multi-Instance Dynamic Ordinal Random Fields (MI-DORF). In this framework, we
treat instance-labels as temporally-dependent latent variables in an Undirected
Graphical Model. Different MIL assumptions are modelled via newly introduced
high-order potentials relating bag and instance-labels within the energy
function of the model. We also extend our framework to address the
Partially-Observed MI-DOR problems, where a subset of instance labels are
available during training. We show on the tasks of weakly-supervised facial
behavior analysis, Facial Action Unit (DISFA dataset) and Pain (UNBC dataset)
Intensity estimation, that the proposed framework outperforms alternative
learning approaches. Furthermore, we show that MIDORF can be employed to reduce
the data annotation efforts in this context by large-scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1&quot;&gt;Adria Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudovic_O/0/1/0/all/0/1&quot;&gt;Ognjen Rudovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binefa_X/0/1/0/all/0/1&quot;&gt;Xavier Binefa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1&quot;&gt;Maja Pantic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00952">
<title>Optimization with Gradient-Boosted Trees and Risk Control. (arXiv:1803.00952v1 [math.OC])</title>
<link>http://arxiv.org/abs/1803.00952</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision trees effectively represent the sparse, high dimensional and noisy
nature of chemical data from experiments. Having learned a function from this
data, we may want to thereafter optimize the function, e.g., picking the best
chemical process catalyst. In this way, we may repurpose legacy predictive
models. This work studies a large-scale, industrially-relevant mixed-integer
quadratic optimization problem involving: (i) gradient-boosted pre-trained
regression trees modeling catalyst behavior, (ii) penalty functions mitigating
risk, and (iii) penalties enforcing composition constraints. We develop
heuristic methods and an exact, branch-and-bound algorithm leveraging
structural properties of gradient-boosted trees and penalty functions. We
numerically test our methods on an industrial instance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mistry_M/0/1/0/all/0/1&quot;&gt;Miten Mistry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Letsios_D/0/1/0/all/0/1&quot;&gt;Dimitrios Letsios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Misener_R/0/1/0/all/0/1&quot;&gt;Ruth Misener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Krennrich_G/0/1/0/all/0/1&quot;&gt;Gerhard Krennrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Robert M. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.05681">
<title>Optimal Warping Paths are unique for almost every Pair of Time Series. (arXiv:1705.05681v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.05681</link>
<description rdf:parseType="Literal">&lt;p&gt;Update rules for learning in dynamic time warping spaces are based on optimal
warping paths between parameter and input time series. In general, optimal
warping paths are not unique resulting in adverse effects in theory and
practice. Under the assumption of squared error local costs, we show that no
two warping paths have identical costs almost everywhere in a measure-theoretic
sense. Two direct consequences of this result are: (i) optimal warping paths
are unique almost everywhere, and (ii) the set of all pairs of time series with
multiple equal-cost warping paths coincides with the union of exponentially
many zero sets of quadratic forms. One implication of the proposed results is
that typical distance-based cost functions such as the k-means objective are
differentiable almost everywhere and can be minimized by subgradient methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Brijnesh J. Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schultz_D/0/1/0/all/0/1&quot;&gt;David Schultz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.02255">
<title>Generative Statistical Models with Self-Emergent Grammar of Chord Sequences. (arXiv:1708.02255v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.02255</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative statistical models of chord sequences play crucial roles in music
processing. To capture syntactic similarities among certain chords (e.g. in C
major key, between G and G7 and between F and Dm), we study hidden Markov
models and probabilistic context-free grammar models with latent variables
describing syntactic categories of chord symbols and their unsupervised
learning techniques for inducing the latent grammar from data. Surprisingly, we
find that these models often outperform conventional Markov models in
predictive power, and the self-emergent categories often correspond to
traditional harmonic functions. This implies the need for chord categories in
harmony models from the informatics perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsushima_H/0/1/0/all/0/1&quot;&gt;Hiroaki Tsushima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamura_E/0/1/0/all/0/1&quot;&gt;Eita Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itoyama_K/0/1/0/all/0/1&quot;&gt;Katsutoshi Itoyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshii_K/0/1/0/all/0/1&quot;&gt;Kazuyoshi Yoshii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06975">
<title>Consequentialist conditional cooperation in social dilemmas with imperfect information. (arXiv:1710.06975v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06975</link>
<description rdf:parseType="Literal">&lt;p&gt;Social dilemmas, where mutual cooperation can lead to high payoffs but
participants face incentives to cheat, are ubiquitous in multi-agent
interaction. We wish to construct agents that cooperate with pure cooperators,
avoid exploitation by pure defectors, and incentivize cooperation from the
rest. However, often the actions taken by a partner are (partially) unobserved
or the consequences of individual actions are hard to predict. We show that in
a large class of games good strategies can be constructed by conditioning one&apos;s
behavior solely on outcomes (ie. one&apos;s past rewards). We call this
consequentialist conditional cooperation. We show how to construct such
strategies using deep reinforcement learning techniques and demonstrate, both
analytically and experimentally, that they are effective in social dilemmas
beyond simple matrix games. We also show the limitations of relying purely on
consequences and discuss the need for understanding both the consequences of
and the intentions behind an action.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peysakhovich_A/0/1/0/all/0/1&quot;&gt;Alexander Peysakhovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1&quot;&gt;Adam Lerer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10967">
<title>Artificial Intelligence as Structural Estimation: Economic Interpretations of Deep Blue, Bonanza, and AlphaGo. (arXiv:1710.10967v3 [econ.EM] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10967</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust&apos;s (1987) nested fixed-point method. AlphaGo&apos;s &quot;supervised-learning policy
network&quot; is a deep neural network implementation of Hotz and Miller&apos;s (1993)
conditional choice probability estimation; its &quot;reinforcement-learning value
network&quot; is equivalent to Hotz, Miller, Sanders, and Smith&apos;s (1994) conditional
choice simulation method. Relaxing these AIs&apos; implicit econometric assumptions
would improve their structural interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Igami_M/0/1/0/all/0/1&quot;&gt;Mitsuru Igami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00851">
<title>Provable defenses against adversarial examples via the convex outer adversarial polytope. (arXiv:1711.00851v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00851</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to learn deep ReLU-based classifiers that are provably
robust against norm-bounded adversarial perturbations on the training data. For
previously unseen examples, the approach is guaranteed to detect all
adversarial examples, though it may flag some non-adversarial examples as well.
The basic idea is to consider a convex outer approximation of the set of
activations reachable through a norm-bounded perturbation, and we develop a
robust optimization procedure that minimizes the worst case loss over this
outer region (via a linear program). Crucially, we show that the dual problem
to this linear program can be represented itself as a deep network similar to
the backpropagation network, leading to very efficient optimization approaches
that produce guaranteed bounds on the robust loss. The end result is that by
executing a few more forward and backward passes through a slightly modified
version of the original network (though possibly with much larger batch sizes),
we can learn a classifier that is provably robust to any norm-bounded
adversarial attack. We illustrate the approach on a number of tasks to train
classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a
convolutional classifier that provably has less than 5.8% test error for any
adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$).
Code for all experiments in the paper is available at
https://github.com/locuslab/convex_adversarial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08697">
<title>Interpretable Counting for Visual Question Answering. (arXiv:1712.08697v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08697</link>
<description rdf:parseType="Literal">&lt;p&gt;Questions that require counting a variety of objects in images remain a major
challenge in visual question answering (VQA). The most common approaches to VQA
involve either classifying answers based on fixed length representations of
both the image and question or summing fractional counts estimated from each
section of the image. In contrast, we treat counting as a sequential decision
process and force our model to make discrete choices of what to count.
Specifically, the model sequentially selects from detected objects and learns
interactions between objects that influence subsequent selections. A
distinction of our approach is its intuitive and interpretable output, as
discrete counts are automatically grounded in the image. Furthermore, our
method outperforms the state of the art architecture for VQA on multiple
metrics that evaluate counting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1&quot;&gt;Alexander Trott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02669">
<title>PPFNet: Global Context Aware Local Features for Robust 3D Point Matching. (arXiv:1802.02669v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02669</link>
<description rdf:parseType="Literal">&lt;p&gt;We present PPFNet - Point Pair Feature NETwork for deeply learning a globally
informed 3D local feature descriptor to find correspondences in unorganized
point clouds. PPFNet learns local descriptors on pure geometry and is highly
aware of the global context, an important cue in deep learning. Our 3D
representation is computed as a collection of point-pair-features combined with
the points and normals within a local vicinity. Our permutation invariant
network design is inspired by PointNet and sets PPFNet to be ordering-free. As
opposed to voxelization, our method is able to consume raw point clouds to
exploit the full sparsity. PPFNet uses a novel $\textit{N-tuple}$ loss and
architecture injecting the global information naturally into the local
descriptor. It shows that context awareness also boosts the local feature
representation. Qualitative and quantitative evaluations of our network suggest
increased recall, improved robustness and invariance as well as a vital step in
the 3D descriptor extraction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Haowen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1&quot;&gt;Tolga Birdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1&quot;&gt;Slobodan Ilic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07687">
<title>Stochastic Video Generation with a Learned Prior. (arXiv:1802.07687v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07687</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating video frames that accurately predict future world states is
challenging. Existing approaches either fail to capture the full distribution
of outcomes, or yield blurry generations, or both. In this paper we introduce
an unsupervised video generation model that learns a prior model of uncertainty
in a given environment. Video frames are generated by drawing samples from this
prior and combining them with a deterministic estimate of the future frame. The
approach is simple and easily trained end-to-end on a variety of datasets.
Sample generations are both varied and sharp, even many frames into the future,
and compare favorably to those from existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1&quot;&gt;Emily Denton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00116">
<title>Separators and Adjustment Sets in Causal Graphs: Complete Criteria and an Algorithmic Framework. (arXiv:1803.00116v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00116</link>
<description rdf:parseType="Literal">&lt;p&gt;Principled reasoning about the identifiability of causal effects from
non-experimental data is an important application of graphical causal models.
We present an algorithmic framework for efficiently testing, constructing, and
enumerating $m$-separators in ancestral graphs (AGs), a class of graphical
causal models that can represent uncertainty about the presence of latent
confounders. Furthermore, we prove a reduction from causal effect
identification by covariate adjustment to $m$-separation in a subgraph for
directed acyclic graphs (DAGs) and maximal ancestral graphs (MAGs). Jointly,
these results yield constructive criteria that characterize all adjustment sets
as well as all minimal and minimum adjustment sets for identification of a
desired causal effect with multivariate exposures and outcomes in the presence
of latent confounding. Our results extend several existing solutions for
special cases of these problems. Our efficient algorithms allowed us to
empirically quantify the identifiability gap between covariate adjustment and
the do-calculus in random DAGs, covering a wide range of scenarios.
Implementations of our algorithms are provided in the R package dagitty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zander_B/0/1/0/all/0/1&quot;&gt;Benito van der Zander&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1&quot;&gt;Maciej Li&amp;#x15b;kiewicz&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Textor_J/0/1/0/all/0/1&quot;&gt;Johannes Textor&lt;/a&gt; (2) ((1) Institute for Theoretical Computer Science, Universit&amp;#xe4;t zu L&amp;#xfc;beck, Germany, (2) Institute for Computing and Information Sciences, Radboud University Nijmegen, Nijmegen, The Netherlands)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00567">
<title>Computational Optimal Transport. (arXiv:1803.00567v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00567</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal Transport (OT) is a mathematical gem at the interface between
probability, analysis and optimization. The goal of that theory is to define
geometric tools that are useful to compare probability distributions. Earlier
contributions originated from Monge&apos;s work in the 18th century, to be later
rediscovered under a different formalism by Tolstoi in the 1920&apos;s, Kantorovich,
Hitchcock and Koopmans in the 1940&apos;s. The problem was solved numerically by
Dantzig in 1949 and others in the 1950&apos;s within the framework of linear
programming, paving the way for major industrial applications in the second
half of the 20th century. OT was later rediscovered under a different light by
analysts in the 90&apos;s, following important work by Brenier and others, as well
as in the computer vision/graphics fields under the name of earth mover&apos;s
distances. Recent years have witnessed yet another revolution in the spread of
OT, thanks to the emergence of approximate solvers that can scale to sizes and
dimensions that are relevant to data sciences. Thanks to this newfound
scalability, OT is being increasingly used to unlock various problems in
imaging sciences (such as color or texture processing), computer vision and
graphics (for shape manipulation) or machine learning (for
regression,classification and density fitting). This short book reviews OT with
a bias toward numerical methods and their applications in data sciences, and
sheds lights on the theoretical properties of OT that make it particularly
useful for some of these applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peyre_G/0/1/0/all/0/1&quot;&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cuturi_M/0/1/0/all/0/1&quot;&gt;Marco Cuturi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00606">
<title>On Polynomial Time PAC Reinforcement Learning with Rich Observations. (arXiv:1803.00606v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00606</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the computational tractability of provably sample-efficient (PAC)
reinforcement learning in episodic environments with high-dimensional
observations. We present new sample efficient algorithms for environments with
deterministic hidden state dynamics but stochastic rich observations. These
methods represent computationally efficient alternatives to prior algorithms
that rely on enumerating exponentially many functions. We show that the only
known statistically efficient algorithm for the more general stochastic
transition setting requires NP-hard computation which cannot be implemented via
standard optimization primitives. We also present several examples that
illustrate fundamental challenges of tractable PAC reinforcement learning in
such general settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1&quot;&gt;Christoph Dann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1&quot;&gt;Akshay Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1&quot;&gt;John Langford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schapire_R/0/1/0/all/0/1&quot;&gt;Robert E. Schapire&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00641">
<title>Re-examination of Bregman functions and new properties of their divergences. (arXiv:1803.00641v1 [math.OC])</title>
<link>http://arxiv.org/abs/1803.00641</link>
<description rdf:parseType="Literal">&lt;p&gt;The Bregman divergence (Bregman distance, Bregman measure of distance) is a
certain useful substitute for a distance, obtained from a well-chosen function
(the &quot;Bregman function&quot;). Bregman functions and divergences have been
extensively investigated during the last decades and have found applications in
optimization, operations research, information theory, nonlinear analysis,
machine learning and more. This paper re-examines various aspects related to
the theory of Bregman functions and divergences. In particular, it presents
many sufficient conditions which allow the construction of Bregman functions in
a general setting and introduces new Bregman functions (such as a negative
iterated log entropy). Moreover, it sheds new light on several known Bregman
functions such as quadratic entropies, the negative Havrda-Charvat-Tsallis
entropy, and the negative Boltzmann-Gibbs-Shannon entropy, and it shows that
the negative Burg entropy, which is not a Bregman function according to the
classical theory but nevertheless is known to have &quot;Bregmanian properties&quot;,
can, by our re-examination of the theory, be considered as a Bregman function.
Our analysis yields several by-products of independent interest such as the
introduction of the concept of relative uniform convexity (a certain
generalization of uniform convexity), new properties of uniformly and strongly
convex functions, and results in Banach space theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Reem_D/0/1/0/all/0/1&quot;&gt;Daniel Reem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Reich_S/0/1/0/all/0/1&quot;&gt;Simeon Reich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pierro_A/0/1/0/all/0/1&quot;&gt;Alvaro De Pierro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00650">
<title>Kernel Embedding Approaches to Orbit Determination of Spacecraft Clusters. (arXiv:1803.00650v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00650</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel formulation and solution of orbit determination
over finite time horizons as a learning problem. We present an approach to
orbit determination under very broad conditions that are satisfied for n-body
problems. These weak conditions allow us to perform orbit determination with
noisy and highly non-linear observations such as those presented by range-rate
only (Doppler only) observations. We show that domain generalization and
distribution regression techniques can learn to estimate orbits of a group of
satellites and identify individual satellites especially with prior
understanding of correlations between orbits and provide asymptotic convergence
conditions. The approach presented requires only visibility and observability
of the underlying state from observations and is particularly useful for
autonomous spacecraft operations using low-cost ground stations or sensors. We
validate the orbit determination approach using observations of two spacecraft
(GRIFEX and MCubed-2) along with synthetic datasets of multiple spacecraft
deployments and lunar orbits. We also provide a comparison with the standard
techniques (EKF) under highly noisy conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Srinagesh Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cutler_J/0/1/0/all/0/1&quot;&gt;James W. Cutler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00651">
<title>Static and Dynamic Robust PCA via Low-Rank + Sparse Matrix Decomposition: A Review. (arXiv:1803.00651v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1803.00651</link>
<description rdf:parseType="Literal">&lt;p&gt;Principal Components Analysis (PCA) is one of the most widely used dimension
reduction techniques. Robust PCA (RPCA) refers to the problem of PCA when the
data may be corrupted by outliers. Recent work by Candes, Wright, Li, and Ma
defined RPCA as a problem of decomposing a given data matrix into the sum of a
low-rank matrix (true data) and a sparse matrix (outliers). The column space of
the low-rank matrix then gives the PCA solution. This simple definition has
lead to a large amount of interesting new work on provably correct, fast, and
practically useful solutions to the RPCA problem. More recently, the dynamic
(time-varying) version of the RPCA problem has been studied and a series of
provably correct, fast, and memory efficient tracking solutions have been
proposed. Dynamic RPCA (or robust subspace tracking) is the problem of tracking
data lying in a (slowly) changing subspace while being robust to sparse
outliers. This article provides an exhaustive review of the last decade of
literature on RPCA and its dynamic counterpart (robust subspace tracking),
along with describing their theoretical guarantees, discussing the pros and
cons of various approaches, and providing empirical comparisons of performance
and speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1&quot;&gt;Namrata Vaswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1&quot;&gt;Praneeth Narayanamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00744">
<title>Clinically Meaningful Comparisons Over Time: An Approach to Measuring Patient Similarity based on Subsequence Alignment. (arXiv:1803.00744v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00744</link>
<description rdf:parseType="Literal">&lt;p&gt;Longitudinal patient data has the potential to improve clinical risk
stratification models for disease. However, chronic diseases that progress
slowly over time are often heterogeneous in their clinical presentation.
Patients may progress through disease stages at varying rates. This leads to
pathophysiological misalignment over time, making it difficult to consistently
compare patients in a clinically meaningful way. Furthermore, patients present
clinically for the first time at different stages of disease. This eliminates
the possibility of simply aligning patients based on their initial
presentation. Finally, patient data may be sampled at different rates due to
differences in schedules or missed visits. To address these challenges, we
propose a robust measure of patient similarity based on subsequence alignment.
Compared to global alignment techniques that do not account for
pathophysiological misalignment, focusing on the most relevant subsequences
allows for an accurate measure of similarity between patients. We demonstrate
the utility of our approach in settings where longitudinal data, while useful,
are limited and lack a clear temporal alignment for comparison. Applied to the
task of stratifying patients for risk of progression to probable Alzheimer&apos;s
Disease, our approach outperforms models that use only snapshot data (AUROC of
0.839 vs. 0.812) and models that use global alignment techniques (AUROC of
0.822). Our results support the hypothesis that patients&apos; trajectories are
useful for quantifying inter-patient similarities and that using subsequence
matching and can help account for heterogeneity and misalignment in
longitudinal data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1&quot;&gt;Dev Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syed_Z/0/1/0/all/0/1&quot;&gt;Zeeshan Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1&quot;&gt;Jenna Wiens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00810">
<title>Detecting non-causal artifacts in multivariate linear regression models. (arXiv:1803.00810v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00810</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider linear models where $d$ potential causes $X_1,...,X_d$ are
correlated with one target quantity $Y$ and propose a method to infer whether
the association is causal or whether it is an artifact caused by overfitting or
hidden common causes. We employ the idea that in the former case the vector of
regression coefficients has &apos;generic&apos; orientation relative to the covariance
matrix $\Sigma_{XX}$ of $X$. Using an ICA based model for confounding, we show
that both confounding and overfitting yield regression vectors that concentrate
mainly in the space of low eigenvalues of $\Sigma_{XX}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Janzing_D/0/1/0/all/0/1&quot;&gt;Dominik Janzing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoelkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Schoelkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00860">
<title>Can we steal your vocal identity from the Internet?: Initial investigation of cloning Obama&apos;s voice using GAN, WaveNet and low-quality found data. (arXiv:1803.00860v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1803.00860</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the growing availability of spoofing databases and rapid advances
in using them, systems for detecting voice spoofing attacks are becoming more
and more capable, and error rates close to zero are being reached for the
ASVspoof2015 database. However, speech synthesis and voice conversion paradigms
that are not considered in the ASVspoof2015 database are appearing. Such
examples include direct waveform modelling and generative adversarial networks.
We also need to investigate the feasibility of training spoofing systems using
only low-quality found data. For that purpose, we developed a generative
adversarial network-based speech enhancement system that improves the quality
of speech data found in publicly available sources. Using the enhanced data, we
trained state-of-the-art text-to-speech and voice conversion models and
evaluated them in terms of perceptual speech quality and speaker similarity.
The results show that the enhancement models significantly improved the SNR of
low-quality degraded data found in publicly available sources and that they
significantly improved the perceptual cleanliness of the source speech without
significantly degrading the naturalness of the voice. However, the results also
show limitations when generating speech with the low-quality found data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fuming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Echizen_I/0/1/0/all/0/1&quot;&gt;Isao Echizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1&quot;&gt;Tomi Kinnunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.04460">
<title>On the Existence of a Sample Mean in Dynamic Time Warping Spaces. (arXiv:1610.04460v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1610.04460</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of sample mean in dynamic time warping (DTW) spaces has been
successfully applied to improve pattern recognition systems and generalize
centroid-based clustering algorithms. Its existence has neither been proved nor
challenged. This article presents sufficient conditions for existence of a
sample mean in DTW spaces. The proposed result justifies prior work on
approximate mean algorithms, sets the stage for constructing exact mean
algorithms, and is a first step towards a statistical theory of DTW spaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Brijnesh J. Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schultz_D/0/1/0/all/0/1&quot;&gt;David Schultz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08999">
<title>Distribution System Voltage Control under Uncertainties using Tractable Chance Constraints. (arXiv:1704.08999v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1704.08999</link>
<description rdf:parseType="Literal">&lt;p&gt;Voltage control plays an important role in the operation of electricity
distribution networks, especially with high penetration of distributed energy
resources. These resources introduce significant and fast varying
uncertainties. In this paper, we focus on reactive power compensation to
control voltage in the presence of uncertainties. We adopt a chance constraint
approach that accounts for arbitrary correlations between renewable resources
at each of the buses. We show how the problem can be solved efficiently using
historical samples via a stochastic quasi gradient method. We also show that
this optimization problem is convex for a wide variety of probabilistic
distributions. Compared to conventional per-bus chance constraints, our
formulation is more robust to uncertainty and more computationally tractable.
We illustrate the results using standard IEEE distribution test feeders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Baihong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baosen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.02436">
<title>Nonlinear Information Bottleneck. (arXiv:1705.02436v4 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1705.02436</link>
<description rdf:parseType="Literal">&lt;p&gt;Information bottleneck [IB] is a technique for extracting information in some
`input&apos; random variable that is relevant for predicting some different &apos;output&apos;
random variable. IB works by encoding the input in a compressed &apos;bottleneck
variable&apos; from which the output can then be accurately decoded. IB can be
difficult to compute in practice, and has been mainly developed for two limited
cases: (1) discrete random variables with small state spaces, and (2)
continuous random variables that are jointly Gaussian distributed (in which
case the encoding and decoding maps are linear). We propose a method to perform
IB in more general domains. Our approach can be applied to discrete or
continuous inputs and outputs, and allows for nonlinear encoding and decoding
maps. The method uses a novel upper bound on the IB objective, derived using a
non-parametric estimator of mutual information and a variational approximation.
We show how to implement the method using neural networks and gradient-based
optimization, and demonstrate its performance on the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolchinsky_A/0/1/0/all/0/1&quot;&gt;Artemy Kolchinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tracey_B/0/1/0/all/0/1&quot;&gt;Brendan D. Tracey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1&quot;&gt;David H. Wolpert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00514">
<title>Selective Inference for Change Point Detection in Multi-dimensional Sequences. (arXiv:1706.00514v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00514</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of detecting change points (CPs) that are characterized
by a subset of dimensions in a multi-dimensional sequence. A method for
detecting those CPs can be formulated as a two-stage method: one for selecting
relevant dimensions, and another for selecting CPs. It has been difficult to
properly control the false detection probability of these CP detection methods
because selection bias in each stage must be properly corrected. Our main
contribution in this paper is to formulate a CP detection problem as a
selective inference problem, and show that exact (non-asymptotic) inference is
possible for a class of CP detection methods. We demonstrate the performances
of the proposed selective inference framework through numerical simulations and
its application to our motivating medical data analysis problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Umezu_Y/0/1/0/all/0/1&quot;&gt;Yuta Umezu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01944">
<title>First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. (arXiv:1711.01944v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01944</link>
<description rdf:parseType="Literal">&lt;p&gt;Two classes of methods have been proposed for escaping from saddle points
with one using the second-order information carried by the Hessian and the
other adding the noise into the first-order information. The existing analysis
for algorithms using noise in the first-order information is quite involved and
hides the essence of added noise, which hinder further improvements of these
algorithms. In this paper, we present a novel perspective of noise-adding
technique, i.e., adding the noise into the first-order information can help
extract the negative curvature from the Hessian matrix, and provide a formal
reasoning of this perspective by analyzing a simple first-order procedure. More
importantly, the proposed procedure enables one to design purely first-order
stochastic algorithms for escaping from non-degenerate saddle points with a
much better time complexity (almost linear time in terms of the problem&apos;s
dimensionality). In particular, we develop a {\bf first-order stochastic
algorithm} based on our new technique and an existing algorithm that only
converges to a first-order stationary point to enjoy a time complexity of
{$\widetilde O(d/\epsilon^{3.5})$ for finding a nearly second-order stationary
point $\bf{x}$ such that $\|\nabla F(bf{x})\|\leq \epsilon$ and $\nabla^2
F(bf{x})\geq -\sqrt{\epsilon}I$ (in high probability), where $F(\cdot)$ denotes
the objective function and $d$ is the dimensionality of the problem. To the
best of our knowledge, this is the best theoretical result of first-order
algorithms for stochastic non-convex optimization, which is even competitive
with if not better than existing stochastic algorithms hinging on the
second-order information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Rong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07077">
<title>Estimation Considerations in Contextual Bandits. (arXiv:1711.07077v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07077</link>
<description rdf:parseType="Literal">&lt;p&gt;Although many contextual bandit algorithms have similar theoretical
guarantees, the characteristics of real-world applications oftentimes result in
large performance dissimilarities across algorithms. We study a consideration
for the exploration vs. exploitation framework that does not arise in
non-contextual bandits: the way exploration is conducted in the present may
affect the bias and variance in the potential outcome model estimation in
subsequent stages of learning. We show that contextual bandit algorithms are
sensitive to the estimation method of the outcome model as well as the
exploration method used, particularly in the presence of rich heterogeneity or
complex outcome models, which can lead to difficult estimation problems along
the path of learning. We propose new contextual bandit designs, combining
parametric and non-parametric statistical estimation methods with causal
inference methods in order to reduce the estimation bias that results from
adaptive treatment assignment. We provide empirical evidence that guides the
choice among the alternatives in different scenarios, such as prejudice
(non-representative user contexts) in the initial training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dimakopoulou_M/0/1/0/all/0/1&quot;&gt;Maria Dimakopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imbens_G/0/1/0/all/0/1&quot;&gt;Guido Imbens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01033">
<title>NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization. (arXiv:1712.01033v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01033</link>
<description rdf:parseType="Literal">&lt;p&gt;Accelerated gradient (AG) methods are breakthroughs in convex optimization,
improving the convergence rate of the gradient descent method for optimization
with smooth functions. However, the analysis of AG methods for non-convex
optimization is still limited. It remains an open question whether AG methods
from convex optimization can accelerate the convergence of the gradient descent
method for finding local minimum of non-convex optimization problems. This
paper provides an affirmative answer to this question. In particular, we
analyze two renowned variants of AG methods (namely Polyak&apos;s Heavy Ball method
and Nesterov&apos;s Accelerated Gradient method) for extracting the negative
curvature from random noise, which is central to escaping from saddle points.
By leveraging the proposed AG methods for extracting the negative curvature, we
present a new AG algorithm with double loops for non-convex
optimization~\footnote{this is in contrast to a single-loop AG algorithm
proposed in a recent manuscript~\citep{AGNON}, which directly analyzed the
Nesterov&apos;s AG method for non-convex optimization and appeared online on
November 29, 2017. However, we emphasize that our work is an independent work,
which is inspired by our earlier work~\citep{NEON17} and is based on a
different novel analysis.}, which converges to second-order stationary point
$\x$ such that $\|\nabla f(\x)\|\leq \epsilon$ and $\nabla^2 f(\x)\geq
-\sqrt{\epsilon} I$ with $\widetilde O(1/\epsilon^{1.75})$ iteration
complexity, improving that of gradient descent method by a factor of
$\epsilon^{-0.25}$ and matching the best iteration complexity of second-order
Hessian-free methods for non-convex optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Rong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08058">
<title>A brain signature highly predictive of future progression to Alzheimer&apos;s dementia. (arXiv:1712.08058v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08058</link>
<description rdf:parseType="Literal">&lt;p&gt;Early prognosis of Alzheimer&apos;s dementia is hard. Mild cognitive impairment
(MCI) typically precedes Alzheimer&apos;s dementia, yet only a fraction of MCI
individuals will progress to dementia, even when screened using biomarkers. We
propose here to identify a subset of individuals who share a common brain
signature highly predictive of oncoming dementia. This signature was composed
of brain atrophy and functional dysconnectivity and discovered using a machine
learning model in patients suffering from dementia. The model recognized the
same brain signature in MCI individuals, 90% of which progressed to dementia
within three years. This result is a marked improvement on the state-of-the-art
in prognostic precision, while the brain signature still identified 47% of all
MCI progressors. We thus discovered a sizable MCI subpopulation which
represents an excellent recruitment target for clinical trials at the prodromal
stage of Alzheimer&apos;s disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dansereau_C/0/1/0/all/0/1&quot;&gt;Christian Dansereau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tam_A/0/1/0/all/0/1&quot;&gt;Angela Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Badhwar_A/0/1/0/all/0/1&quot;&gt;AmanPreet Badhwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Urchs_S/0/1/0/all/0/1&quot;&gt;Sebastian Urchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Orban_P/0/1/0/all/0/1&quot;&gt;Pierre Orban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rosa_Neto_P/0/1/0/all/0/1&quot;&gt;Pedro Rosa-Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bellec_P/0/1/0/all/0/1&quot;&gt;Pierre Bellec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03848">
<title>Region Detection in Markov Random Fields: Gaussian Case. (arXiv:1802.03848v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03848</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we consider the problem of model selection in Gaussian Markov
fields in the sample deficient scenario. The benchmark information-theoretic
results in the case of d-regular graphs require the number of samples to be at
least proportional to the logarithm of the number of vertices to allow
consistent graph recovery. When the number of samples is less than this amount,
reliable detection of all edges is impossible. In many applications, it is more
important to learn the distribution of the edge (coupling) parameters over the
network than the specific locations of the edges. Assuming that the entire
graph can be partitioned into a number of spatial regions with similar edge
parameters and reasonably regular boundaries, we develop new
information-theoretic sample complexity bounds and show that even bounded
number of samples can be enough to consistently recover these regions. We also
introduce and analyze an efficient region growing algorithm capable of
recovering the regions with high accuracy. We show that it is consistent and
demonstrate its performance benefits in synthetic simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soloveychik_I/0/1/0/all/0/1&quot;&gt;Ilya Soloveychik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07528">
<title>Subspace-Induced Gaussian Processes. (arXiv:1802.07528v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new Gaussian process (GP) regression model where the covariance
kernel is indexed or parameterized by a sufficient dimension reduction subspace
of a reproducing kernel Hilbert space. The covariance kernel will be low-rank
while capturing the statistical dependency of the response to the covariates,
this affords significant improvement in computational efficiency as well as
potential reduction in the variance of predictions. We develop a fast
Expectation-Maximization algorithm for estimating the parameters of the
subspace-induced Gaussian process (SIGP). Extensive results on real data show
that SIGP can outperform the standard full GP even with a low rank-$m$, $m\leq
3$, inducing subspace.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zilong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Sayan Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09568">
<title>Shampoo: Preconditioned Stochastic Tensor Optimization. (arXiv:1802.09568v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09568</link>
<description rdf:parseType="Literal">&lt;p&gt;Preconditioned gradient methods are among the most general and powerful tools
in optimization. However, preconditioning requires storing and manipulating
prohibitively large matrices. We describe and analyze a new structure-aware
preconditioning algorithm, called Shampoo, for stochastic optimization over
tensor spaces. Shampoo maintains a set of preconditioning matrices, each of
which operates on a single dimension, contracting over the remaining
dimensions. We establish convergence guarantees in the stochastic convex
setting, the proof of which builds upon matrix trace inequalities. Our
experiments with state-of-the-art deep learning models show that Shampoo is
capable of converging considerably faster than commonly used optimizers.
Although it involves a more complex update rule, Shampoo&apos;s runtime per step is
comparable to that of simple gradient methods such as SGD, AdaGrad, and Adam.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vineet Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1&quot;&gt;Tomer Koren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singer_Y/0/1/0/all/0/1&quot;&gt;Yoram Singer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10582">
<title>Evaluating Overfit and Underfit in Models of Network Community Structure. (arXiv:1802.10582v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.10582</link>
<description rdf:parseType="Literal">&lt;p&gt;A common data mining task on networks is community detection, which seeks an
unsupervised decomposition of a network into structural groups based on
statistical regularities in the network&apos;s connectivity. Although many methods
exist, the No Free Lunch theorem for community detection implies that each
makes some kind of tradeoff, and no algorithm can be optimal on all inputs.
Thus, different algorithms will over or underfit on different inputs, finding
more, fewer, or just different communities than is optimal, and evaluation
methods that use a metadata partition as a ground truth will produce misleading
conclusions about general accuracy. Here, we present a broad evaluation of over
and underfitting in community detection, comparing the behavior of 16
state-of-the-art community detection algorithms on a novel and structurally
diverse corpus of 406 real-world networks. We find that (i) algorithms vary
widely both in the number of communities they find and in their corresponding
composition, given the same input, (ii) algorithms can be clustered into
distinct high-level groups based on similarities of their outputs on real-world
networks, and (iii) these differences induce wide variation in accuracy on link
prediction and link description tasks. We introduce a new diagnostic for
evaluating overfitting and underfitting in practice, and use it to roughly
divide community detection methods into general and specialized learning
algorithms. Across methods and inputs, Bayesian techniques based on the
stochastic block model and a minimum description length approach to
regularization represent the best general learning approach, but can be
outperformed under specific circumstances. These results introduce both a
theoretically principled approach to evaluate over and underfitting in models
of network community structure and a realistic benchmark by which new methods
may be evaluated and compared.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghasemian_A/0/1/0/all/0/1&quot;&gt;Amir Ghasemian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hosseinmardi_H/0/1/0/all/0/1&quot;&gt;Homa Hosseinmardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Clauset_A/0/1/0/all/0/1&quot;&gt;Aaron Clauset&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00183">
<title>Learning with Correntropy-induced Losses for Regression with Mixture of Symmetric Stable Noise. (arXiv:1803.00183v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00183</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, correntropy and its applications in machine learning have
been drawing continuous attention owing to its merits in dealing with
non-Gaussian noise and outliers. However, theoretical understanding of
correntropy, especially in the statistical learning context, is still limited.
In this study, within the statistical learning framework, we investigate
correntropy based regression in the presence of non-Gaussian noise or outliers.
To this purpose, we first introduce mixture of symmetric stable noise, which
include Gaussian noise, Cauchy noise, and the mixture of Gaussian noise as
special cases, to model non-Gaussian noise and outliers. We demonstrate that
under the mixture of symmetric stable noise assumption, correntropy based
regression can learn the conditional mean function or the conditional median
function well without requiring the finite variance assumption of the noise. In
particular, we establish learning rates for correntropy based regression
estimators that are asymptotically of type $\mathcal{O}(n^{-1})$. We believe
that the present study completes our understanding towards correntropy based
regression from a statistical learning viewpoint, and may also shed some light
on robust statistical learning for regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunlong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1&quot;&gt;Yiming Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00276">
<title>Model-Based Clustering and Classification of Functional Data. (arXiv:1803.00276v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00276</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of complex data analysis is a central topic of modern statistical
science and learning systems and is becoming of broader interest with the
increasing prevalence of high-dimensional data. The challenge is to develop
statistical models and autonomous algorithms that are able to acquire knowledge
from raw data for exploratory analysis, which can be achieved through
clustering techniques or to make predictions of future data via classification
(i.e., discriminant analysis) techniques. Latent data models, including mixture
model-based approaches are one of the most popular and successful approaches in
both the unsupervised context (i.e., clustering) and the supervised one (i.e,
classification or discrimination). Although traditionally tools of multivariate
analysis, they are growing in popularity when considered in the framework of
functional data analysis (FDA). FDA is the data analysis paradigm in which the
individual data units are functions (e.g., curves, surfaces), rather than
simple vectors. In many areas of application, the analyzed data are indeed
often available in the form of discretized values of functions or curves (e.g.,
time series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data).
This functional aspect of the data adds additional difficulties compared to the
case of a classical multivariate (non-functional) data analysis. We review and
present approaches for model-based clustering and classification of functional
data. We derive well-established statistical models along with efficient
algorithmic tools to address problems regarding the clustering and the
classification of these high-dimensional data, including their heterogeneity,
missing information, and dynamical hidden structure. The presented models and
algorithms are illustrated on real-world functional data analysis problems from
several application area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chamroukhi_F/0/1/0/all/0/1&quot;&gt;Faicel Chamroukhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien D. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/0808.3416">
<title>Uncertainty quantification in complex systems using approximate solvers. (arXiv:0808.3416v1 [stat.CO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/0808.3416</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel uncertainty quantification framework for
computationally demanding systems characterized by a large vector of
non-Gaussian uncertainties. It combines state-of-the-art techniques in advanced
Monte Carlo sampling with Bayesian formulations. The key departure from
existing works is the use of inexpensive, approximate computational models in a
rigorous manner. Such models can readily be derived by coarsening the
discretization size in the solution of the governing PDEs, increasing the time
step when integration of ODEs is performed, using fewer iterations if a
non-linear solver is employed or making use of lower order models. It is shown
that even in cases where the inexact models provide very poor approximations of
the exact response, statistics of the latter can be quantified accurately with
significant reductions in the computational effort. Multiple approximate models
can be used and rigorous confidence bounds of the estimates produced are
provided at all stages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koutsourelakis_P/0/1/0/all/0/1&quot;&gt;Phaedon-Stelios Koutsourelakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03824">
<title>Physics-constrained, data-driven discovery of coarse-grained dynamics. (arXiv:1802.03824v1 [physics.comp-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.03824</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of high-dimensionality and disparity of time scales
encountered in many problems in computational physics has motivated the
development of coarse-grained (CG) models. In this paper, we advocate the
paradigm of data-driven discovery for extract- ing governing equations by
employing fine-scale simulation data. In particular, we cast the
coarse-graining process under a probabilistic state-space model where the
transition law dic- tates the evolution of the CG state variables and the
emission law the coarse-to-fine map. The directed probabilistic graphical model
implied, suggests that given values for the fine- grained (FG) variables,
probabilistic inference tools must be employed to identify the cor- responding
values for the CG states and to that end, we employ Stochastic Variational In-
ference. We advocate a sparse Bayesian learning perspective which avoids
overfitting and reveals the most salient features in the CG evolution law. The
formulation adopted enables the quantification of a crucial, and often
neglected, component in the CG process, i.e. the pre- dictive uncertainty due
to information loss. Furthermore, it is capable of reconstructing the evolution
of the full, fine-scale system. We demonstrate the efficacy of the proposed
frame- work in high-dimensional systems of random walkers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Felsberger_L/0/1/0/all/0/1&quot;&gt;L. Felsberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Koutsourelakis_P/0/1/0/all/0/1&quot;&gt;P.S. Koutsourelakis&lt;/a&gt;</dc:creator>
</item></rdf:RDF>