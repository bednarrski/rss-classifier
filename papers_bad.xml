<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09331"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09709"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.06987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09344"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09356"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09381"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09657"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10011"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.01014"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10241"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09376"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09520"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09592"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09641"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09677"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09713"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09776"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09913"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1503.00214"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1507.03538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.07947"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.04849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.05780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.03536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08324"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.09331">
<title>Learning Based on CC1 and CC4 Neural Networks. (arXiv:1712.09331v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.09331</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose that a general learning system should have three kinds of agents
corresponding to sensory, short-term, and long-term memory that implicitly will
facilitate context-free and context-sensitive aspects of learning. These three
agents perform mututally complementary functions that capture aspects of the
human cognition system. We investigate the use of CC1 and CC4 networks for use
as models of short-term and sensory memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kak_S/0/1/0/all/0/1&quot;&gt;Subhash Kak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09709">
<title>Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro Gesture. (arXiv:1712.09709v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.09709</link>
<description rdf:parseType="Literal">&lt;p&gt;In the research of the impact of gestures using by a lecturer, one
challenging task is to infer the attention of a group of audiences. Two
important measurements that can help infer the level of attention are eye
movement data and Electroencephalography (EEG) data. Under the fundamental
assumption that a group of people would look at the same place if they all pay
attention at the same time, we apply a method, &quot;Time Warp Edit Distance&quot;, to
calculate the similarity of their eye movement trajectories. Moreover, we also
cluster eye movement pattern of audiences based on these pair-wised similarity
metrics. Besides, since we don&apos;t have a direct metric for the &quot;attention&quot;
ground truth, a visual assessment would be beneficial to evaluate the
gesture-attention relationship. Thus we also implement a visualization tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1511.06987">
<title>Evolutionary algorithms. (arXiv:1511.06987v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1511.06987</link>
<description rdf:parseType="Literal">&lt;p&gt;This manuscript contains an outline of lectures course &quot;Evolutionary
Algorithms&quot; read by the author in Omsk State University n.a. F.M.Dostoevsky.
The course covers Canonic Genetic Algorithm and various other genetic
algorithms as well as evolutionary strategies, genetic programming, tabu search
and the class of evolutionary algorithms in general. Some facts, such as the
Rotation Property of crossover, the Schemata Theorem, GA performance as a local
search and &quot;almost surely&quot; convergence of evolutionary algorithms are given
with complete proofs. The text is in Russian.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eremeev_A/0/1/0/all/0/1&quot;&gt;Anton V. Eremeev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09344">
<title>Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger. (arXiv:1712.09344v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.09344</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments have established the vulnerability of deep Reinforcement
Learning (RL) to policy manipulation attacks via adversarial perturbations. In
this paper, we investigate the robustness and resilience of deep RL to
training-time and test-time attacks. Through experimental results, we
demonstrate that under noncontiguous training-time attacks, Deep Q-Network
(DQN) agents can recover and adapt to the adversarial conditions by reactively
adjusting the policy. Our results also show that policies learned under
adversarial perturbations are more robust to test-time attacks. Furthermore, we
compare the performance of $\epsilon$-greedy and parameter-space noise
exploration methods in terms of robustness and resilience against adversarial
perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behzadan_V/0/1/0/all/0/1&quot;&gt;Vahid Behzadan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munir_A/0/1/0/all/0/1&quot;&gt;Arslan Munir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09356">
<title>An Online Ride-Sharing Path Planning Strategy for Public Vehicle Systems. (arXiv:1712.09356v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.09356</link>
<description rdf:parseType="Literal">&lt;p&gt;As efficient traffic-management platforms, public vehicle (PV) systems are
envisioned to be a promising approach to solving traffic congestions and
pollutions for future smart cities. PV systems provide online/dynamic
peer-to-peer ride-sharing services with the goal of serving sufficient number
of customers with minimum number of vehicles and lowest possible cost. A key
component of the PV system is the online ride-sharing scheduling strategy. In
this paper, we propose an efficient path planning strategy that focuses on a
limited potential search area for each vehicle by filtering out the requests
that violate passenger service quality level, so that the global search is
reduced to local search. We analyze the performance of the proposed solution
such as reduction ratio of computational complexity. Simulations based on the
Manhattan taxi data set show that, the computing time is reduced by 22%
compared with the exhaustive search method under the same service quality
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Ming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao-Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09374">
<title>SLAC: A Sparsely Labeled Dataset for Action Classification and Localization. (arXiv:1712.09374v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.09374</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a procedure for the creation of large-scale video
datasets for action classification and localization from unconstrained,
realistic web data. The scalability of the proposed procedure is demonstrated
by building a novel video benchmark, named SLAC (Sparsely Labeled ACtions),
consisting of over 520K untrimmed videos and 1.75M clip annotations spanning
200 action categories. Using our proposed framework, annotating a clip takes
merely 8.8 seconds on average. This represents a saving in labeling time of
over 95% compared to the traditional procedure of manual trimming and
localization of actions. Our approach dramatically reduces the amount of human
labeling by automatically identifying hard clips, i.e., clips that contain
coherent actions but lead to prediction disagreement between action
classifiers. A human annotator can disambiguate whether such a clip truly
contains the hypothesized action in a handful of seconds, thus generating
labels for highly informative samples at little cost. We show that our
large-scale dataset can be used to effectively pre-train action recognition
models, significantly improving final metrics on smaller-scale benchmarks after
fine-tuning. On Kinetics, UCF-101 and HMDB-51, models pre-trained on SLAC
outperform baselines trained from scratch, by 2.0%, 20.1% and 35.4% in top-1
accuracy, respectively when RGB input is used. Furthermore, we introduce a
simple procedure that leverages the sparse labels in SLAC to pre-train action
localization models. On THUMOS14 and ActivityNet-v1.3, our localization model
improves the mAP of baseline model by 8.6% and 2.5%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1&quot;&gt;Lorenzo Torresani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09381">
<title>Ray RLLib: A Composable and Scalable Reinforcement Learning Library. (arXiv:1712.09381v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.09381</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) algorithms involve the deep nesting of distinct
components, where each component typically exhibits opportunities for
distributed computation. Current RL libraries offer parallelism at the level of
the entire program, coupling all the components together and making existing
implementations difficult to extend, combine, and reuse. We argue for building
composable RL components by encapsulating parallelism and resource requirements
within individual components, which can be achieved by building on top of a
flexible task-based programming model. We demonstrate this principle by
building Ray RLLib on top of Ray and show that we can implement a wide range of
state-of-the-art algorithms by composing and reusing a handful of standard
components. This composability does not come at the cost of performance --- in
our experiments, RLLib matches or exceeds the performance of highly optimized
reference implementations. Ray RLLib is available as part of Ray at
https://github.com/ray-project/ray/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_E/0/1/0/all/0/1&quot;&gt;Eric Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liaw_R/0/1/0/all/0/1&quot;&gt;Richard Liaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishihara_R/0/1/0/all/0/1&quot;&gt;Robert Nishihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moritz_P/0/1/0/all/0/1&quot;&gt;Philipp Moritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1&quot;&gt;Roy Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1&quot;&gt;Ken Goldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09657">
<title>The information bottleneck and geometric clustering. (arXiv:1712.09657v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.09657</link>
<description rdf:parseType="Literal">&lt;p&gt;The information bottleneck (IB) approach to clustering takes a joint
distribution $P\!\left(X,Y\right)$ and maps the data $X$ to cluster labels $T$
which retain maximal information about $Y$ (Tishby et al., 1999). This
objective results in an algorithm that clusters data points based upon the
similarity of their conditional distributions $P\!\left(Y\mid X\right)$. This
is in contrast to classic &quot;geometric clustering&quot; algorithms such as $k$-means
and gaussian mixture models (GMMs) which take a set of observed data points
$\left\{ \mathbf{x}_{i}\right\}_{i=1:N}$ and cluster them based upon their
geometric (typically Euclidean) distance from one another. Here, we show how to
use the deterministic information bottleneck (DIB) (Strouse and Schwab, 2017),
a variant of IB, to perform geometric clustering, by choosing cluster labels
that preserve information about data point location on a smoothed dataset. We
also introduce a novel intuitive method to choose the number of clusters, via
kinks in the information curve. We apply this approach to a variety of simple
clustering problems, showing that DIB with our model selection procedure
recovers the generative cluster labels. We also show that, for one simple case,
DIB interpolates between the cluster boundaries of GMMs and $k$-means in the
large data limit. Thus, our IB approach to clustering also provides an
information-theoretic perspective on these classic algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strouse_D/0/1/0/all/0/1&quot;&gt;D J Strouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schwab_D/0/1/0/all/0/1&quot;&gt;David J Schwab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10011">
<title>The Merits of Sharing a Ride. (arXiv:1712.10011v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1712.10011</link>
<description rdf:parseType="Literal">&lt;p&gt;The culture of sharing instead of ownership is sharply increasing in
individuals behaviors. Particularly in transportation, concepts of sharing a
ride in either carpooling or ridesharing have been recently adopted. An
efficient optimization approach to match passengers in real-time is the core of
any ridesharing system. In this paper, we model ridesharing as an online
matching problem on general graphs such that passengers do not drive private
cars and use shared taxis. We propose an optimization algorithm to solve it.
The outlined algorithm calculates the optimal waiting time when a passenger
arrives. This leads to a matching with minimal overall overheads while
maximizing the number of partnerships. To evaluate the behavior of our
algorithm, we used NYC taxi real-life data set. Results represent a substantial
reduction in overall overheads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsani_P/0/1/0/all/0/1&quot;&gt;Pooyan Ehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jia Yuan Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.01014">
<title>An Ontological Architecture for Orbital Debris Data. (arXiv:1704.01014v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1704.01014</link>
<description rdf:parseType="Literal">&lt;p&gt;The orbital debris problem presents an opportunity for inter-agency and
international cooperation toward the mutually beneficial goals of debris
prevention, mitigation, remediation, and improved space situational awareness
(SSA). Achieving these goals requires sharing orbital debris and other SSA
data. Toward this, I present an ontological architecture for the orbital debris
domain, taking steps in the creation of an orbital debris ontology (ODO). The
purpose of this ontological system is to (I) represent general orbital debris
and SSA domain knowledge, (II) structure, and standardize where needed, orbital
data and terminology, and (III) foster semantic interoperability and
data-sharing. In doing so I hope to (IV) contribute to solving the orbital
debris problem, improving peaceful global SSA, and ensuring safe space travel
for future generations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rovetto_R/0/1/0/all/0/1&quot;&gt;Robert J. Rovetto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01829">
<title>Declarative Statistics. (arXiv:1708.01829v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01829</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we introduce declarative statistics, a suite of declarative
modelling tools for statistical analysis. Statistical constraints represent the
key building block of declarative statistics. First, we introduce a range of
relevant counting and matrix constraints and associated decompositions, some of
which novel, that are instrumental in the design of statistical constraints.
Second, we introduce a selection of novel statistical constraints and
associated decompositions, which constitute a self-contained toolbox that can
be used to tackle a wide range of problems typically encountered by
statisticians. Finally, we deploy these statistical constraints to a wide range
of application areas drawn from classical statistics and we contrast our
framework against established practices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Roberto Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akgun_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;zg&amp;#xfc;r Akg&amp;#xfc;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prestwich_S/0/1/0/all/0/1&quot;&gt;Steven Prestwich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarim_S/0/1/0/all/0/1&quot;&gt;S. Armagan Tarim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05706">
<title>Memory Augmented Control Networks. (arXiv:1709.05706v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05706</link>
<description rdf:parseType="Literal">&lt;p&gt;Planning problems in partially observable environments cannot be solved
directly with convolutional networks and require some form of memory. But, even
memory networks with sophisticated addressing schemes are unable to learn
intelligent reasoning satisfactorily due to the complexity of simultaneously
learning to access memory and plan. To mitigate these challenges we introduce
the Memory Augmented Control Network (MACN). The proposed network architecture
consists of three main parts. The first part uses convolutions to extract
features and the second part uses a neural network-based planning module to
pre-plan in the environment. The third part uses a network controller that
learns to store those specific instances of past information that are necessary
for planning. The performance of the network is evaluated in discrete grid
world environments for path planning in the presence of simple and complex
obstacles. We show that our network learns to plan and can generalize to new
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Arbaaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Clark Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1&quot;&gt;Nikolay Atanasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karydis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Karydis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Daniel D. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06196">
<title>POMCPOW: An online algorithm for POMDPs with continuous state, action, and observation spaces. (arXiv:1709.06196v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06196</link>
<description rdf:parseType="Literal">&lt;p&gt;Online solvers for partially observable Markov decision processes have been
applied to problems with large discrete state spaces, but continuous state,
action, and observation spaces remain a challenge. This paper begins by
investigating double progressive widening (DPW) as a solution to this
challenge. However, we prove that this modification alone is not sufficient
because the belief representations in the search tree collapse to a single
particle causing the algorithm to converge to a policy that is suboptimal
regardless of the computation time. The main contribution of the paper is to
propose a new algorithm, POMCPOW, that incorporates DPW and weighted particle
filtering to overcome this deficiency and attack continuous problems.
Simulation results show that these modifications allow the algorithm to be
successful where previous approaches fail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunberg_Z/0/1/0/all/0/1&quot;&gt;Zachary Sunberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10241">
<title>Diversity Constraints in Public Housing Allocation. (arXiv:1711.10241v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10241</link>
<description rdf:parseType="Literal">&lt;p&gt;The state of Singapore operates a national public housing program, accounting
for over 80% of its residential real estate. Singapore uses its housing
allocation program to ensure ethnic diversity in its neighborhoods; it does so
by imposing ethnic quotas: every ethnic group must not own more than a certain
percentage in a housing project, thus ensuring that every neighborhood contains
members from each ethnic group. However, imposing diversity constraints
naturally results in some welfare loss. Our work studies the tradeoff between
diversity and social welfare from the perspective of computational economics.
We model the problem as an extension of the classic assignment problem, with
additional diversity constraints. While the classic assignment program is
poly-time computable, we show that adding diversity constraints makes the
problem computationally intractable; however, we identify a
$\tfrac{1}{2}$-approximation algorithm, as well as reasonable agent utility
models which admit poly-time algorithms. In addition, we study the price of
diversity: this is the loss in welfare incurred by imposing diversity
constraints; we provide upper bounds on the price of diversity as a function of
natural problem parameters; next, we analyze public data from Singapore&apos;s
Housing and Development Board, and create a simulated framework testing the
welfare loss due to diversity constraints in realistic large-scale scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benabbou_N/0/1/0/all/0/1&quot;&gt;Nawal Benabbou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1&quot;&gt;Mithun Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_V/0/1/0/all/0/1&quot;&gt;Vinh Ho Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sliwinski_J/0/1/0/all/0/1&quot;&gt;Jakub Sliwinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1&quot;&gt;Yair Zick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09376">
<title>Entropy-SGD optimizes the prior of a PAC-Bayes bound: Data-dependent PAC-Bayes priors via differential privacy. (arXiv:1712.09376v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.09376</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that Entropy-SGD (Chaudhari et al., 2016), when viewed as a learning
algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior)
classifier, i.e., a randomized classifier obtained by a risk-sensitive
perturbation of the weights of a learned classifier. Entropy-SGD works by
optimizing the bound&apos;s prior, violating the hypothesis of the PAC-Bayes theorem
that the prior is chosen independently of the data. Indeed, available
implementations of Entropy-SGD rapidly obtain zero training error on random
labels and the same holds of the Gibbs posterior. In order to obtain a valid
generalization bound, we show that an $\epsilon$-differentially private prior
yields a valid PAC-Bayes bound, a straightforward consequence of results
connecting generalization with differential privacy. Using stochastic gradient
Langevin dynamics (SGLD) to approximate the well-known exponential release
mechanism, we observe that generalization error on MNIST (measured on held out
data) falls within the (empirically nonvacuous) bounds computed under the
assumption that SGLD produces perfect samples. In particular, Entropy-SGLD can
be configured to yield relatively tight generalization bounds and still fit
real labels, although these same settings do not obtain state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dziugaite_G/0/1/0/all/0/1&quot;&gt;Gintare Karolina Dziugaite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1&quot;&gt;Daniel M. Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09379">
<title>IHT dies hard: Provable accelerated Iterative Hard Thresholding. (arXiv:1712.09379v1 [math.OC])</title>
<link>http://arxiv.org/abs/1712.09379</link>
<description rdf:parseType="Literal">&lt;p&gt;We study --both in theory and practice-- the use of momentum motions in
classic iterative hard thresholding (IHT) methods. By simply modifying plain
IHT, we investigate its convergence behavior on convex optimization criteria
with non-convex constraints, under standard assumptions. In diverse scenaria,
we observe that acceleration in IHT leads to significant improvements, compared
to state of the art projected gradient descent and Frank-Wolfe variants. As a
byproduct of our inspection, we study the impact of selecting the momentum
parameter: similar to convex settings, two modes of behavior are observed
--&quot;rippling&quot; and linear-- depending on the level of momentum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Khanna_R/0/1/0/all/0/1&quot;&gt;Rajiv Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09473">
<title>Sketching for Kronecker Product Regression and P-splines. (arXiv:1712.09473v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1712.09473</link>
<description rdf:parseType="Literal">&lt;p&gt;TensorSketch is an oblivious linear sketch introduced in Pagh&apos;13 and later
used in Pham, Pagh&apos;13 in the context of SVMs for polynomial kernels. It was
shown in Avron, Nguyen, Woodruff&apos;14 that TensorSketch provides a subspace
embedding, and therefore can be used for canonical correlation analysis, low
rank approximation, and principal component regression for the polynomial
kernel. We take TensorSketch outside of the context of polynomials kernels, and
show its utility in applications in which the underlying design matrix is a
Kronecker product of smaller matrices. This allows us to solve Kronecker
product regression and non-negative Kronecker product regression, as well as
regularized spline regression. Our main technical result is then in extending
TensorSketch to other norms. That is, TensorSketch only provides input sparsity
time for Kronecker product regression with respect to the $2$-norm. We show how
to solve Kronecker product regression with respect to the $1$-norm in time
sublinear in the time required for computing the Kronecker product, as well as
for more general $p$-norms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_H/0/1/0/all/0/1&quot;&gt;Huaian Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09520">
<title>Tensor Regression Networks with various Low-Rank Tensor Approximations. (arXiv:1712.09520v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.09520</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor regression networks achieve high rate of compression of model
parameters in multilayer perceptrons (MLP) while having slight impact on
performances. Tensor regression layer imposes low-rank constraints on the
tensor regression layer which replaces the flattening operation of traditional
MLP. We investigate tensor regression networks using various low-rank tensor
approximations, aiming to leverage the multi-modal structure of high
dimensional data by enforcing efficient low-rank constraints. We provide a
theoretical analysis giving insights on the choice of the rank parameters. We
evaluated performance of proposed model with state-of-the-art deep
convolutional models. For CIFAR-10 dataset, we achieved the compression rate of
0.018 with the sacrifice of accuracy less than 1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xingwei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1&quot;&gt;Guillaume Rabusseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1&quot;&gt;Joelle Pineau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09592">
<title>An Artificial Neural Network-based Stock Trading System Using Technical Analysis and Big Data Framework. (arXiv:1712.09592v1 [cs.CE])</title>
<link>http://arxiv.org/abs/1712.09592</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a neural network-based stock price prediction and trading
system using technical analysis indicators is presented. The model developed
first converts the financial time series data into a series of buy-sell-hold
trigger signals using the most commonly preferred technical analysis
indicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN)
model is trained in the learning stage on the daily stock prices between 1997
and 2007 for all of the Dow30 stocks. Apache Spark big data framework is used
in the training stage. The trained model is then tested with data from 2007 to
2017. The results indicate that by choosing the most appropriate technical
indicators, the neural network model can achieve comparable results against the
Buy and Hold strategy in most of the cases. Furthermore, fine tuning the
technical indicators and/or optimization strategy can enhance the overall
trading performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sezer_O/0/1/0/all/0/1&quot;&gt;O.B. Sezer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozbayoglu_M/0/1/0/all/0/1&quot;&gt;M. Ozbayoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dogdu_E/0/1/0/all/0/1&quot;&gt;E. Dogdu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09641">
<title>A Composite Quantile Fourier Neural Network for Multi-Horizon Probabilistic Forecasting. (arXiv:1712.09641v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.09641</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel quantile Fourier neural network is presented for nonparametric
probabilistic forecasting. Prediction are provided in the form of composite
quantiles using time as the only input to the model. This effectively is a form
of extrapolation based quantile regression applied for forecasting. Empirical
results showcase that for time series data that have clear seasonality and
trend, the model provides high quality probabilistic predictions. This work
introduces a new class of forecasting of using only time as the input versus
using past data such as an autoregressive model. Extrapolation based regression
has not been studied before for probabilistic forecasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hatalis_K/0/1/0/all/0/1&quot;&gt;Kostas Hatalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kishore_S/0/1/0/all/0/1&quot;&gt;Shalinee Kishore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09677">
<title>Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods. (arXiv:1712.09677v1 [math.OC])</title>
<link>http://arxiv.org/abs/1712.09677</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we study several classes of stochastic optimization algorithms
enriched with heavy ball momentum. Among the methods studied are: stochastic
gradient descent, stochastic Newton, stochastic proximal point and stochastic
dual subspace ascent. This is the first time momentum variants of several of
these methods are studied. We choose to perform our analysis in a setting in
which all of the above methods are equivalent. We prove global nonassymptotic
linear convergence rates for all methods and various measures of success,
including primal function values, primal iterates (in L2 sense), and dual
function values. We also show that the primal iterates converge at an
accelerated linear rate in the L1 sense. This is the first time a linear rate
is shown for the stochastic heavy ball method (i.e., stochastic gradient
descent method with momentum). Under somewhat weaker conditions, we establish a
sublinear convergence rate for Cesaro averages of primal iterates. Moreover, we
propose a novel concept, which we call stochastic momentum, aimed at decreasing
the cost of performing the momentum step. We prove linear convergence of
several stochastic methods with stochastic momentum, and show that in some
sparse data regimes and for sufficiently small momentum parameters, these
methods enjoy better overall complexity than methods with deterministic
momentum. Finally, we perform extensive numerical testing on artificial and
real datasets, including data coming from average consensus problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Loizou_N/0/1/0/all/0/1&quot;&gt;Nicolas Loizou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09694">
<title>A note on estimation in a simple probit model under dependency. (arXiv:1712.09694v1 [math.ST])</title>
<link>http://arxiv.org/abs/1712.09694</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a probit model without covariates, but the latent Gaussian
variables having compound symmetry covariance structure with a single parameter
characterizing the common correlation. We study the parameter estimation
problem under such one-parameter probit models. As a surprise, we demonstrate
that the likelihood function does not yield consistent estimates for the
correlation. We then formally prove the parameter&apos;s nonestimability by deriving
a non-vanishing minimax lower bound. This counter-intuitive phenomenon provides
an interesting insight that one bit information of the latent Gaussian
variables is not sufficient to consistently recover their correlation. On the
other hand, we further show that trinary data generated from the Gaussian
variables can consistently estimate the correlation with parametric convergence
rate. Hence we reveal a phase transition phenomenon regarding the
discretization of latent Gaussian variables while preserving the estimability
of the correlation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Weng_H/0/1/0/all/0/1&quot;&gt;Haolei Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09707">
<title>Deep learning for universal linear embeddings of nonlinear dynamics. (arXiv:1712.09707v1 [math.DS])</title>
<link>http://arxiv.org/abs/1712.09707</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying coordinate transformations that make strongly nonlinear dynamics
approximately linear is a central challenge in modern dynamical systems. These
transformations have the potential to enable prediction, estimation, and
control of nonlinear systems using standard linear theory. The Koopman operator
has emerged as a leading data-driven embedding, as eigenfunctions of this
operator provide intrinsic coordinates that globally linearize the dynamics.
However, identifying and representing these eigenfunctions has proven to be
mathematically and computationally challenging. This work leverages the power
of deep learning to discover representations of Koopman eigenfunctions from
trajectory data of dynamical systems. Our network is parsimonious and
interpretable by construction, embedding the dynamics on a low-dimensional
manifold that is of the intrinsic rank of the dynamics and parameterized by the
Koopman eigenfunctions. In particular, we identify nonlinear coordinates on
which the dynamics are globally linear using a modified auto-encoder. We also
generalize Koopman representations to include a ubiquitous class of systems
that exhibit continuous spectra, ranging from the simple pendulum to nonlinear
optics and broadband turbulence. Our framework parametrizes the continuous
frequency using an auxiliary network, enabling a compact and efficient
embedding at the intrinsic rank, while connecting our models to half a century
of asymptotics. In this way, we benefit from the power and generality of deep
learning, while retaining the physical interpretability of Koopman embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lusch_B/0/1/0/all/0/1&quot;&gt;Bethany Lusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J. Nathan Kutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brunton_S/0/1/0/all/0/1&quot;&gt;Steven L. Brunton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09713">
<title>Extrapolating Expected Accuracies for Large Multi-Class Problems. (arXiv:1712.09713v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.09713</link>
<description rdf:parseType="Literal">&lt;p&gt;The difficulty of multi-class classification generally increases with the
number of classes. Using data from a subset of the classes, can we predict how
well a classifier will scale with an increased number of classes? Under the
assumptions that the classes are sampled identically and independently from a
population, and that the classifier is based on independently learned scoring
functions, we show that the expected accuracy when the classifier is trained on
k classes is the (k-1)st moment of a certain distribution that can be estimated
from data. We present an unbiased estimation method based on the theory, and
demonstrate its application on a facial recognition example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Charles Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Achanta_R/0/1/0/all/0/1&quot;&gt;Rakesh Achanta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benjamini_Y/0/1/0/all/0/1&quot;&gt;Yuval Benjamini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09771">
<title>Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures. (arXiv:1712.09771v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.09771</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: A clinical decision support tool that automatically interprets
EEGs can reduce time to diagnosis and enhance real-time applications such as
ICU monitoring. Clinicians have indicated that a sensitivity of 95% with a
specificity below 5% was the minimum requirement for clinical acceptance. We
propose a highperformance classification system based on principles of big data
and machine learning. Methods: A hybrid machine learning system that uses
hidden Markov models (HMM) for sequential decoding and deep learning networks
for postprocessing is proposed. These algorithms were trained and evaluated
using the TUH EEG Corpus, which is the world&apos;s largest publicly available
database of clinical EEG data. Results: Our approach delivers a sensitivity
above 90% while maintaining a specificity below 5%. This system detects three
events of clinical interest: (1) spike and/or sharp waves, (2) periodic
lateralized epileptiform discharges, (3) generalized periodic epileptiform
discharges. It also detects three events used to model background noise: (1)
artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep
learning system can deliver a low false alarm rate on EEG event detection,
making automated analysis a viable option for clinicians. Significance: The TUH
EEG Corpus enables application of highly data consumptive machine learning
algorithms to EEG analysis. Performance is approaching clinical acceptance for
real-time applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torbati_A/0/1/0/all/0/1&quot;&gt;Amir Hossein Harati Nejad Torbati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diego_S/0/1/0/all/0/1&quot;&gt;Silvia Lopez de Diego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09776">
<title>Deep Architectures for Automated Seizure Detection in Scalp EEGs. (arXiv:1712.09776v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.09776</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated seizure detection using clinical electroencephalograms is a
challenging machine learning problem because the multichannel signal often has
an extremely low signal to noise ratio. Events of interest such as seizures are
easily confused with signal artifacts (e.g, eye movements) or benign variants
(e.g., slowing). Commercially available systems suffer from unacceptably high
false alarm rates. Deep learning algorithms that employ high dimensional models
have not previously been effective due to the lack of big data resources. In
this paper, we use the TUH EEG Seizure Corpus to evaluate a variety of hybrid
deep structures including Convolutional Neural Networks and Long Short-Term
Memory Networks. We introduce a novel recurrent convolutional architecture that
delivers 30% sensitivity at 7 false alarms per 24 hours. We have also evaluated
our system on a held-out evaluation set based on the Duke University Seizure
Corpus and demonstrate that performance trends are similar to the TUH EEG
Seizure Corpus. This is a significant finding because the Duke corpus was
collected with different instrumentation and at different hospitals. Our work
shows that deep learning architectures that integrate spatial and temporal
contexts are critical to achieving state of the art performance and will enable
a new generation of clinically-acceptable technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziyabari_S/0/1/0/all/0/1&quot;&gt;Saeedeh Ziyabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Vinit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diego_S/0/1/0/all/0/1&quot;&gt;Silvia Lopez de Diego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09913">
<title>Visualizing the Loss Landscape of Neural Nets. (arXiv:1712.09913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.09913</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network training relies on our ability to find &quot;good&quot; minimizers of
highly non-convex loss functions. It is well known that certain network
architecture designs (e.g., skip connections) produce loss functions that train
easier, and well-chosen training parameters (batch size, learning rate,
optimizer) produce minimizers that generalize better. However, the reasons for
these differences, and their effect on the underlying loss landscape, is not
well understood. In this paper, we explore the structure of neural loss
functions, and the effect of loss landscapes on generalization, using a range
of visualization methods. First, we introduce a simple &quot;filter normalization&quot;
method that helps us visualize loss function curvature, and make meaningful
side-by-side comparisons between loss functions. Then, using a variety of
visualizations, we explore how network architecture affects the loss landscape,
and how training parameters affect the shape of minimizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Gavin Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09983">
<title>Online Ensemble Multi-kernel Learning Adaptive to Non-stationary and Adversarial Environments. (arXiv:1712.09983v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.09983</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel-based methods exhibit well-documented performance in various nonlinear
learning tasks. Most of them rely on a preselected kernel, whose prudent choice
presumes task-specific prior information. To cope with this limitation,
multi-kernel learning has gained popularity thanks to its flexibility in
choosing kernels from a prescribed kernel dictionary. Leveraging the random
feature approximation and its recent orthogonality-promoting variant, the
present contribution develops an online multi-kernel learning scheme to infer
the intended nonlinear function `on the fly.&apos; To further boost performance in
non-stationary environments, an adaptive multi-kernel learning scheme (termed
AdaRaker) is developed with affordable computation and memory complexity.
Performance is analyzed in terms of both static and dynamic regret. To our best
knowledge, AdaRaker is the first algorithm that can optimally track nonlinear
functions in non-stationary settings with theoretical guarantees. Numerical
tests on real datasets are carried out to showcase the effectiveness of the
proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanning Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09988">
<title>Orthogonal Machine Learning for Demand Estimation: High Dimensional Causal Inference in Dynamic Panels. (arXiv:1712.09988v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.09988</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been growing interest in how economists can import machine learning
tools designed for prediction to facilitate, optimize and automate the model
selection process, while still retaining desirable inference properties for
causal parameters. Focusing on partially linear models, we extend the Double ML
framework to allow for (1) a number of treatments that may grow with the sample
size and (2) the analysis of panel data under sequentially exogenous errors.
Our low-dimensional treatment (LD) regime directly extends the work in
Chernozhukov et al. (2016), by showing that the coefficients from a second
stage, ordinary least squares estimator attain root-n convergence and desired
coverage even if the dimensionality of treatment is allowed to grow at a rate
of O(N/ log N ). Additionally we consider a high-dimensional sparse (HDS)
regime in which we show that second stage orthogonal LASSO and debiased
orthogonal LASSO have asymptotic properties equivalent to oracle estimators
with known first stage estimators. We argue that these advances make Double ML
methods a desirable alternative for practitioners estimating short-term demand
elasticities in non-contractual settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1&quot;&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goldman_M/0/1/0/all/0/1&quot;&gt;Matt Goldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Semenova_V/0/1/0/all/0/1&quot;&gt;Vira Semenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taddy_M/0/1/0/all/0/1&quot;&gt;Matt Taddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1503.00214">
<title>Matrix Completion with Noisy Entries and Outliers. (arXiv:1503.00214v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1503.00214</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the problem of matrix completion when the observed
entries are noisy and contain outliers. It begins with introducing a new
optimization criterion for which the recovered matrix is defined as its
solution. This criterion uses the celebrated Huber function from the robust
statistics literature to downweigh the effects of outliers. A practical
algorithm is developed to solve the optimization involved. This algorithm is
fast, straightforward to implement, and monotonic convergent. Furthermore, the
proposed methodology is theoretically shown to be stable in a well defined
sense. Its promising empirical performance is demonstrated via a sequence of
simulation experiments, including image inpainting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wong_R/0/1/0/all/0/1&quot;&gt;Raymond K. W. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Thomas C. M. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1507.03538">
<title>Classifying X-ray Binaries: A Probabilistic Approach. (arXiv:1507.03538v2 [astro-ph.HE] UPDATED)</title>
<link>http://arxiv.org/abs/1507.03538</link>
<description rdf:parseType="Literal">&lt;p&gt;In X-ray binary star systems consisting of a compact object that accretes
material from an orbiting secondary star, there is no straightforward means to
decide if the compact object is a black hole or a neutron star. To assist this
classification, we develop a Bayesian statistical model that makes use of the
fact that X-ray binary systems appear to cluster based on their compact object
type when viewed from a 3-dimensional coordinate system derived from X-ray
spectral data. The first coordinate of this data is the ratio of counts in mid
to low energy band (color 1), the second coordinate is the ratio of counts in
high to low energy band (color 2), and the third coordinate is the sum of
counts in all three bands. We use this model to estimate the probabilities that
an X-ray binary system contains a black hole, non-pulsing neutron star, or
pulsing neutron star. In particular, we utilize a latent variable model in
which the latent variables follow a Gaussian process prior distribution, and
hence we are able to induce the spatial correlation we believe exists between
systems of the same type. The utility of this approach is evidenced by the
accurate prediction of system types using Rossi X-ray Timing Explorer All Sky
Monitor data, but it is not flawless. In particular, non-pulsing neutron
systems containing &quot;bursters&quot; that are close to the boundary demarcating
systems containing black holes tend to be classified as black hole systems. As
a byproduct of our analyses, we provide the astronomer with public R code that
can be used to predict the compact object type of X-ray binaries given training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gopalan_G/0/1/0/all/0/1&quot;&gt;Giri Gopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Vrtilek_S/0/1/0/all/0/1&quot;&gt;Saeqa Dil Vrtilek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bornn_L/0/1/0/all/0/1&quot;&gt;Luke Bornn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.07947">
<title>Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear Subspace Tracking. (arXiv:1601.07947v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1601.07947</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel-based methods enjoy powerful generalization capabilities in handling a
variety of learning tasks. When such methods are provided with sufficient
training data, broadly-applicable classes of nonlinear functions can be
approximated with desired accuracy. Nevertheless, inherent to the nonparametric
nature of kernel-based estimators are computational and memory requirements
that become prohibitive with large-scale datasets. In response to this
formidable challenge, the present work puts forward a low-rank, kernel-based,
feature extraction approach that is particularly tailored for online operation,
where data streams need not be stored in memory. A novel generative model is
introduced to approximate high-dimensional (possibly infinite) features via a
low-rank nonlinear subspace, the learning of which leads to a direct kernel
function approximation. Offline and online solvers are developed for the
subspace learning task, along with affordable versions, in which the number of
stored data vectors is confined to a predefined budget. Analytical results
provide performance bounds on how well the kernel matrix as well as
kernel-based classification and regression tasks can be approximated by
leveraging budgeted online subspace learning and feature extraction schemes.
Tests on synthetic and real datasets demonstrate and benchmark the efficiency
of the proposed method when linear classification and regression is applied to
the extracted features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheikholeslami_F/0/1/0/all/0/1&quot;&gt;Fatemeh Sheikholeslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berberidis_D/0/1/0/all/0/1&quot;&gt;Dimitris Berberidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B.Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.04849">
<title>Predicting Shot Making in Basketball Learnt from Adversarial Multiagent Trajectories. (arXiv:1609.04849v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.04849</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we predict the likelihood of a player making a shot in
basketball from multiagent trajectories. Previous approaches to similar
problems center on hand-crafting features to capture domain specific knowledge.
Although intuitive, recent work in deep learning has shown this approach is
prone to missing important predictive features. To circumvent this issue, we
present a convolutional neural network (CNN) approach where we initially
represent the multiagent behavior as an image. To encode the adversarial nature
of basketball, we use a multi-channel image which we then feed into a CNN.
Additionally, to capture the temporal aspect of the trajectories we &quot;fade&quot; the
player trajectories. We find that this approach is superior to a traditional
FFN model. By using gradient ascent to create images using an already trained
CNN, we discover what features the CNN filters learn. Last, we find that a
combined CNN+FFN is the best performing network with an error rate of 39%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harmon_M/0/1/0/all/0/1&quot;&gt;Mark Harmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucey_P/0/1/0/all/0/1&quot;&gt;Patrick Lucey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.05780">
<title>Gap Safe screening rules for sparsity enforcing penalties. (arXiv:1611.05780v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.05780</link>
<description rdf:parseType="Literal">&lt;p&gt;In high dimensional regression settings, sparsity enforcing penalties have
proved useful to regularize the data-fitting term. A recently introduced
technique called screening rules propose to ignore some variables in the
optimization leveraging the expected sparsity of the solutions and consequently
leading to faster solvers. When the procedure is guaranteed not to discard
variables wrongly the rules are said to be safe. In this work, we propose a
unifying framework for generalized linear models regularized with standard
sparsity enforcing penalties such as $\ell_1$ or $\ell_1/\ell_2$ norms. Our
technique allows to discard safely more variables than previously considered
safe rules, particularly for low regularization parameters. Our proposed Gap
Safe rules (so called because they rely on duality gap computation) can cope
with any iterative solver but are particularly well suited to (block)
coordinate descent methods. Applied to many standard learning tasks, Lasso,
Sparse-Group Lasso, multi-task Lasso, binary and multinomial logistic
regression, etc., we report significant speed-ups compared to previously
proposed safe rules on all tested data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ndiaye_E/0/1/0/all/0/1&quot;&gt;Eugene Ndiaye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fercoq_O/0/1/0/all/0/1&quot;&gt;Olivier Fercoq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gramfort_A/0/1/0/all/0/1&quot;&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salmon_J/0/1/0/all/0/1&quot;&gt;Joseph Salmon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.03536">
<title>SILVar: Single Index Latent Variable Models. (arXiv:1705.03536v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.03536</link>
<description rdf:parseType="Literal">&lt;p&gt;A semi-parametric, non-linear regression model in the presence of latent
variables is introduced. These latent variables can correspond to unmodeled
phenomena or unmeasured agents in a complex networked system. This new
formulation allows joint estimation of certain non-linearities in the system,
the direct interactions between measured variables, and the effects of
unmodeled elements on the observed system. The particular form of the model
adopted is justified, and learning is posed as a regularized maximum likelihood
estimation. This leads to classes of structured convex optimization problems
with a &quot;sparse plus low-rank&quot; flavor. Relations between the proposed model and
several common model paradigms, such as those of Robust Principal Component
Analysis (PCA) and Vector Autoregression (VAR), are established. Particularly
in the VAR setting, the low-rank contributions can come from broad trends
exhibited in the time series. Details of the algorithm for learning the model
are presented. Experiments demonstrate the performance of the model and the
estimation algorithm on simulated and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jonathan Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moura_J/0/1/0/all/0/1&quot;&gt;Jose&amp;#x27; M.F. Moura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09416">
<title>Dual Based DSP Bidding Strategy and its Application. (arXiv:1705.09416v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09416</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, RTB(Real Time Bidding) becomes a popular online
advertisement trading method. During the auction, each DSP(Demand Side
Platform) is supposed to evaluate current opportunity and respond with an ad
and corresponding bid price. It&apos;s essential for DSP to find an optimal ad
selection and bid price determination strategy which maximizes revenue or
performance under budget and ROI(Return On Investment) constraints in P4P(Pay
For Performance) or P4U(Pay For Usage) mode. We solve this problem by 1)
formalizing the DSP problem as a constrained optimization problem, 2) proposing
the augmented MMKP(Multi-choice Multi-dimensional Knapsack Problem) with
general solution, 3) and demonstrating the DSP problem is a special case of the
augmented MMKP and deriving specialized strategy. Our strategy is verified
through simulation and outperforms state-of-the-art strategies in real
application. To the best of our knowledge, our solution is the first dual based
DSP bidding framework that is derived from strict second price auction
assumption and generally applicable to the multiple ads scenario with various
objectives and constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huahui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingrui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiaonan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08324">
<title>Towards dense object tracking in a 2D honeybee hive. (arXiv:1712.08324v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.08324</link>
<description rdf:parseType="Literal">&lt;p&gt;From human crowds to cells in tissue, the detection and efficient tracking of
multiple objects in dense configurations is an important and unsolved problem.
In the past, limitations of image analysis have restricted studies of dense
groups to tracking a single or subset of marked individuals, or to
coarse-grained group-level dynamics, all of which yield incomplete information.
Here, we combine convolutional neural networks (CNNs) with the model
environment of a honeybee hive to automatically recognize all individuals in a
dense group from raw image data. We create new, adapted individual labeling and
use the segmentation architecture U-Net with a loss function dependent on both
object identity and orientation. We additionally exploit temporal regularities
of the video recording in a recurrent manner and achieve near human-level
performance while reducing the network size by 94% compared to the original
U-Net architecture. Given our novel application of CNNs, we generate extensive
problem-specific image data in which labeled examples are produced through a
custom interface with Amazon Mechanical Turk. This dataset contains over
375,000 labeled bee instances across 720 video frames at 2 FPS, representing an
extensive resource for the development and testing of tracking methods. We
correctly detect 96% of individuals with a location error of ~7% of a typical
body dimension, and orientation error of 12 degrees, approximating the
variability of human raters. Our results provide an important step towards
efficient image-based dense object tracking by allowing for the accurate
determination of object location and orientation across time-series image data
efficiently within one network architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozek_K/0/1/0/all/0/1&quot;&gt;Katarzyna Bozek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1&quot;&gt;Laetitia Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikheyev_A/0/1/0/all/0/1&quot;&gt;Alexander S Mikheyev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephens_G/0/1/0/all/0/1&quot;&gt;Greg J Stephens&lt;/a&gt;</dc:creator>
</item></rdf:RDF>