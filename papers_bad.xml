<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05865"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05971"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07524"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08594"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05767"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06299"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05722"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05730"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05738"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05817"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05823"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05938"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07352"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02332"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07167"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03201"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.04567"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.05794">
<title>RAPIDNN: In-Memory Deep Neural Network Acceleration Framework. (arXiv:1806.05794v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.05794</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNN) have demonstrated effectiveness for various
applications such as image processing, video segmentation, and speech
recognition. Running state-of-the-art DNNs on current systems mostly relies on
either general purpose processors, ASIC designs, or FPGA accelerators, all of
which suffer from data movements due to the limited on chip memory and data
transfer bandwidth. In this work, we propose a novel framework, called RAPIDNN,
which processes all DNN operations within the memory to minimize the cost of
data movement. To enable in-memory processing, RAPIDNN reinterprets a DNN model
and maps it into a specialized accelerator, which is designed using
non-volatile memory blocks that model four fundamental DNN operations, i.e.,
multiplication, addition, activation functions, and pooling. The framework
extracts representative operands of a DNN model, e.g., weights and input
values, using clustering methods to optimize the model for in-memory
processing. Then, it maps the extracted operands and their precomputed results
into the accelerator memory blocks. At runtime, the accelerator identifies
computation results based on efficient in-memory search capability which also
provides tunability of approximation to further improve computation efficiency.
Our evaluation shows that RAPIDNN achieves 382.6x, 13.4x energy improvement and
211.5x, 5.6x performance speedup as compared to GPU-based DNN and the
state-of-the-art DNN accelerator, while ensuring less than 0.3% of quality
loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imani_M/0/1/0/all/0/1&quot;&gt;Mohsen Imani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1&quot;&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeseong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Saransh Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosing_T/0/1/0/all/0/1&quot;&gt;Tajana Rosing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05845">
<title>A Covariance Matrix Self-Adaptation Evolution Strategy for Linear Constrained Optimization. (arXiv:1806.05845v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the development of a covariance matrix self-adaptation
evolution strategy (CMSA-ES) for solving optimization problems with linear
constraints. The proposed algorithm is referred to as Linear Constraint CMSA-ES
(lcCMSA-ES). It uses a specially built mutation operator together with repair
by projection to satisfy the constraints. The lcCMSA-ES evolves itself on a
linear manifold defined by the constraints. The objective function is only
evaluated at feasible search points (interior point method). This is a property
often required in application domains such as simulation optimization and
finite element methods. The algorithm is tested on a variety of different test
problems revealing considerable results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spettel_P/0/1/0/all/0/1&quot;&gt;Patrick Spettel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyer_H/0/1/0/all/0/1&quot;&gt;Hans-Georg Beyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellwig_M/0/1/0/all/0/1&quot;&gt;Michael Hellwig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05865">
<title>Data-Efficient Design Exploration through Surrogate-Assisted Illumination. (arXiv:1806.05865v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.05865</link>
<description rdf:parseType="Literal">&lt;p&gt;Design optimization techniques are often used at the beginning of the design
process to explore the space of possible designs. In these domains illumination
algorithms, such as MAP-Elites, are promising alternatives to classic
optimization algorithms because they produce diverse, high-quality solutions in
a single run, instead of only a single near-optimal solution. Unfortunately,
these algorithms currently require a large number of function evaluations,
limiting their applicability. In this article we introduce a new illumination
algorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate
modeling techniques to create a map of the design space according to
user-defined features while minimizing the number of fitness evaluations. On a
2-dimensional airfoil optimization problem SAIL produces hundreds of diverse
but high-performing designs with several orders of magnitude fewer evaluations
than MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of
producing maps of high-performing designs in realistic 3-dimensional
aerodynamic tasks with an accurate flow simulation. Data-efficient design
exploration with SAIL can help designers understand what is possible, beyond
what is optimal, by considering more than pure objective-based optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaier_A/0/1/0/all/0/1&quot;&gt;Adam Gaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Asteroth_A/0/1/0/all/0/1&quot;&gt;Alexander Asteroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05971">
<title>An Enhanced BPSO based Approach for Service Placement in Hybrid Cloud. (arXiv:1806.05971v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1806.05971</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the challenges of competition and the rapidly evolving market,
companies need to be innovative and agile, particularly in regard of web
applications as used by customers. Nowadays, hybrid cloud stands as an
attractive solution as organizations tend to use a combination of private and
public cloud implementations, in accordance with their appropriate needs to
profitably apply the available resources and speed of execution. In such a
case, deploying the new applications would certainly entail opting for placing
and consecrating some components to the private cloud option, while reserving
some others to the public cloud option. In this respect, our primary goal in
this paper consists in minimizing the extra costs likely to be incurred by
applying the public cloud related options, along with those costs involved in
maintaining communication between the private cloud system and the public cloud
framework. As for our second targeted objective, it lies in reducing the
decision process relating to the execution time, necessary for selecting the
optimal service placement solution. For this purpose, a novel Binary Particle
Swarm Optimization (BPSO) based approach is proposed, useful for an effective
service placement optimization within hybrid cloud to take place. Using a real
benchmark, the experimental results appear to reveal that our proposed approach
reached results that outperform those documented in the state of the art both
in terms of cost and time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbes_W/0/1/0/all/0/1&quot;&gt;Wissem Abbes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kechaou_Z/0/1/0/all/0/1&quot;&gt;Zied Kechaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alimi_A/0/1/0/all/0/1&quot;&gt;Adel M. Alimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06010">
<title>A Self-Replication Basis for Designing Complex Agents. (arXiv:1806.06010v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.06010</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we describe a self-replication-based mechanism for designing
agents of increasing complexity. We demonstrate the validity of this approach
by solving simple, standard evolutionary computation problems in simulation. In
the context of these simulation results, we describe the fundamental
differences of this approach when compared to traditional approaches. Further,
we highlight the possible advantages of applying this approach to the problem
of designing complex artificial agents, along with the potential drawbacks and
issues to be addressed in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimpanal_T/0/1/0/all/0/1&quot;&gt;Thommen George Karimpanal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06062">
<title>Distributed Optimization Strategy for Multi Area Economic Dispatch Based on Electro Search Optimization Algorithm. (arXiv:1806.06062v1 [cs.OH])</title>
<link>http://arxiv.org/abs/1806.06062</link>
<description rdf:parseType="Literal">&lt;p&gt;A new adopted evolutionary algorithm is presented in this paper to solve the
non-smooth, non-convex and non-linear multi-area economic dispatch (MAED). MAED
includes some areas which contains its own power generation and loads. By
transmitting the power from the area with lower cost to the area with higher
cost, the total cost function can be minimized greatly. The tie line capacity,
multi-fuel generator and the prohibited operating zones are satisfied in this
study. In addition, a new algorithm based on electro search optimization
algorithm (ESOA) is proposed to solve the MAED optimization problem with
considering all the constraints. In ESOA algorithm all probable moving states
for individuals to get away from or move towards the worst or best solution
needs to be considered. To evaluate the performance of the ESOA algorithm, the
algorithm is applied to both the original economic dispatch with 40 generator
systems and the multi-area economic dispatch with 3 different systems such as:
6 generators in 2 areas; and 40 generators in 4 areas. It can be concluded
that, ESOA algorithm is more accurate and robust in comparison with other
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdandoost_M/0/1/0/all/0/1&quot;&gt;Mina Yazdandoost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khazaei_P/0/1/0/all/0/1&quot;&gt;Peyman Khazaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadatian_S/0/1/0/all/0/1&quot;&gt;Salar Saadatian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamali_R/0/1/0/all/0/1&quot;&gt;Rahim Kamali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07524">
<title>Supervised Speech Separation Based on Deep Learning: An Overview. (arXiv:1708.07524v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07524</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech separation is the task of separating target speech from background
interference. Traditionally, speech separation is studied as a signal
processing problem. A more recent approach formulates speech separation as a
supervised learning problem, where the discriminative patterns of speech,
speakers, and background noise are learned from training data. Over the past
decade, many supervised separation algorithms have been put forward. In
particular, the recent introduction of deep learning to supervised speech
separation has dramatically accelerated progress and boosted separation
performance. This article provides a comprehensive overview of the research on
deep learning based supervised speech separation in the last several years. We
first introduce the background of speech separation and the formulation of
supervised separation. Then we discuss three main components of supervised
separation: learning machines, training targets, and acoustic features. Much of
the overview is on separation algorithms where we review monaural methods,
including speech enhancement (speech-nonspeech separation), speaker separation
(multi-talker separation), and speech dereverberation, as well as
multi-microphone techniques. The important issue of generalization, unique to
supervised learning, is discussed. This overview provides a historical
perspective on how advances are made. In addition, we discuss a number of
conceptual issues, including what constitutes the target source.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;DeLiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jitong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04487">
<title>Better Runtime Guarantees Via Stochastic Domination. (arXiv:1801.04487v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04487</link>
<description rdf:parseType="Literal">&lt;p&gt;Apart from few exceptions, the mathematical runtime analysis of evolutionary
algorithms is mostly concerned with expected runtimes. In this work, we argue
that stochastic domination is a notion that should be used more frequently in
this area. Stochastic domination allows to formulate much more informative
performance guarantees, it allows to decouple the algorithm analysis into the
true algorithmic part of detecting a domination statement and the
probability-theoretical part of deriving the desired probabilistic guarantees
from this statement, and it helps finding simpler and more natural proofs.
&lt;/p&gt;
&lt;p&gt;As particular results, we prove a fitness level theorem which shows that the
runtime is dominated by a sum of independent geometric random variables, we
prove the first tail bounds for several classic runtime problems, and we give a
short and natural proof for Witt&apos;s result that the runtime of any $(\mu,p)$
mutation-based algorithm on any function with unique optimum is subdominated by
the runtime of a variant of the \oea on the \onemax function.
&lt;/p&gt;
&lt;p&gt;As side-products, we determine the fastest unbiased (1+1) algorithm for the
\leadingones benchmark problem, both in the general case and when restricted to
static mutation operators, and we prove a Chernoff-type tail bound for sums of
independent coupon collector distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06733">
<title>Probabilistic Tools for the Analysis of Randomized Optimization Heuristics. (arXiv:1801.06733v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06733</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter collects several probabilistic tools that proved to be useful in
the analysis of randomized search heuristics. This includes classic material
like Markov, Chebyshev and Chernoff inequalities, but also lesser known topics
like stochastic domination and coupling or Chernoff bounds for geometrically
distributed random variables and for negatively correlated random variables.
Most of the results presented here have appeared previously, some, however,
only in recent conference publications. While the focus is on collecting tools
for the analysis of randomized search heuristics, many of these may be useful
as well in the analysis of classic randomized algorithms or discrete random
structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08594">
<title>Neural Generative Models for Global Optimization with Gradients. (arXiv:1805.08594v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08594</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of global optimization is to find the global optimum of arbitrary
classes of functions, possibly highly multimodal ones. In this paper we focus
on the subproblem of global optimization for differentiable functions and we
propose an Evolutionary Search-inspired solution where we model point search
distributions via Generative Neural Networks. This approach enables us to model
diverse and complex search distributions based on which we can efficiently
explore complicated objective landscapes. In our experiments we show the
practical superiority of our algorithm versus classical Evolutionary Search and
gradient-based solutions on a benchmark set of multimodal functions, and
demonstrate how it can be used to accelerate Bayesian Optimization with
Gaussian Processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faury_L/0/1/0/all/0/1&quot;&gt;Louis Faury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1&quot;&gt;Flavian Vasile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calauzenes_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Calauz&amp;#xe8;nes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fercoq_O/0/1/0/all/0/1&quot;&gt;Olivier Fercoq&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05767">
<title>Motion Planning Networks. (arXiv:1806.05767v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1806.05767</link>
<description rdf:parseType="Literal">&lt;p&gt;Fast and efficient motion planning algorithms are crucial for many
state-of-the-art robotics applications such as self-driving cars. Existing
motion planning methods such as RRT*, A*, and D*, become ineffective as their
computational complexity increases exponentially with the dimensionality of the
motion planning problem. To address this issue, we present a neural
network-based novel planning algorithm which generates end-to-end
collision-free paths irrespective of the obstacles&apos; geometry. The proposed
method, called MPNet (Motion Planning Network), comprises of a Contractive
Autoencoder which encodes the given workspaces directly from a point cloud
measurement, and a deep feedforward neural network which takes the workspace
encoding, start and goal configuration, and generates end-to-end feasible
motion trajectories for the robot to follow. We evaluate MPNet on multiple
planning problems such as planning of a point-mass robot, rigid-body, and 7 DOF
Baxter robot manipulators in various 2D and 3D environments. The results show
that MPNet is not only consistently computationally efficient in all 2D and 3D
environments but also show remarkable generalization to completely unseen
environments. The results also show that computation time of MPNet consistently
remains less than 1 second which is significantly lower than existing
state-of-the-art motion planning algorithms. Furthermore, through transfer
learning, the MPNet trained in one scenario (e.g., indoor living places) can
also quickly adapt to new scenarios (e.g., factory floors) with a little amount
of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_A/0/1/0/all/0/1&quot;&gt;Ahmed H. Qureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bency_M/0/1/0/all/0/1&quot;&gt;Mayur J. Bency&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1&quot;&gt;Michael C. Yip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05892">
<title>Learning Front-end Filter-bank Parameters using Convolutional Neural Networks for Abnormal Heart Sound Detection. (arXiv:1806.05892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.05892</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic heart sound abnormality detection can play a vital role in the
early diagnosis of heart diseases, particularly in low-resource settings. The
state-of-the-art algorithms for this task utilize a set of Finite Impulse
Response (FIR) band-pass filters as a front-end followed by a Convolutional
Neural Network (CNN) model. In this work, we propound a novel CNN architecture
that integrates the front-end bandpass filters within the network using
time-convolution (tConv) layers, which enables the FIR filter-bank parameters
to become learnable. Different initialization strategies for the learnable
filters, including random parameters and a set of predefined FIR filter-bank
coefficients, are examined. Using the proposed tConv layers, we add constraints
to the learnable FIR filters to ensure linear and zero phase responses.
Experimental evaluations are performed on a balanced 4-fold cross-validation
task prepared using the PhysioNet/CinC 2016 dataset. Results demonstrate that
the proposed models yield superior performance compared to the state-of-the-art
system, while the linear phase FIR filterbank method provides an absolute
improvement of 9.54% over the baseline in terms of an overall accuracy metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1&quot;&gt;Ahmed Imtiaz Humayun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_S/0/1/0/all/0/1&quot;&gt;Shabnam Ghaffarzadegan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhe Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1&quot;&gt;Taufiq Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05898">
<title>Improving width-based planning with compact policies. (arXiv:1806.05898v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.05898</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal action selection in decision problems characterized by sparse,
delayed rewards is still an open challenge. For these problems, current deep
reinforcement learning methods require enormous amounts of data to learn
controllers that reach human-level performance. In this work, we propose a
method that interleaves planning and learning to address this issue. The
planning step hinges on the Iterated-Width (IW) planner, a state of the art
planner that makes explicit use of the state representation to perform
structured exploration. IW is able to scale up to problems independently of the
size of the state space. From the state-actions visited by IW, the learning
step estimates a compact policy, which in turn is used to guide the planning
step. The type of exploration used by our method is radically different than
the standard random exploration used in RL. We evaluate our method in simple
problems where we show it to have superior performance than the
state-of-the-art reinforcement learning algorithms A2C and Alpha Zero. Finally,
we present preliminary results in a subset of the Atari games suite.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junyent_M/0/1/0/all/0/1&quot;&gt;Miquel Junyent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1&quot;&gt;Anders Jonsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1&quot;&gt;Vicen&amp;#xe7; G&amp;#xf3;mez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06003">
<title>On Machine Learning and Structure for Mobile Robots. (arXiv:1806.06003v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.06003</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to recent advances - compute, data, models - the role of learning in
autonomous systems has expanded significantly, rendering new applications
possible for the first time. While some of the most significant benefits are
obtained in the perception modules of the software stack, other aspects
continue to rely on known manual procedures based on prior knowledge on
geometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in
these modules when data collection and curation become easier than manual rule
design. Building on this coarse and broad survey of current research, the final
sections aim to provide insights into future potentials and challenges as well
as the necessity of structure in current practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wulfmeier_M/0/1/0/all/0/1&quot;&gt;Markus Wulfmeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06055">
<title>Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees. (arXiv:1806.06055v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06055</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing classification algorithms that are fair with respect to sensitive
attributes of the data has become an important problem due to the growing
deployment of classification algorithms in various social contexts. Several
recent works have focused on fairness with respect to a specific metric,
modeled the corresponding fair classification problem as a constrained
optimization problem, and developed tailored algorithms to solve them. Despite
this, there still remain important metrics for which we do not have fair
classifiers and many of the aforementioned algorithms do not come with
theoretical guarantees; perhaps because the resulting optimization problem is
non-convex. The main contribution of this paper is a new meta-algorithm for
classification that takes as input a large class of fairness constraints, with
respect to multiple non-disjoint sensitive attributes, and which comes with
provable guarantees. This is achieved by first developing a meta-algorithm for
a large family of classification problems with convex constraints, and then
showing that classification problems with general types of fairness constraints
can be reduced to those in this family. We present empirical results that show
that our algorithm can achieve near-perfect fairness with respect to various
fairness metrics, and that the loss in accuracy due to the imposed fairness
constraints is often small. Overall, this work unifies several prior works on
fair classification, presents a practical algorithm with theoretical
guarantees, and can handle fairness metrics that were previously not possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1&quot;&gt;L. Elisa Celis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1&quot;&gt;Vijay Keswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07303">
<title>Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D Representations. (arXiv:1708.07303v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07303</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on the problem of learning 6-DOF grasping with a parallel
jaw gripper in simulation. We propose the notion of a geometry-aware
representation in grasping based on the assumption that knowledge of 3D
geometry is at the heart of interaction. Our key idea is constraining and
regularizing grasping interaction learning through 3D geometry prediction.
Specifically, we formulate the learning of deep geometry-aware grasping model
in two steps: First, we learn to build mental geometry-aware representation by
reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via
generative 3D shape modeling. Second, we learn to predict grasping outcome with
its internal geometry-aware representation. The learned outcome prediction
model is used to sequentially propose grasping solutions via
analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best
of our knowledge, we are presenting for the first time a method to learn a
6-DOF grasping net from RGBD input; (2) We build a grasping dataset from
demonstrations in virtual reality with rich sensory and interaction
annotations. This dataset includes 101 everyday objects spread across 7
categories, additionally, we propose a data augmentation strategy for effective
learning; (3) We demonstrate that the learned geometry-aware representation
leads to about 10 percent relative performance improvement over the baseline
CNN on grasping objects from our dataset. (4) We further demonstrate that the
model generalizes to novel viewpoints and object instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xinchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1&quot;&gt;Jasmine Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khansari_M/0/1/0/all/0/1&quot;&gt;Mohi Khansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunfei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1&quot;&gt;Arkanath Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_J/0/1/0/all/0/1&quot;&gt;James Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06299">
<title>Bayesian Best-Arm Identification for Selecting Influenza Mitigation Strategies. (arXiv:1711.06299v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06299</link>
<description rdf:parseType="Literal">&lt;p&gt;Pandemic influenza has the epidemic potential to kill millions of people.
While various preventive measures exist (i.a., vaccination and school
closures), deciding on strategies that lead to their most effective and
efficient use remains challenging. To this end, individual-based
epidemiological models are essential to assist decision makers in determining
the best strategy to curb epidemic spread. However, individual-based models are
computationally intensive and it is therefore pivotal to identify the optimal
strategy using a minimal amount of model evaluations. Additionally, as
epidemiological modeling experiments need to be planned, a computational budget
needs to be specified a priori. Consequently, we present a new sampling
technique to optimize the evaluation of preventive strategies using fixed
budget best-arm identification algorithms. We use epidemiological modeling
theory to derive knowledge about the reward distribution which we exploit using
Bayesian best-arm identification algorithms (i.e., Top-two Thompson sampling
and BayesGap). We evaluate these algorithms in a realistic experimental setting
and demonstrate that it is possible to identify the optimal strategy using only
a limited number of model evaluations, i.e., 2-to-3 times faster compared to
the uniform sampling method, the predominant technique used for epidemiological
decision making in the literature. Finally, we contribute and evaluate a
statistic for Top-two Thompson sampling to inform the decision makers about the
confidence of an arm recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Libin_P/0/1/0/all/0/1&quot;&gt;Pieter Libin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verstraeten_T/0/1/0/all/0/1&quot;&gt;Timothy Verstraeten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roijers_D/0/1/0/all/0/1&quot;&gt;Diederik M. Roijers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grujic_J/0/1/0/all/0/1&quot;&gt;Jelena Grujic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theys_K/0/1/0/all/0/1&quot;&gt;Kristof Theys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemey_P/0/1/0/all/0/1&quot;&gt;Philippe Lemey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1&quot;&gt;Ann Now&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00779">
<title>Gradient Descent Learns One-hidden-layer CNN: Don&apos;t be Afraid of Spurious Local Minima. (arXiv:1712.00779v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00779</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a one-hidden-layer neural network with
non-overlapping convolutional layer and ReLU activation, i.e., $f(\mathbf{Z},
\mathbf{w}, \mathbf{a}) = \sum_j a_j\sigma(\mathbf{w}^T\mathbf{Z}_j)$, in which
both the convolutional weights $\mathbf{w}$ and the output weights $\mathbf{a}$
are parameters to be learned. When the labels are the outputs from a teacher
network of the same architecture with fixed weights $(\mathbf{w}^*,
\mathbf{a}^*)$, we prove that with Gaussian input $\mathbf{Z}$, there is a
spurious local minimizer. Surprisingly, in the presence of the spurious local
minimizer, gradient descent with weight normalization from randomly initialized
weights can still be proven to recover the true parameters with constant
probability, which can be boosted to probability $1$ with multiple restarts. We
also show that with constant probability, the same procedure could also
converge to the spurious local minimum, showing that the local minimum plays a
non-trivial role in the dynamics of gradient descent. Furthermore, a
quantitative analysis shows that the gradient descent dynamics has two phases:
it starts off slow, but converges much faster after several iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnabas Poczos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06924">
<title>Safe Policy Improvement with Baseline Bootstrapping. (arXiv:1712.06924v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06924</link>
<description rdf:parseType="Literal">&lt;p&gt;A common goal in Reinforcement Learning is to derive a good strategy given a
limited batch of data. In this paper, we adopt the safe policy improvement
(SPI) approach: we compute a target policy guaranteed to perform at least as
well as a given baseline policy, approximately and with high probability. Our
SPI strategy, inspired by the knows-what-it-knows paradigm, consists in
bootstrapping the target with the baseline when the target does not know. We
develop two computationally efficient bootstrapping algorithms, one value-based
and one policy-based, both accompanied by theoretical SPI bounds for the
tabular case. We empirically show the limits of the existing algorithms on a
small stochastic gridworld problem, and then demonstrate that our algorithms
not only improve the worst-case scenario but also the mean performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trichelair_P/0/1/0/all/0/1&quot;&gt;Paul Trichelair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01206">
<title>On the Power of Over-parametrization in Neural Networks with Quadratic Activation. (arXiv:1803.01206v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01206</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide new theoretical insights on why over-parametrization is effective
in learning neural networks. For a $k$ hidden node shallow network with
quadratic activation and $n$ training data points, we show as long as $ k \ge
\sqrt{2n}$, over-parametrization enables local search algorithms to find a
\emph{globally} optimal solution for general smooth and convex loss functions.
Further, despite that the number of parameters may exceed the sample size,
using theory of Rademacher complexity, we show with weight decay, the solution
also generalizes well if the data is sampled from a regular distribution such
as Gaussian. To prove when $k\ge \sqrt{2n}$, the loss function has benign
landscape properties, we adopt an idea from smoothed analysis, which may have
other applications in studying loss surfaces of neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11324">
<title>Bayesian Inference with Anchored Ensembles of Neural Networks, and Application to Reinforcement Learning. (arXiv:1805.11324v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11324</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of ensembles of neural networks (NNs) for the quantification of
predictive uncertainty is widespread. However, the current justification is
intuitive rather than analytical. This work proposes one minor modification to
the normal ensembling methodology, which we prove allows the ensemble to
perform Bayesian inference, hence converging to the corresponding Gaussian
Process as both the total number of NNs, and the size of each, tend to
infinity. This working paper provides early-stage results in a reinforcement
learning setting, analysing the practicality of the technique for an ensemble
of small, finite number. Using the uncertainty estimates produced by anchored
ensembles to govern the exploration-exploitation process results in steadier,
more stable learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pearce_T/0/1/0/all/0/1&quot;&gt;Tim Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anastassacos_N/0/1/0/all/0/1&quot;&gt;Nicolas Anastassacos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaki_M/0/1/0/all/0/1&quot;&gt;Mohamed Zaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neely_A/0/1/0/all/0/1&quot;&gt;Andy Neely&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04225">
<title>PAC-Bayes Control: Synthesizing Controllers that Provably Generalize to Novel Environments. (arXiv:1806.04225v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04225</link>
<description rdf:parseType="Literal">&lt;p&gt;Our goal is to synthesize controllers for robots that provably generalize
well to novel environments given a dataset of example environments. The key
technical idea behind our approach is to leverage tools from generalization
theory in machine learning by exploiting a precise analogy (which we present in
the form of a reduction) between robustness of controllers to novel
environments and generalization of hypotheses in supervised learning. In
particular, we utilize the Probably Approximately Correct (PAC)-Bayes
framework, which allows us to obtain upper bounds (that hold with high
probability) on the expected cost of (stochastic) controllers across novel
environments. We propose control synthesis algorithms that explicitly seek to
minimize this upper bound. The corresponding optimization problem can be solved
using convex optimization (Relative Entropy Programming in particular) in the
setting where we are optimizing over a finite control policy space. In the more
general setting of continuously parameterized controllers, we minimize this
upper bound using stochastic gradient descent. We present examples of our
approach in the context of obstacle avoidance control with depth measurements.
Our simulated examples demonstrate the potential of our approach to provide
strong generalization guarantees on controllers for robotic systems with
continuous state and action spaces, complicated (e.g., nonlinear) dynamics, and
rich sensory inputs (e.g., depth measurements).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Anirudha Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_M/0/1/0/all/0/1&quot;&gt;Maxwell Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05502">
<title>Neural Stethoscopes: Unifying Analytic, Auxiliary and Adversarial Network Probing. (arXiv:1806.05502v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05502</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability and systematic, targeted model adaptation present
central tenets in machine learning for addressing limited or biased datasets.
In this paper, we introduce neural stethoscopes as a framework for quantifying
the degree of importance of specific factors of influence in deep networks as
well as for actively promoting and suppressing information as appropriate. In
doing so we unify concepts from multitask learning as well as training with
auxiliary and adversarial losses. We showcase the efficacy of neural
stethoscopes in an intuitive physics domain. Specifically, we investigate the
challenge of visually predicting stability of block towers and demonstrate that
the network uses visual cues which makes it susceptible to biases in the
dataset. Through the use of stethoscopes we interrogate the accessibility of
specific information throughout the network stack and show that we are able to
actively de-bias network predictions as well as enhance performance via
suitable auxiliary and adversarial stethoscope losses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fuchs_F/0/1/0/all/0/1&quot;&gt;Fabian B. Fuchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Groth_O/0/1/0/all/0/1&quot;&gt;Oliver Groth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kosiorek_A/0/1/0/all/0/1&quot;&gt;Adam R. Kosiorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bewley_A/0/1/0/all/0/1&quot;&gt;Alex Bewley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wulfmeier_M/0/1/0/all/0/1&quot;&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05694">
<title>Discovering Latent Patterns of Urban Cultural Interactions in WeChat for Modern City Planning. (arXiv:1806.05694v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.05694</link>
<description rdf:parseType="Literal">&lt;p&gt;Cultural activity is an inherent aspect of urban life and the success of a
modern city is largely determined by its capacity to offer generous cultural
entertainment to its citizens. To this end, the optimal allocation of cultural
establishments and related resources across urban regions becomes of vital
importance, as it can reduce financial costs in terms of planning and improve
quality of life in the city, more generally. In this paper, we make use of a
large longitudinal dataset of user location check-ins from the online social
network WeChat to develop a data-driven framework for cultural planning in the
city of Beijing. We exploit rich spatio-temporal representations on user
activity at cultural venues and use a novel extended version of the traditional
latent Dirichlet allocation model that incorporates temporal information to
identify latent patterns of urban cultural interactions. Using the
characteristic typologies of mobile user cultural activities emitted by the
model, we determine the levels of demand for different types of cultural
resources across urban areas. We then compare those with the corresponding
levels of supply as driven by the presence and spatial reach of cultural venues
in local areas to obtain high resolution maps that indicate urban regions with
lack of cultural resources, and thus give suggestions for further urban
cultural planning and investment optimisation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noulas_A/0/1/0/all/0/1&quot;&gt;Anastasios Noulas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascoloo_C/0/1/0/all/0/1&quot;&gt;Cecilia Mascoloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongxiang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05722">
<title>Non-asymptotic Identification of LTI Systems from a Single Trajectory. (arXiv:1806.05722v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.05722</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a realization for a linear time-invariant
(LTI) dynamical system from input/output data. Given a single input/output
trajectory, we provide finite time analysis for learning the system&apos;s Markov
parameters, from which a balanced realization is obtained using the classical
Ho-Kalman algorithm. By proving a stability result for the Ho-Kalman algorithm
and combining it with the sample complexity results for Markov parameters, we
show how much data is needed to learn a balanced realization of the system up
to a desired accuracy with high probability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozay_N/0/1/0/all/0/1&quot;&gt;Necmiye Ozay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05730">
<title>Learning Influence-Receptivity Network Structure with Guarantee. (arXiv:1806.05730v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.05730</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional works on community detection from observations of information
cascade assume that a single adjacency matrix parametrizes all the observed
cascades. However, in reality the connection structure usually does not stay
the same across cascades. For example, different people have different topics
of interest, therefore the connection structure would depend on the
information/topic content of the cascade. In this paper we consider the case
where we observe a sequence of noisy adjacency matrices triggered by
information/events with different topic distributions. We propose a novel
latent model using the intuition that the connection is more likely to exist
between two nodes if they are interested in similar topics, which are common
with the information/event. Specifically, we endow each node two node-topic
vectors: an influence vector that measures how much influential/authoritative
they are on each topic; and a receptivity vector that measures how much
receptive/susceptible they are to each topic. We show how these two node-topic
structures can be estimated from observed adjacency matrices with theoretical
guarantee, in cases where the topic distributions of the information/events are
known, as well as when they are unknown. Extensive experiments on synthetic and
real data demonstrate the effectiveness of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Ming Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Varun Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1&quot;&gt;Mladen Kolar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05738">
<title>Efficient sampling for Gaussian linear regression with arbitrary priors. (arXiv:1806.05738v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1806.05738</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops a slice sampler for Bayesian linear regression models
with arbitrary priors. The new sampler has two advantages over current
approaches. One, it is faster than many custom implementations that rely on
auxiliary latent variables, if the number of regressors is large. Two, it can
be used with any prior with a density function that can be evaluated up to a
normalizing constant, making it ideal for investigating the properties of new
shrinkage priors without having to develop custom sampling algorithms. The new
sampler takes advantage of the special structure of the linear regression
likelihood, allowing it to produce better effective sample size per second than
common alternative approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hahn_P/0/1/0/all/0/1&quot;&gt;P. Richard Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopes_H/0/1/0/all/0/1&quot;&gt;Hedibert Lopes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05791">
<title>Monaural source enhancement maximizing source-to-distortion ratio via automatic differentiation. (arXiv:1806.05791v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.05791</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep neural network (DNN) has made a breakthrough in monaural
source enhancement. Through a training step by using a large amount of data,
DNN estimates a mapping between mixed signals and clean signals. At this time,
we use an objective function that numerically expresses the quality of a
mapping by DNN. In the conventional methods, L1 norm, L2 norm, and
Itakura-Saito divergence are often used as objective functions. Recently, an
objective function based on short-time objective intelligibility (STOI) has
also been proposed. However, these functions only indicate similarity between
the clean signal and the estimated signal by DNN. In other words, they do not
show the quality of noise reduction or source enhancement. Motivated by the
fact, this paper adopts signal-to-distortion ratio (SDR) as the objective
function. Since SDR virtually shows signal-to-noise ratio (SNR), maximizing SDR
solves the above problem. The experimental results revealed that the proposed
method achieved better performance than the conventional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakajima_H/0/1/0/all/0/1&quot;&gt;Hiroaki Nakajima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_Y/0/1/0/all/0/1&quot;&gt;Yu Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_K/0/1/0/all/0/1&quot;&gt;Kazunobu Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hisaminato_Y/0/1/0/all/0/1&quot;&gt;Yuji Hisaminato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05817">
<title>Safe Active Feature Selection for Sparse Learning. (arXiv:1806.05817v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.05817</link>
<description rdf:parseType="Literal">&lt;p&gt;We present safe active incremental feature selection~(SAIF) to scale up the
computation of LASSO solutions. SAIF does not require a solution from a heavier
penalty parameter as in sequential screening or updating the full model for
each iteration as in dynamic screening. Different from these existing screening
methods, SAIF starts from a small number of features and incrementally recruits
active features and updates the significantly reduced model. Hence, it is much
more computationally efficient and scalable with the number of features. More
critically, SAIF has the safe guarantee as it has the convergence guarantee to
the optimal solution to the original full LASSO problem. Such an incremental
procedure and theoretical convergence guarantee can be extended to fused LASSO
problems. Compared with state-of-the-art screening methods as well as working
set and homotopy methods, which may not always guarantee the optimal solution,
SAIF can achieve superior or comparable efficiency and high scalability with
the safe guarantee when facing extremely high dimensional data sets.
Experiments with both synthetic and real-world data sets show that SAIF can be
up to 50 times faster than dynamic screening, and hundreds of times faster than
computing LASSO or fused LASSO solutions without screening.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shaogang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianhua Z. Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shuai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaoning Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05823">
<title>Primal-dual residual networks. (arXiv:1806.05823v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.05823</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a deep neural network architecture motivated by
primal-dual splitting methods from convex optimization. We show theoretically
that there exists a close relation between the derived architecture and
residual networks, and further investigate this connection in numerical
experiments. Moreover, we demonstrate how our approach can be used to unroll
optimization algorithms for certain problems with hard constraints. Using the
example of speech dequantization, we show that our method can outperform
classical splitting methods when both are applied to the same task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brauer_C/0/1/0/all/0/1&quot;&gt;Christoph Brauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lorenz_D/0/1/0/all/0/1&quot;&gt;Dirk Lorenz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05824">
<title>Three dimensional Deep Learning approach for remote sensing image classification. (arXiv:1806.05824v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.05824</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a variety of approaches has been enriching the field of Remote
Sensing (RS) image processing and analysis. Unfortunately, existing methods
remain limited faced to the rich spatio-spectral content of today&apos;s large
datasets. It would seem intriguing to resort to Deep Learning (DL) based
approaches at this stage with regards to their ability to offer accurate
semantic interpretation of the data. However, the specificity introduced by the
coexistence of spectral and spatial content in the RS datasets widens the scope
of the challenges presented to adapt DL methods to these contexts. Therefore,
the aim of this paper is firstly to explore the performance of DL architectures
for the RS hyperspectral dataset classification and secondly to introduce a new
three-dimensional DL approach that enables a joint spectral and spatial
information process. A set of three-dimensional schemes is proposed and
evaluated. Experimental results based on well knownhyperspectral datasets
demonstrate that the proposed method is able to achieve a better classification
rate than state of the art methods with lower computational costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamida_A/0/1/0/all/0/1&quot;&gt;Amina Ben Hamida&lt;/a&gt; (LISTIC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benoit_A/0/1/0/all/0/1&quot;&gt;A Benoit&lt;/a&gt; (LISTIC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_P/0/1/0/all/0/1&quot;&gt;Patrick Lambert&lt;/a&gt; (LISTIC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amar_C/0/1/0/all/0/1&quot;&gt;Chokri Ben Amar&lt;/a&gt; (REGIM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05833">
<title>On the exact minimization of saturated loss functions for robust regression and subspace estimation. (arXiv:1806.05833v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.05833</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with robust regression and subspace estimation and more
precisely with the problem of minimizing a saturated loss function. In
particular, we focus on computational complexity issues and show that an exact
algorithm with polynomial time-complexity with respect to the number of data
can be devised for robust regression and subspace estimation. This result is
obtained by adopting a classification point of view and relating the problems
to the search for a linear model that can approximate the maximal number of
points with a given error. Approximate variants of the algorithms based on
ramdom sampling are also discussed and experiments show that it offers an
accuracy gain over the traditional RANSAC for a similar algorithmic simplicity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauer_F/0/1/0/all/0/1&quot;&gt;Fabien Lauer&lt;/a&gt; (ABC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05924">
<title>Robust Bayesian Model Selection for Variable Clustering with the Gaussian Graphical Model. (arXiv:1806.05924v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.05924</link>
<description rdf:parseType="Literal">&lt;p&gt;Variable clustering is important for explanatory analysis. However, only few
dedicated methods for variable clustering with the Gaussian graphical model
have been proposed. Even more severe, small insignificant partial correlations
due to noise can dramatically change the clustering result when evaluating for
example with the Bayesian Information Criteria (BIC). In this work, we try to
address this issue by proposing a Bayesian model that accounts for negligible
small, but not necessarily zero, partial correlations. Based on our model, we
propose to evaluate a variable clustering result using the marginal likelihood.
To address the intractable calculation of the marginal likelihood, we propose
two solutions: one based on a variational approximation, and another based on
MCMC. Experiments on simulated data shows that the proposed method is similarly
accurate as BIC in the no noise setting, but considerably more accurate when
there are noisy partial correlations. Furthermore, on real data the proposed
method provides clustering results that are intuitively sensible, which is not
always the case when using BIC or its extensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Andrade_D/0/1/0/all/0/1&quot;&gt;Daniel Andrade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takeda_A/0/1/0/all/0/1&quot;&gt;Akiko Takeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fukumizu_K/0/1/0/all/0/1&quot;&gt;Kenji Fukumizu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05938">
<title>Query K-means Clustering and the Double Dixie Cup Problem. (arXiv:1806.05938v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.05938</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of approximate $K$-means clustering with outliers and
side information provided by same-cluster queries and possibly noisy answers.
Our solution shows that, under some mild assumptions on the smallest cluster
size, one can obtain an $(1+\epsilon)$-approximation for the optimal potential
with probability at least $1-\delta$, where $\epsilon&amp;gt;0$ and $\delta\in(0,1)$,
using an expected number of $O(\frac{K^3}{\epsilon \delta})$ noiseless
same-cluster queries and comparison-based clustering of complexity $O(ndK +
\frac{K^3}{\epsilon \delta})$, here, $n$ denotes the number of points and $d$
the dimension of space. Compared to a handful of other known approaches that
perform importance sampling to account for small cluster sizes, the proposed
query technique reduces the number of queries by a factor of roughly
$O(\frac{K^6}{\epsilon^3})$, at the cost of possibly missing very small
clusters. We extend this settings to the case where some queries to the oracle
produce erroneous information, and where certain points, termed outliers, do
not belong to any clusters. Our proof techniques differ from previous methods
used for $K$-means clustering analysis, as they rely on estimating the sizes of
the clusters and the number of points needed for accurate centroid estimation
and subsequent nontrivial generalizations of the double Dixie cup problem. We
illustrate the performance of the proposed algorithm both on synthetic and real
datasets, including MNIST and CIFAR $10$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chien_I/0/1/0/all/0/1&quot;&gt;I Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Milenkovic_O/0/1/0/all/0/1&quot;&gt;Olgica Milenkovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05953">
<title>Controllable Semantic Image Inpainting. (arXiv:1806.05953v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.05953</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a method for user-controllable semantic image inpainting: Given an
arbitrary set of observed pixels, the unobserved pixels can be imputed in a
user-controllable range of possibilities, each of which is semantically
coherent and locally consistent with the observed pixels. We achieve this using
a deep generative model bringing together: an encoder which can encode an
arbitrary set of observed pixels, latent variables which are trained to
represent disentangled factors of variations, and a bidirectional PixelCNN
model. We experimentally demonstrate that our method can generate plausible
inpainting results matching the user-specified semantics, but is still coherent
with observed pixels. We justify our choices of architecture and training
regime through more experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06063">
<title>Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet Process Switching Linear Dynamical Systems. (arXiv:1806.06063v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.06063</link>
<description rdf:parseType="Literal">&lt;p&gt;Using movement primitive libraries is an effective means to enable robots to
solve more complex tasks. In order to build these movement libraries, current
algorithms require a prior segmentation of the demonstration trajectories. A
promising approach is to model the trajectory as being generated by a set of
Switching Linear Dynamical Systems and inferring a meaningful segmentation by
inspecting the transition points characterized by the switching dynamics. With
respect to the learning, a nonparametric Bayesian approach is employed
utilizing a Gibbs sampler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sieb_M/0/1/0/all/0/1&quot;&gt;Maximilian Sieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schultheis_M/0/1/0/all/0/1&quot;&gt;Matthias Schultheis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Szelag_S/0/1/0/all/0/1&quot;&gt;Sebastian Szelag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07352">
<title>Structured low-rank matrix learning: algorithms and applications. (arXiv:1704.07352v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07352</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a low-rank matrix, constrained to lie in
a linear subspace, and introduce a novel factorization for modeling such
matrices. A salient feature of the proposed factorization scheme is it
decouples the low-rank and the structural constraints onto separate factors. We
formulate the optimization problem on the Riemannian spectrahedron manifold,
where the Riemannian framework allows to develop computationally efficient
conjugate gradient and trust-region algorithms. Experiments on problems such as
standard/robust/non-negative matrix completion, Hankel matrix learning and
multi-task learning demonstrate the efficacy of our approach. A shorter version
of this work has been published in ICML&apos;18.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jawanpuria_P/0/1/0/all/0/1&quot;&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02332">
<title>Low-shot learning with large-scale diffusion. (arXiv:1706.02332v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02332</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the problem of inferring image labels from images when
only a few annotated examples are available at training time. This setup is
often referred to as low-shot learning, where a standard approach is to
re-train the last few layers of a convolutional neural network learned on
separate classes for which training examples are abundant. We consider a
semi-supervised setting based on a large collection of images to support label
propagation. This is possible by leveraging the recent advances on large-scale
similarity graph construction.
&lt;/p&gt;
&lt;p&gt;We show that despite its conceptual simplicity, scaling label propagation up
to hundred millions of images leads to state of the art accuracy in the
low-shot learning regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1&quot;&gt;Matthijs Douze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1&quot;&gt;Arthur Szlam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1&quot;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; J&amp;#xe9;gou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04165">
<title>Temporal Stability in Predictive Process Monitoring. (arXiv:1712.04165v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04165</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive process monitoring is concerned with the analysis of events
produced during the execution of a business process in order to predict as
early as possible the final outcome of an ongoing case. Traditionally,
predictive process monitoring methods are optimized with respect to accuracy.
However, in environments where users make decisions and take actions in
response to the predictions they receive, it is equally important to optimize
the stability of the successive predictions made for each case. To this end,
this paper defines a notion of temporal stability for binary classification
tasks in predictive process monitoring and evaluates existing methods with
respect to both temporal stability and accuracy. We find that methods based on
XGBoost and LSTM neural networks exhibit the highest temporal stability. We
then show that temporal stability can be enhanced by hyperparameter-optimizing
random forests and XGBoost classifiers with respect to inter-run stability.
Finally, we show that time series smoothing techniques can further enhance
temporal stability at the expense of slightly lower accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teinemaa_I/0/1/0/all/0/1&quot;&gt;Irene Teinemaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leontjeva_A/0/1/0/all/0/1&quot;&gt;Anna Leontjeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07167">
<title>High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach. (arXiv:1802.07167v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07167</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the generation of prediction intervals (PIs) by neural
networks for quantifying uncertainty in regression tasks. It is axiomatic that
high-quality PIs should be as narrow as possible, whilst capturing a specified
portion of data. We derive a loss function directly from this axiom that
requires no distributional assumption. We show how its form derives from a
likelihood principle, that it can be used with gradient descent, and that model
uncertainty is accounted for in ensembled form. Benchmark experiments show the
method outperforms current state-of-the-art uncertainty quantification methods,
reducing average PI width by over 10%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pearce_T/0/1/0/all/0/1&quot;&gt;Tim Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaki_M/0/1/0/all/0/1&quot;&gt;Mohamed Zaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brintrup_A/0/1/0/all/0/1&quot;&gt;Alexandra Brintrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neely_A/0/1/0/all/0/1&quot;&gt;Andy Neely&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07309">
<title>Detection limits in the high-dimensional spiked rectangular model. (arXiv:1802.07309v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07309</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of detecting the presence of a single unknown spike in a
rectangular data matrix, in a high-dimensional regime where the spike has fixed
strength and the aspect ratio of the matrix converges to a finite limit. This
setup includes Johnstone&apos;s spiked covariance model. We analyze the likelihood
ratio of the spiked model against an &quot;all noise&quot; null model of reference, and
show it has asymptotically Gaussian fluctuations in a region below---but in
general not up to---the so-called BBP threshold from random matrix theory. Our
result parallels earlier findings of Onatski et al.\ (2013) and
Johnstone-Onatski (2015) for spherical spikes. We present a probabilistic
approach capable of treating generic product priors. In particular, sparsity in
the spike is allowed. Our approach is based on Talagrand&apos;s interpretation of
the cavity method from spin-glass theory. The question of the maximal parameter
region where asymptotic normality is expected to hold is left open. This region
is shaped by the prior in a non-trivial way. We conjecture that this is the
entire paramagnetic phase of an associated spin-glass model, and is defined by
the vanishing of the replica-symmetric solution of Lesieur et al.\ (2015).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Alaoui_A/0/1/0/all/0/1&quot;&gt;Ahmed El Alaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01785">
<title>Differentiable Submodular Maximization. (arXiv:1803.01785v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01785</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider learning of submodular functions from data. These functions are
important in machine learning and have a wide range of applications, e.g. data
summarization, feature selection and active learning. Despite their
combinatorial nature, submodular functions can be maximized approximately with
strong theoretical guarantees in polynomial time. Typically, learning the
submodular function and optimization of that function are treated separately,
i.e. the function is first learned using a proxy objective and subsequently
maximized. In contrast, we show how to perform learning and optimization
jointly. By interpreting the output of greedy maximization algorithms as
distributions over sequences of items and smoothening these distributions, we
obtain a differentiable objective. In this way, we can differentiate through
the maximization algorithms and optimize the model to work well with the
optimization algorithm. We theoretically characterize the error made by our
approach, yielding insights into the tradeoff of smoothness and accuracy. We
demonstrate the effectiveness of our approach for jointly learning and
optimizing on synthetic maximum cut data, and on real world applications such
as product recommendation and image collection summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tschiatschek_S/0/1/0/all/0/1&quot;&gt;Sebastian Tschiatschek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sahin_A/0/1/0/all/0/1&quot;&gt;Aytunc Sahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08178">
<title>Boosted Density Estimation Remastered. (arXiv:1803.08178v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08178</link>
<description rdf:parseType="Literal">&lt;p&gt;There has recently been a steadily increase in the iterative approaches to
boosted density estimation and sampling, usually proceeding by adding candidate
&quot;iterate&quot; densities to a model that gets more accurate with iterations. The
relative accompanying burst of formal convergence results has not yet changed a
striking picture: all results essentially pay the price of heavy assumptions on
iterates, often unrealistic or hard to check, and offer a blatant contrast with
the original boosting theory where such assumptions would be the weakest
possible. In this paper, we show that all that suffices to achieve boosting for
density estimation is a weak learner in the original boosting theory sense,
that is, an oracle that supplies classifiers. We provide converge rates that
comply with boosting requirements, being better and / or relying on
substantially weaker assumptions than the state of the art. One of our rates is
to our knowledge the first to rely on not just weak but also empirically
testable assumptions. We show that the model fit belongs to exponential
families, and obtain in the course of our results a variational
characterization of $f$-divergences better than $f$-GAN&apos;s. Experimental results
on several simulated problems display significantly better results than AdaGAN
during early boosting rounds, in particular for mode capture, and using
architectures less than the fifth&apos;s of AdaGAN&apos;s size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cranko_Z/0/1/0/all/0/1&quot;&gt;Zac Cranko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03201">
<title>Scalable Factorized Hierarchical Variational Autoencoder Training. (arXiv:1804.03201v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03201</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have achieved great success in unsupervised learning
with the ability to capture complex nonlinear relationships between latent
generating factors and observations. Among them, a factorized hierarchical
variational autoencoder (FHVAE) is a variational inference-based model that
formulates a hierarchical generative process for sequential data. Specifically,
an FHVAE model can learn disentangled and interpretable representations, which
have been proven useful for numerous speech applications, such as speaker
verification, robust speech recognition, and voice conversion. However, as we
will elaborate in this paper, the training algorithm proposed in the original
paper is not scalable to datasets of thousands of hours, which makes this model
less applicable on a larger scale. After identifying limitations in terms of
runtime, memory, and hyperparameter optimization, we propose a hierarchical
sampling training algorithm to address all three issues. Our proposed method is
evaluated comprehensively on a wide variety of datasets, ranging from 3 to
1,000 hours and involving different types of generating factors, such as
recording conditions and noise types. In addition, we also present a new
visualization method for qualitatively evaluating the performance with respect
to the interpretability and disentanglement. Models trained with our proposed
algorithm demonstrate the desired characteristics on all the datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.04567">
<title>Regression-based reduced-order models to predict transient thermal output for enhanced geothermal systems. (arXiv:1606.04567v3 [cs.CE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1606.04567</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this paper is to assess the utility of Reduced-Order Models
(ROMs) developed from 3D physics-based models for predicting transient thermal
power output for an enhanced geothermal reservoir while explicitly accounting
for uncertainties in the subsurface system and site-specific details. Numerical
simulations are performed based on Latin Hypercube Sampling (LHS) of model
inputs drawn from uniform probability distributions. Key sensitive parameters
are identified from these simulations, which are fracture zone permeability,
well/skin factor, bottom hole pressure, and injection flow rate. The inputs for
ROMs are based on these key sensitive parameters. The ROMs are then used to
evaluate the influence of subsurface attributes on thermal power production
curves. The resulting ROMs are compared with field-data and the detailed
physics-based numerical simulations. We propose three different ROMs with
different levels of model parsimony, each describing key and essential features
of the power production curves. ROM-1 is able to accurately reproduce the power
output of numerical simulations for low values of permeabilities and certain
features of the field-scale data, and is relatively parsimonious. ROM-2 is a
more complex model than ROM-1 but it accurately describes the field-data. At
higher permeabilities, ROM-2 reproduces numerical results better than ROM-1,
however, there is a considerable deviation at low fracture zone permeabilities.
ROM-3 is developed by taking the best aspects of ROM-1 and ROM-2 and provides a
middle ground for model parsimony. It is able to describe various features of
numerical simulations and field-data. From the proposed workflow, we
demonstrate that the proposed simple ROMs are able to capture various complex
features of the power production curves of Fenton Hill HDR system. For typical
EGS applications, ROM-2 and ROM-3 outperform ROM-1.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudunuru_M/0/1/0/all/0/1&quot;&gt;M. K. Mudunuru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karra_S/0/1/0/all/0/1&quot;&gt;S. Karra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harp_D/0/1/0/all/0/1&quot;&gt;D. R. Harp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guthrie_G/0/1/0/all/0/1&quot;&gt;G. D. Guthrie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_H/0/1/0/all/0/1&quot;&gt;H. S. Viswanathan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>