<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00962"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01011"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01019"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01035"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00886"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00973"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01079"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01227"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01279"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00977"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00867"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00882"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00905"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00939"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01069"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01134"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01297"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.02588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04674"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12421"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09918"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.00962">
<title>Neuro-memristive Circuits for Edge Computing: A review. (arXiv:1807.00962v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1807.00962</link>
<description rdf:parseType="Literal">&lt;p&gt;The volume, veracity, variability and velocity of data produced from the ever
increasing network of sensors connected to Internet pose challenges for power
management, scalability and sustainability of cloud computing infrastructure.
Increasing the data processing capability of edge computing devices at lower
power requirements can reduce the overheads for cloud computing solutions. This
paper provides the review of neuromorphic CMOS-memristive architectures that
can be integrated into edge computing devices. We discuss why the neuromorphic
architectures are useful for edge devices and show the advantages, drawbacks
and open problems in the field of memristive circuit and architectures in terms
of edge computing perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krestinskaya_O/0/1/0/all/0/1&quot;&gt;Olga Krestinskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_A/0/1/0/all/0/1&quot;&gt;Alex Pappachen James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_L/0/1/0/all/0/1&quot;&gt;Leon O. Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00981">
<title>Stochastic optimization approaches to learning concise representations. (arXiv:1807.00981v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.00981</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and study a method for learning interpretable features via
stochastic optimization of feature architectures. Features are represented as
multi-type expression trees using a set of activation functions common in
neural networks in addition to other elementary functions. Continuous features
are trained via gradient descent, and the performance of features in ML models
is used to weight the rate of change among subcomponents of representations.
The search process maintains an archive of representations with
accuracy-complexity trade-offs to assist in generalization and interpretation.
We compare several stochastic optimization approaches within this framework. We
benchmark these variants on many real-world regression problems in comparison
to other machine learning approaches. The best results across methods are
obtained by search methods that directly optimize the accuracy-complexity
trade-off in order to find simple architectures that generalize well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cava_W/0/1/0/all/0/1&quot;&gt;William La Cava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_T/0/1/0/all/0/1&quot;&gt;Tilak Raj Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taggart_J/0/1/0/all/0/1&quot;&gt;James Taggart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suri_S/0/1/0/all/0/1&quot;&gt;Srinivas Suri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jason H. Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01011">
<title>A First Analysis of Kernels for Kriging-based Optimization in Hierarchical Search Spaces. (arXiv:1807.01011v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.01011</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world optimization problems require significant resources for
objective function evaluations. This is a challenge to evolutionary algorithms,
as it limits the number of available evaluations. One solution are surrogate
models, which replace the expensive objective. A particular issue in this
context are hierarchical variables. Hierarchical variables only influence the
objective function if other variables satisfy some condition. We study how this
kind of hierarchical structure can be integrated into the model based
optimization framework. We discuss an existing kernel and propose alternatives.
An artificial test function is used to investigate how different kernels and
assumptions affect model quality and search performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1&quot;&gt;Martin Zaefferer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horn_D/0/1/0/all/0/1&quot;&gt;Daniel Horn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01013">
<title>Is Neuromorphic MNIST neuromorphic? Analyzing the discriminative power of neuromorphic datasets in the time domain. (arXiv:1807.01013v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.01013</link>
<description rdf:parseType="Literal">&lt;p&gt;The advantage of spiking neural networks (SNNs) over their predecessors is
their ability to spike, enabling them to use spike timing for coding and
efficient computing. A neuromorphic dataset should allow a neuromorphic
algorithm to clearly show that a SNN is able to perform better on the dataset
than an ANN. We have analyzed both N-MNIST and N-Caltech101 along these lines,
but focus our study on N-MNIST. First we evaluate if additional information is
encoded in the time domain in a neuromoprhic dataset. We show that an ANN
trained with backpropagation on frame based versions of N-MNIST and
N-Caltech101 images achieve 99.23% and 78.01% accuracy. These are the best
classification accuracies obtained on these datasets to date. Second we present
the first unsupervised SNN to be trained on N-MNIST and demonstrate results of
91.78%. We also use this SNN for further experiments on N-MNIST to show that
rate based SNNs perform better, and precise spike timings are not important in
N-MNIST. N-MNIST does not, therefore, highlight the unique ability of SNNs. The
conclusion of this study opens an important question in neuromorphic
engineering - what, then, constitutes a good neuromorphic dataset?
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_L/0/1/0/all/0/1&quot;&gt;Laxmi R. Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1&quot;&gt;Yansong Chua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haizhou Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01019">
<title>Linear Combination of Distance Measures for Surrogate Models in Genetic Programming. (arXiv:1807.01019v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.01019</link>
<description rdf:parseType="Literal">&lt;p&gt;Surrogate models are a well established approach to reduce the number of
expensive function evaluations in continuous optimization. In the context of
genetic programming, surrogate modeling still poses a challenge, due to the
complex genotype-phenotype relationships. We investigate how different
genotypic and phenotypic distance measures can be used to learn Kriging models
as surrogates. We compare the measures and suggest to use their linear
combination in a kernel.
&lt;/p&gt;
&lt;p&gt;We test the resulting model in an optimization framework, using symbolic
regression problem instances as a benchmark. Our experiments show that the
model provides valuable information. Firstly, the model enables an improved
optimization performance compared to a model-free algorithm. Furthermore, the
model provides information on the contribution of different distance measures.
The data indicates that a phenotypic distance measure is important during the
early stages of an optimization run when less data is available. In contrast,
genotypic measures, such as the tree edit distance, contribute more during the
later stages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1&quot;&gt;Martin Zaefferer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stork_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Stork&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flasch_O/0/1/0/all/0/1&quot;&gt;Oliver Flasch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1&quot;&gt;Thomas Bartz-Beielstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01035">
<title>Deep Neural Object Analysis by Interactive Auditory Exploration with a Humanoid Robot. (arXiv:1807.01035v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.01035</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach for interactive auditory object analysis with a
humanoid robot. The robot elicits sensory information by physically shaking
visually indistinguishable plastic capsules. It gathers the resulting audio
signals from microphones that are embedded into the robotic ears. A neural
network architecture learns from these signals to analyze properties of the
contents of the containers. Specifically, we evaluate the material
classification and weight prediction accuracy and demonstrate that the
framework is fairly robust to acoustic real-world noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eppe_M/0/1/0/all/0/1&quot;&gt;Manfred Eppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1&quot;&gt;Matthias Kerzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strahl_E/0/1/0/all/0/1&quot;&gt;Erik Strahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00886">
<title>Hypertree Decompositions Revisited for PGMs. (arXiv:1807.00886v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00886</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit the classical problem of exact inference on probabilistic
graphical models (PGMs). Our algorithm is based on recent \emph{worst-case
optimal database join} algorithms, which can be asymptotically faster than
traditional data processing methods. We present the first empirical evaluation
of these algorithms via JoinInfer -- a new exact inference engine. We
empirically explore the properties of the data for which our engine can be
expected to outperform traditional inference engines, refining current
theoretical notions. Further, JoinInfer outperforms existing state-of-the-art
inference engines (ACE, IJGP and libDAI) on some standard benchmark datasets by
up to a factor of 630x. Finally, we propose a promising data-driven heuristic
that extends JoinInfer to automatically tailor its parameters and/or switch to
the traditional inference algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1&quot;&gt;Aarthy Shivram Arun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_S/0/1/0/all/0/1&quot;&gt;Sai Vikneshwar Mani Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudra_A/0/1/0/all/0/1&quot;&gt;Atri Rudra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00973">
<title>Scalable Structure Learning for Probabilistic Soft Logic. (arXiv:1807.00973v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00973</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical relational frameworks such as Markov logic networks and
probabilistic soft logic (PSL) encode model structure with weighted first-order
logical clauses. Learning these clauses from data is referred to as structure
learning. Structure learning alleviates the manual cost of specifying models.
However, this benefit comes with high computational costs; structure learning
typically requires an expensive search over the space of clauses which involves
repeated optimization of clause weights. In this paper, we propose the first
two approaches to structure learning for PSL. We introduce a greedy
search-based algorithm and a novel optimization method that trade-off
scalability and approximations to the structure learning problem in varying
ways. The highly scalable optimization method combines data-driven generation
of clauses with a piecewise pseudolikelihood (PPLL) objective that learns model
structure by optimizing clause weights only once. We compare both methods
across five real-world tasks, showing that PPLL achieves an order of magnitude
runtime speedup and AUC gains up to 15% over greedy search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Embar_V/0/1/0/all/0/1&quot;&gt;Varun Embar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1&quot;&gt;Dhanya Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farnadi_G/0/1/0/all/0/1&quot;&gt;Golnoosh Farnadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1&quot;&gt;Lise Getoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00975">
<title>A Spatial and Temporal Features Mixture Model with Body Parts for Video-based Person Re-Identification. (arXiv:1807.00975v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00975</link>
<description rdf:parseType="Literal">&lt;p&gt;The video-based person re-identification is to recognize a person under
different cameras, which is a crucial task applied in visual surveillance
system. Most previous methods mainly focused on the feature of full body in the
frame. In this paper we propose a novel Spatial and Temporal Features Mixture
Model (STFMM) based on convolutional neural network (CNN) and recurrent neural
network (RNN), in which the human body is split into $N$ parts in horizontal
direction so that we can obtain more specific features. The proposed method
skillfully integrates features of each part to achieve more expressive
representation of each person. We first split the video sequence into $N$ part
sequences which include the information of head, waist, legs and so on. Then
the features are extracted by STFMM whose $2N$ inputs are obtained from the
developed Siamese network, and these features are combined into a
discriminative representation for one person. Experiments are conducted on the
iLIDS-VID and PRID-2011 datasets. The results demonstrate that our approach
outperforms existing methods for video-based person re-identification. It
achieves a rank-1 CMC accuracy of 74\% on the iLIDS-VID dataset, exceeding the
the most recently developed method ASTPN by 12\%. For the cross-data testing,
our method achieves a rank-1 CMC accuracy of 48\% exceeding the ASTPN method by
18\%, which shows that our model has significant stability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Cheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Baomin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shuangyuan Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01079">
<title>Stochastic Constraint Optimization using Propagation on Ordered Binary Decision Diagrams. (arXiv:1807.01079v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.01079</link>
<description rdf:parseType="Literal">&lt;p&gt;A number of problems in relational Artificial Intelligence can be viewed as
Stochastic Constraint Optimization Problems (SCOPs). These are constraint
optimization problems that involve objectives or constraints with a stochastic
component. Building on the recently proposed language SC-ProbLog for modeling
SCOPs, we propose a new method for solving these problems. Earlier methods used
Probabilistic Logic Programming (PLP) techniques to create Ordered Binary
Decision Diagrams (OBDDs), which were decomposed into smaller constraints in
order to exploit existing constraint programming (CP) solvers. We argue that
this approach has as drawback that a decomposed representation of an OBDD does
not guarantee domain consistency during search, and hence limits the efficiency
of the solver. For the specific case of monotonic distributions, we suggest an
alternative method for using CP in SCOP, based on the development of a new
propagator; we show that this propagator is linear in the size of the OBDD, and
has the potential to be more efficient than the decomposition method, as it
maintains domain consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latour_A/0/1/0/all/0/1&quot;&gt;Anna L.D. Latour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaki_B/0/1/0/all/0/1&quot;&gt;Behrouz Babaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nijssen_S/0/1/0/all/0/1&quot;&gt;Siegfried Nijssen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01227">
<title>Providing Explanations for Recommendations in Reciprocal Environments. (arXiv:1807.01227v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated platforms which support users in finding a mutually beneficial
match, such as online dating and job recruitment sites, are becoming
increasingly popular. These platforms often include recommender systems that
assist users in finding a suitable match. While recommender systems which
provide explanations for their recommendations have shown many benefits,
explanation methods have yet to be adapted and tested in recommending suitable
matches. In this paper, we introduce and extensively evaluate the use of
&quot;reciprocal explanations&quot; -- explanations which provide reasoning as to why
both parties are expected to benefit from the match. Through an extensive
empirical evaluation, in both simulated and real-world dating platforms with
287 human participants, we find that when the acceptance of a recommendation
involves a significant cost (e.g., monetary or emotional), reciprocal
explanations outperform standard explanation methods which consider the
recommendation receiver alone. However, contrary to what one may expect, when
the cost of accepting a recommendation is negligible, reciprocal explanations
are shown to be less effective than the traditional explanation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinerman_A/0/1/0/all/0/1&quot;&gt;Akiva Kleinerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_A/0/1/0/all/0/1&quot;&gt;Ariel Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraus_S/0/1/0/all/0/1&quot;&gt;Sarit Kraus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01279">
<title>Dynamic Control of Explore/Exploit Trade-Off In Bayesian Optimization. (arXiv:1807.01279v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.01279</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization offers the possibility of optimizing black-box
operations not accessible through traditional techniques. The success of
Bayesian optimization methods such as Expected Improvement (EI) are
significantly affected by the degree of trade-off between exploration and
exploitation. Too much exploration can lead to inefficient optimization
protocols, whilst too much exploitation leaves the protocol open to strong
initial biases, and a high chance of getting stuck in a local minimum.
Typically, a constant margin is used to control this trade-off, which results
in yet another hyper-parameter to be optimized. We propose contextual
improvement as a simple, yet effective heuristic to counter this - achieving a
one-shot optimization strategy. Our proposed heuristic can be swiftly
calculated and improves both the speed and robustness of discovery of optimal
solutions. We demonstrate its effectiveness on both synthetic and real world
problems and explore the unaccounted for uncertainty in the pre-determination
of search hyperparameters controlling explore-exploit trade-off.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jasrasaria_D/0/1/0/all/0/1&quot;&gt;Dipti Jasrasaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pyzer_Knapp_E/0/1/0/all/0/1&quot;&gt;Edward O. Pyzer-Knapp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00193">
<title>InclusiveFaceNet: Improving Face Attribute Detection with Race and Gender Diversity. (arXiv:1712.00193v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00193</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate an approach to face attribute detection that retains or
improves attribute detection accuracy across gender and race subgroups by
learning demographic information prior to learning the attribute detection
task. The system, which we call InclusiveFaceNet, detects face attributes by
transferring race and gender representations learned from a held-out dataset of
public race and gender identities. Leveraging learned demographic
representations while withholding demographic inference from the downstream
face attribute detection task preserves potential users&apos; demographic privacy
while resulting in some of the best reported numbers to date on attribute
detection in the Faces of the World and CelebA datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hee Jung Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Margaret Mitchell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00977">
<title>Pose Flow: Efficient Online Pose Tracking. (arXiv:1802.00977v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00977</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-person articulated pose tracking in unconstrained videos is an
important while challenging problem. In this paper, going along the road of
top-down approaches, we propose a decent and efficient pose tracker based on
pose flows. First, we design an online optimization framework to build the
association of cross-frame poses and form pose flows (PF-Builder). Second, a
novel pose flow non-maximum suppression (PF-NMS) is designed to robustly reduce
redundant pose flows and re-link temporal disjoint ones. Extensive experiments
show that our method significantly outperforms best-reported results on two
standard Pose Tracking datasets by 13 mAP 25 MOTA and 6 mAP 3 MOTA
respectively. Moreover, in the case of working on detected poses in individual
frames, the extra computation of pose tracker is very minor, guaranteeing
online 10FPS tracking. Our source codes are made publicly
available(https://github.com/YuliangXiu/PoseFlow).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Xiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiefeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yinghong Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02096">
<title>Cooperative Tracking of Cyclists Based on Smart Devices and Infrastructure. (arXiv:1803.02096v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02096</link>
<description rdf:parseType="Literal">&lt;p&gt;In future traffic scenarios, vehicles and other traffic participants will be
interconnected and equipped with various types of sensors, allowing for
cooperation based on data or information exchange. This article presents an
approach to cooperative tracking of cyclists using smart devices and
infrastructure-based sensors. A smart device is carried by the cyclists and an
intersection is equipped with a wide angle stereo camera system. Two tracking
models are presented and compared. The first model is based on the stereo
camera system detections only, whereas the second model cooperatively combines
the camera based detections with velocity and yaw rate data provided by the
smart device. Our aim is to overcome limitations of tracking approaches based
on single data sources. We show in numerical evaluations on scenes where
cyclists are starting or turning right that the cooperation leads to an
improvement in both the ability to keep track of a cyclist and the accuracy of
the track particularly when it comes to occlusions in the visual system. We,
therefore, contribute to the safety of vulnerable road users in future traffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reitberger_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nther Reitberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1&quot;&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bieshaar_M/0/1/0/all/0/1&quot;&gt;Maarten Bieshaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1&quot;&gt;Konrad Doll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_E/0/1/0/all/0/1&quot;&gt;Erich Fuchs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04242">
<title>The Potential of the Return Distribution for Exploration in RL. (arXiv:1806.04242v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04242</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the potential of the return distribution for exploration
in deterministic reinforcement learning (RL) environments. We study network
losses and propagation mechanisms for Gaussian, Categorical and Gaussian
mixture distributions. Combined with exploration policies that leverage this
return distribution, we solve, for example, a randomized Chain task of length
100, which has not been reported before when learning with neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1&quot;&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broekens_J/0/1/0/all/0/1&quot;&gt;Joost Broekens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1&quot;&gt;Catholijn M. Jonker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10293">
<title>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. (arXiv:1806.10293v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10293</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of learning vision-based dynamic
manipulation skills using a scalable reinforcement learning approach. We study
this problem in the context of grasping, a longstanding challenge in robotic
manipulation. In contrast to static learning behaviors that choose a grasp
point and then execute the desired grasp, our method enables closed-loop
vision-based control, whereby the robot continuously updates its grasp strategy
based on the most recent observations to optimize long-horizon grasp success.
To that end, we introduce QT-Opt, a scalable self-supervised vision-based
reinforcement learning framework that can leverage over 580k real-world grasp
attempts to train a deep neural network Q-function with over 1.2M parameters to
perform closed-loop, real-world grasping that generalizes to 96% grasp success
on unseen objects. Aside from attaining a very high success rate, our method
exhibits behaviors that are quite distinct from more standard grasping systems:
using only RGB vision-based perception from an over-the-shoulder camera, our
method automatically learns regrasping strategies, probes objects to find the
most effective grasps, learns to reposition objects and perform other
non-prehensile pre-grasp manipulations, and responds dynamically to
disturbances and perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1&quot;&gt;Dmitry Kalashnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1&quot;&gt;Alex Irpan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pastor_P/0/1/0/all/0/1&quot;&gt;Peter Pastor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1&quot;&gt;Julian Ibarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzog_A/0/1/0/all/0/1&quot;&gt;Alexander Herzog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1&quot;&gt;Eric Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quillen_D/0/1/0/all/0/1&quot;&gt;Deirdre Quillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holly_E/0/1/0/all/0/1&quot;&gt;Ethan Holly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalakrishnan_M/0/1/0/all/0/1&quot;&gt;Mrinal Kalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1&quot;&gt;Vincent Vanhoucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00275">
<title>Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera. (arXiv:1807.00275v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00275</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth completion, the technique of estimating a dense depth image from sparse
depth measurements, has a variety of applications in robotics and autonomous
driving. However, depth completion faces 3 main challenges: the irregularly
spaced pattern in the sparse depth input, the difficulty in handling multiple
sensor modalities (when color images are available), as well as the lack of
dense, pixel-level ground truth depth labels. In this work, we address all
these challenges. Specifically, we develop a deep regression model to learn a
direct mapping from sparse depth (and color images) to dense depth. We also
propose a self-supervised training framework that requires only sequences of
color and sparse depth images, without the need for dense depth labels. Our
experiments demonstrate that our network, when trained with semi-dense
annotations, attains state-of-the- art accuracy and is the winning approach on
the KITTI depth completion benchmark at the time of submission. Furthermore,
the self-supervised framework outperforms a number of existing solutions
trained with semi- dense annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fangchang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalheiro_G/0/1/0/all/0/1&quot;&gt;Guilherme Venturelli Cavalheiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1&quot;&gt;Sertac Karaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00867">
<title>Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access. (arXiv:1807.00867v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00867</link>
<description rdf:parseType="Literal">&lt;p&gt;A stochastic multi-user multi-armed bandit framework is used to develop
algorithms for uncoordinated spectrum access. In contrast to prior work, the
number of users is assumed to be unknown to each user and can possibly exceed
the number of channels. Also, in contrast to prior work, it is assumed that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
i.e., even when the number of users is less than the number of channels. The
algorithm is extended to the dynamic case where the number of users in the
system evolves over time and our algorithm leads to sub-linear regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bande_M/0/1/0/all/0/1&quot;&gt;Meghana Bande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeravalli_V/0/1/0/all/0/1&quot;&gt;Venugopal V. Veeravalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00882">
<title>Deep convolutional encoder-decoder networks for uncertainty quantification of dynamic multiphase flow in heterogeneous media. (arXiv:1807.00882v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00882</link>
<description rdf:parseType="Literal">&lt;p&gt;Surrogate strategies are used widely for uncertainty quantification of
groundwater models in order to improve computational efficiency. However, their
application to dynamic multiphase flow problems is hindered by the curse of
dimensionality, the saturation discontinuity due to capillarity effects, and
the time-dependence of the multi-output responses. In this paper, we propose a
deep convolutional encoder-decoder neural network methodology to tackle these
issues. The surrogate modeling task is transformed to an image-to-image
regression strategy. This approach extracts high-level coarse features from the
high-dimensional input permeability images using an encoder, and then refines
the coarse features to provide the output pressure/saturation images through a
decoder. A training strategy combining a regression loss and a segmentation
loss is proposed in order to better approximate the discontinuous saturation
field. To characterize the high-dimensional time-dependent outputs of the
dynamic system, time is treated as an additional input to the network that is
trained using pairs of input realizations and of the corresponding system
outputs at a limited number of time instances. The proposed method is evaluated
using a geological carbon storage process-based multiphase flow model with a
2500-dimensional stochastic permeability field. With a relatively small number
of training data, the surrogate model is capable of accurately characterizing
the spatio-temporal evolution of the pressure and discontinuous CO2 saturation
fields and can be used efficiently to compute the statistics of the system
responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shaoxing Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zabaras_N/0/1/0/all/0/1&quot;&gt;Nicholas Zabaras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jichun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00891">
<title>Optimality and Sub-optimality of PCA I: Spiked Random Matrix Models. (arXiv:1807.00891v1 [math.ST])</title>
<link>http://arxiv.org/abs/1807.00891</link>
<description rdf:parseType="Literal">&lt;p&gt;A central problem of random matrix theory is to understand the eigenvalues of
spiked random matrix models, introduced by Johnstone, in which a prominent
eigenvector (or &quot;spike&quot;) is planted into a random matrix. These distributions
form natural statistical models for principal component analysis (PCA) problems
throughout the sciences. Baik, Ben Arous and Peche showed that the spiked
Wishart ensemble exhibits a sharp phase transition asymptotically: when the
spike strength is above a critical threshold, it is possible to detect the
presence of a spike based on the top eigenvalue, and below the threshold the
top eigenvalue provides no information. Such results form the basis of our
understanding of when PCA can detect a low-rank signal in the presence of
noise. However, under structural assumptions on the spike, not all information
is necessarily contained in the spectrum. We study the statistical limits of
tests for the presence of a spike, including non-spectral tests. Our results
leverage Le Cam&apos;s notion of contiguity, and include:
&lt;/p&gt;
&lt;p&gt;i) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal
detection threshold for certain natural priors for the spike.
&lt;/p&gt;
&lt;p&gt;ii) For any non-Gaussian Wigner ensemble, PCA is sub-optimal for detection.
However, an efficient variant of PCA achieves the optimal threshold (for
natural priors) by pre-transforming the matrix entries.
&lt;/p&gt;
&lt;p&gt;iii) For the Gaussian Wishart ensemble, the PCA threshold is optimal for
positive spikes (for natural priors) but this is not always the case for
negative spikes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Perry_A/0/1/0/all/0/1&quot;&gt;Amelia Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wein_A/0/1/0/all/0/1&quot;&gt;Alexander S. Wein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bandeira_A/0/1/0/all/0/1&quot;&gt;Afonso S. Bandeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moitra_A/0/1/0/all/0/1&quot;&gt;Ankur Moitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00905">
<title>Learning under selective labels in the presence of expert consistency. (arXiv:1807.00905v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00905</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the problem of learning under selective labels in the context of
algorithm-assisted decision making. Selective labels is a pervasive selection
bias problem that arises when historical decision making blinds us to the true
outcome for certain instances. Examples of this are common in many
applications, ranging from predicting recidivism using pre-trial release data
to diagnosing patients. In this paper we discuss why selective labels often
cannot be effectively tackled by standard methods for adjusting for sample
selection bias, even if there are no unobservables. We propose a data
augmentation approach that can be used to either leverage expert consistency to
mitigate the partial blindness that results from selective labels, or to
empirically validate whether learning under such framework may lead to
unreliable models prone to systemic discrimination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1&quot;&gt;Maria De-Arteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1&quot;&gt;Artur Dubrawski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouldechova_A/0/1/0/all/0/1&quot;&gt;Alexandra Chouldechova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00939">
<title>A Deep Learning Based Illegal Insider-Trading Detection and Prediction Technique in Stock Market. (arXiv:1807.00939v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/1807.00939</link>
<description rdf:parseType="Literal">&lt;p&gt;The stock market is a nonlinear, nonstationary, dynamic, and complex system.
There are several factors that affect the stock market conditions, such as
news, social media, expert opinion, political transitions, and natural
disasters. In addition, the market must also be able to handle the situation of
illegal insider trading, which impacts the integrity and value of stocks.
Illegal insider trading occurs when trading is performed based on non-public
(private, leaked, tipped) information (e.g., new product launch, quarterly
financial report, acquisition or merger plan) before the information is made
public. Preventing illegal insider trading is a priority of the regulatory
authorities (e.g., SEC) as it involves billions of dollars, and is very
difficult to detect. In this work, we present different types of insider
trading approaches, techniques and our proposed approach for detecting and
predicting insider trader using a deep-learning based approach combined with
discrete signal processing on time series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Islam_S/0/1/0/all/0/1&quot;&gt;Sheikh Rabiul Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00944">
<title>Structure Learning of Markov Random Fields through Grow-Shrink Maximum Pseudolikelihood Estimation. (arXiv:1807.00944v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00944</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning the structure of Markov random fields (MRFs) plays an important role
in multivariate analysis. The importance has been increasing with the recent
rise of statistical relational models since the MRF serves as a building block
of these models such as Markov logic networks. There are two fundamental ways
to learn structures of MRFs: methods based on parameter learning and those
based on independence test. The former methods more or less assume certain
forms of distribution, so they potentially perform poorly when the assumption
is not satisfied. The latter can learn an MRF structure without a strong
distributional assumption, but sometimes it is unclear what objective function
is maximized/minimized in these methods. In this paper, we follow the latter,
but we explicitly define the optimization problem of MRF structure learning as
maximum pseudolikelihood estimation (MPLE) with respect to the edge set. As a
result, the proposed solution successfully deals with the {\em symmetricity} in
MRFs, whereas such symmetricity is not taken into account in most existing
independence test techniques. The proposed method achieved higher accuracy than
previous methods when there were asymmetric dependencies in our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takashina_Y/0/1/0/all/0/1&quot;&gt;Yuya Takashina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nakatani_S/0/1/0/all/0/1&quot;&gt;Shuyo Nakatani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Inoue_M/0/1/0/all/0/1&quot;&gt;Masato Inoue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01065">
<title>When Gaussian Process Meets Big Data: A Review of Scalable GPs. (arXiv:1807.01065v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.01065</link>
<description rdf:parseType="Literal">&lt;p&gt;The vast quantity of information brought by big data as well as the evolving
computer hardware encourages success stories in the machine learning community.
In the meanwhile, it poses challenges for the Gaussian process (GP), a
well-known non-parametric and interpretable Bayesian model, which suffers from
cubic complexity to training size. To improve the scalability while retaining
the desirable prediction quality, a variety of scalable GPs have been
presented. But they have not yet been comprehensively reviewed and discussed in
a unifying way in order to be well understood by both academia and industry. To
this end, this paper devotes to reviewing state-of-the-art scalable GPs
involving two main categories: global approximations which distillate the
entire data and local approximations which divide the data for subspace
learning. Particularly, for global approximations, we mainly focus on sparse
approximations comprising prior approximations which modify the prior but
perform exact inference, and posterior approximations which retain exact prior
but perform approximate inference; for local approximations, we highlight the
mixture/product of experts that conducts model averaging from multiple local
experts to boost predictions. To present a complete review, recent advances for
improving the scalability and model capability of scalable GPs are reviewed.
Finally, the extensions and open issues regarding the implementation of
scalable GPs in various scenarios are reviewed and discussed to inspire novel
ideas for future research avenues.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haitao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ong_Y/0/1/0/all/0/1&quot;&gt;Yew-Soon Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaobo Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01066">
<title>Behaviour Policy Estimation in Off-Policy Policy Evaluation: Calibration Matters. (arXiv:1807.01066v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01066</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we consider the problem of estimating a behaviour policy for
use in Off-Policy Policy Evaluation (OPE) when the true behaviour policy is
unknown. Via a series of empirical studies, we demonstrate how accurate OPE is
strongly dependent on the calibration of estimated behaviour policy models: how
precisely the behaviour policy is estimated from data. We show how powerful
parametric models such as neural networks can result in highly uncalibrated
behaviour policy models on a real-world medical dataset, and illustrate how a
simple, non-parametric, k-nearest neighbours model produces better calibrated
behaviour policy estimates and can be used to obtain superior importance
sampling-based OPE estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghu_A/0/1/0/all/0/1&quot;&gt;Aniruddh Raghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottesman_O/0/1/0/all/0/1&quot;&gt;Omer Gottesman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komorowski_M/0/1/0/all/0/1&quot;&gt;Matthieu Komorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1&quot;&gt;Aldo Faisal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma Brunskill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01069">
<title>Adversarial Robustness Toolbox v0.2.2. (arXiv:1807.01069v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01069</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples have become an indisputable threat to the security of
modern AI systems based on deep neural networks (DNNs). The Adversarial
Robustness Toolbox (ART) is a Python library designed to support researchers
and developers in creating novel defence techniques, as well as in deploying
practical defences of real-world AI systems. Researchers can use ART to
benchmark novel defences against the state-of-the-art. For developers, the
library provides interfaces which support the composition of comprehensive
defence systems using individual methods as building blocks. The Adversarial
Robustness Toolbox supports machine learning models (and deep neural networks
(DNNs) specifically) implemented in any of the most popular deep learning
frameworks (TensorFlow, Keras, PyTorch). Currently, the library is primarily
intended to improve the adversarial robustness of visual recognition systems,
however, future releases that will comprise adaptations to other data modes
(such as speech, text or time series) are envisioned. The ART source code is
released (https://github.com/IBM/adversarial-robustness-toolbox) under an MIT
license. The release includes code examples and extensive documentation
(&lt;a href=&quot;http://adversarial-robustness-toolbox.readthedocs.io&quot;&gt;this http URL&lt;/a&gt;) to help researchers and
developers get quickly started.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolae_M/0/1/0/all/0/1&quot;&gt;Maria-Irina Nicolae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinn_M/0/1/0/all/0/1&quot;&gt;Mathieu Sinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh Ngoc Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1&quot;&gt;Ambrish Rawat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1&quot;&gt;Martin Wistuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1&quot;&gt;Valentina Zantedeschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_I/0/1/0/all/0/1&quot;&gt;Ian M. Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_B/0/1/0/all/0/1&quot;&gt;Ben Edwards&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01085">
<title>One-Class Kernel Spectral Regression for Outlier Detection. (arXiv:1807.01085v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01085</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces a new efficient nonlinear one-class classifier
formulated as the Rayleigh quotient criterion. The method, operating in a
reproducing kernel Hilbert subspace, minimises the scatter of target
distribution along an optimal projection direction while at the same time
keeping projections of target observations as distant as possible from the
origin which serves as an artificial outlier with respect to the data. We
provide a graph embedding view of the problem which can then be solved
efficiently using the spectral regression approach. In this sense, unlike
previous similar methods which often require costly eigen-computations of dense
matrices, the proposed approach casts the problem under consideration into a
regression framework which avoids eigen-decomposition computations. In
particular, it is shown that the dominant complexity of the proposed method is
the complexity of computing the kernel matrix. Additional appealing
characteristics of the proposed one-class classifier are: 1-the ability to be
trained in an incremental fashion (allowing for application in streaming data
scenarios while also reducing computational complexity in the non-streaming
operation mode); 2-being unsupervised while also providing the ability for the
user to specify the expected fraction of outliers in the training set in
advance; And last but not least 3-the deployment of the kernel trick allowing
for a large class of functions by nonlinearly mapping the data into a
high-dimensional feature space. Extensive experiments conducted on several
datasets verifies the merits of the proposed approach in comparison with some
other alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arashloo_S/0/1/0/all/0/1&quot;&gt;Shervin Rahimzadeh Arashloo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01134">
<title>Welfare and Distributional Impacts of Fair Classification. (arXiv:1807.01134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01134</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methodologies in machine learning analyze the effects of various
statistical parity notions of fairness primarily in light of their impacts on
predictive accuracy and vendor utility loss. In this paper, we propose a new
framework for interpreting the effects of fairness criteria by converting the
constrained loss minimization problem into a social welfare maximization
problem. This translation moves a classifier and its output into utility space
where individuals, groups, and society at-large experience different welfare
changes due to classification assignments. Under this characterization,
predictions and fairness constraints are seen as shaping societal welfare and
distribution and revealing individuals&apos; implied welfare weights in
society--weights that may then be interpreted through a fairness lens. The
social welfare formulation of the fairness problem brings to the fore concerns
of distributive justice that have always had a central albeit more implicit
role in standard algorithmic fairness approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lily Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiling Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01176">
<title>Credit Default Mining Using Combined Machine Learning and Heuristic Approach. (arXiv:1807.01176v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01176</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting potential credit default accounts in advance is challenging.
Traditional statistical techniques typically cannot handle large amounts of
data and the dynamic nature of fraud and humans. To tackle this problem, recent
research has focused on artificial and computational intelligence based
approaches. In this work, we present and validate a heuristic approach to mine
potential default accounts in advance where a risk probability is precomputed
from all previous data and the risk probability for recent transactions are
computed as soon they happen. Beside our heuristic approach, we also apply a
recently proposed machine learning approach that has not been applied
previously on our targeted dataset [15]. As a result, we find that these
applied approaches outperform existing state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1&quot;&gt;Sheikh Rabiul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eberle_W/0/1/0/all/0/1&quot;&gt;William Eberle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghafoor_S/0/1/0/all/0/1&quot;&gt;Sheikh Khaled Ghafoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01280">
<title>On the Computational Power of Online Gradient Descent. (arXiv:1807.01280v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01280</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove that the evolution of weight vectors in online gradient descent can
encode arbitrary polynomial-space computations, even in the special case of
soft-margin support vector machines. Our results imply that, under weak
complexity-theoretic assumptions, it is impossible to reason efficiently about
the fine-grained behavior of online gradient descent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatziafratis_V/0/1/0/all/0/1&quot;&gt;Vaggos Chatziafratis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roughgarden_T/0/1/0/all/0/1&quot;&gt;Tim Roughgarden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Joshua R. Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01297">
<title>Generalizable Protein Interface Prediction with End-to-End Learning. (arXiv:1807.01297v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/1807.01297</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting how proteins interact with one another - that is, which surfaces
of one protein bind to which surfaces of another protein - is a central problem
in biology. Here we present Siamese Atomic Surfacelet Network (SASNet), the
first end-to-end learning method for protein interface prediction. Despite
using only spatial coordinates and identities of atoms as inputs, SASNet
outperforms state-of-the-art methods that rely on complex, hand-selected
features. These results are particularly striking because we train the method
entirely on a significantly biased data set that does not account for the fact
that proteins deform when binding to one another. Nonetheless, our network
maintains high performance, without retraining, when tested on real cases in
which proteins do deform. This suggests that it has learned fundamental
properties of protein structure and dynamics, which has important implications
for a variety of key problems related to biomolecular structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Townshend_R/0/1/0/all/0/1&quot;&gt;Raphael J. L. Townshend&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bedi_R/0/1/0/all/0/1&quot;&gt;Rishi Bedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dror_R/0/1/0/all/0/1&quot;&gt;Ron O. Dror&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01298">
<title>Generalized Bilinear Deep Convolutional Neural Networks for Multimodal Biometric Identification. (arXiv:1807.01298v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01298</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose to employ a bank of modality-dedicated
Convolutional Neural Networks (CNNs), fuse, train, and optimize them together
for person classification tasks. A modality-dedicated CNN is used for each
modality to extract modality-specific features. We demonstrate that, rather
than spatial fusion at the convolutional layers, the fusion can be performed on
the outputs of the fully-connected layers of the modality-specific CNNs without
any loss of performance and with significant reduction in the number of
parameters. We show that, using multiple CNNs with multimodal fusion at the
feature-level, we significantly outperform systems that use unimodal
representation. We study weighted feature, bilinear, and compact bilinear
feature-level fusion algorithms for multimodal biometric person identification.
Finally, We propose generalized compact bilinear fusion algorithm to deploy
both the weighted feature fusion and compact bilinear schemes. We provide the
results for the proposed algorithms on three challenging databases: CMU
Multi-PIE, BioCop, and BIOMDATA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1&quot;&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torfi_A/0/1/0/all/0/1&quot;&gt;Amirsina Torfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1&quot;&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1&quot;&gt;Nasser M. Nasrabadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.02588">
<title>Iterative proportional scaling revisited: a modern optimization perspective. (arXiv:1610.02588v4 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1610.02588</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper revisits the classic iterative proportional scaling (IPS) from a
modern optimization perspective. In contrast to the criticisms made in the
literature, we show that based on a coordinate descent characterization, IPS
can be slightly modified to deliver coefficient estimates, and from a
majorization-minimization standpoint, IPS can be extended to handle log-affine
models with features not necessarily binary-valued or nonnegative. Furthermore,
some state-of-the-art optimization techniques such as block-wise computation,
randomization and momentum-based acceleration can be employed to provide more
scalable IPS algorithms, as well as some regularized variants of IPS for
concurrent feature selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+She_Y/0/1/0/all/0/1&quot;&gt;Yiyuan She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shao Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04674">
<title>Model Criticism in Latent Space. (arXiv:1711.04674v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04674</link>
<description rdf:parseType="Literal">&lt;p&gt;Model criticism is usually carried out by assessing if replicated data
generated under the fitted model looks similar to the observed data, see e.g.
Gelman, Carlin, Stern, and Rubin [2004, p. 165]. This paper presents a method
for latent variable models by pulling back the data into the space of latent
variables, and carrying out model criticism in that space. Making use of a
model&apos;s structure enables a more direct assessment of the assumptions made in
the prior and likelihood. We demonstrate the method with examples of model
criticism in latent space applied to factor analysis, linear dynamical systems
and Gaussian processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seth_S/0/1/0/all/0/1&quot;&gt;Sohan Seth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1&quot;&gt;Iain Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Williams_C/0/1/0/all/0/1&quot;&gt;Christopher K. I. Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04846">
<title>State Space Gaussian Processes with Non-Gaussian Likelihood. (arXiv:1802.04846v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04846</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a comprehensive overview and tooling for GP modeling with
non-Gaussian likelihoods using state space methods. The state space formulation
allows for solving one-dimensional GP models in $\mathcal{O}(n)$ time and
memory complexity. While existing literature has focused on the connection
between GP regression and state space methods, the computational primitives
allowing for inference using general likelihoods in combination with the
Laplace approximation (LA), variational Bayes (VB), and assumed density
filtering (ADF, a.k.a. single-sweep expectation propagation, EP) schemes has
been largely overlooked. We present means of combining the efficient
$\mathcal{O}(n)$ state space methodology with existing inference methods. We
extend existing methods, and provide unifying code implementing all approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nickisch_H/0/1/0/all/0/1&quot;&gt;Hannes Nickisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grigorievskiy_A/0/1/0/all/0/1&quot;&gt;Alexander Grigorievskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12421">
<title>HOPF: Higher Order Propagation Framework for Deep Collective Classification. (arXiv:1805.12421v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12421</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a graph where every node has certain attributes associated with it and
some nodes have labels associated with them, Collective Classification (CC) is
the task of assigning labels to every unlabeled node using information from the
node as well as its neighbors. It is often the case that a node is not only
influenced by its immediate neighbors but also by higher order neighbors,
multiple hops away. Recent state-of-the-art models for CC learn end-to-end
differentiable variations of Weisfeiler-Lehman (WL) kernels to aggregate
multi-hop neighborhood information. In this work, we propose a Higher Order
Propagation Framework, HOPF, which provides an iterative inference mechanism
for these powerful differentiable kernels. Such combination of classical
iterative inference mechanism with recent differentiable kernels allows the
framework to learn graph convolutional filters that simultaneously exploit the
attribute and label information available in the neighborhood. Further, these
iterative differentiable kernels can scale to larger hops beyond the memory
limitations of existing differentiable kernels. We also show that existing WL
kernel-based models suffer from the problem of Node Information Morphing where
the information of the node is morphed or overwhelmed by the information of its
neighbors when considering multiple hops. To address this, we propose a
specific instantiation of HOPF, called the NIP models, which preserves the node
information at every propagation step. The iterative formulation of NIP models
further helps in incorporating distant hop information concisely as summaries
of the inferred labels. We do an extensive evaluation across 11 datasets from
different domains. We show that existing CC models do not provide consistent
performance, while the proposed NIP model with iterative inference is robust
with a minimal overall shortfall in performance across datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1&quot;&gt;Priyesh Vijayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandak_Y/0/1/0/all/0/1&quot;&gt;Yash Chandak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1&quot;&gt;Mitesh M. Khapra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1&quot;&gt;Balaraman Ravindran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00543">
<title>The Externalities of Exploration and How Data Diversity Helps Exploitation. (arXiv:1806.00543v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00543</link>
<description rdf:parseType="Literal">&lt;p&gt;Online learning algorithms, widely used to power search and content
optimization on the web, must balance exploration and exploitation, potentially
sacrificing the experience of current users for information that will lead to
better decisions in the future. Recently, concerns have been raised about
whether the process of exploration could be viewed as unfair, placing too much
burden on certain individuals or groups. Motivated by these concerns, we
initiate the study of the externalities of exploration - the undesirable side
effects that the presence of one party may impose on another - under the linear
contextual bandits model. We introduce the notion of a group externality,
measuring the extent to which the presence of one population of users impacts
the rewards of another. We show that this impact can in some cases be negative,
and that, in a certain sense, no algorithm can avoid it. We then study
externalities at the individual level, interpreting the act of exploration as
an externality imposed on the current user of a system by future users. This
drives us to ask under what conditions inherent diversity in the data makes
explicit exploration unnecessary. We build on a recent line of work on the
smoothed analysis of the greedy algorithm that always chooses the action that
currently looks optimal, improving on prior results to show that a greedy
approach almost matches the best possible Bayesian regret rate of any other
algorithm on the same problem instance whenever the diversity conditions hold,
and that this regret is at most $\tilde{O}(T^{1/3})$. Returning to group-level
effects, we show that under the same conditions, negative group externalities
essentially vanish under the greedy algorithm. Together, our results uncover a
sharp contrast between the high externalities that exist in the worst case, and
the ability to remove all externalities if the data is sufficiently diverse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_M/0/1/0/all/0/1&quot;&gt;Manish Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1&quot;&gt;Aleksandrs Slivkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1&quot;&gt;Jennifer Wortman Vaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09918">
<title>Hierarchical VampPrior Variational Fair Auto-Encoder. (arXiv:1806.09918v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09918</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision making is a process that is extremely prone to different biases. In
this paper we consider learning fair representations that aim at removing
nuisance (sensitive) information from the decision process. For this purpose,
we propose to use deep generative modeling and adapt a hierarchical Variational
Auto-Encoder to learn these fair representations. Moreover, we utilize the
mutual information as a useful regularizer for enforcing fairness of a
representation. In experiments on two benchmark datasets and two scenarios
where the sensitive variables are fully and partially observable, we show that
the proposed approach either outperforms or performs on par with the current
best model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botros_P/0/1/0/all/0/1&quot;&gt;Philip Botros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tomczak_J/0/1/0/all/0/1&quot;&gt;Jakub M. Tomczak&lt;/a&gt;</dc:creator>
</item></rdf:RDF>