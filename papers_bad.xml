<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05249"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05264"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05336"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00121"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05334"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05480"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04936"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11408"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.05238">
<title>AnatomyNet: Deep 3D Squeeze-and-excitation U-Nets for fast and fully automated whole-volume anatomical segmentation. (arXiv:1808.05238v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.05238</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiation therapy (RT) is a common treatment for head and neck (HaN) cancer
where therapists are often required to manually delineate boundaries of the
organs-at-risks (OARs). Automated head and neck anatomical segmentation
provides a way to speed up and improve the reproducibility of radiation therapy
planning. In this work, we propose the AnatomyNet, an end-to-end and atlas-free
three dimensional squeeze-and-excitation U-Net (3D SE U-Net), for fast and
fully automated whole-volume HaN anatomical segmentation.
&lt;/p&gt;
&lt;p&gt;There are two main challenges for fully automated HaN OARs segmentation: 1)
challenge in segmenting small anatomies (i.e., optic chiasm and optic nerves)
occupying only a few slices, and 2) training model with inconsistent data
annotations with missing ground truth for some anatomical structures because of
different RT planning. We propose the AnatomyNet that has one down-sampling
layer with the trade-off between GPU memory and feature representation
capacity, and 3D SE residual blocks for effective feature learning to alleviate
these challenges. Moreover, we design a hybrid loss function with the Dice loss
and the focal loss. The Dice loss is a class level distribution loss that
depends less on the number of voxels in the anatomy, and the focal loss is
designed to deal with highly unbalanced segmentation. For missing annotations,
we propose masked loss and weighted loss for accurate and balanced weights
updating in the learning of the AnatomyNet.
&lt;/p&gt;
&lt;p&gt;We collect 261 HaN CT images to train the AnatomyNet for segmenting nine
anatomies. Compared to previous state-of-the-art methods for each anatomy from
the MICCAI 2015 competition, the AnatomyNet increases Dice similarity
coefficient (DSC) by 3.3% on average. The proposed AnatomyNet takes only 0.12
seconds on average to segment a whole-volume HaN CT image of an average
dimension of 178x302x225.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1&quot;&gt;Nan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05566">
<title>The linear hidden subset problem for the (1+1) EA with scheduled and adaptive mutation rates. (arXiv:1808.05566v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.05566</link>
<description rdf:parseType="Literal">&lt;p&gt;We study unbiased $(1+1)$ evolutionary algorithms on linear functions with an
unknown number $n$ of bits with non-zero weight. Static algorithms achieve an
optimal runtime of $O(n (\ln n)^{2+\epsilon})$, however, it remained unclear
whether more dynamic parameter policies could yield better runtime guarantees.
We consider two setups: one where the mutation rate follows a fixed schedule,
and one where it may be adapted depending on the history of the run. For the
first setup, we give a schedule that achieves a runtime of $(1\pm o(1))\beta n
\ln n$, where $\beta \approx 3.552$, which is an asymptotic improvement over
the runtime of the static setup. Moreover, we show that no schedule admits a
better runtime guarantee and that the optimal schedule is essentially unique.
For the second setup, we show that the runtime can be further improved to
$(1\pm o(1)) e n \ln n$, which matches the performance of algorithms that know
$n$ in advance.
&lt;/p&gt;
&lt;p&gt;Finally, we study the related model of initial segment uncertainty with
static position-dependent mutation rates, and derive asymptotically optimal
lower bounds. This answers a question by Doerr, Doerr, and K\&quot;otzing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Einarsson_H/0/1/0/all/0/1&quot;&gt;Hafsteinn Einarsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauy_M/0/1/0/all/0/1&quot;&gt;Marcelo Matheus Gauy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengler_J/0/1/0/all/0/1&quot;&gt;Johannes Lengler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_F/0/1/0/all/0/1&quot;&gt;Florian Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mujika_A/0/1/0/all/0/1&quot;&gt;Asier Mujika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steger_A/0/1/0/all/0/1&quot;&gt;Angelika Steger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weissenberger_F/0/1/0/all/0/1&quot;&gt;Felix Weissenberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05249">
<title>LSTM-Based Goal Recognition in Latent Space. (arXiv:1808.05249v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.05249</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaches to goal recognition have progressively relaxed the requirements
about the amount of domain knowledge and available observations, yielding
accurate and efficient algorithms capable of recognizing goals. However, to
recognize goals in raw data, recent approaches require either human engineered
domain knowledge, or samples of behavior that account for almost all actions
being observed to infer possible goals. This is clearly too strong a
requirement for real-world applications of goal recognition, and we develop an
approach that leverages advances in recurrent neural networks to perform goal
recognition as a classification task, using encoded plan traces for training.
We empirically evaluate our approach against the state-of-the-art in goal
recognition with image-based domains, and discuss under which conditions our
approach is superior to previous ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amado_L/0/1/0/all/0/1&quot;&gt;Leonardo Amado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aires_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Paulo Aires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1&quot;&gt;Ramon Fraga Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magnaguagno_M/0/1/0/all/0/1&quot;&gt;Maur&amp;#xed;cio C. Magnaguagno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granada_R/0/1/0/all/0/1&quot;&gt;Roger Granada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneguzzi_F/0/1/0/all/0/1&quot;&gt;Felipe Meneguzzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05264">
<title>DeepDownscale: a Deep Learning Strategy for High-Resolution Weather Forecast. (arXiv:1808.05264v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05264</link>
<description rdf:parseType="Literal">&lt;p&gt;Running high-resolution physical models is computationally expensive and
essential for many disciplines. Agriculture, transportation, and energy are
sectors that depend on high-resolution weather models, which typically consume
many hours of large High Performance Computing (HPC) systems to deliver timely
results. Many users cannot afford to run the desired resolution and are forced
to use low resolution output. One simple solution is to interpolate results for
visualization. It is also possible to combine an ensemble of low resolution
models to obtain a better prediction. However, these approaches fail to capture
the redundant information and patterns in the low-resolution input that could
help improve the quality of prediction. In this paper, we propose and evaluate
a strategy based on a deep neural network to learn a high-resolution
representation from low-resolution predictions using weather forecast as a
practical use case. We take a supervised learning approach, since obtaining
labeled data can be done automatically. Our results show significant
improvement when compared with standard practices and the strategy is still
lightweight enough to run on modest computer systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_E/0/1/0/all/0/1&quot;&gt;Eduardo R. Rodrigues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_I/0/1/0/all/0/1&quot;&gt;Igor Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_R/0/1/0/all/0/1&quot;&gt;Renato L. F. Cunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netto_M/0/1/0/all/0/1&quot;&gt;Marco A. S. Netto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05322">
<title>Decision-Making with Belief Functions: a Review. (arXiv:1808.05322v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.05322</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaches to decision-making under uncertainty in the belief function
framework are reviewed. Most methods are shown to blend criteria for decision
under ignorance with the maximum expected utility principle of Bayesian
decision theory. A distinction is made between methods that construct a
complete preference relation among acts, and those that allow incomparability
of some acts due to lack of information. Methods developed in the imprecise
probability framework are applicable in the Dempster-Shafer context and are
also reviewed. Shafer&apos;s constructive decision theory, which substitutes the
notion of goal for that of utility, is described and contrasted with other
approaches. The paper ends by pointing out the need to carry out deeper
investigation of fundamental issues related to decision-making with belief
functions and to assess the descriptive, normative and prescriptive values of
the different approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denoeux_T/0/1/0/all/0/1&quot;&gt;Thierry Denoeux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05336">
<title>Simultaneous Localization And Mapping with depth Prediction using Capsule Networks for UAVs. (arXiv:1808.05336v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.05336</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an novel implementation of a simultaneous
localization and mapping (SLAM) system based on a monocular camera from an
unmanned aerial vehicle (UAV) using Depth prediction performed with Capsule
Networks (CapsNet), which possess improvements over the drawbacks of the more
widely-used Convolutional Neural Networks (CNN). An Extended Kalman Filter will
assist in estimating the position of the UAV so that we are able to update the
belief for the environment. Results will be evaluated on a benchmark dataset to
portray the accuracy of our intended approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1&quot;&gt;Sunil Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1&quot;&gt;Gaelan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05577">
<title>Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images. (arXiv:1808.05577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.05577</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we address the memory demands that come with the processing of
3-dimensional, high-resolution, multi-channeled medical images in deep
learning. We exploit memory-efficient backpropagation techniques, to reduce the
memory complexity of network training from being linear in the network&apos;s depth,
to being roughly constant $ - $ permitting us to elongate deep architectures
with negligible memory increase. We evaluate our methodology in the paradigm of
Image Quality Transfer, whilst noting its potential application to various
tasks that use deep learning. We study the impact of depth on accuracy and show
that deeper models have more predictive power, which may exploit larger
training sets. We obtain substantially better results than the previous
state-of-the-art model with a slight memory increase, reducing the
root-mean-squared-error by $ 13\% $. Our code is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blumberg_S/0/1/0/all/0/1&quot;&gt;Stefano B. Blumberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokkinos_I/0/1/0/all/0/1&quot;&gt;Iasonas Kokkinos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00121">
<title>A Missing Information Loss function for implicit feedback datasets. (arXiv:1805.00121v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00121</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent factor models for Recommender Systems with implicit feedback typically
treat unobserved user-item interactions (i.e. missing information) as negative
feedback. This is frequently done either through negative sampling (point--wise
loss) or with a ranking loss function (pair-- or list--wise estimation). Since
a zero preference recommendation is a valid solution for most common objective
functions, regarding unknown values as actual zeros results in users having a
zero preference recommendation for most of the available items. In this paper
we propose a novel objective function, the \emph{Missing Information Loss}
(MIL), that explicitly forbids treating unobserved user-item interactions as
positive or negative feedback. We apply this loss to both traditional Matrix
Factorization and user--based Denoising Autoencoder, and compare it with other
established objective functions such as cross-entropy (both point- and
pair-wise) or the recently proposed multinomial log-likelihood. MIL achieves
competitive performance in ranking-aware metrics when applied to three
datasets. Furthermore, we show that such a relevance in the recommendation is
obtained while displaying popular items less frequently (up to a $20 \%$
decrease with respect to the best competing method). This debiasing from the
recommendation of popular items favours the appearance of infrequent items (up
to a $50 \%$ increase of long-tail recommendations), a valuable feature for
Recommender Systems with a large catalogue of products.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arevalo_J/0/1/0/all/0/1&quot;&gt;Juan Ar&amp;#xe9;valo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duque_J/0/1/0/all/0/1&quot;&gt;Juan Ram&amp;#xf3;n Duque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creatura_M/0/1/0/all/0/1&quot;&gt;Marco Creatura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05240">
<title>Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks. (arXiv:1808.05240v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05240</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantized deep neural networks (QDNNs) are attractive due to their much lower
memory storage and faster inference speed than their regular full precision
counterparts. To maintain the same performance level especially at low
bit-widths, QDNNs must be retrained. Their training involves piecewise constant
activation functions and discrete weights, hence mathematical challenges arise.
We introduce the notion of coarse derivative and propose the blended coarse
gradient descent (BCGD) algorithm, for training fully quantized neural
networks. Coarse gradient is generally not a gradient of any function but an
artificial descent direction. The weight update of BCGD goes by coarse gradient
correction of a weighted average of the full precision weights and their
quantization (the so-called blending), which yields sufficient descent in the
objective value and thus accelerates the training. Our experiments demonstrate
that this simple blending technique is very effective for quantization at
extremely low bit-width such as binarization. In full quantization of ResNet-18
for ImageNet classification task, BCGD gives 64.36% top-1 accuracy with binary
weights across all layers and 4-bit adaptive activation. If the weights in the
first and last layers are kept in full precision, this number increases to
65.46%. As theoretical justification, we provide the convergence analysis of
coarse gradient descent for a two-layer neural network model with Gaussian
input data, and prove that the expected coarse gradient correlates positively
with the underlying true gradient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Penghang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1&quot;&gt;Stanley Osher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yingyong Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Jack Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05274">
<title>Frank-Wolfe Style Algorithms for Large Scale Optimization. (arXiv:1808.05274v1 [math.OC])</title>
<link>http://arxiv.org/abs/1808.05274</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a few variants on Frank-Wolfe style algorithms suitable for
large scale optimization. We show how to modify the standard Frank-Wolfe
algorithm using stochastic gradients, approximate subproblem solutions, and
sketched decision variables in order to scale to enormous problems while
preserving (up to constants) the optimal convergence rate
$\mathcal{O}(\frac{1}{k})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Lijun Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Udell_M/0/1/0/all/0/1&quot;&gt;Madeleine Udell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05334">
<title>Active Distribution Learning from Indirect Samples. (arXiv:1808.05334v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05334</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of {\em learning} the probability distribution
$P_X$ of a discrete random variable $X$ using indirect and sequential samples.
At each time step, we choose one of the possible $K$ functions, $g_1, \ldots,
g_K$ and observe the corresponding sample $g_i(X)$. The goal is to estimate the
probability distribution of $X$ by using a minimum number of such sequential
samples. This problem has several real-world applications including inference
under non-precise information and privacy-preserving statistical estimation. We
establish necessary and sufficient conditions on the functions $g_1, \ldots,
g_K$ under which asymptotically consistent estimation is possible. We also
derive lower bounds on the estimation error as a function of total samples and
show that it is order-wise achievable. Leveraging these results, we propose an
iterative algorithm that i) chooses the function to observe at each step based
on past observations; and ii) combines the obtained samples to estimate $p_X$.
The performance of this algorithm is investigated numerically under various
scenarios, and shown to outperform baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Samarth Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yagan_O/0/1/0/all/0/1&quot;&gt;Osman Ya&amp;#x11f;an&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05347">
<title>Tool Breakage Detection using Deep Learning. (arXiv:1808.05347v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05347</link>
<description rdf:parseType="Literal">&lt;p&gt;In manufacture, steel and other metals are mainly cut and shaped during the
fabrication process by computer numerical control (CNC) machines. To keep high
productivity and efficiency of the fabrication process, engineers need to
monitor the real-time process of CNC machines, and the lifetime management of
machine tools. In a real manufacturing process, breakage of machine tools
usually happens without any indication, this problem seriously affects the
fabrication process for many years. Previous studies suggested many different
approaches for monitoring and detecting the breakage of machine tools. However,
there still exists a big gap between academic experiments and the complex real
fabrication processes such as the high demands of real-time detections, the
difficulty in data acquisition and transmission. In this work, we use the
spindle current approach to detect the breakage of machine tools, which has the
high performance of real-time monitoring, low cost, and easy to install. We
analyze the features of the current of a milling machine spindle through tools
wearing processes, and then we predict the status of tool breakage by a
convolutional neural network(CNN). In addition, we use a BP neural network to
understand the reliability of the CNN. The results show that our CNN approach
can detect tool breakage with an accuracy of 93%, while the best performance of
BP is 80%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Duanbing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_A/0/1/0/all/0/1&quot;&gt;Anxing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuke Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junlin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05403">
<title>Nonconvex Regularization Based Sparse and Low-Rank Recovery in Signal Processing, Statistics, and Machine Learning. (arXiv:1808.05403v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1808.05403</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decade, sparse and low-rank recovery have drawn much attention in
many areas such as signal/image processing, statistics, bioinformatics and
machine learning. To achieve sparsity and/or low-rankness inducing, the
$\ell_1$ norm and nuclear norm are of the most popular regularization penalties
due to their convexity. While the $\ell_1$ and nuclear norm are convenient as
the related convex optimization problems are usually tractable, it has been
shown in many applications that a nonconvex penalty can yield significantly
better performance. In recent, nonconvex regularization based sparse and
low-rank recovery is of considerable interest and it in fact is a main driver
of the recent progress in nonconvex and nonsmooth optimization. This paper
gives an overview of this topic in various fields in signal processing,
statistics and machine learning, including compressive sensing (CS), sparse
regression and variable selection, sparse signals separation, sparse principal
component analysis (PCA), large covariance and inverse covariance matrices
estimation, matrix completion, and robust PCA. We present recent developments
of nonconvex regularization based sparse and low-rank recovery in these fields,
addressing the issues of penalty selection, applications and the convergence of
nonconvex algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1&quot;&gt;Fei Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1&quot;&gt;Lei Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peilin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1&quot;&gt;Robert C. Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05480">
<title>A novel Empirical Bayes with Reversible Jump Markov Chain in User-Movie Recommendation system. (arXiv:1808.05480v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05480</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we select the unknown dimension of the feature by re-
versible jump MCMC inside a simulated annealing in bayesian set up of
collaborative filter. We implement the same in MovieLens small dataset. We also
tune the hyper parameter by using a modified empirical bayes. It can also be
used to guess an initial choice for hyper-parameters in grid search procedure
even for the datasets where MCMC oscillates around the true value or takes long
time to converge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dey_A/0/1/0/all/0/1&quot;&gt;Arabin Kumar Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jhamb_H/0/1/0/all/0/1&quot;&gt;Himanshu Jhamb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05527">
<title>Deep Learning for Energy Markets. (arXiv:1808.05527v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05527</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning (DL) provides a methodology to predict extreme loads observed
in energy grids. Forecasting energy loads and prices is challenging due to
sharp peaks and troughs that arise from intraday system constraints due to
supply and demand fluctuations. We propose the use of deep spatio-temporal
models and extreme value theory (DL-EVT) to capture the tail behavior of load
spikes. Deep architectures such as ReLU and LSTM can model generation trends
and temporal dependencies, while EVT captures highly volatile load spikes. To
illustrate our methodology, we use hourly price and demand data from the PJM
interconnection, for 4719 nodes and develop deep predictor. DL-EVT outperforms
traditional Fourier and time series methods, both in-and out-of-sample, by
capturing the nonlinearities in prices. Finally, we conclude with directions
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polson_M/0/1/0/all/0/1&quot;&gt;Michael Polson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sokolov_V/0/1/0/all/0/1&quot;&gt;Vadim Sokolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05535">
<title>Combining time-series and textual data for taxi demand prediction in event areas: a deep learning approach. (arXiv:1808.05535v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05535</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate time-series forecasting is vital for numerous areas of application
such as transportation, energy, finance, economics, etc. However, while modern
techniques are able to explore large sets of temporal data to build forecasting
models, they typically neglect valuable information that is often available
under the form of unstructured text. Although this data is in a radically
different format, it often contains contextual explanations for many of the
patterns that are observed in the temporal data. In this paper, we propose two
deep learning architectures that leverage word embeddings, convolutional layers
and attention mechanisms for combining text information with time-series data.
We apply these approaches for the problem of taxi demand forecasting in event
areas. Using publicly available taxi data from New York, we empirically show
that by fusing these two complementary cross-modal sources of information, the
proposed models are able to significantly reduce the error in the forecasts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodrigues_F/0/1/0/all/0/1&quot;&gt;Filipe Rodrigues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Markou_I/0/1/0/all/0/1&quot;&gt;Ioulia Markou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pereira_F/0/1/0/all/0/1&quot;&gt;Francisco Pereira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05537">
<title>Distributionally Adversarial Attack. (arXiv:1808.05537v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05537</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on adversarial attack has shown that Projected Gradient Descent
(PGD) Adversary is a universal first-order adversary, and the classifier
adversarially trained by PGD is robust against a wide range of first-order
attacks. However, it is worth noting that the objective of an attacking/defense
model relies on a data distribution, typically in the form of risk
maximization/minimization: $\max\!/\!\min \mathbb{E}_{p(\mathbf{x})}
\mathcal{L}(\mathbf{x})$, with $p(\mathbf{x})$ the data distribution and
$\mathcal{L}(\cdot)$ a loss function. While PGD generates attack samples
independently for each data point, the procedure does not necessary lead to
good generalization in terms of risk maximization. In the paper, we achieve the
goal by proposing distributionally adversarial attack (DAA), a framework to
solve an optimal {\em adversarial data distribution}, a perturbed distribution
that is close to the original data distribution but increases the
generalization risk maximally. Algorithmically, DAA performs optimization on
the space of probability measures, which introduces direct dependency between
all data points when generating adversarial samples. DAA is evaluated by
attacking state-of-the-art defense models, including the adversarially trained
models provided by MadryLab. Notably, DAA outperforms all the attack algorithms
listed in MadryLab&apos;s white-box leaderboard, reducing the accuracy of their
secret MNIST model to $88.79\%$ (with $l_\infty$ perturbations of $\epsilon =
0.3$) and the accuracy of their secret CIFAR model to $44.73\%$ (with
$l_\infty$ perturbations of $\epsilon = 8.0$). Code for the experiments is
released on https://github.com/tianzheng4/Distributionally-Adversarial-Attack
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianhang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kui Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09055">
<title>Solving for multi-class using orthogonal coding matrices. (arXiv:1801.09055v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09055</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a method of solving for the conditional probabilities in
multi-class classification using orthogonal error-correcting codes. The method
is tested on six different datasets using support vector machines as the binary
classifiers and both the classification results as well as the probability
estimates are found to be more accurate than using random coding matrices, as
predicted by recent literature. Probability estimates are desirable in
statistical classification both for gauging the accuracy of a classification
result and for calibration. Probability estimation using orthogonal codes is
simple and elegant making it faster than the more general constrained
optimization solutions required for arbitrary codes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mills_P/0/1/0/all/0/1&quot;&gt;Peter Mills&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11271">
<title>Gaussian Process Behaviour in Wide Deep Neural Networks. (arXiv:1804.11271v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.11271</link>
<description rdf:parseType="Literal">&lt;p&gt;Whilst deep neural networks have shown great empirical success, there is
still much work to be done to understand their theoretical properties. In this
paper, we study the relationship between random, wide, fully connected,
feedforward networks with more than one hidden layer and Gaussian processes
with a recursive kernel definition. We show that, under broad conditions, as we
make the architecture increasingly wide, the implied random function converges
in distribution to a Gaussian process, formalising and extending existing
results by Neal (1996) to deep networks. To evaluate convergence rates
empirically, we use maximum mean discrepancy. We then compare finite Bayesian
deep networks from the literature to Gaussian processes in terms of the key
predictive quantities of interest, finding that in some cases the agreement can
be very close. We discuss the desirability of Gaussian process behaviour and
review non-Gaussian alternative models from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matthews_A/0/1/0/all/0/1&quot;&gt;Alexander G. de G. Matthews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1&quot;&gt;Mark Rowland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hron_J/0/1/0/all/0/1&quot;&gt;Jiri Hron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04936">
<title>Non-Gaussian Component Analysis using Entropy Methods. (arXiv:1807.04936v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04936</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-Gaussian component analysis (NGCA) is a problem in multidimensional data
analysis. Since its formulation in 2006, NGCA has attracted considerable
attention in statistics and machine learning. In this problem, we have a random
variable $X$ in $n$-dimensional Euclidean space. There is an unknown subspace
$U$ of the $n$-dimensional Euclidean space such that the orthogonal projection
of $X$ onto $U$ is standard multidimensional Gaussian and the orthogonal
projection of $X$ onto $V$, the orthogonal complement of $U$, is non-Gaussian,
in the sense that all its one-dimensional marginals are different from the
Gaussian in a certain metric defined in terms of moments. The NGCA problem is
to approximate the non-Gaussian subspace $V$ given samples of $X$.
&lt;/p&gt;
&lt;p&gt;Vectors in $V$ corresponds to &quot;interesting&quot; directions, whereas vectors in
$U$ correspond to the directions where data is very noisy. The most interesting
applications of the NGCA model is for the case when the magnitude of the noise
is comparable to that of the true signal, a setting in which traditional noise
reduction techniques such as PCA don&apos;t apply directly. NGCA is also related to
dimensionality reduction and to other data analysis problems such as ICA.
NGCA-like problems have been studied in statistics for a long time using
techniques such as projection pursuit.
&lt;/p&gt;
&lt;p&gt;We give an algorithm that takes polynomial time in the dimension $n$ and has
an inverse polynomial dependence on the error parameter measuring the angle
distance between the non-Gaussian subspace and the subspace output by the
algorithm. Our algorithm is based on relative entropy as the contrast function
and fits under the projection pursuit framework. The techniques we develop for
analyzing our algorithm maybe of use for other related problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1&quot;&gt;Navin Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1&quot;&gt;Abhishek Shetty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10588">
<title>A Modality-Adaptive Method for Segmenting Brain Tumors and Organs-at-Risk in Radiation Therapy Planning. (arXiv:1807.10588v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10588</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a method for simultaneously segmenting brain tumors
and an extensive set of organs-at-risk for radiation therapy planning of
glioblastomas. The method combines a contrast-adaptive generative model for
whole-brain segmentation with a new spatial regularization model of tumor shape
using convolutional restricted Boltzmann machines. We demonstrate
experimentally that the method is able to adapt to image acquisitions that
differ substantially from any available training data, ensuring its
applicability across treatment sites; that its tumor segmentation accuracy is
comparable to that of the current state of the art; and that it captures most
organs-at-risk sufficiently well for radiation therapy planning purposes. The
proposed method may be a valuable step towards automating the delineation of
brain tumors and organs-at-risk in glioblastoma patients undergoing radiation
therapy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agn_M/0/1/0/all/0/1&quot;&gt;Mikael Agn&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenschold_P/0/1/0/all/0/1&quot;&gt;Per Munck af Rosensch&amp;#xf6;ld&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puonti_O/0/1/0/all/0/1&quot;&gt;Oula Puonti&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lundemann_M/0/1/0/all/0/1&quot;&gt;Michael J. Lundemann&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mancini_L/0/1/0/all/0/1&quot;&gt;Laura Mancini&lt;/a&gt; (5 and 6), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadaki_A/0/1/0/all/0/1&quot;&gt;Anastasia Papadaki&lt;/a&gt; (5 and 6), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thust_S/0/1/0/all/0/1&quot;&gt;Steffi Thust&lt;/a&gt; (5 and 6), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashburner_J/0/1/0/all/0/1&quot;&gt;John Ashburner&lt;/a&gt; (7), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Law_I/0/1/0/all/0/1&quot;&gt;Ian Law&lt;/a&gt; (8), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leemput_K/0/1/0/all/0/1&quot;&gt;Koen Van Leemput&lt;/a&gt; (1 and 9) ((1) Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark, (2) Radiation Physics, Department of Hematology, Oncology and Radiation Physics, Sk&amp;#xe5;ne University Hospital, Lund, Sweden, (3) Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre, Denmark, (4) Department of Oncology, Copenhagen University Hospital Rigshospitalet, Denmark, (5) Neuroradiological Academic Unit, Department of Brain Repair and Rehabilitation, UCL Institute of Neurology, University College London, UK, (6) Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS Foundation Trust, UK, (7) Wellcome Centre for Human Neuroimaging, UCL Institute of Neurology, University College London, UK, (8) Department of Clinical Physiology, Nuclear Medicine and PET, Copenhagen University Hospital Rigshospitalet, Denmark, (9) Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Harvard Medical School, USA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03300">
<title>$\alpha$-Approximation Density-based Clustering of Multi-valued Objects. (arXiv:1808.03300v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03300</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-valued data are commonly found in many real applications. During the
process of clustering multi-valued data, most existing methods use sampling or
aggregation mechanisms that cannot reflect the real distribution of objects and
their instances and thus fail to obtain high-quality clusters. In this paper, a
concept of $\alpha$-approximation distance is introduced to measure the
connectivity between multi-valued objects by taking account of the distribution
of the instances. An $\alpha$-approximation density-based clustering algorithm
(DBCMO) is proposed to efficiently cluster the multi-valued objects by using
global and local R* tree structures. To speed up the algorithm, four pruning
rules on the tree structures are implemented. Empirical studies on synthetic
and real datasets demonstrate that DBCMO can efficiently and effectively
discover the multi-valued object clusters. A comparison with two existing
methods further shows that DBCMO can better handle a continuous decrease in the
cluster density and detect clusters of varying density.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03333">
<title>Linked Causal Variational Autoencoder for Inferring Paired Spillover Effects. (arXiv:1808.03333v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03333</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling spillover effects from observational data is an important problem in
economics, business, and other fields of research. % It helps us infer the
causality between two seemingly unrelated set of events. For example, if
consumer spending in the United States declines, it has spillover effects on
economies that depend on the U.S. as their largest export market. In this
paper, we aim to infer the causation that results in spillover effects between
pairs of entities (or units), we call this effect as \textit{paired spillover}.
To achieve this, we leverage the recent developments in variational inference
and deep learning techniques to propose a generative model called Linked Causal
Variational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA
incorporates an encoder neural network to learn the latent attributes and a
decoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the
\textit{latent attributes as confounders that are assumed to affect both the
treatment and the outcome of units}. Specifically, given a pair of units $u$
and $\bar{u}$, their individual treatment and outcomes, the encoder network of
LCVA samples the confounders by conditioning on the observed covariates of $u$,
the treatments of both $u$ and $\bar{u}$ and the outcome of $u$. Once inferred,
the latent attributes (or confounders) of $u$ captures the spillover effect of
$\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde
(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that
LCVA is significantly more robust than existing methods in capturing spillover
effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakesh_V/0/1/0/all/0/1&quot;&gt;Vineeth Rakesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Ruocheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1&quot;&gt;Raha Moraffah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Nitin Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11408">
<title>Local Linear Forests. (arXiv:1807.11408v1 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.11408</link>
<description rdf:parseType="Literal">&lt;p&gt;Random forests are a powerful method for non-parametric regression, but are
limited in their ability to fit smooth signals, and can show poor predictive
performance in the presence of strong, smooth effects. Taking the perspective
of random forests as an adaptive kernel method, we pair the forest kernel with
a local linear regression adjustment to better capture smoothness. The
resulting procedure, local linear forests, enables us to improve on asymptotic
rates of convergence for random forests with smooth signals, and provides
substantial gains in accuracy on both real and simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Friedberg_R/0/1/0/all/0/1&quot;&gt;Rina Friedberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tibshirani_J/0/1/0/all/0/1&quot;&gt;Julie Tibshirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wager_S/0/1/0/all/0/1&quot;&gt;Stefan Wager&lt;/a&gt;</dc:creator>
</item></rdf:RDF>