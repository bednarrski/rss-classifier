<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-22T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.04231"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11443"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07569"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07610"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07665"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07741"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07879"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.03764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.00960"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.06603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.04812"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04932"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06732"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.07627">
<title>Rapid Time Series Prediction with a Hardware-Based Reservoir Computer. (arXiv:1807.07627v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07627</link>
<description rdf:parseType="Literal">&lt;p&gt;Reservoir computing is a neural network approach for processing
time-dependent signals that has seen rapid development in recent years.
Physical implementations of the technique using optical reservoirs have
demonstrated remarkable accuracy and processing speed at benchmark tasks.
However, these approaches require an electronic output layer to maintain high
performance, which limits their use in tasks such as time-series prediction,
where the output is fed back into the reservoir. We present here a reservoir
computing scheme that has rapid processing speed both by the reservoir and the
output layer. The reservoir is realized by an autonomous, time-delay, Boolean
network configured on a field-programmable gate array. We investigate the
dynamical properties of the network and observe the fading memory property that
is critical for successful reservoir computing. We demonstrate the utility of
the technique by training a reservoir to learn the short- and long-term
behavior of a chaotic system. We find accuracy comparable to state-of-the-art
software approaches of similar network size, but with a superior real-time
prediction rate up to 160 MHz.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canaday_D/0/1/0/all/0/1&quot;&gt;Daniel Canaday&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffith_A/0/1/0/all/0/1&quot;&gt;Aaron Griffith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1&quot;&gt;Daniel Gauthier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07839">
<title>Distance-based Kernels for Surrogate Model-based Neuroevolution. (arXiv:1807.07839v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.07839</link>
<description rdf:parseType="Literal">&lt;p&gt;The topology optimization of artificial neural networks can be particularly
difficult if the fitness evaluations require expensive experiments or
simulations. For that reason, the optimization methods may need to be supported
by surrogate models. We propose different distances for a suitable surrogate
model, and compare them in a simple numerical test scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stork_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Stork&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1&quot;&gt;Martin Zaefferer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1&quot;&gt;Thomas Bartz-Beielstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07868">
<title>The Deep Kernelized Autoencoder. (arXiv:1807.07868v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07868</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoencoders learn data representations (codes) in such a way that the input
is reproduced at the output of the network. However, it is not always clear
what kind of properties of the input data need to be captured by the codes.
Kernel machines have experienced great success by operating via inner-products
in a theoretically well-defined reproducing kernel Hilbert space, hence
capturing topological properties of input data. In this paper, we enhance the
autoencoder&apos;s ability to learn effective data representations by aligning inner
products between codes with respect to a kernel matrix. By doing so, the
proposed kernelized autoencoder allows learning similarity-preserving
embeddings of input data, where the notion of similarity is explicitly
controlled by the user and encoded in a positive semi-definite kernel matrix.
Experiments are performed for evaluating both reconstruction and kernel
alignment performance in classification tasks and visualization of
high-dimensional data. Additionally, we show that our method is capable to
emulate kernel principal component analysis on a denoising task, obtaining
competitive results at a much lower computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kampffmeyer_M/0/1/0/all/0/1&quot;&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lokse_S/0/1/0/all/0/1&quot;&gt;Sigurd L&amp;#xf8;kse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bianchi_F/0/1/0/all/0/1&quot;&gt;Filippo M. Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Livi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Livi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.04231">
<title>Identity Matters in Deep Learning. (arXiv:1611.04231v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1611.04231</link>
<description rdf:parseType="Literal">&lt;p&gt;An emerging design principle in deep learning is that each layer of a deep
artificial neural network should be able to easily express the identity
transformation. This idea not only motivated various normalization techniques,
such as \emph{batch normalization}, but was also key to the immense success of
\emph{residual networks}.
&lt;/p&gt;
&lt;p&gt;In this work, we put the principle of \emph{identity parameterization} on a
more solid theoretical footing alongside further empirical progress. We first
give a strikingly simple proof that arbitrarily deep linear residual networks
have no spurious local optima. The same result for linear feed-forward networks
in their standard parameterization is substantially more delicate. Second, we
show that residual networks with ReLu activations have universal finite-sample
expressivity in the sense that the network can represent any function of its
sample provided that the model has more parameters than the sample size.
&lt;/p&gt;
&lt;p&gt;Directly inspired by our theory, we experiment with a radically simple
residual architecture consisting of only residual convolutional layers and ReLu
activations, but no batch normalization, dropout, or max pool. Our model
improves significantly on previous all-convolutional networks on the CIFAR10,
CIFAR100, and ImageNet classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1&quot;&gt;Moritz Hardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07896">
<title>Towards a general mathematical theory of experimental science. (arXiv:1807.07896v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.07896</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we lay the groundwork for a general mathematical theory of
experimental science. The starting point will be the notion of verifiable
statements, those assertions that can be shown to be true with an experimental
test. We study the algebra of such objects and show how it is closed only under
finite conjunction and countable disjunction. With simple constructions, we
show that the set of possible cases distinguishable by verifiable statements is
equipped with a natural Kolmogorov and second countable topology and a natural
$\sigma$-algebra. This gives a clear physical meaning to those mathematical
structures and provides a strong justification for their use in science. It is
our hope and belief that such an approach can be extended to many areas of
fundamental physics and will provide a consistent vocabulary across scientific
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carcassi_G/0/1/0/all/0/1&quot;&gt;Gabriele Carcassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aidala_C/0/1/0/all/0/1&quot;&gt;Christine A. Aidala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07957">
<title>Decentralized Task Allocation in Multi-Robot Systems via Bipartite Graph Matching Augmented with Fuzzy Clustering. (arXiv:1807.07957v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1807.07957</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic systems, working together as a team, are becoming valuable players in
different real-world applications, from disaster response to warehouse
fulfillment services. Centralized solutions for coordinating multi-robot teams
often suffer from poor scalability and vulnerability to communication
disruptions. This paper develops a decentralized multi-agent task allocation
(Dec-MATA) algorithm for multi-robot applications. The task planning problem is
posed as a maximum-weighted matching of a bipartite graph, the solution of
which using the blossom algorithm allows each robot to autonomously identify
the optimal sequence of tasks it should undertake. The graph weights are
determined based on a soft clustering process, which also plays a problem
decomposition role seeking to reduce the complexity of the individual-agents&apos;
task assignment problems. To evaluate the new Dec-MATA algorithm, a series of
case studies (of varying complexity) are performed, with tasks being
distributed randomly over an observable 2D environment. A centralized approach,
based on a state-of-the-art MILP formulation of the multi-Traveling Salesman
problem is used for comparative analysis. While getting within 7-28% of the
optimal cost obtained by the centralized algorithm, the Dec-MATA algorithm is
found to be 1-3 orders of magnitude faster and minimally sensitive to
task-to-robot ratios, unlike the centralized algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_P/0/1/0/all/0/1&quot;&gt;Payam Ghassemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Souma Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11443">
<title>ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases. (arXiv:1711.11443v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11443</link>
<description rdf:parseType="Literal">&lt;p&gt;ConvNets and Imagenet have driven the recent success of deep learning for
image classification. However, the marked slowdown in performance improvement
combined with the lack of robustness of neural networks to adversarial examples
and their tendency to exhibit undesirable biases question the reliability of
these methods. This work investigates these questions from the perspective of
the end-user by using human subject studies and explanations. The contribution
of this study is threefold. We first experimentally demonstrate that the
accuracy and robustness of ConvNets measured on Imagenet are vastly
underestimated. Next, we show that explanations can mitigate the impact of
misclassified adversarial examples from the perspective of the end-user. We
finally introduce a novel tool for uncovering the undesirable biases learned by
a model. These contributions also show that explanations are a valuable tool
both for improving our understanding of ConvNets&apos; predictions and for designing
more reliable models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1&quot;&gt;Pierre Stock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cisse_M/0/1/0/all/0/1&quot;&gt;Moustapha Cisse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06918">
<title>RARD II: The 2nd Related-Article Recommendation Dataset. (arXiv:1807.06918v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06918</link>
<description rdf:parseType="Literal">&lt;p&gt;The main contribution of this paper is to introduce and describe a new
recommender-systems dataset (RARD II). It is based on data from a
recommender-system in the digital library and reference management software
domain. As such, it complements datasets from other domains such as books,
movies, and music. The RARD II dataset encompasses 89m recommendations,
covering an item-space of 24m unique items. RARD II provides a range of rich
recommendation data, beyond conventional ratings. For example, in addition to
the usual ratings matrices, RARD II includes the original recommendation logs,
which provide a unique insight into many aspects of the algorithms that
generated the recommendations. In this paper, we summarise the key features of
this dataset release, describing how it was generated and discussing some of
its unique features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1&quot;&gt;Joeran Beel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyth_B/0/1/0/all/0/1&quot;&gt;Barry Smyth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_A/0/1/0/all/0/1&quot;&gt;Andrew Collins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07569">
<title>Fully Convolutional Pixel Adaptive Image Denoiser. (arXiv:1807.07569v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.07569</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new denoising algorithm, dubbed as Fully Convolutional Adaptive
Image DEnoiser (FC-AIDE), that can learn from offline supervised training set
with a fully convolutional neural network architecture as well as adaptively
fine-tune the denoiser for each given noisy image. We mainly follow the
framework of the recently proposed Neural AIDE, which formulates the denoiser
to be context-based pixelwise affine mappings and utilizes the unbiased
estimator of MSE of such denoisers. The three main contributions we make to
significantly improve upon the original Neural AIDE are the followings; 1)
implementing a novel fully convolutional architecture that boosts the base
supervised model, 2) introducing data augmentation for adaptive fine-tuning to
achieve much stronger adaptivity, and 3) proposing an effective unknown noise
level estimation method. As a result, FC-AIDE is shown to significantly
outperform the state-of-the-art CNN-based denoisers on two standard benchmark
dataset as well as on a much challenging blind denoising dataset, in which
nothing is known about the noise level, noise distribution, or image
characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1&quot;&gt;Sungmin Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1&quot;&gt;Taesup Moon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07603">
<title>Doubly Stochastic Adversarial Autoencoder. (arXiv:1807.07603v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07603</link>
<description rdf:parseType="Literal">&lt;p&gt;Any autoencoder network can be turned into a generative model by imposing an
arbitrary prior distribution on its hidden code vector. Variational Autoencoder
(VAE) [2] uses a KL divergence penalty to impose the prior, whereas Adversarial
Autoencoder (AAE) [1] uses {\it generative adversarial networks} GAN [3]. GAN
trades the complexities of {\it sampling} algorithms with the complexities of
{\it searching} Nash equilibrium in minimax games. Such minimax architectures
get trained with the help of data examples and gradients flowing through a
generator and an adversary. A straightforward modification of AAE is to replace
the adversary with the maximum mean discrepancy (MMD) test [4-5]. This
replacement leads to a new type of probabilistic autoencoder, which is also
discussed in our paper. We propose a novel probabilistic autoencoder in which
the adversary of AAE is replaced with a space of {\it stochastic} functions.
This replacement introduces a new source of randomness, which can be considered
as a continuous control for encouraging {\it explorations}. This prevents the
adversary from fitting too closely to the generator and therefore leads to a
more diverse set of generated samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azarafrooz_M/0/1/0/all/0/1&quot;&gt;Mahdi Azarafrooz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07610">
<title>Unrolling Swiss Cheese: Metric repair on manifolds with holes. (arXiv:1807.07610v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07610</link>
<description rdf:parseType="Literal">&lt;p&gt;For many machine learning tasks, the input data lie on a low-dimensional
manifold embedded in a high dimensional space and, because of this
high-dimensional struc- ture, most algorithms inefficient. The typical solution
is to reduce the dimension of the input data using a standard dimension
reduction algorithms such as ISOMAP, LAPLACIAN EIGENMAPS or LLES. This
approach, however, does not always work in practice as these algorithms require
that we have somewhat ideal data. Unfortunately, most data sets either have
missing entries or unacceptably noisy values. That is, real data are far from
ideal and we cannot use these algorithms directly. In this paper, we focus on
the case when we have missing data. Some techniques, such as matrix completion,
can be used to fill in missing data but these methods do not capture the
non-linear structure of the manifold. Here, we present a new algorithm
MR-MISSING that extends these previous algorithms and can be used to compute
low dimensional representation on data sets with missing entries. We
demonstrate the effectiveness of our algorithm by running three different
experiments. We visually verify the effectiveness of our algorithm on synthetic
manifolds, we numerically compare our projections against those computed by
first filling in data using nlPCA and mDRUR on the MNIST data set, and we also
show that we can do classification on MNIST with missing data. We also provide
a theoretical guarantee for MR-MISSING under some simplifying assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Anna C. Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonthalia_R/0/1/0/all/0/1&quot;&gt;Rishi Sonthalia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07612">
<title>Adaptive Variational Particle Filtering in Non-stationary Environments. (arXiv:1807.07612v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07612</link>
<description rdf:parseType="Literal">&lt;p&gt;Online convex optimization is a sequential prediction framework with the goal
to track and adapt to the environment through evaluating proper convex loss
functions. We study efficient particle filtering methods from the perspective
of such a framework.
&lt;/p&gt;
&lt;p&gt;We formulate an efficient particle filtering methods for the non-stationary
environment by making connections with the online mirror descent algorithm
which is known to be a universal online convex optimization algorithm.
&lt;/p&gt;
&lt;p&gt;As a result of this connection, our proposed particle filtering algorithm
proves to achieve optimal particle efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azarafrooz_M/0/1/0/all/0/1&quot;&gt;Mahdi Azarafrooz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07621">
<title>Approximate Collapsed Gibbs Clustering with Expectation Propagation. (arXiv:1807.07621v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07621</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a framework for approximating collapsed Gibbs sampling in
generative latent variable cluster models. Collapsed Gibbs is a popular MCMC
method, which integrates out variables in the posterior to improve mixing.
Unfortunately for many complex models, integrating out these variables is
either analytically or computationally intractable. We efficiently approximate
the necessary collapsed Gibbs integrals by borrowing ideas from expectation
propagation. We present two case studies where exact collapsed Gibbs sampling
is intractable: mixtures of Student-t&apos;s and time series clustering. Our
experiments on real and synthetic data show that our approximate sampler
enables a runtime-accuracy tradeoff in sampling these types of models,
providing results with competitive accuracy much more rapidly than the naive
Gibbs samplers one would otherwise rely on in these scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aicher_C/0/1/0/all/0/1&quot;&gt;Christopher Aicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily B. Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07623">
<title>An Optimal Algorithm for Stochastic and Adversarial Bandits. (arXiv:1807.07623v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07623</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide an algorithm that achieves the optimal (up to constants) finite
time regret in both adversarial and stochastic multi-armed bandits without
prior knowledge of the regime and time horizon. The result provides a negative
answer to the open problem of whether extra price has to be paid for the lack
of information about the adversariality/stochasticity of the environment. We
provide a complete characterization of online mirror descent algorithms based
on Tsallis entropy and show that the power ${\alpha} = \frac{1}{2}$ achieves
the goal. In addition, the proposed algorithm enjoys improved regret guarantees
in two intermediate regimes: the moderately contaminated stochastic regime
defined by Seldin and Slivkins (2014) and the stochastically constrained
adversary studied by Wei and Luo (2018). The algorithm also obtains adversarial
and stochastic optimality in the utility-based dueling bandit setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmert_J/0/1/0/all/0/1&quot;&gt;Julian Zimmert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seldin_Y/0/1/0/all/0/1&quot;&gt;Yevgeny Seldin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07663">
<title>Automatically Designing CNN Architectures for Medical Image Segmentation. (arXiv:1807.07663v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07663</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network architectures have traditionally been designed and
explored with human expertise in a long-lasting trial-and-error process. This
process requires huge amount of time, expertise, and resources. To address this
tedious problem, we propose a novel algorithm to optimally find hyperparameters
of a deep network architecture automatically. We specifically focus on
designing neural architectures for medical image segmentation task. Our
proposed method is based on a policy gradient reinforcement learning for which
the reward function is assigned a segmentation evaluation utility (i.e., dice
index). We show the efficacy of the proposed method with its low computational
cost in comparison with the state-of-the-art medical image segmentation
networks. We also present a new architecture design, a densely connected
encoder-decoder CNN, as a strong baseline architecture to apply the proposed
hyperparameter search algorithm. We apply the proposed algorithm to each layer
of the baseline architectures. As an application, we train the proposed system
on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC)
MICCAI 2017. Starting from a baseline segmentation architecture, the resulting
network architecture obtains the state-of-the-art results in accuracy without
performing any trial-and-error based architecture design approaches or close
supervision of the hyperparameters changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mortazi_A/0/1/0/all/0/1&quot;&gt;Aliasghar Mortazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07665">
<title>Multitask Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies. (arXiv:1807.07665v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07665</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new RL problem where the agent is required to execute a given
subtask graph which describes a set of subtasks and their dependency. Unlike
existing multitask RL approaches that explicitly describe what the agent should
do, a subtask graph in our problem only describes properties of subtasks and
relationships among them, which requires the agent to perform complex reasoning
to find the optimal sequence of subtasks to execute. To tackle this problem, we
propose a neural subtask graph solver (NSS) which encodes the subtask graph
using a recursive neural network. To overcome the difficulty of training, we
propose a novel non-parametric gradient-based policy to pre-train our NSS
agent. % and further finetune it through actor-critic method. The experimental
results on two 2D visual domains show that our agent can perform complex
reasoning to find a near-optimal way of executing the subtask graph and
generalize well to the unseen subtask graphs. In addition, we compare our agent
with a Monte-Carlo tree search (MCTS) method showing that (1) our method is
much more efficient than MCTS and (2) combining MCTS with NSS dramatically
improves the search performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Sungryull Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Junhyuk Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07706">
<title>Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model. (arXiv:1807.07706v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07706</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework that enables efficient probabilistic inference
in large-scale scientific models by allowing the execution of existing
domain-specific simulators as probabilistic programs, resulting in highly
interpretable posterior inference. Our framework is general purpose and
scalable, and is based on a cross-platform probabilistic execution protocol
through which an inference engine can control simulators in a language-agnostic
way. We demonstrate the technique in particle physics, on a scientifically
accurate simulation of the tau lepton decay, which is a key ingredient in
establishing the properties of the Higgs boson. High-energy physics has a rich
set of simulators based on quantum field theory and the interaction of
particles in matter. We show how to use probabilistic programming to perform
Bayesian inference in these existing simulator codebases directly, in
particular conditioning on observable outputs from a simulated particle
detector to directly produce an interpretable posterior distribution over decay
pathways. Inference efficiency is achieved via inference compilation where a
deep recurrent neural network is trained to parameterize proposal distributions
and control the stochastic simulator in a sequential importance sampling
scheme, at a fraction of the computational cost of Markov chain Monte Carlo
sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1&quot;&gt;Atilim Gunes Baydin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_L/0/1/0/all/0/1&quot;&gt;Lukas Heinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhimji_W/0/1/0/all/0/1&quot;&gt;Wahid Bhimji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gram_Hansen_B/0/1/0/all/0/1&quot;&gt;Bradley Gram-Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Lei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhat/0/1/0/all/0/1&quot;&gt;Prabhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07741">
<title>Learning Representations for Soft Skill Matching. (arXiv:1807.07741v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.07741</link>
<description rdf:parseType="Literal">&lt;p&gt;Employers actively look for talents having not only specific hard skills but
also various soft skills. To analyze the soft skill demands on the job market,
it is important to be able to detect soft skill phrases from job advertisements
automatically. However, a naive matching of soft skill phrases can lead to
false positive matches when a soft skill phrase, such as friendly, is used to
describe a company, a team, or another entity, rather than a desired candidate.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a phrase-matching-based approach which
differentiates between soft skill phrases referring to a candidate vs.
something else. The disambiguation is formulated as a binary text
classification problem where the prediction is made for the potential soft
skill based on the context where it occurs. To inform the model about the soft
skill for which the prediction is made, we develop several approaches,
including soft skill masking and soft skill tagging.
&lt;/p&gt;
&lt;p&gt;We compare several neural network based approaches, including CNN, LSTM and
Hierarchical Attention Model. The proposed tagging-based input representation
using LSTM achieved the highest recall of 83.92% on the job dataset when fixing
a precision to 95%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayfullina_L/0/1/0/all/0/1&quot;&gt;Luiza Sayfullina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1&quot;&gt;Eric Malmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1&quot;&gt;Juho Kannala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07754">
<title>Learning the effect of latent variables in Gaussian Graphical models with unobserved variables. (arXiv:1807.07754v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07754</link>
<description rdf:parseType="Literal">&lt;p&gt;The edge structure of the graph defining an undirected graphical model
describes precisely the structure of dependence between the variables in the
graph. In many applications, the dependence structure is unknown and it is
desirable to learn it from data, often because it is a preliminary step to be
able to ascertain causal effects. This problem, known as structure learning, is
hard in general, but for Gaussian graphical models it is slightly easier
because the structure of the graph is given by the sparsity pattern of the
precision matrix of the joint distribution, and because independence coincides
with decorrelation. A major difficulty too often ignored in structure learning
is the fact that if some variables are not observed, the marginal dependence
graph over the observed variables will possibly be significantly more complex
and no longer reflect the direct dependencies that are potentially associated
with causal effects. In this work, we consider a family of latent variable
Gaussian graphical models in which the graph of the joint distribution between
observed and unobserved variables is sparse, and the unobserved variables are
conditionally independent given the others. Prior work was able to recover the
connectivity between observed variables, but could only identify the subspace
spanned by unobserved variables, whereas we propose a convex optimization
formulation based on structured matrix sparsity to estimate the complete
connectivity of the complete graph including unobserved variables, given the
knowledge of the number of missing variables, and a priori knowledge of their
level of connectivity. Our formulation is supported by a theoretical result of
identifiability of the latent dependence structure for sparse graphs in the
infinite data limit. We propose an algorithm leveraging recent active set
methods, which performs well in the experiments on synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vinyes_M/0/1/0/all/0/1&quot;&gt;Marina Vinyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Obozinski_G/0/1/0/all/0/1&quot;&gt;Guillaume Obozinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07789">
<title>Escaping the Curse of Dimensionality in Similarity Learning: Efficient Frank-Wolfe Algorithm and Generalization Bounds. (arXiv:1807.07789v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07789</link>
<description rdf:parseType="Literal">&lt;p&gt;Similarity and metric learning provides a principled approach to construct a
task-specific similarity from weakly supervised data. However, these methods
are subject to the curse of dimensionality: as the number of features grows
large, poor generalization is to be expected and training becomes intractable
due to high computational and memory costs. In this paper, we propose a
similarity learning method that can efficiently deal with high-dimensional
sparse data. This is achieved through a parameterization of similarity
functions by convex combinations of sparse rank-one matrices, together with the
use of a greedy approximate Frank-Wolfe algorithm which provides an efficient
way to control the number of active features. We show that the convergence rate
of the algorithm, as well as its time and memory complexity, are independent of
the data dimension. We further provide a theoretical justification of our
modeling choices through an analysis of the generalization error, which depends
logarithmically on the sparsity of the solution rather than on the number of
features. Our experiments on datasets with up to one million features
demonstrate the ability of our approach to generalize well despite the high
dimensionality as well as its superiority compared to several competing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bellet_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07879">
<title>Semi-Generative Modelling: Domain Adaptation with Cause and Effect Features. (arXiv:1807.07879v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07879</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel, causally-inspired approach to domain adaptation
which aims to also include unlabelled data in the model fitting when labelled
data is scarce. We consider a case of covariate-shift adaptation with cause and
effect features, and--drawing from recent ideas in causal modelling and
invariant prediction--show how this setting leads to, what we will refer to as,
a semi-generative model: P(Y, X_eff; X_cau,\theta). Our proposed approach is
robust to changes in the distribution over causal features, and naturally
allows to impose model constraints by unsupervised learning of a map from
causes to effects.
&lt;/p&gt;
&lt;p&gt;In experiments on synthetic datasets we demonstrate a significant improvement
in classification performance of our semi-generative model over
purely-supervised and importance-weighting baselines when the amount of
labelled data is small. Moreover, we apply our approach for regression on
real-world protein-count data and compare it to feature transformation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1&quot;&gt;Julius von Kugelgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mey_A/0/1/0/all/0/1&quot;&gt;Alexander Mey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loog_M/0/1/0/all/0/1&quot;&gt;Marco Loog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07909">
<title>Boosting algorithms for uplift modeling. (arXiv:1807.07909v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07909</link>
<description rdf:parseType="Literal">&lt;p&gt;Uplift modeling is an area of machine learning which aims at predicting the
causal effect of some action on a given individual. The action may be a medical
procedure, marketing campaign, or any other circumstance controlled by the
experimenter. Building an uplift model requires two training sets: the
treatment group, where individuals have been subject to the action, and the
control group, where no action has been performed. An uplift model allows then
to assess the gain resulting from taking the action on a given individual, such
as the increase in probability of patient recovery or of a product being
purchased. This paper describes an adaptation of the well-known boosting
techniques to the uplift modeling case. We formulate three desirable properties
which an uplift boosting algorithm should have. Since all three properties
cannot be satisfied simultaneously, we propose three uplift boosting
algorithms, each satisfying two of them. Experiments demonstrate the usefulness
of the proposed methods, which often dramatically improve performance of the
base models and are thus new and powerful tools for uplift modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltys_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; So&amp;#x142;tys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaroszewicz_S/0/1/0/all/0/1&quot;&gt;Szymon Jaroszewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07924">
<title>Optimal Bounds on the VC-dimension. (arXiv:1807.07924v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07924</link>
<description rdf:parseType="Literal">&lt;p&gt;The VC-dimension of a set system is a way to capture its complexity and has
been a key parameter studied extensively in machine learning and geometry
communities. In this paper, we resolve two longstanding open problems on
bounding the VC-dimension of two fundamental set systems: $k$-fold
unions/intersections of half-spaces, and the simplices set system. Among other
implications, it settles an open question in machine learning that was first
studied in the 1989 foundational paper of Blumer, Ehrenfeucht, Haussler and
Warmuth as well as by Eisenstat and Angluin and Johnson.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Csikos_M/0/1/0/all/0/1&quot;&gt;Monika Csikos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kupavskii_A/0/1/0/all/0/1&quot;&gt;Andrey Kupavskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_N/0/1/0/all/0/1&quot;&gt;Nabil H. Mustafa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.03764">
<title>Linear Algebraic Structure of Word Senses, with Applications to Polysemy. (arXiv:1601.03764v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1601.03764</link>
<description rdf:parseType="Literal">&lt;p&gt;Word embeddings are ubiquitous in NLP and information retrieval, but it is
unclear what they represent when the word is polysemous. Here it is shown that
multiple word senses reside in linear superposition within the word embedding
and simple sparse coding can recover vectors that approximately capture the
senses. The success of our approach, which applies to several embedding
methods, is mathematically explained using a variant of the random walk on
discourses model (Arora et al., 2016). A novel aspect of our technique is that
each extracted word sense is accompanied by one of about 2000 &quot;discourse atoms&quot;
that gives a succinct description of which other words co-occur with that word
sense. Discourse atoms can be of independent interest, and make the method
potentially more useful. Empirical tests are used to verify and support the
theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1&quot;&gt;Sanjeev Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1&quot;&gt;Andrej Risteski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.00960">
<title>An Inexact Variable Metric Proximal Point Algorithm for Generic Quasi-Newton Acceleration. (arXiv:1610.00960v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.00960</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an inexact variable-metric proximal point algorithm to accelerate
gradient-based optimization algorithms. The proposed scheme, called QNing can
be notably applied to incremental first-order methods such as the stochastic
variance-reduced gradient descent algorithm (SVRG) and other randomized
incremental optimization algorithms. QNing is also compatible with composite
objectives, meaning that it has the ability to provide exactly sparse solutions
when the objective involves a sparsity-inducing regularization. When combined
with limited-memory BFGS rules, QNing is particularly effective to solve
high-dimensional optimization problems, while enjoying a worst-case linear
convergence rate for strongly convex problems. We present experimental results
where QNing gives significant improvements over competing methods for training
machine learning methods on large samples and in high dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongzhou Lin&lt;/a&gt; (Thoth), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mairal_J/0/1/0/all/0/1&quot;&gt;Julien Mairal&lt;/a&gt; (Thoth), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1&quot;&gt;Zaid Harchaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.06603">
<title>Combinatorial Multi-Armed Bandit with General Reward Functions. (arXiv:1610.06603v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1610.06603</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the stochastic combinatorial multi-armed bandit
(CMAB) framework that allows a general nonlinear reward function, whose
expected value may not depend only on the means of the input random variables
but possibly on the entire distributions of these variables. Our framework
enables a much larger class of reward functions such as the $\max()$ function
and nonlinear utility functions. Existing techniques relying on accurate
estimations of the means of random variables, such as the upper confidence
bound (UCB) technique, do not work directly on these functions. We propose a
new algorithm called stochastically dominant confidence bound (SDCB), which
estimates the distributions of underlying random variables and their
stochastically dominant confidence bounds. We prove that SDCB can achieve
$O(\log{T})$ distribution-dependent regret and $\tilde{O}(\sqrt{T})$
distribution-independent regret, where $T$ is the time horizon. We apply our
results to the $K$-MAX problem and expected utility maximization problems. In
particular, for $K$-MAX, we provide the first polynomial-time approximation
scheme (PTAS) for its offline problem, and give the first $\tilde{O}(\sqrt T)$
bound on the $(1-\epsilon)$-approximation regret of its online problem, for any
$\epsilon&amp;gt;0$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pinyan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.04812">
<title>k-Means as a Variational EM Approximation of Gaussian Mixture Models. (arXiv:1704.04812v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.04812</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that $k$-means (Lloyd&apos;s algorithm) is obtained as a special case when
truncated variational EM approximations are applied to Gaussian Mixture Models
(GMM) with isotropic Gaussians. In contrast to the standard way to relate
$k$-means and GMMs, the provided derivation shows that it is not required to
consider Gaussians with small variances or the limit case of zero variances.
There are a number of consequences that directly follow from our approach: (A)
$k$-means can be shown to increase a free energy associated with truncated
distributions and this free energy can directly be reformulated in terms of the
$k$-means objective; (B) $k$-means generalizations can directly be derived by
considering the 2nd closest, 3rd closest etc. cluster in addition to just the
closest one; and (C) the embedding of $k$-means into a free energy framework
allows for theoretical interpretations of other $k$-means generalizations in
the literature. In general, truncated variational EM provides a natural and
rigorous quantitative link between $k$-means-like clustering and GMM clustering
algorithms which may be very relevant for future theoretical and empirical
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucke_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg L&amp;#xfc;cke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Forster_D/0/1/0/all/0/1&quot;&gt;Dennis Forster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09150">
<title>Variational Bayes Estimation of Discrete-Margined Copula Models with Application to Time Series. (arXiv:1712.09150v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09150</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new variational Bayes estimator for high-dimensional copulas
with discrete, or a combination of discrete and continuous, margins. The method
is based on a variational approximation to a tractable augmented posterior, and
is faster than previous likelihood-based approaches. We use it to estimate
drawable vine copulas for univariate and multivariate Markov ordinal and mixed
time series. These have dimension $rT$, where $T$ is the number of observations
and $r$ is the number of series, and are difficult to estimate using previous
methods. The vine pair-copulas are carefully selected to allow for
heteroskedasticity, which is a feature of most ordinal time series data. When
combined with flexible margins, the resulting time series models also allow for
other common features of ordinal data, such as zero inflation, multiple modes
and under- or over-dispersion. Using six example series, we illustrate both the
flexibility of the time series copula models, and the efficacy of the
variational Bayes estimator for copulas of up to 792 dimensions and 60
parameters. This far exceeds the size and complexity of copula models for
discrete data that can be estimated using previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loaiza_Maya_R/0/1/0/all/0/1&quot;&gt;Ruben Loaiza-Maya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Michael Stanley Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09737">
<title>DJAM: distributed Jacobi asynchronous method for learning personal models. (arXiv:1803.09737v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09737</link>
<description rdf:parseType="Literal">&lt;p&gt;Processing data collected by a network of agents often boils down to solving
an optimization problem. The distributed nature of these problems calls for
methods that are, themselves, distributed. While most collaborative learning
problems require agents to reach a common (or consensus) model, there are
situations in which the consensus solution may not be optimal. For instance,
agents may want to reach a compromise between agreeing with their neighbors and
minimizing a personal loss function. We present DJAM, a Jacobi-like distributed
algorithm for learning personalized models. This method is
implementation-friendly: it has no hyperparameters that need tuning, it is
asynchronous, and its updates only require single-neighbor interactions. We
prove that DJAM converges with probability one to the solution, provided that
the personal loss functions are strongly convex and have Lipschitz gradient. We
then give evidence that DJAM is on par with state-of-the-art methods: our
method reaches a solution with error similar to the error of a carefully tuned
ADMM in about the same number of single-neighbor interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_I/0/1/0/all/0/1&quot;&gt;In&amp;#xea;s Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xavier_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Xavier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08716">
<title>Learning Qualitatively Diverse and Interpretable Rules for Classification. (arXiv:1806.08716v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.08716</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been growing interest in developing accurate models that can also
be explained to humans. Unfortunately, if there exist multiple distinct but
accurate models for some dataset, current machine learning methods are unlikely
to find them: standard techniques will likely recover a complex model that
combines them. In this work, we introduce a way to identify a maximal set of
distinct but accurate models for a dataset. We demonstrate empirically that, in
situations where the data supports multiple accurate classifiers, we tend to
recover simpler, more interpretable classifiers rather than more complex ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Andrew Slavin Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Weiwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04932">
<title>Sequential sampling of Gaussian process latent variable models. (arXiv:1807.04932v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04932</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of inferring a latent function in a probabilistic
model of data. When dependencies of the latent function are specified by a
Gaussian process and the data likelihood is complex, efficient computation
often involve Markov chain Monte Carlo sampling with limited applicability to
large data sets. We extend some of these techniques to scale efficiently when
the problem exhibits a sequential structure. We propose an approximation that
enables sequential sampling of both latent variables and associated parameters.
We demonstrate strong performance in growing-data settings that would otherwise
be unfeasible with naive, non-sequential sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tegner_M/0/1/0/all/0/1&quot;&gt;Martin Tegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bloem_Reddy_B/0/1/0/all/0/1&quot;&gt;Benjamin Bloem-Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06732">
<title>Motivating the Rules of the Game for Adversarial Example Research. (arXiv:1807.06732v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06732</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in machine learning have led to broad deployment of systems with
impressive performance on important problems. Nonetheless, these systems can be
induced to make errors on data that are surprisingly similar to examples the
learned system handles correctly. The existence of these errors raises a
variety of questions about out-of-sample generalization and whether bad actors
might use such examples to abuse deployed systems. As a result of these
security concerns, there has been a flurry of recent papers proposing
algorithms to defend against such malicious perturbations of correctly handled
examples. It is unclear how such misclassifications represent a different kind
of security problem than other errors, or even other attacker-produced examples
that have no specific relationship to an uncorrupted input. In this paper, we
argue that adversarial example defense papers have, to date, mostly considered
abstract, toy games that do not relate to any specific security concern.
Furthermore, defense papers have not yet precisely described all the abilities
and limitations of attackers that would be relevant in practical security.
Towards this end, we establish a taxonomy of motivations, constraints, and
abilities for more plausible adversaries. Finally, we provide a series of
recommendations outlining a path forward for future work to more clearly
articulate the threat model and perform more meaningful evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1&quot;&gt;Justin Gilmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1&quot;&gt;Ryan P. Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_D/0/1/0/all/0/1&quot;&gt;David Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_G/0/1/0/all/0/1&quot;&gt;George E. Dahl&lt;/a&gt;</dc:creator>
</item></rdf:RDF>