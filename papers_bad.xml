<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11091"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10847"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10935"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11061"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11174"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11227"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11236"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11284"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11367"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.03625"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02867"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03223"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08237"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10956"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11014"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11027"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11143"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11167"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11169"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11228"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11320"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11398"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11407"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11408"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11455"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.05889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05506"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04145"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01189"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10278"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02109"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10320"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.11091">
<title>ADAM-ADMM: A Unified, Systematic Framework of Structured Weight Pruning for DNNs. (arXiv:1807.11091v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.11091</link>
<description rdf:parseType="Literal">&lt;p&gt;Weight pruning methods of deep neural networks (DNNs) have been demonstrated
to achieve a good model pruning ratio without loss of accuracy, thereby
alleviating the significant computation/storage requirements of large-scale
DNNs. Structured weight pruning methods have been proposed to overcome the
limitation of irregular network structure and demonstrated actual GPU
acceleration. However, the pruning ratio (degree of sparsity) and GPU
acceleration are limited (to less than 50%) when accuracy needs to be
maintained.
&lt;/p&gt;
&lt;p&gt;In this work, we overcome pruning ratio and GPU acceleration limitations by
proposing a unified, systematic framework of structured weight pruning for
DNNs, named ADAM-ADMM (Adaptive Moment Estimation-Alternating Direction Method
of Multipliers). It is a framework that can be used to induce different types
of structured sparsity, such as filter-wise, channel-wise, and shape-wise
sparsity, as well non-structured sparsity. The proposed framework incorporates
stochastic gradient descent with ADMM, and can be understood as a dynamic
regularization method in which the regularization target is analytically
updated in each iteration. A significant improvement in weight pruning ratio is
achieved without loss of accuracy, along with fast convergence rate. With a
small sparsity degree of 33% on the convolutional layers, we achieve 1.64%
accuracy enhancement for the AlexNet (CaffeNet) model. This is obtained by
mitigation of overfitting. Without loss of accuracy on the AlexNet model, we
achieve 2.6 times and 3.65 times average measured speedup on two GPUs, clearly
outperforming the prior work. The average speedups reach 2.77 times and 7.5
times when allowing a moderate accuracy loss of 2%. In this case the model
compression for convolutional layers is 13.2 times, corresponding to 10.5 times
CPU speedup. Our models and codes are released at
https://github.com/KaiqiZhang/ADAM-ADMM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Shaokai Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wujie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xue Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fardad_M/0/1/0/all/0/1&quot;&gt;Makan Fardad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11215">
<title>CAKE: Compact and Accurate K-dimensional representation of Emotion. (arXiv:1807.11215v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11215</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by works from the psychology community, we first study the link
between the compact two-dimensional representation of the emotion known as
arousal-valence, and discrete emotion classes (e.g. anger, happiness, sadness,
etc.) used in the computer vision community. It enables to assess the benefits
-- in terms of discrete emotion inference -- of adding an extra dimension to
arousal-valence (usually named dominance). Building on these observations, we
propose CAKE, a 3-dimensional representation of emotion learned in a
multi-domain fashion, achieving accurate emotion recognition on several public
datasets. Moreover, we visualize how emotions boundaries are organized inside
DNN representations and show that DNNs are implicitly learning
arousal-valence-like descriptions of emotions. Finally, we use the CAKE
representation to compare the quality of the annotations of different public
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1&quot;&gt;Corentin Kervadec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1&quot;&gt;Valentin Vielzeuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pateux_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Pateux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1&quot;&gt;Alexis Lechervy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Jurie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02679">
<title>Semi-Supervised Learning via Compact Latent Space Clustering. (arXiv:1806.02679v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02679</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel cost function for semi-supervised learning of neural
networks that encourages compact clustering of the latent space to facilitate
separation. The key idea is to dynamically create a graph over embeddings of
labeled and unlabeled samples of a training batch to capture underlying
structure in feature space, and use label propagation to estimate its high and
low density regions. We then devise a cost function based on Markov chains on
the graph that regularizes the latent space to form a single compact cluster
per class, while avoiding to disturb existing clusters during optimization. We
evaluate our approach on three benchmarks and compare to state-of-the art with
promising results. Our approach combines the benefits of graph-based
regularization with efficient, inductive inference, does not require
modifications to a network architecture, and can thus be easily applied to
existing networks to enable an effective use of unlabeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1&quot;&gt;Konstantinos Kamnitsas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1&quot;&gt;Daniel C. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1&quot;&gt;Loic Le Folgoc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_I/0/1/0/all/0/1&quot;&gt;Ian Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Criminisi_A/0/1/0/all/0/1&quot;&gt;Antonio Criminisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1&quot;&gt;Aditya Nori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10756">
<title>False Positive Reduction by Actively Mining Negative Samples for Pulmonary Nodule Detection in Chest Radiographs. (arXiv:1807.10756v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.10756</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating large quantities of quality labeled data in medical imaging is
very time consuming and expensive. The performance of supervised algorithms for
various tasks on imaging has improved drastically over the years, however the
availability of data to train these algorithms have become one of the main
bottlenecks for implementation. To address this, we propose a semi-supervised
learning method where pseudo-negative labels from unlabeled data are used to
further refine the performance of a pulmonary nodule detection network in chest
radiographs. After training with the proposed network, the false positive rate
was reduced to 0.1266 from 0.4864 while maintaining sensitivity at 0.89.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sejin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1&quot;&gt;Woochan Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kyu Hwan Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joon Beom Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Namkug Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10760">
<title>Deep nested level sets: Fully automated segmentation of cardiac MR images in patients with pulmonary hypertension. (arXiv:1807.10760v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.10760</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a novel and accurate optimisation method for
segmentation of cardiac MR (CMR) images in patients with pulmonary hypertension
(PH). The proposed method explicitly takes into account the image features
learned from a deep neural network. To this end, we estimate simultaneous
probability maps over region and edge locations in CMR images using a fully
convolutional network. Due to the distinct morphology of the heart in patients
with PH, these probability maps can then be incorporated in a single nested
level set optimisation framework to achieve multi-region segmentation with high
efficiency. The proposed method uses an automatic way for level set
initialisation and thus the whole optimisation is fully automated. We
demonstrate that the proposed deep nested level set (DNLS) method outperforms
existing state-of-the-art methods for CMR segmentation in PH patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinming Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlemper_J/0/1/0/all/0/1&quot;&gt;Jo Schlemper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Wenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawes_T/0/1/0/all/0/1&quot;&gt;Timothy J W Dawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_G/0/1/0/all/0/1&quot;&gt;Ghalib Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doumou_G/0/1/0/all/0/1&quot;&gt;Georgia Doumou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvao_A/0/1/0/all/0/1&quot;&gt;Antonio De Marvao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ORegan_D/0/1/0/all/0/1&quot;&gt;Declan P O&amp;#x27;Regan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10847">
<title>Agent cognition through micro-simulations: Adaptive and tunable intelligence with NetLogo LevelSpace. (arXiv:1807.10847v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.10847</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method of endowing agents in an agent-based model (ABM) with
sophisticated cognitive capabilities and a naturally tunable level of
intelligence. Often, ABMs use random behavior or greedy algorithms for
maximizing objectives (such as a predator always chasing after the closest
prey). However, random behavior is too simplistic in many circumstances and
greedy algorithms, as well as classic AI planning techniques, can be brittle in
the context of the unpredictable and emergent situations in which agents may
find themselves. Our method, called agent-centric Monte Carlo cognition
(ACMCC), centers around using a separate agent-based model to represent the
agents&apos; cognition. This model is then used by the agents in the primary model
to predict the outcomes of their actions, and thus guide their behavior. To
that end, we have implemented our method in the NetLogo agent-based modeling
platform, using the recently released LevelSpace extension, which we developed
to allow NetLogo models to interact with other NetLogo models. As an
illustrative example, we extend the Wolf Sheep Predation model (included with
NetLogo) by using ACMCC to guide animal behavior, and analyze the impact on
agent performance and model dynamics. We find that ACMCC provides a reliable
and understandable method of controlling agent intelligence, and has a large
impact on agent performance and model dynamics even at low settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Head_B/0/1/0/all/0/1&quot;&gt;Bryan Head&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilensky_U/0/1/0/all/0/1&quot;&gt;Uri Wilensky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10857">
<title>A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition. (arXiv:1807.10857v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1807.10857</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention-based recurrent neural encoder-decoder models present an elegant
solution to the automatic speech recognition problem. This approach folds the
acoustic model, pronunciation model, and language model into a single network
and requires only a parallel corpus of speech and text for training. However,
unlike in conventional approaches that combine separate acoustic and language
models, it is not clear how to use additional (unpaired) text. While there has
been previous work on methods addressing this problem, a thorough comparison
among methods is still lacking. In this paper, we compare a suite of past
methods and some of our own proposed methods for using unpaired text data to
improve encoder-decoder models. For evaluation, we use the medium-sized
Switchboard data set and the large-scale Google voice search and dictation data
sets. Our results confirm the benefits of using unpaired text across a range of
methods and data sets. Surprisingly, for first-pass decoding, the rather simple
approach of shallow fusion performs best across data sets. However, for Google
data sets we find that cold fusion has a lower oracle error rate and
outperforms other approaches after second-pass rescoring on the Google voice
search data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toshniwal_S/0/1/0/all/0/1&quot;&gt;Shubham Toshniwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anjuli Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Chung-Cheng Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1&quot;&gt;Tara N Sainath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Livescu_K/0/1/0/all/0/1&quot;&gt;Karen Livescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10934">
<title>Bike Flow Prediction with Multi-Graph Convolutional Networks. (arXiv:1807.10934v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10934</link>
<description rdf:parseType="Literal">&lt;p&gt;One fundamental issue in managing bike sharing systems is the bike flow
prediction. Due to the hardness of predicting the flow for a single station,
recent research works often predict the bike flow at cluster-level. While such
studies gain satisfactory prediction accuracy, they cannot directly guide some
fine-grained bike sharing system management issues at station-level. In this
paper, we revisit the problem of the station-level bike flow prediction, aiming
to boost the prediction accuracy leveraging the breakthroughs of deep learning
techniques. We propose a new multi-graph convolutional neural network model to
predict the bike flow at station-level, where the key novelty is viewing the
bike sharing system from the graph perspective. More specifically, we construct
multiple inter-station graphs for a bike sharing system. In each graph, nodes
are stations, and edges are a certain type of relations between stations. Then,
multiple graphs are constructed to reflect heterogeneous relationships (e.g.,
distance, ride record correlation). Afterward, we fuse the multiple graphs and
then apply the convolutional layers on the fused graph to predict station-level
future bike flow. In addition to the estimated bike flow value, our model also
gives the prediction confidence interval so as to help the bike sharing system
managers make decisions. Using New York City and Chicago bike sharing data for
experiments, our model can outperform state-of-the-art station-level prediction
models by reducing 25.1% and 17.0% of prediction error in New York City and
Chicago, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_D/0/1/0/all/0/1&quot;&gt;Di Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10935">
<title>Towards Explainable Inference about Object Motion using Qualitative Reasoning. (arXiv:1807.10935v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.10935</link>
<description rdf:parseType="Literal">&lt;p&gt;The capability of making explainable inferences regarding physical processes
has long been desired. One fundamental physical process is object motion.
Inferring what causes the motion of a group of objects can even be a
challenging task for experts, e.g., in forensics science. Most of the work in
the literature relies on physics simulation to draw such infer- ences. The
simulation requires a precise model of the under- lying domain to work well and
is essentially a black-box from which one can hardly obtain any useful
explanation. By contrast, qualitative reasoning methods have the advan- tage in
making transparent inferences with ambiguous infor- mation, which makes it
suitable for this task. However, there has been no suitable qualitative theory
proposed for object motion in three-dimensional space. In this paper, we take
this challenge and develop a qualitative theory for the motion of rigid
objects. Based on this theory, we develop a reasoning method to solve a very
interesting problem: Assuming there are several objects that were initially at
rest and now have started to move. We want to infer what action causes the
movement of these objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1&quot;&gt;Jochen Renz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1&quot;&gt;Hua Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11024">
<title>Opinion Spam Recognition Method for Online Reviews using Ontological Features. (arXiv:1807.11024v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.11024</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, there are a lot of people using social media opinions to make their
decision on buying products or services. Opinion spam detection is a hard
problem because fake reviews can be made by organizations as well as
individuals for different purposes. They write fake reviews to mislead readers
or automated detection system by promoting or demoting target products to
promote them or to damage their reputations. In this paper, we pro-pose a new
approach using knowledge-based Ontology to detect opinion spam with high
accuracy (higher than 75%). Keywords: Opinion spam, Fake review, E-commercial,
Ontology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;L.H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1&quot;&gt;N.T.H. Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_V/0/1/0/all/0/1&quot;&gt;V.M. Ngo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11061">
<title>Clause Vivification by Unit Propagation in CDCL SAT Solvers. (arXiv:1807.11061v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11061</link>
<description rdf:parseType="Literal">&lt;p&gt;Original and learnt clauses in Conflict-Driven Clause Learning (CDCL) SAT
solvers often contain redundant literals. This may have a negative impact on
performance because redundant literals may deteriorate both the effectiveness
of Boolean constraint propagation and the quality of subsequent learnt clauses.
To overcome this drawback, we propose a clause vivification approach that
eliminates redundant literals by applying unit propagation. The proposed clause
vivification is activated before the SAT solver triggers some selected
restarts, and only affects a subset of original and learnt clauses, which are
considered to be more relevant according to metrics like the literal block
distance (LBD). Moreover, we conducted an empirical investigation with
instances coming from the hard combinatorial and application categories of
recent SAT competitions. The results show that a remarkable number of
additional instances are solved when the proposed approach is incorporated into
five of the best performing CDCL SAT solvers (Glucose, TC_Glucose, COMiniSatPS,
MapleCOMSPS and MapleCOMSPS_LRB). More importantly, the empirical investigation
includes an in-depth analysis of the effectiveness of clause vivification. It
is worth mentioning that one of the SAT solvers described here was ranked first
in the main track of SAT Competition 2017 thanks to the incorporation of the
proposed clause vivification. That solver was further improved in this paper
and won the bronze medal in the main track of SAT Competition 2018.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chu-Min Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Fan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Mao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manya_F/0/1/0/all/0/1&quot;&gt;Felip Many&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng L&amp;#xfc;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11112">
<title>While Tuning is Good, No Tuner is Best. (arXiv:1807.11112v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11112</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperparameter tuning is the black art of automatically finding a good
combination of control parameters for a data miner. While widely applied in
Software Engineering, there has not been much discussion on which
hyperparameter tuner is best for software analytics.
&lt;/p&gt;
&lt;p&gt;To address this gap in the literature, this paper applied a range of
hyperparameter optimizers (grid search, differential evolution, random search,
SMAC) to defect prediction. No hyperparameter optimizer was observed to be
&quot;best&quot; and, for one of the two evaluation measures studied here (F-measure),
hyperparameter optimization, in 50\% cases, was no better than using default
configurations.
&lt;/p&gt;
&lt;p&gt;We conclude that hyperparameter optimization is more nuanced than previously
believed. While such optimization can certainly lead to large improvements in
the performance of classifiers used in software analytics, it remains to be
seen which specific optimizers should be endorsed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1&quot;&gt;Huy Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1&quot;&gt;Vivek Nair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11113">
<title>Reinforced Auto-Zoom Net: Towards Accurate and Fast Breast Cancer Segmentation in Whole-slide Images. (arXiv:1807.11113v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11113</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks have led to significant breakthroughs in the
domain of medical image analysis. However, the task of breast cancer
segmentation in whole-slide images (WSIs) is still underexplored. WSIs are
large histopathological images with extremely high resolution. Constrained by
the hardware and field of view, using high-magnification patches can slow down
the inference process and using low-magnification patches can cause the loss of
information. In this paper, we aim to achieve two seemingly conflicting goals
for breast cancer segmentation: accurate and fast prediction. We propose a
simple yet efficient framework Reinforced Auto-Zoom Net (RAZN) to tackle this
task. Motivated by the zoom-in operation of a pathologist using a digital
microscope, RAZN learns a policy network to decide whether zooming is required
in a given region of interest. Because the zoom-in action is selective, RAZN is
robust to unbalanced and noisy ground truth labels and can efficiently reduce
overfitting. We evaluate our method on a public breast cancer dataset. RAZN
outperforms both single-scale and multi-scale baseline approaches, achieving
better accuracy at low inference cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Nanqing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1&quot;&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeya Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wei Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11150">
<title>Learning to Interrupt: A Hierarchical Deep Reinforcement Learning Framework for Efficient Exploration. (arXiv:1807.11150v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11150</link>
<description rdf:parseType="Literal">&lt;p&gt;To achieve scenario intelligence, humans must transfer knowledge to robots by
developing goal-oriented algorithms, which are sometimes insensitive to
dynamically changing environments. While deep reinforcement learning achieves
significant success recently, it is still extremely difficult to be deployed in
real robots directly. In this paper, we propose a hybrid structure named
Option-Interruption in which human knowledge is embedded into a hierarchical
reinforcement learning framework. Our architecture has two key components:
options, represented by existing human-designed methods, can significantly
speed up the training process and interruption mechanism, based on learnable
termination functions, enables our system to quickly respond to the external
environment. To implement this architecture, we derive a set of update rules
based on policy gradient methods and present a complete training process. In
the experiment part, our method is evaluated in Four-room navigation and
exploration task, which shows the efficiency and flexibility of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tingguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jin Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Delong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Max Q.-H. Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11174">
<title>Active Object Perceiver: Recognition-guided Policy Learning for Object Searching on Mobile Robots. (arXiv:1807.11174v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11174</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning a navigation policy for a robot to actively
search for an object of interest in an indoor environment solely from its
visual inputs. While scene-driven visual navigation has been widely studied,
prior efforts on learning navigation policies for robots to find objects are
limited. The problem is often more challenging than target scene finding as the
target objects can be very small in the view and can be in an arbitrary pose.
We approach the problem from an active perceiver perspective, and propose a
novel framework that integrates a deep neural network based object recognition
module and a deep reinforcement learning based action prediction mechanism. To
validate our method, we conduct experiments on both a simulation dataset
(AI2-THOR) and a real-world environment with a physical robot. We further
propose a new decaying reward function to learn the control policy specific to
the object searching task. Experimental results validate the efficacy of our
method, which outperforms competing methods in both average trajectory length
and success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shibin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yezhou Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11227">
<title>YouTube AV 50K: an Annotated Corpus for Comments in Autonomous Vehicles. (arXiv:1807.11227v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.11227</link>
<description rdf:parseType="Literal">&lt;p&gt;With one billion monthly viewers, and millions of users discussing and
sharing opinions, comments below YouTube videos are rich sources of data for
opinion mining and sentiment analysis. We introduce the YouTube AV 50K dataset,
a freely-available collections of more than 50,000 YouTube comments and
metadata below autonomous vehicle (AV)-related videos. We describe its creation
process, its content and data format, and discuss its possible usages.
Especially, we do a case study of the first self-driving car fatality to
evaluate the dataset, and show how we can use this dataset to better understand
public attitudes toward self-driving cars and public reactions to the accident.
Future developments of the dataset are also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kaiming Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Siyuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11236">
<title>Semantic Labeling in Very High Resolution Images via a Self-Cascaded Convolutional Neural Network. (arXiv:1807.11236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11236</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic labeling for very high resolution (VHR) images in urban areas, is of
significant importance in a wide range of remote sensing applications. However,
many confusing manmade objects and intricate fine-structured objects make it
very difficult to obtain both coherent and accurate labeling results. For this
challenging task, we propose a novel deep model with convolutional neural
networks (CNNs), i.e., an end-to-end self-cascaded network (ScasNet).
Specifically, for confusing manmade objects, ScasNet improves the labeling
coherence with sequential global-to-local contexts aggregation. Technically,
multi-scale contexts are captured on the output of a CNN encoder, and then they
are successively aggregated in a self-cascaded manner. Meanwhile, for
fine-structured objects, ScasNet boosts the labeling accuracy with a
coarse-to-fine refinement strategy. It progressively refines the target objects
using the low-level features learned by CNN&apos;s shallow layers. In addition, to
correct the latent fitting residual caused by multi-feature fusion inside
ScasNet, a dedicated residual correction scheme is proposed. It greatly
improves the effectiveness of ScasNet. Extensive experimental results on three
public datasets, including two challenging benchmarks, show that ScasNet
achieves the state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1&quot;&gt;Bin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jun Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Shiming Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chunhong Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11272">
<title>Uncertainty Quantification in CNN-Based Surface Prediction Using Shape Priors. (arXiv:1807.11272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11272</link>
<description rdf:parseType="Literal">&lt;p&gt;Surface reconstruction is a vital tool in a wide range of areas of medical
image analysis and clinical research. Despite the fact that many methods have
proposed solutions to the reconstruction problem, most, due to their
deterministic nature, do not directly address the issue of quantifying
uncertainty associated with their predictions. We remedy this by proposing a
novel probabilistic deep learning approach capable of simultaneous surface
reconstruction and associated uncertainty prediction. The method incorporates
prior shape information in the form of a principal component analysis (PCA)
model. Experiments using the UK Biobank data show that our probabilistic
approach outperforms an analogous deterministic PCA-based method in the task of
2D organ delineation and quantifies uncertainty by formulating distributions
over predicted surface vertex positions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tothova_K/0/1/0/all/0/1&quot;&gt;Katar&amp;#xed;na T&amp;#xf3;thov&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisot_S/0/1/0/all/0/1&quot;&gt;Sarah Parisot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Matthew C. H. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puyol_Anton_E/0/1/0/all/0/1&quot;&gt;Esther Puyol-Ant&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1&quot;&gt;Lisa M. Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1&quot;&gt;Andrew P. King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1&quot;&gt;Ender Konukoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11284">
<title>Unsupervised Domain Adaptation by Adversarial Learning for Robust Speech Recognition. (arXiv:1807.11284v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1807.11284</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the use of adversarial learning for
unsupervised adaptation to unseen recording conditions, more specifically,
single microphone far-field speech. We adapt neural networks based acoustic
models trained with close-talk clean speech to the new recording conditions
using untranscribed adaptation data. Our experimental results on Italian
SPEECON data set show that our proposed method achieves 19.8% relative word
error rate (WER) reduction compared to the unadapted models. Furthermore, this
adaptation method is beneficial even when performed on data from another
language (i.e. French) giving 12.6% relative WER reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denisov_P/0/1/0/all/0/1&quot;&gt;Pavel Denisov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vu_N/0/1/0/all/0/1&quot;&gt;Ngoc Thang Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Font_M/0/1/0/all/0/1&quot;&gt;Marc Ferras Font&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11367">
<title>Fairly Allocating Many Goods with Few Queries. (arXiv:1807.11367v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1807.11367</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the query complexity of the fair allocation of indivisible
goods. For two agents with arbitrary monotonic valuations, we design an
algorithm that computes an allocation satisfying envy-freeness up to one good
(EF1), a relaxation of envy-freeness, using a logarithmic number of queries. We
show that the logarithmic query complexity bound also holds for three agents
with additive valuations. These results suggest that it is possible to fairly
allocate goods in practice even when the number of goods is extremely large. By
contrast, we prove that computing an allocation satisfying envy-freeness and
another of its relaxations, envy-freeness up to any good (EFX), requires a
linear number of queries even when there are only two agents with identical
additive valuations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1&quot;&gt;Hoon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Procaccia_A/0/1/0/all/0/1&quot;&gt;Ariel D. Procaccia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suksompong_W/0/1/0/all/0/1&quot;&gt;Warut Suksompong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11456">
<title>Norms, Institutions, and Robots. (arXiv:1807.11456v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11456</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactions within human societies are usually regulated by social norms. If
robots are to be accepted into human society, it is essential that they are
aware of and capable of reasoning about social norms. In this paper, we focus
on how to represent social norms in societies with humans and robots, and how
artificial agents such as robots can reason about social norms in order to plan
appropriate behavior. We use the notion of institution as a way to formally
define and encapsulate norms. We provide a formal framework built around the
notion of institution. The framework distinguishes between abstract norms and
their semantics in a concrete domain, hence allowing the use of the same
institution across physical domains and agent types. It also provides a formal
computational framework for norm verification, planning, and plan execution in
a domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomic_S/0/1/0/all/0/1&quot;&gt;Stevan Tomic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pecora_F/0/1/0/all/0/1&quot;&gt;Federico Pecora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saffiotti_A/0/1/0/all/0/1&quot;&gt;Alessandro Saffiotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.03625">
<title>Budgeted Experiment Design for Causal Structure Learning. (arXiv:1709.03625v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.03625</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of causal structure learning when the experimenter is
limited to perform at most $k$ non-adaptive experiments of size $1$. We
formulate the problem of finding the best intervention target set as an
optimization problem, which aims to maximize the average number of edges whose
directions are resolved. We prove that the corresponding objective function is
submodular and a greedy algorithm suffices to achieve
$(1-\frac{1}{e})$-approximation of the optimal value. We further present an
accelerated variant of the greedy algorithm, which can lead to orders of
magnitude performance speedup. We validate our proposed approach on synthetic
and real graphs. The results show that compared to the purely observational
setting, our algorithm orients the majority of the edges through a considerably
small number of interventions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1&quot;&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehkaleybar_S/0/1/0/all/0/1&quot;&gt;Saber Salehkaleybar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1&quot;&gt;Elias Bareinboim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10519">
<title>Exploiting Points and Lines in Regression Forests for RGB-D Camera Relocalization. (arXiv:1710.10519v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10519</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera relocalization plays a vital role in many robotics and computer vision
tasks, such as global localization, recovery from tracking failure and loop
closure detection. Recent random forests based methods exploit randomly sampled
pixel comparison features to predict 3D world locations for 2D image locations
to guide the camera pose optimization. However, these image features are only
sampled randomly in the images, without considering the spatial structures or
geometric information, leading to large errors or failure cases with the
existence of poorly textured areas or in motion blur. Line segment features are
more robust in these environments. In this work, we propose to jointly exploit
points and lines within the framework of uncertainty driven regression forests.
The proposed approach is thoroughly evaluated on three publicly available
datasets against several strong state-of-the-art baselines in terms of several
different error metrics. Experimental results prove the efficacy of our method,
showing superior or on-par state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lili Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tung_F/0/1/0/all/0/1&quot;&gt;Frederick Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_J/0/1/0/all/0/1&quot;&gt;James J. Little&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valentin_J/0/1/0/all/0/1&quot;&gt;Julien Valentin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1&quot;&gt;Clarence de Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04378">
<title>Fairness in Supervised Learning: An Information Theoretic Approach. (arXiv:1801.04378v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04378</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated decision making systems are increasingly being used in real-world
applications. In these systems for the most part, the decision rules are
derived by minimizing the training error on the available historical data.
Therefore, if there is a bias related to a sensitive attribute such as gender,
race, religion, etc. in the data, say, due to cultural/historical
discriminatory practices against a certain demographic, the system could
continue discrimination in decisions by including the said bias in its decision
rule. We present an information theoretic framework for designing fair
predictors from data, which aim to prevent discrimination against a specified
sensitive attribute in a supervised learning setting. We use equalized odds as
the criterion for discrimination, which demands that the prediction should be
independent of the protected attribute conditioned on the actual label. To
ensure fairness and generalization simultaneously, we compress the data to an
auxiliary variable, which is used for the prediction task. This auxiliary
variable is chosen such that it is decontaminated from the discriminatory
attribute in the sense of equalized odds. The final predictor is obtained by
applying a Bayesian decision rule to the auxiliary variable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1&quot;&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1&quot;&gt;Sajad Khodadadian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08328">
<title>On Looking for Local Expansion Invariants in Argumentation Semantics: a Preliminary Report. (arXiv:1802.08328v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08328</link>
<description rdf:parseType="Literal">&lt;p&gt;We study invariant local expansion operators for conflict-free and admissible
sets in Abstract Argumentation Frameworks (AFs). Such operators are directly
applied on AFs, and are invariant with respect to a chosen &quot;semantics&quot; (that is
w.r.t. each of the conflict free/admissible set of arguments). Accordingly, we
derive a definition of robustness for AFs in terms of the number of times such
operators can be applied without producing any change in the chosen semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bistarelli_S/0/1/0/all/0/1&quot;&gt;Stefano Bistarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santini_F/0/1/0/all/0/1&quot;&gt;Francesco Santini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taticchi_C/0/1/0/all/0/1&quot;&gt;Carlo Taticchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00119">
<title>Integrating Human-Provided Information Into Belief State Representation Using Dynamic Factorization. (arXiv:1803.00119v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00119</link>
<description rdf:parseType="Literal">&lt;p&gt;In partially observed environments, it can be useful for a human to provide
the robot with declarative information that represents probabilistic relational
constraints on properties of objects in the world, augmenting the robot&apos;s
sensory observations. For instance, a robot tasked with a search-and-rescue
mission may be informed by the human that two victims are probably in the same
room. An important question arises: how should we represent the robot&apos;s
internal knowledge so that this information is correctly processed and combined
with raw sensory information? In this paper, we provide an efficient belief
state representation that dynamically selects an appropriate factoring,
combining aspects of the belief when they are correlated through information
and separating them when they are not. This strategy works in open domains, in
which the set of possible objects is not known in advance, and provides
significant improvements in inference time over a static factoring, leading to
more efficient planning for complex partially observed tasks. We validate our
approach experimentally in two open-domain planning problems: a 2D discrete
gridworld task and a 3D continuous cooking task. A supplementary video can be
found at &lt;a href=&quot;http://tinyurl.com/chitnis-iros-18.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1&quot;&gt;Rohan Chitnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02867">
<title>Online normalizer calculation for softmax. (arXiv:1805.02867v2 [cs.PF] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02867</link>
<description rdf:parseType="Literal">&lt;p&gt;The Softmax function is ubiquitous in machine learning, multiple previous
works suggested faster alternatives for it. In this paper we propose a way to
compute classical Softmax with fewer memory accesses and hypothesize that this
reduction in memory accesses should improve Softmax performance on actual
hardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to
1.3x and Softmax+TopK combined and fused by up to 5x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milakov_M/0/1/0/all/0/1&quot;&gt;Maxim Milakov&lt;/a&gt; (NVIDIA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimelshein_N/0/1/0/all/0/1&quot;&gt;Natalia Gimelshein&lt;/a&gt; (NVIDIA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03223">
<title>Entropy Maximization for Markov Decision Processes Under Temporal Logic Constraints. (arXiv:1807.03223v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03223</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of synthesizing a policy that maximizes the entropy of a
Markov decision process (MDP) subject to a temporal logic constraint. Such a
policy minimizes the predictability of the paths it generates, or dually,
maximizes the continual exploration of different paths in an MDP while ensuring
the satisfaction of a temporal logic specification. We first show that the
maximum entropy of an MDP can be finite, infinite or unbounded. We provide
necessary and sufficient conditions under which the maximum entropy of an MDP
is finite, infinite or unbounded. We then present an algorithm to synthesize a
policy that maximizes the entropy of an MDP. The proposed algorithm is based on
a convex optimization problem and runs in time polynomial in the size of the
MDP. We also show that maximizing the entropy of an MDP is equivalent to
maximizing the entropy of the paths that reach a certain set of states in the
MDP. Finally, we extend the algorithm to an MDP subject to a temporal logic
specification. In numerical examples, we demonstrate the proposed method on
different motion planning scenarios and illustrate that as the restrictions
imposed on the paths by a specification increase, the maximum entropy
decreases, which in turn, increases the predictability of paths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Savas_Y/0/1/0/all/0/1&quot;&gt;Yagiz Savas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ornik_M/0/1/0/all/0/1&quot;&gt;Melkior Ornik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cubuktepe_M/0/1/0/all/0/1&quot;&gt;Murat Cubuktepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08237">
<title>Learning Deep Hidden Nonlinear Dynamics from Aggregate Data. (arXiv:1807.08237v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08237</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning nonlinear dynamics from diffusion data is a challenging problem
since the individuals observed may be different at different time points,
generally following an aggregate behaviour. Existing work cannot handle the
tasks well since they model such dynamics either directly on observations or
enforce the availability of complete longitudinal individual-level
trajectories. However, in most of the practical applications, these
requirements are unrealistic: the evolving dynamics may be too complex to be
modeled directly on observations, and individual-level trajectories may not be
available due to technical limitations, experimental costs and/or privacy
issues. To address these challenges, we formulate a model of diffusion dynamics
as the {\em hidden stochastic process} via the introduction of hidden variables
for flexibility, and learn the hidden dynamics directly on {\em aggregate
observations} without any requirement for individual-level trajectories. We
propose a dynamic generative model with Wasserstein distance for LEarninG dEep
hidden Nonlinear Dynamics (LEGEND) and prove its theoretical guarantees as
well. Experiments on a range of synthetic and real-world datasets illustrate
that LEGEND has very strong performance compared to state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingkai Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Monazam Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10876">
<title>Transportation Modes Classification Using Feature Engineering. (arXiv:1807.10876v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10876</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting transportation modes from GPS (Global Positioning System) records
is a hot topic in the trajectory mining domain. Each GPS record is called a
trajectory point and a trajectory is a sequence of these points. Trajectory
mining has applications including but not limited to transportation mode
detection, tourism, traffic congestion, smart cities management, animal
behaviour analysis, environmental preservation, and traffic dynamics are some
of the trajectory mining applications. Transportation modes prediction as one
of the tasks in human mobility and vehicle mobility applications plays an
important role in resource allocation, traffic management systems, tourism
planning and accident detection. In this work, the proposed framework in Etemad
et al. is extended to consider other aspects in the task of transportation
modes prediction. Wrapper search and information retrieval methods were
investigated to find the best subset of trajectory features. Finding the best
classifier and the best feature subset, the framework is compared against two
related papers that applied deep learning methods. The results show that our
framework achieved better performance. Moreover, the ground truth noise removal
improved accuracy of transportation modes prediction task; however, the
assumption of having access to test set labels in pre-processing task is
invalid. Furthermore, the cross validation approaches were investigated and the
performance results show that the random cross validation method provides
optimistic results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etemad_M/0/1/0/all/0/1&quot;&gt;Mohammad Etemad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10956">
<title>Group-sparse SVD Models and Their Applications in Biological Data. (arXiv:1807.10956v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.10956</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse Singular Value Decomposition (SVD) models have been proposed for
biclustering high dimensional gene expression data to identify block patterns
with similar expressions. However, these models do not take into account prior
group effects upon variable selection. To this end, we first propose
group-sparse SVD models with group Lasso (GL1-SVD) and group L0-norm penalty
(GL0-SVD) for non-overlapping group structure of variables. However, such
group-sparse SVD models limit their applicability in some problems with
overlapping structure. Thus, we also propose two group-sparse SVD models with
overlapping group Lasso (OGL1-SVD) and overlapping group L0-norm penalty
(OGL0-SVD). We first adopt an alternating iterative strategy to solve GL1-SVD
based on a block coordinate descent method, and GL0-SVD based on a projection
method. The key of solving OGL1-SVD is a proximal operator with overlapping
group Lasso penalty. We employ an alternating direction method of multipliers
(ADMM) to solve the proximal operator. Similarly, we develop an approximate
method to solve OGL0-SVD. Applications of these methods and comparison with
competing ones using simulated data demonstrate their effectiveness. Extensive
applications of them onto several real gene expression data with gene prior
group knowledge identify some biologically interpretable gene modules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Min_W/0/1/0/all/0/1&quot;&gt;Wenwen Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Juan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10957">
<title>Improving Sequential Determinantal Point Processes for Supervised Video Summarization. (arXiv:1807.10957v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10957</link>
<description rdf:parseType="Literal">&lt;p&gt;It is now much easier than ever before to produce videos. While the
ubiquitous video data is a great source for information discovery and
extraction, the computational challenges are unparalleled. Automatically
summarizing the videos has become a substantial need for browsing, searching,
and indexing visual content. This paper is in the vein of supervised video
summarization using sequential determinantal point process (SeqDPP), which
models diversity by a probabilistic distribution. We improve this model in two
folds. In terms of learning, we propose a large-margin algorithm to address the
exposure bias problem in SeqDPP. In terms of modeling, we design a new
probabilistic distribution such that, when it is integrated into SeqDPP, the
resulting model accepts user input about the expected length of the summary.
Moreover, we also significantly extend a popular video summarization dataset by
1) more egocentric videos, 2) dense user annotations, and 3) a refined
evaluation scheme. We conduct extensive experiments on this dataset (about 60
hours of videos in total) and compare our approach to several competitive
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharghi_A/0/1/0/all/0/1&quot;&gt;Aidean Sharghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1&quot;&gt;Ali Borji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengtao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1&quot;&gt;Boqing Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11014">
<title>A Margin-based MLE for Crowdsourced Partial Ranking. (arXiv:1807.11014v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11014</link>
<description rdf:parseType="Literal">&lt;p&gt;A preference order or ranking aggregated from pairwise comparison data is
commonly understood as a strict total order. However, in real-world scenarios,
some items are intrinsically ambiguous in comparisons, which may very well be
an inherent uncertainty of the data. In this case, the conventional total order
ranking can not capture such uncertainty with mere global ranking or utility
scores. In this paper, we are specifically interested in the recent surge in
crowdsourcing applications to predict partial but more accurate (i.e., making
less incorrect statements) orders rather than complete ones. To do so, we
propose a novel framework to learn some probabilistic models of partial orders
as a \emph{margin-based Maximum Likelihood Estimate} (MLE) method. We prove
that the induced MLE is a joint convex optimization problem with respect to all
the parameters, including the global ranking scores and margin parameter.
Moreover, three kinds of generalized linear models are studied, including the
basic uniform model, Bradley-Terry model, and Thurstone-Mosteller model,
equipped with some theoretical analysis on FDR and Power control for the
proposed methods. The validity of these models are supported by experiments
with both simulated and real-world datasets, which shows that the proposed
models exhibit improvements compared with traditional state-of-the-art
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qianqian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jiechao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11027">
<title>Consistent polynomial-time unseeded graph matching for Lipschitz graphons. (arXiv:1807.11027v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.11027</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a consistent polynomial-time method for the unseeded node matching
problem for networks with smooth underlying structures. Despite widely
conjectured by the research community that the structured graph matching
problem to be significantly easier than its worst case counterpart, well-known
to be NP-hard, the statistical version of the problem has stood a challenge
that resisted any solution both provable and polynomial-time. The closest
existing work requires quasi-polynomial time. Our method is based on the latest
advances in graphon estimation techniques and analysis on the concentration of
empirical Wasserstein distances. Its core is a simple yet unconventional
sampling-and-matching scheme that reduces the problem from unseeded to seeded.
Our method allows flexible efficiencies, is convenient to analyze and
potentially can be extended to more general settings. Our work enables a rich
variety of subsequent estimations and inferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11143">
<title>ARM: Augment-REINFORCE-Merge Gradient for Discrete Latent Variable Models. (arXiv:1807.11143v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.11143</link>
<description rdf:parseType="Literal">&lt;p&gt;To backpropagate the gradients through discrete stochastic layers, we encode
the true gradients into a multiplication between random noises and the
difference of the same function of two different sets of discrete latent
variables, which are correlated with these random noises. The expectations of
that multiplication over iterations are zeros combined with spikes from time to
time. To modulate the frequencies, amplitudes, and signs of the spikes to
capture the temporal evolution of the true gradients, we propose the
augment-REINFORCE-merge (ARM) estimator that combines data augmentation, the
score-function estimator, permutation of the indices of latent variables, and
variance reduction for Monte Carlo integration using common random numbers. The
ARM estimator provides low-variance and unbiased gradient estimates for the
parameters of discrete distributions, leading to state-of-the-art performance
in both auto-encoding variational Bayes and maximum likelihood inference, for
discrete latent variable models with one or multiple discrete stochastic
layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Mingzhang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11167">
<title>A Group-Theoretic Approach to Abstraction: Hierarchical, Interpretable, and Task-Free Clustering. (arXiv:1807.11167v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11167</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstraction plays a key role in concept learning and knowledge discovery.
While pervasive in both human and artificial intelligence, it remains
mysterious how concepts are abstracted in the first place. We study the nature
of abstraction through a group-theoretic approach, formalizing it as a
hierarchical, interpretable, and task-free clustering problem. This clustering
framework is data-free, feature-free, similarity-free, and globally
hierarchical---the four key features that distinguish it from common clustering
models. Beyond a theoretical foundation for abstraction, we also present a
top-down and a bottom-up approach to establish an algorithmic foundation for
practical abstraction-generating methods. Lastly, using both a theoretical
explanation and a real-world application, we show that the coupling of our
abstraction framework with statistics realizes Shannon&apos;s information lattice
and even further, brings learning into the picture. This gives a first step
towards a principled and cognitive way of automatic concept learning and
knowledge discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haizi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mineyev_I/0/1/0/all/0/1&quot;&gt;Igor Mineyev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1&quot;&gt;Lav R. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11169">
<title>Online Learning with an Almost Perfect Expert. (arXiv:1807.11169v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11169</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the online learning problem where a forecaster makes a sequence of
binary predictions using the advice of $n$ experts. Our main contribution is to
analyze the regime where the best expert makes at most $b$ mistakes and to show
that when $b = o(\log_4{n})$, the expected number of mistakes made by the
optimal forecaster is at most $\log_4{n} + o(\log_4{n})$. We also describe an
adversary strategy showing that this bound is tight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branzei_S/0/1/0/all/0/1&quot;&gt;Simina Br&amp;#xe2;nzei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peres_Y/0/1/0/all/0/1&quot;&gt;Yuval Peres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11228">
<title>Predicting Conversion of Mild Cognitive Impairments to Alzheimer&apos;s Disease and Exploring Impact of Neuroimaging. (arXiv:1807.11228v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11228</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, a lot of scientific efforts are concentrated on the diagnosis of
Alzheimer&apos;s Disease (AD) applying deep learning methods to neuroimaging data.
Even for 2017, there were published more than a hundred papers dedicated to AD
diagnosis, whereas only a few works considered a problem of mild cognitive
impairments (MCI) conversion to the AD. However, the conversion prediction is
an important problem since approximately 15% of patients with MCI converges to
the AD every year. In the current work, we are focusing on the conversion
prediction using brain Magnetic Resonance Imaging and clinical data, such as
demographics, cognitive assessments, genetic, and biochemical markers. First of
all, we applied state-of-the-art deep learning algorithms on the neuroimaging
data and compared these results with two machine learning algorithms that we
fit using the clinical data. As a result, the models trained on the clinical
data outperform the deep learning algorithms applied to the MR images. To
explore the impact of neuroimaging further, we trained a deep feed-forward
embedding using similarity learning with Histogram loss on all available MRIs
and obtained 64-dimensional vector representation of neuroimaging data. The use
of learned representation from the deep embedding allowed to increase the
quality of prediction based on the neuroimaging. Finally, the current results
on this dataset show that the neuroimaging does affect conversion prediction,
however, cannot noticeably increase the quality of the prediction. The best
results of predicting MCI-to-AD conversion are provided by XGBoost algorithm
trained on the clinical and embedding data. The resulting accuracy is 0.76 +-
0.01 and the area under the ROC curve - 0.86 +- 0.01.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmulev_Y/0/1/0/all/0/1&quot;&gt;Yaroslav Shmulev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1&quot;&gt;Mikhail Belyaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11320">
<title>Kernel Density Estimation-Based Markov Models with Hidden State. (arXiv:1807.11320v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11320</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider Markov models of stochastic processes where the next-step
conditional distribution is defined by a kernel density estimator (KDE),
similar to Markov forecast densities and certain time-series bootstrap schemes.
The KDE Markov models (KDE-MMs) we discuss are nonlinear, nonparametric, fully
probabilistic representations of stationary processes, based on techniques with
strong asymptotic consistency properties. The models generate new data by
concatenating points from the training data sequences in a context-sensitive
manner, together with some additive driving noise. We present novel EM-type
maximum-likelihood algorithms for data-driven bandwidth selection in KDE-MMs.
Additionally, we augment the KDE-MMs with a hidden state, yielding a new model
class, KDE-HMMs. The added state variable captures non-Markovian long memory
and signal structure (e.g., slow oscillations), complementing the short-range
dependences described by the Markov process. The resulting joint Markov and
hidden-Markov structure is appealing for modelling complex real-world processes
such as speech signals. We present guaranteed-ascent EM-update equations for
model parameters in the case of Gaussian kernels, as well as relaxed update
formulas that greatly accelerate training in practice. Experiments demonstrate
increased held-out set probability for KDE-HMMs on several challenging natural
and synthetic data series, compared to traditional techniques such as
autoregressive models, HMMs, and their combinations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leijon_A/0/1/0/all/0/1&quot;&gt;Arne Leijon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. Bastiaan Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11398">
<title>Preference-based Online Learning with Dueling Bandits: A Survey. (arXiv:1807.11398v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11398</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, the notion of multi-armed bandits refers to a class of
online learning problems, in which an agent is supposed to simultaneously
explore and exploit a given set of choice alternatives in the course of a
sequential decision process. In the standard setting, the agent learns from
stochastic feedback in the form of real-valued rewards. In many applications,
however, numerical reward signals are not readily available -- instead, only
weaker information is provided, in particular relative preferences in the form
of qualitative comparisons between pairs of alternatives. This observation has
motivated the study of variants of the multi-armed bandit problem, in which
more general representations are used both for the type of feedback to learn
from and the target of prediction. The aim of this paper is to provide a survey
of the state of the art in this field, referred to as preference-based
multi-armed bandits or dueling bandits. To this end, we provide an overview of
problems that have been considered in the literature as well as methods for
tackling them. Our taxonomy is mainly based on the assumptions made by these
methods about the data-generating process and, related to this, the properties
of the preference-based feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busa_Fekete_R/0/1/0/all/0/1&quot;&gt;Robert Busa-Fekete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesaoudi_Paul_A/0/1/0/all/0/1&quot;&gt;Adil El Mesaoudi-Paul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11407">
<title>HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning. (arXiv:1807.11407v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11407</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a new model for leveraging unlabeled data to
improve generalization performances of image classifiers: a two-branch
encoder-decoder architecture called HybridNet. The first branch receives
supervision signal and is dedicated to the extraction of invariant
class-related representations. The second branch is fully unsupervised and
dedicated to model information discarded by the first branch to reconstruct
input data. To further support the expected behavior of our model, we propose
an original training objective. It favors stability in the discriminative
branch and complementarity between the learned representations in the two
branches. HybridNet is able to outperform state-of-the-art results on CIFAR-10,
SVHN and STL-10 in various semi-supervised settings. In addition,
visualizations and ablation studies validate our contributions and the behavior
of the model on both CIFAR-10 and STL-10 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robert_T/0/1/0/all/0/1&quot;&gt;Thomas Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11408">
<title>Local Linear Forests. (arXiv:1807.11408v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.11408</link>
<description rdf:parseType="Literal">&lt;p&gt;Random forests are a powerful method for non-parametric regression, but are
limited in their ability to fit smooth signals, and can show poor predictive
performance in the presence of strong, smooth effects. Taking the perspective
of random forests as an adaptive kernel method, we pair the forest kernel with
a local linear regression adjustment to better capture smoothness. The
resulting procedure, local linear forests, enables us to improve on asymptotic
rates of convergence for random forests with smooth signals, and provides
substantial gains in accuracy on both real and simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Friedberg_R/0/1/0/all/0/1&quot;&gt;Rina Friedberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tibshirani_J/0/1/0/all/0/1&quot;&gt;Julie Tibshirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wager_S/0/1/0/all/0/1&quot;&gt;Stefan Wager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11419">
<title>High-dimensional estimation via sum-of-squares proofs. (arXiv:1807.11419v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1807.11419</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimation is the computational task of recovering a hidden parameter $x$
associated with a distribution $D_x$, given a measurement $y$ sampled from the
distribution. High dimensional estimation problems arise naturally in
statistics, machine learning, and complexity theory.
&lt;/p&gt;
&lt;p&gt;Many high dimensional estimation problems can be formulated as systems of
polynomial equations and inequalities, and thus give rise to natural
probability distributions over polynomial systems. Sum-of-squares proofs
provide a powerful framework to reason about polynomial systems, and further
there exist efficient algorithms to search for low-degree sum-of-squares
proofs.
&lt;/p&gt;
&lt;p&gt;Understanding and characterizing the power of sum-of-squares proofs for
estimation problems has been a subject of intense study in recent years. On one
hand, there is a growing body of work utilizing sum-of-squares proofs for
recovering solutions to polynomial systems when the system is feasible. On the
other hand, a general technique referred to as pseudocalibration has been
developed towards showing lower bounds on the degree of sum-of-squares proofs.
Finally, the existence of sum-of-squares refutations of a polynomial system has
been shown to be intimately connected to the existence of spectral algorithms.
In this article we survey these developments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavendra_P/0/1/0/all/0/1&quot;&gt;Prasad Raghavendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1&quot;&gt;Tselil Schramm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steurer_D/0/1/0/all/0/1&quot;&gt;David Steurer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11455">
<title>Factor analysis of dynamic PET images: beyond Gaussian noise. (arXiv:1807.11455v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1807.11455</link>
<description rdf:parseType="Literal">&lt;p&gt;Factor analysis has proven to be a relevant tool for extracting tissue
time-activity curves (TACs) in dynamic PET images, since it allows for an
unsupervised analysis of the data. To provide reliable and interpretable
outputs, it requires to be conducted with respect to a suitable noise
statistics. However, the noise in reconstructed dynamic PET images is very
difficult to characterize, despite the Poissonian nature of the count-rates.
Rather than explicitly modeling the noise distribution, this work proposes to
study the relevance of several divergence measures to be used within a factor
analysis framework. To this end, the $\beta$-divergence, widely used in other
applicative domains, is considered to design the data-fitting term involved in
three different factor models. The performances of the resulting algorithms are
evaluated for different values of $\beta$, in a range covering Gaussian,
Poissonian and Gamma-distributed noises. The results obtained on two different
types of synthetic images and one real image show the interest of applying
non-standard values of $\beta$ to improve factor analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cavalcanti_Y/0/1/0/all/0/1&quot;&gt;Yanna Cruz Cavalcanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oberlin_T/0/1/0/all/0/1&quot;&gt;Thomas Oberlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dobigeon_N/0/1/0/all/0/1&quot;&gt;Nicolas Dobigeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stute_S/0/1/0/all/0/1&quot;&gt;Simon Stute&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ribeiro_M/0/1/0/all/0/1&quot;&gt;Maria-Joao Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tauber_C/0/1/0/all/0/1&quot;&gt;Clovis Tauber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11470">
<title>Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis. (arXiv:1807.11470v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1807.11470</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating versatile and appropriate synthetic speech requires control over
the output expression separate from the spoken text. Important non-textual
speech variation is seldom annotated, in which case output control must be
learned in an unsupervised fashion. In this paper, we perform an in-depth study
of methods for unsupervised learning of control in statistical speech
synthesis. For example, we show that popular unsupervised training heuristics
can be interpreted as variational inference in certain autoencoder models. We
additionally connect these models to VQ-VAEs, another, recently-proposed class
of deep variational autoencoders, which we show can be derived from a very
similar mathematical argument. The implications of these new probabilistic
interpretations are discussed. We illustrate the utility of the various
approaches with an application to emotional speech synthesis, where the
unsupervised methods for learning expression control (without access to
emotional labels) are found to give results that in many aspects match or
surpass the previous best supervised approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.05889">
<title>Tight Performance Bounds for Compressed Sensing With Conventional and Group Sparsity. (arXiv:1606.05889v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1606.05889</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of recovering a group sparse vector from
a small number of linear measurements. In the past the common approach has been
to use various &quot;group sparsity-inducing&quot; norms such as the Group LASSO norm for
this purpose. By using the theory of convex relaxations, we show that it is
also possible to use $\ell_1$-norm minimization for group sparse recovery. We
introduce a new concept called group robust null space property (GRNSP), and
show that, under suitable conditions, a group version of the restricted
isometry property (GRIP) implies the GRNSP, and thus leads to group sparse
recovery. When all groups are of equal size, our bounds are less conservative
than known bounds. Moreover, our results apply even to situations where where
the groups have different sizes. When specialized to conventional sparsity, our
bounds reduce to one of the well-known &quot;best possible&quot; conditions for sparse
recovery. This relationship between GRNSP and GRIP is new even for conventional
sparsity, and substantially streamlines the proofs of some known results. Using
this relationship, we derive bounds on the $\ell_p$-norm of the residual error
vector for all $p \in [1,2]$, and not just when $p = 2$. When the measurement
matrix consists of random samples of a sub-Gaussian random variable, we present
bounds on the number of measurements, which are less conservative than
currently known bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ranjan_S/0/1/0/all/0/1&quot;&gt;Shashank Ranjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vidyasagar_M/0/1/0/all/0/1&quot;&gt;Mathukumalli Vidyasagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05506">
<title>A statistical interpretation of spectral embedding: the generalised random dot product graph. (arXiv:1709.05506v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05506</link>
<description rdf:parseType="Literal">&lt;p&gt;A generalisation of a latent position network model known as the random dot
product graph model is considered. The resulting model may be of independent
interest because it has the unique property of representing a mixture of
connectivity behaviours as the corresponding convex combination in latent
space. We show that, whether the normalised Laplacian or adjacency matrix is
used, the vector representations of nodes obtained by spectral embedding
provide strongly consistent latent position estimates with asymptotically
Gaussian error. Direct methodological consequences follow from the observation
that the well-known mixed membership and standard stochastic block models are
special cases where the latent positions live respectively inside or on the
vertices of a simplex. Estimation via spectral embedding can therefore be
achieved by respectively estimating this simplicial support, or fitting a
Gaussian mixture model. In the latter case, the use of $K$-means, as has been
previously recommended, is suboptimal and for identifiability reasons unsound.
Empirical improvements in link prediction, as well as the potential to uncover
much richer latent structure (than available under the mixed membership or
standard stochastic block models) are demonstrated in a cyber-security example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rubin_Delanchy_P/0/1/0/all/0/1&quot;&gt;Patrick Rubin-Delanchy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Minh Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cape_J/0/1/0/all/0/1&quot;&gt;Joshua Cape&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06688">
<title>Property Testing in High Dimensional Ising models. (arXiv:1709.06688v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06688</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the information-theoretic limitations of graph property
testing in zero-field Ising models. Instead of learning the entire graph
structure, sometimes testing a basic graph property such as connectivity, cycle
presence or maximum clique size is a more relevant and attainable objective.
Since property testing is more fundamental than graph recovery, any necessary
conditions for property testing imply corresponding conditions for graph
recovery, while custom property tests can be statistically and/or
computationally more efficient than graph recovery based algorithms.
Understanding the statistical complexity of property testing requires the
distinction of ferromagnetic (i.e., positive interactions only) and general
Ising models. Using combinatorial constructs such as graph packing and strong
monotonicity, we characterize how target properties affect the corresponding
minimax upper and lower bounds within the realm of ferromagnets. On the other
hand, by studying the detection of an antiferromagnetic (i.e., negative
interactions only) Curie-Weiss model buried in Rademacher noise, we show that
property testing is strictly more challenging over general Ising models. In
terms of methodological development, we propose two types of correlation based
tests: computationally efficient screening for ferromagnets, and score type
tests for general models, including a fast cycle presence test. Our correlation
screening tests match the information-theoretic bounds for property testing in
ferromagnets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Neykov_M/0/1/0/all/0/1&quot;&gt;Matey Neykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10733">
<title>Attacking the Madry Defense Model with $L_1$-based Adversarial Examples. (arXiv:1710.10733v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10733</link>
<description rdf:parseType="Literal">&lt;p&gt;The Madry Lab recently hosted a competition designed to test the robustness
of their adversarially trained MNIST model. Attacks were constrained to perturb
each pixel of the input image by a scaled maximal $L_\infty$ distortion
$\epsilon$ = 0.3. This discourages the use of attacks which are not optimized
on the $L_\infty$ distortion metric. Our experimental results demonstrate that
by relaxing the $L_\infty$ constraint of the competition, the elastic-net
attack to deep neural networks (EAD) can generate transferable adversarial
examples which, despite their high average $L_\infty$ distortion, have minimal
visual distortion. These results call into question the use of $L_\infty$ as a
sole measure for visual distortion, and further demonstrate the power of EAD at
generating robust adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1&quot;&gt;Yash Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07230">
<title>Regret Analysis for Adaptive Linear-Quadratic Policies. (arXiv:1711.07230v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07230</link>
<description rdf:parseType="Literal">&lt;p&gt;In the classical problem of Linear-Quadratic (LQ) control, when the
parameters of the system&apos;s dynamics are unknown, an adaptive policy is needed
to learn those parameters and also plan a control action. The resulting
trade-off between accurate parameter estimation (exploration) and effective
control (exploitation) represents the main challenge in the area of adaptive
control. Asymptotic approaches have been extensively studied in the literature,
but there is a dearth of non-asymptotic results that in addition are rather
incomplete.
&lt;/p&gt;
&lt;p&gt;This study establishes high probability regret bounds for the aforementioned
problem that are optimal up to logarithmic factors. The results on finite time
analysis of the regret are obtained under very mild assumptions, requiring: (i)
stabilizability of the system&apos;s dynamics, and (ii) limiting the degree of
heaviness of the noise distribution. To establish such bounds, certain novel
techniques are introduced to comprehensively address the probabilistic behavior
of dependent random matrices with heavy-tailed distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faradonbeh_M/0/1/0/all/0/1&quot;&gt;Mohamad Kazem Shirani Faradonbeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ambuj Tewari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michailidis_G/0/1/0/all/0/1&quot;&gt;George Michailidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08364">
<title>ForestHash: Semantic Hashing With Shallow Random Forests and Tiny Convolutional Networks. (arXiv:1711.08364v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08364</link>
<description rdf:parseType="Literal">&lt;p&gt;Hash codes are efficient data representations for coping with the ever
growing amounts of data. In this paper, we introduce a random forest semantic
hashing scheme that embeds tiny convolutional neural networks (CNN) into
shallow random forests, with near-optimal information-theoretic code
aggregation among trees. We start with a simple hashing scheme, where random
trees in a forest act as hashing functions by setting `1&apos; for the visited tree
leaf, and `0&apos; for the rest. We show that traditional random forests fail to
generate hashes that preserve the underlying similarity between the trees,
rendering the random forests approach to hashing challenging. To address this,
we propose to first randomly group arriving classes at each tree split node
into two groups, obtaining a significantly simplified two-class classification
problem, which can be handled using a light-weight CNN weak learner. Such
random class grouping scheme enables code uniqueness by enforcing each class to
share its code with different classes in different trees. A non-conventional
low-rank loss is further adopted for the CNN weak learners to encourage code
consistency by minimizing intra-class variations and maximizing inter-class
distance for the two random class groups. Finally, we introduce an
information-theoretic approach for aggregating codes of individual trees into a
single hash code, producing a near-optimal unique hash for each class. The
proposed approach significantly outperforms state-of-the-art hashing methods
for image retrieval tasks on large-scale public datasets, while performing at
the level of other state-of-the-art image classification techniques while
utilizing a more compact and efficient scalable representation. This work
proposes a principled and robust procedure to train and deploy in parallel an
ensemble of light-weight CNNs, instead of simply going deeper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lezama_J/0/1/0/all/0/1&quot;&gt;Jose Lezama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1&quot;&gt;Alex Bronstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapiro_G/0/1/0/all/0/1&quot;&gt;Guillermo Sapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04145">
<title>DCFNet: Deep Neural Network with Decomposed Convolutional Filters. (arXiv:1802.04145v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04145</link>
<description rdf:parseType="Literal">&lt;p&gt;Filters in a Convolutional Neural Network (CNN) contain model parameters
learned from enormous amounts of data. In this paper, we suggest to decompose
convolutional filters in CNN as a truncated expansion with pre-fixed bases,
namely the Decomposed Convolutional Filters network (DCFNet), where the
expansion coefficients remain learned from data. Such a structure not only
reduces the number of trainable parameters and computation, but also imposes
filter regularity by bases truncation. Through extensive experiments, we
consistently observe that DCFNet maintains accuracy for image classification
tasks with a significant reduction of model parameters, particularly with
Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we
analyze the representation stability of DCFNet with respect to input
variations, and prove representation stability under generic assumptions on the
expansion coefficients. The analysis is consistent with the empirical
observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Calderbank_R/0/1/0/all/0/1&quot;&gt;Robert Calderbank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sapiro_G/0/1/0/all/0/1&quot;&gt;Guillermo Sapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06328">
<title>Nesting Probabilistic Programs. (arXiv:1803.06328v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06328</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the notion of nesting probabilistic programming queries and
investigate the resulting statistical implications. We demonstrate that while
query nesting allows the definition of models which could not otherwise be
expressed, such as those involving agents reasoning about other agents,
existing systems take approaches which lead to inconsistent estimates. We show
how to correct this by delineating possible ways one might want to nest queries
and asserting the respective conditions required for convergence. We further
introduce a new online nested Monte Carlo estimator that makes it substantially
easier to ensure these conditions are met, thereby providing a simple framework
for designing statistically correct inference engines. We prove the correctness
of this online estimator and show that, when using the recommended setup, its
asymptotic variance is always better than that of the equivalent fixed
estimator, while its bias is always within a factor of two.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07519">
<title>DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems. (arXiv:1803.07519v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07519</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) defines a new data-driven programming paradigm that
constructs the internal system logic of a crafted neuron network through a set
of training data. We have seen wide adoption of DL in many safety-critical
scenarios. However, a plethora of studies have shown that the state-of-the-art
DL systems suffer from various vulnerabilities which can lead to severe
consequences when applied to real-world applications. Currently, the testing
adequacy of a DL system is usually measured by the accuracy of test data.
Considering the limitation of accessible high quality test data, good accuracy
performance on test data can hardly provide confidence to the testing adequacy
and generality of DL systems. Unlike traditional software systems that have
clear and controllable logic and functionality, the lack of interpretability in
a DL system makes system analysis and defect detection difficult, which could
potentially hinder its real-world deployment. In this paper, we propose
DeepGauge, a set of multi-granularity testing criteria for DL systems, which
aims at rendering a multi-faceted portrayal of the testbed. The in-depth
evaluation of our proposed testing criteria is demonstrated on two well-known
datasets, five DL systems, and with four state-of-the-art adversarial attack
techniques against DL. The potential usefulness of DeepGauge sheds light on the
construction of more generic and robust DL systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1&quot;&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiyuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1&quot;&gt;Ting Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yadong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01189">
<title>Real-Time Prediction of the Duration of Distribution System Outages. (arXiv:1804.01189v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01189</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of predicting duration of unplanned power
outages, using historical outage records to train a series of neural network
predictors. The initial duration prediction is made based on environmental
factors, and it is updated based on incoming field reports using natural
language processing to automatically analyze the text. Experiments using 15
years of outage records show good initial results and improved performance
leveraging text. Case studies show that the language processing identifies
phrases that point to outage causes and repair steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaech_A/0/1/0/all/0/1&quot;&gt;Aaron Jaech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baosen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1&quot;&gt;Mari Ostendorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschen_D/0/1/0/all/0/1&quot;&gt;Daniel S. Kirschen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02034">
<title>Degrees of Freedom and Model Selection for kmeans Clustering. (arXiv:1806.02034v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02034</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the problem of model selection for kmeans clustering,
based on conservative estimates of the model degrees of freedom. An extension
of Stein&apos;s lemma, which is used in unbiased risk estimation, is used to obtain
an expression which allows one to approximate the degrees of freedom.
Empirically based estimates of this approximation are obtained. The degrees of
freedom estimates are then used within the popular Bayesian Information
Criterion to perform model selection. The proposed estimation procedure is
validated in a thorough simulation study, and the robustness is assessed
through relaxations of the modelling assumptions and on data from real
applications. Comparisons with popular existing techniques suggest that this
approach performs extremely well when the modelling assumptions
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hofmeyr_D/0/1/0/all/0/1&quot;&gt;David P. Hofmeyr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08317">
<title>Fashion-Gen: The Generative Fashion Dataset and Challenge. (arXiv:1806.08317v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.08317</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new dataset of 293,008 high definition (1360 x 1360 pixels)
fashion images paired with item descriptions provided by professional stylists.
Each item is photographed from a variety of angles. We provide baseline results
on 1) high-resolution image generation, and 2) image generation conditioned on
the given text descriptions. We invite the community to improve upon these
baselines. In this paper, we also outline the details of a challenge that we
are launching based upon this dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rostamzadeh_N/0/1/0/all/0/1&quot;&gt;Negar Rostamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hosseini_S/0/1/0/all/0/1&quot;&gt;Seyedarian Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boquet_T/0/1/0/all/0/1&quot;&gt;Thomas Boquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stokowiec_W/0/1/0/all/0/1&quot;&gt;Wojciech Stokowiec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jauvin_C/0/1/0/all/0/1&quot;&gt;Christian Jauvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Chris Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07525">
<title>Emulating malware authors for proactive protection using GANs over a distributed image visualization of dynamic file behavior. (arXiv:1807.07525v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07525</link>
<description rdf:parseType="Literal">&lt;p&gt;Malware authors have always been at an advantage of being able to
adversarially test and augment their malicious code, before deploying the
payload, using anti-malware products at their disposal. The anti-malware
developers and threat experts, on the other hand, do not have such a privilege
of tuning anti-malware products against zero-day attacks pro-actively. This
allows the malware authors to being a step ahead of the anti-malware products,
fundamentally biasing the cat and mouse game played by the two parties. In this
paper, we propose a way that would enable machine learning based threat
prevention models to bridge that gap by being able to tune against a deep
generative adversarial network (GAN), which takes up the role of a malware
author and generates new types of malware. The GAN is trained over a reversible
distributed RGB image representation of known malware behaviors, encoding the
sequence of API call ngrams and the corresponding term frequencies. The
generated images represent synthetic malware that can be decoded back to the
underlying API call sequence information. The image representation is not only
demonstrated as a general technique of incorporating necessary priors for
exploiting convolutional neural network architectures for generative or
discriminative modeling, but also as a visualization method for easy manual
software or malware categorization, by having individual API ngram information
distributed across the image space. In addition, we also propose using
smart-definitions for detecting malwares based on perceptual hashing of these
images. Such hashes are potentially more effective than cryptographic hashes
that do not carry any meaningful similarity metric, and hence, do not
generalize well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhaskara_V/0/1/0/all/0/1&quot;&gt;Vineeth S. Bhaskara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhattacharyya_D/0/1/0/all/0/1&quot;&gt;Debanjan Bhattacharyya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09386">
<title>On the Randomized Complexity of Minimizing a Convex Quadratic Function. (arXiv:1807.09386v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.09386</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizing a convex, quadratic objective is a fundamental problem in machine
learning and optimization. In this work, we study prove
$\textit{information-theoretic}$, gradient query complexity lower bounds for
minimizing convex quadratic functions, which, unlike prior works, apply even
for $\textit{randomized}$ algorithms. Specifically, we construct a distribution
over quadratic functions that witnesses lower bounds which match those known
for deterministic algorithms, up to multiplicative constants. The distribution
which witnesses our lower bound is in fact quite benign: it is both closed
form, and derived from classical ensembles in random matrix theory. We believe
that our construction constitutes a plausible &quot;average case&quot; setting, and thus
provides compelling evidence that the worst case and average case complexity of
convex-quadratic optimization are essentially identical.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10278">
<title>Structured Point Cloud Data Analysis via Regularized Tensor Regression for Process Modeling and Optimization. (arXiv:1807.10278v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10278</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced 3D metrology technologies such as Coordinate Measuring Machine (CMM)
and laser 3D scanners have facilitated the collection of massive point cloud
data, beneficial for process monitoring, control and optimization. However, due
to their high dimensionality and structure complexity, modeling and analysis of
point clouds are still a challenge. In this paper, we utilize multilinear
algebra techniques and propose a set of tensor regression approaches to model
the variational patterns of point clouds and to link them to process variables.
The performance of the proposed methods is evaluated through simulations and a
real case study of turning process optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paynabar_K/0/1/0/all/0/1&quot;&gt;Kamran Paynabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacella_M/0/1/0/all/0/1&quot;&gt;Massimo Pacella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10623">
<title>Learning low dimensional word based linear classifiers using Data Shared Adaptive Bootstrap Aggregated Lasso with application to IMDb data. (arXiv:1807.10623v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10623</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we propose a new supervised ensemble learning method called
Data Shared Adaptive Bootstrap Aggregated (AdaBag) Lasso for capturing low
dimensional useful features for word based sentiment analysis and mining
problems. The literature on ensemble methods is very rich in both statistics
and machine learning. The algorithm is a substantial upgrade of the Data Shared
Lasso uplift algorithm. The most significant conceptual addition to the
existing literature lies in the final selection of bag of predictors through a
special bootstrap aggregation scheme. We apply the algorithm to one simulated
data and perform dimension reduction in grouped IMDb data (drama, comedy and
horror) to extract reduced set of word features for predicting sentiment
ratings of movie reviews demonstrating different aspects. We also compare the
performance of the present method with the classical Principal Components with
associated Linear Discrimination (PCA-LD) as baseline. There are few
limitations in the algorithm. Firstly, the algorithm workflow does not
incorporate online sequential data acquisition and it does not use sentence
based models which are common in ANN algorithms . Our results produce slightly
higher error rate compare to the reported state-of-the-art as a consequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maurya_A/0/1/0/all/0/1&quot;&gt;Ashutosh K. Maurya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02109">
<title>Predicting Race and Ethnicity From the Sequence of Characters in a Name. (arXiv:1805.02109v1 [stat.AP] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.02109</link>
<description rdf:parseType="Literal">&lt;p&gt;To answer questions about racial inequality, we often need a way to infer
race and ethnicity from a name. Until now, a bulk of the focus has been on
optimally exploiting the last names list provided by the Census Bureau. But
there is more information in the first names, especially for African Americans.
To estimate the relationship between full names and race, we exploit the
Florida voter registration data and the Wikipedia data. In particular, we model
the relationship between the sequence of characters in a name, and race and
ethnicity using Long Short Term Memory Networks. Our out of sample (OOS)
precision and recall for the full name model estimated on the Florida Voter
Registration data is .83 and .84 respectively. This compares to OOS precision
and recall of .79 and .81 for the last name only model. Commensurate numbers
for Wikipedia data are .73 and .73 for the full name model and .66 and .67 for
the last name model. To illustrate the use of this method, we apply our method
to the campaign finance data to estimate the share of donations made by people
of various racial groups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sood_G/0/1/0/all/0/1&quot;&gt;Gaurav Sood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laohaprapanon_S/0/1/0/all/0/1&quot;&gt;Suriyan Laohaprapanon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10320">
<title>High Dimensional Model Representation as a Glass Box in Supervised Machine Learning. (arXiv:1807.10320v1 [stat.ME] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.10320</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction and explanation are key objects in supervised machine learning,
where predictive models are known as black boxes and explanatory models are
known as glass boxes. Explanation provides the necessary and sufficient
information to interpret the model output in terms of the model input. It
includes assessments of model output dependence on important input variables
and measures of input variable importance to model output. High dimensional
model representation (HDMR), also known as the generalized functional ANOVA
expansion, provides useful insight into the input-output behavior of supervised
machine learning models. This article gives applications of HDMR in supervised
machine learning. The first application is characterizing information leakage
in ``big-data&apos;&apos; settings. The second application is reduced-order
representation of elementary symmetric polynomials. The third application is
analysis of variance with correlated variables. The last application is
estimation of HDMR from kernel machine and decision tree black box
representations. These results suggest HDMR to have broad utility within
machine learning as a glass box representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bastian_C/0/1/0/all/0/1&quot;&gt;Caleb Deen Bastian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rabitz_H/0/1/0/all/0/1&quot;&gt;Herschel Rabitz&lt;/a&gt;</dc:creator>
</item></rdf:RDF>