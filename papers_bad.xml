<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07116"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07312"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.09902"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07452"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.05632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10903"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07177"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07325"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07454"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.02068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.03063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.00661"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.07056"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02184"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07019"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08826"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.02691"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07106"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01145"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.07116">
<title>Detection and classification of masses in mammographic images in a multi-kernel approach. (arXiv:1712.07116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.07116</link>
<description rdf:parseType="Literal">&lt;p&gt;According to the World Health Organization, breast cancer is the main cause
of cancer death among adult women in the world. Although breast cancer occurs
indiscriminately in countries with several degrees of social and economic
development, among developing and underdevelopment countries mortality rates
are still high, due to low availability of early detection technologies. From
the clinical point of view, mammography is still the most effective diagnostic
technology, given the wide diffusion of the use and interpretation of these
images. Herein this work we propose a method to detect and classify
mammographic lesions using the regions of interest of images. Our proposal
consists in decomposing each image using multi-resolution wavelets. Zernike
moments are extracted from each wavelet component. Using this approach we can
combine both texture and shape features, which can be applied both to the
detection and classification of mammary lesions. We used 355 images of fatty
breast tissue of IRMA database, with 233 normal instances (no lesion), 72
benign, and 83 malignant cases. Classification was performed by using SVM and
ELM networks with modified kernels, in order to optimize accuracy rates,
reaching 94.11%. Considering both accuracy rates and training times, we defined
the ration between average percentage accuracy and average training time in a
reverse order. Our proposal was 50 times higher than the ratio obtained using
the best method of the state-of-the-art. As our proposed model can combine high
accuracy rate with low learning time, whenever a new data is received, our work
will be able to save a lot of time, hours, in learning process in relation to
the best method of the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_S/0/1/0/all/0/1&quot;&gt;Sidney Marlon Lopes de Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filho_A/0/1/0/all/0/1&quot;&gt;Abel Guilhermino da Silva Filho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_W/0/1/0/all/0/1&quot;&gt;Wellington Pinheiro dos Santos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07165">
<title>Scale-invariant temporal history (SITH): optimal slicing of the past in an uncertain world. (arXiv:1712.07165v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07165</link>
<description rdf:parseType="Literal">&lt;p&gt;In both the human brain and any general artificial intelligence (AI), a
representation of the past is necessary to predict the future. However, perfect
storage of all experiences is not possible. One possibility, utilized in many
applications, is to retain information about the past in a buffer. A limitation
of this approach is that although events in the buffer are represented with
perfect accuracy, the resources necessary to represent information at a
particular time scale go up rapidly. Here we present a neurally-plausible,
compressed, scale-free memory representation we call Scale-Invariant Temporal
History (SITH). This representation covers an exponentially large period of
time in the past at the cost of sacrificing temporal accuracy for events
further in the past. The form of this decay is scale-invariant and can be shown
to be optimal in that it is able to respond to worlds with a wide range of time
scales. We demonstrate the utility of this representation in learning to play a
simple video game. In this environment, SITH exhibits better learning
performance than a fixed-size buffer history representation. Whereas the buffer
performs well as long as the temporal dependencies can be represented within
the buffer, SITH performs well over a much larger range of time scales for the
same amount of resources. Finally, we discuss how the application of SITH,
along with other human-inspired models of cognition, could improve
reinforcement and machine learning algorithms in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spears_T/0/1/0/all/0/1&quot;&gt;Tyler A. Spears&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacques_B/0/1/0/all/0/1&quot;&gt;Brandon G. Jacques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_M/0/1/0/all/0/1&quot;&gt;Marc W. Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sederberg_P/0/1/0/all/0/1&quot;&gt;Per B. Sederberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07199">
<title>Cognitive Database: A Step towards Endowing Relational Databases with Artificial Intelligence Capabilities. (arXiv:1712.07199v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1712.07199</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Cognitive Databases, an approach for transparently enabling
Artificial Intelligence (AI) capabilities in relational databases. A novel
aspect of our design is to first view the structured data source as meaningful
unstructured text, and then use the text to build an unsupervised neural
network model using a Natural Language Processing (NLP) technique called word
embedding. This model captures the hidden inter-/intra-column relationships
between database tokens of different types. For each database token, the model
includes a vector that encodes contextual semantic relationships. We seamlessly
integrate the word embedding model into existing SQL query infrastructure and
use it to enable a new class of SQL-based analytics queries called cognitive
intelligence (CI) queries. CI queries use the model vectors to enable complex
queries such as semantic matching, inductive reasoning queries such as
analogies, predictive queries using entities not present in a database, and,
more generally, using knowledge from external sources. We demonstrate unique
capabilities of Cognitive Databases using an Apache Spark based prototype to
execute inductive reasoning CI queries over a multi-modal database containing
text and images. We believe our first-of-a-kind system exemplifies using AI
functionality to endow relational databases with capabilities that were
previously very hard to realize in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordawekar_R/0/1/0/all/0/1&quot;&gt;Rajesh Bordawekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_B/0/1/0/all/0/1&quot;&gt;Bortik Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmueli_O/0/1/0/all/0/1&quot;&gt;Oded Shmueli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07257">
<title>Multi-shot Pedestrian Re-identification via Sequential Decision Making. (arXiv:1712.07257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.07257</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-shot pedestrian re-identification problem is at the core of
surveillance video analysis. It matches two tracks of pedestrians from
different cameras. In contrary to existing works that aggregate single frames
features by time series model such as recurrent neural network, in this paper,
we propose an interpretable reinforcement learning based approach to this
problem. Particularly, we train an agent to verify a pair of images at each
time. The agent could choose to output the result (same or different) or
request another pair of images to see (unsure). By this way, our model
implicitly learns the difficulty of image pairs, and postpone the decision when
the model does not accumulate enough evidence. Moreover, by adjusting the
reward for unsure action, we can easily trade off between speed and accuracy.
In three open benchmarks, our method are competitive with the state-of-the-art
methods while only using 3% to 6% images. These promising results demonstrate
that our method is favorable in both efficiency and performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianfu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Naiyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liqing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07312">
<title>Analysis of supervised and semi-supervised GrowCut applied to segmentation of masses in mammography images. (arXiv:1712.07312v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.07312</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast cancer is already one of the most common form of cancer worldwide.
Mammography image analysis is still the most effective diagnostic method to
promote the early detection of breast cancer. Accurately segmenting tumors in
digital mammography images is important to improve diagnosis capabilities of
health specialists and avoid misdiagnosis. In this work, we evaluate the
feasibility of applying GrowCut to segment regions of tumor and we propose two
GrowCut semi-supervised versions. All the analysis was performed by evaluating
the application of segmentation techniques to a set of images obtained from the
Mini-MIAS mammography image database. GrowCut segmentation was compared to
Region Growing, Active Contours, Random Walks and Graph Cut techniques.
Experiments showed that GrowCut, when compared to the other techniques, was
able to acquire better results for the metrics analyzed. Moreover, the proposed
semi-supervised versions of GrowCut was proved to have a clinically
satisfactory quality of segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordeiro_F/0/1/0/all/0/1&quot;&gt;Filipe Rolim Cordeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_W/0/1/0/all/0/1&quot;&gt;Wellington Pinheiro dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filho_A/0/1/0/all/0/1&quot;&gt;Abel Guilhermino da Silva Filho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07447">
<title>Dataflow Matrix Machines and V-values: a Bridge between Programs and Neural Nets. (arXiv:1712.07447v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.07447</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataflow matrix machines generalize neural nets by replacing streams of
numbers with streams of vectors (or other kinds of linear streams admitting a
notion of linear combination of several streams) and adding a few more changes
on top of that, namely arbitrary input and output arities for activation
functions, countable-sized networks with finite dynamically changeable active
part capable of unbounded growth, and a very expressive self-referential
mechanism.
&lt;/p&gt;
&lt;p&gt;While recurrent neural networks are Turing-complete, they form an esoteric
programming platform, not conductive for practical general-purpose programming.
Dataflow matrix machines are more suitable as a general-purpose programming
platform, although it remains to be seen whether this platform can be made
fully competitive with more traditional programming platforms currently in use.
At the same time, dataflow matrix machines retain the key property of recurrent
neural networks: programs are expressed via matrices of real numbers, and
continuous changes to those matrices produce arbitrarily small variations in
the programs associated with those matrices.
&lt;/p&gt;
&lt;p&gt;Spaces of vector-like elements are of particular importance in this context.
In particular, we focus on the vector space $V$ of finite linear combinations
of strings, which can be also understood as the vector space of finite prefix
trees with numerical leaves, the vector space of &quot;mixed rank tensors&quot;, or the
vector space of recurrent maps.
&lt;/p&gt;
&lt;p&gt;This space, and a family of spaces of vector-like elements derived from it,
are sufficiently expressive to cover all cases of interest we are currently
aware of, and allow a compact and streamlined version of dataflow matrix
machines based on a single space of vector-like elements and variadic neurons.
We call elements of these spaces V-values. Their role in our context is
somewhat similar to the role of S-expressions in Lisp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bukatin_M/0/1/0/all/0/1&quot;&gt;Michael Bukatin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthony_J/0/1/0/all/0/1&quot;&gt;Jon Anthony&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.09902">
<title>Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. (arXiv:1703.09902v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1703.09902</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper surveys the current state of the art in Natural Language
Generation (NLG), defined as the task of generating text or speech from
non-linguistic input. A survey of NLG is timely in view of the changes that the
field has undergone over the past decade or so, especially in relation to new
(usually data-driven) methods, as well as new applications of NLG technology.
This survey therefore aims to (a) give an up-to-date synthesis of research on
the core tasks in NLG and the architectures adopted in which such tasks are
organised; (b) highlight a number of relatively recent research topics that
have arisen partly as a result of growing synergies between NLG and other areas
of artificial intelligence; (c) draw attention to the challenges in NLG
evaluation, relating them to similar challenges faced in other areas of Natural
Language Processing, with an emphasis on different evaluation methods and the
relationships between them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1&quot;&gt;Albert Gatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krahmer_E/0/1/0/all/0/1&quot;&gt;Emiel Krahmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07294">
<title>Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning. (arXiv:1712.07294v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07294</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning policies for complex tasks that require multiple different skills is
a major challenge in reinforcement learning (RL). It is also a requirement for
its deployment in real-world scenarios. This paper proposes a novel framework
for efficient multi-task reinforcement learning. Our framework trains agents to
employ hierarchical policies that decide when to use a previously learned
policy and when to learn a new skill. This enables agents to continually
acquire new skills during different stages of training. Each learned task
corresponds to a human language description. Because agents can only access
previously learned skills through these descriptions, the agent can always
provide a human-interpretable description of its choices. In order to help the
agent learn the complex temporal dependencies necessary for the hierarchical
policy, we provide it with a stochastic temporal grammar that modulates when to
rely on previously learned skills and when to execute new skills. We validate
our approach on Minecraft games designed to explicitly test the ability to
reuse previously learned skills while simultaneously learning new skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1&quot;&gt;Tianmin Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07452">
<title>Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via Mental Simulation. (arXiv:1712.07452v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1712.07452</link>
<description rdf:parseType="Literal">&lt;p&gt;Everyday robotics are challenged to deal with autonomous product handling in
applications like logistics or retail, possibly causing damage on the items
during manipulation. Traditionally, most approaches try to minimize physical
interaction with goods. However, we propose to take into account any unintended
motion of objects in the scene and to learn manipulation strategies in a
self-supervised way which minimize the potential damage. The presented approach
consists of a planning method that determines the optimal sequence to
manipulate a number of objects in a scene with respect to possible damage by
simulating interaction and hence anticipating scene dynamics. The planned
manipulation sequences are taken as input to a machine learning process which
generalizes to new, unseen scenes in the same application scenario. This
learned manipulation strategy is continuously refined in a self-supervised
optimization cycle dur- ing load-free times of the system. Such a
simulation-in-the-loop setup is commonly known as mental simulation and allows
for efficient, fully automatic generation of training data as opposed to
classical supervised learning approaches. In parallel, the generated
manipulation strategies can be deployed in near-real time in an anytime
fashion. We evaluate our approach on one industrial scenario (autonomous
container unloading) and one retail scenario (autonomous shelf replenishment).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fromm_T/0/1/0/all/0/1&quot;&gt;Tobias Fromm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07525">
<title>Use of Deep Learning in Modern Recommendation System: A Summary of Recent Works. (arXiv:1712.07525v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07525</link>
<description rdf:parseType="Literal">&lt;p&gt;With the exponential increase in the amount of digital information over the
internet, online shops, online music, video and image libraries, search engines
and recommendation system have become the most convenient ways to find relevant
information within a short time. In the recent times, deep learning&apos;s advances
have gained significant attention in the field of speech recognition, image
processing and natural language processing. Meanwhile, several recent studies
have shown the utility of deep learning in the area of recommendation systems
and information retrieval as well. In this short review, we cover the recent
advances made in the field of recommendation using various variants of deep
learning technology. We organize the review in three parts: Collaborative
system, Content based system and Hybrid system. The review also discusses the
contribution of deep learning integrated recommendation systems into several
application domains. The review concludes by discussion of the impact of deep
learning in recommendation system in various domain and whether deep learning
has shown any significant improvement over the conventional systems for
recommendation. Finally, we also provide future directions of research which
are possible based on the current state of use of deep learning in
recommendation systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_A/0/1/0/all/0/1&quot;&gt;Ayush Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_P/0/1/0/all/0/1&quot;&gt;Pradeep Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pant_R/0/1/0/all/0/1&quot;&gt;Rakesh Pant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.05632">
<title>On the adoption of abductive reasoning for time series interpretation. (arXiv:1609.05632v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1609.05632</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series interpretation aims to provide an explanation of what is observed
in terms of its underlying processes. The present work is based on the
assumption that common classification-based approaches to time series
interpretation suffer from a set of inherent weaknesses whose ultimate cause
lies in the monotonic nature of the deductive reasoning paradigm. In this
document we propose a new approach to this problem based on the initial
hypothesis that abductive reasoning properly accounts for the human ability to
identify and characterize patterns appearing in a time series. The result of
the interpretation is a set of conjectures in the form of observations,
organized into an abstraction hierarchy, and explaining what has been observed.
A knowledge-based framework and a set of algorithms for the interpretation task
are provided, implementing a hypothesize-and-test cycle guided by an
attentional mechanism. As a representative application domain, the
interpretation of the electrocardiogram allows us to highlight the strengths of
the proposed approach in comparison with traditional classification-based
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teijeiro_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Teijeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felix_P/0/1/0/all/0/1&quot;&gt;Paulo F&amp;#xe9;lix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10903">
<title>Graph Attention Networks. (arXiv:1710.10903v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10903</link>
<description rdf:parseType="Literal">&lt;p&gt;We present graph attention networks (GATs), novel neural network
architectures that operate on graph-structured data, leveraging masked
self-attentional layers to address the shortcomings of prior methods based on
graph convolutions or their approximations. By stacking layers in which nodes
are able to attend over their neighborhoods&apos; features, we enable (implicitly)
specifying different weights to different nodes in a neighborhood, without
requiring any kind of costly matrix operation (such as inversion) or depending
on knowing the graph structure upfront. In this way, we address several key
challenges of spectral-based graph neural networks simultaneously, and make our
model readily applicable to inductive as well as transductive problems. Our GAT
models have achieved or matched state-of-the-art results across four
established transductive and inductive graph benchmarks: the Cora, Citeseer and
Pubmed citation network datasets, as well as a protein-protein interaction
dataset (wherein test graphs remain unseen during training).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cucurull_G/0/1/0/all/0/1&quot;&gt;Guillem Cucurull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Casanova_A/0/1/0/all/0/1&quot;&gt;Arantxa Casanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Romero_A/0/1/0/all/0/1&quot;&gt;Adriana Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06365">
<title>&apos;Indifference&apos; methods for managing agent rewards. (arXiv:1712.06365v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06365</link>
<description rdf:parseType="Literal">&lt;p&gt;Indifference is a class of methods that are used to control a reward based
agent, by, for example, safely changing their reward or policy, or making the
agent behave as if a certain outcome could never happen. These methods of
control work even if the implications of the agent&apos;s reward are otherwise not
fully understood. Though they all come out of similar ideas, indifference
techniques can be classified as way of achieving one or more of three distinct
goals: rewards dependent on certain events (with no motivation for the agent to
manipulate the probability of those events), effective disbelief that an event
will ever occur, and seamless transition from one behaviour to another. There
are five basic methods to achieve these three goals. This paper classifies and
analyses these methods on POMDPs (though the methods are highly portable to
other agent designs), and establishes their uses, strengths, and limitations.
It aims to make the tools of indifference generally accessible and usable to
agent designers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Stuart Armstrong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07177">
<title>Approximate Profile Maximum Likelihood. (arXiv:1712.07177v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.07177</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an efficient algorithm for approximate computation of the profile
maximum likelihood (PML), a variant of maximum likelihood maximizing the
probability of observing a sufficient statistic rather than the empirical
sample. The PML has appealing theoretical properties, but is difficult to
compute exactly. Inspired by observations gleaned from exactly solvable cases,
we look for an approximate PML solution, which, intuitively, clumps comparably
frequent symbols into one symbol. This amounts to lower-bounding a certain
matrix permanent by summing over a subgroup of the symmetric group rather than
the whole group during the computation. We extensively experiment with the
approximate solution, and find the empirical performance of our approach is
competitive and sometimes significantly better than state-of-the-art
performance for various estimation problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlichin_D/0/1/0/all/0/1&quot;&gt;Dmitri S. Pavlichin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weissman_T/0/1/0/all/0/1&quot;&gt;Tsachy Weissman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07230">
<title>Fusing Multifaceted Transaction Data for User Modeling and Demographic Prediction. (arXiv:1712.07230v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1712.07230</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring user characteristics such as demographic attributes is of the
utmost importance in many user-centric applications. Demographic data is an
enabler of personalization, identity security, and other applications. Despite
that, this data is sensitive and often hard to obtain. Previous work has shown
that purchase history can be used for multi-task prediction of many demographic
fields such as gender and marital status. Here we present an embedding based
method to integrate multifaceted sequences of transaction data, together with
auxiliary relational tables, for better user modeling and demographic
prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resheff_Y/0/1/0/all/0/1&quot;&gt;Yehezkel S. Resheff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahar_M/0/1/0/all/0/1&quot;&gt;Moni Shahar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07233">
<title>Hyperparameters Optimization in Deep Convolutional Neural Network / Bayesian Approach with Gaussian Process Prior. (arXiv:1712.07233v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.07233</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Network is known as ConvNet have been extensively used
in many complex machine learning tasks. However, hyperparameters optimization
is one of a crucial step in developing ConvNet architectures, since the
accuracy and performance are reliant on the hyperparameters. This multilayered
architecture parameterized by a set of hyperparameters such as the number of
convolutional layers, number of fully connected dense layers &amp;amp; neurons, the
probability of dropout implementation, learning rate. Hence the searching the
hyperparameter over the hyperparameter space are highly difficult to build such
complex hierarchical architecture. Many methods have been proposed over the
decade to explore the hyperparameter space and find the optimum set of
hyperparameter values. Reportedly, Gird search and Random search are said to be
inefficient and extremely expensive, due to a large number of hyperparameters
of the architecture. Hence, Sequential model-based Bayesian Optimization is a
promising alternative technique to address the extreme of the unknown cost
function. The recent study on Bayesian Optimization by Snoek in nine
convolutional network parameters is achieved the lowerest error report in the
CIFAR-10 benchmark. This article is intended to provide the overview of the
mathematical concept behind the Bayesian Optimization over a Gaussian prior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugan_P/0/1/0/all/0/1&quot;&gt;Pushparaja Murugan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07325">
<title>Model-Based Clustering of Time-Evolving Networks through Temporal Exponential-Family Random Graph Models. (arXiv:1712.07325v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.07325</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic networks are a general language for describing time-evolving complex
systems, and discrete time network models provide an emerging statistical
technique for various applications. It is a fundamental research question to
detect the community structure in time-evolving networks. However, due to
significant computational challenges and difficulties in modeling communities
of time-evolving networks, there is little progress in the current literature
to effectively find communities in time-evolving networks. In this work, we
propose a novel model-based clustering framework for time-evolving networks
based on discrete time exponential-family random graph models. To choose the
number of communities, we use conditional likelihood to construct an effective
model selection criterion. Furthermore, we propose an efficient variational
expectation-maximization (EM) algorithm to find approximate maximum likelihood
estimates of network parameters and mixing proportions. By using variational
methods and minorization-maximization (MM) techniques, our method has appealing
scalability for large-scale time-evolving networks. The power of our method is
demonstrated in simulation studies and empirical applications to international
trade networks and the collaboration networks of a large American research
university.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kevin H. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Lingzhou Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hunter_D/0/1/0/all/0/1&quot;&gt;David R. Hunter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07364">
<title>Transformation Models in High-Dimensions. (arXiv:1712.07364v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.07364</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformation models are a very important tool for applied statisticians and
econometricians. In many applications, the dependent variable is transformed so
that homogeneity or normal distribution of the error holds. In this paper, we
analyze transformation models in a high-dimensional setting, where the set of
potential covariates is large. We propose an estimator for the transformation
parameter and we show that it is asymptotically normally distributed using an
orthogonalized moment condition where the nuisance functions depend on the
target parameter. In a simulation study, we show that the proposed estimator
works well in small samples. A common practice in labor economics is to
transform wage with the log-function. In this study, we test if this
transformation holds in CPS data from the United States.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klaassen_S/0/1/0/all/0/1&quot;&gt;Sven Klaassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kueck_J/0/1/0/all/0/1&quot;&gt;Jannis Kueck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spindler_M/0/1/0/all/0/1&quot;&gt;Martin Spindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07374">
<title>Adversarial Structured Prediction for Multivariate Measures. (arXiv:1712.07374v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07374</link>
<description rdf:parseType="Literal">&lt;p&gt;Many predicted structured objects (e.g., sequences, matchings, trees) are
evaluated using the F-score, alignment error rate (AER), or other multivariate
performance measures. Since inductively optimizing these measures using
training data is typically computationally difficult, empirical risk
minimization of surrogate losses is employed, using, e.g., the hinge loss for
(structured) support vector machines. These approximations often introduce a
mismatch between the learner&apos;s objective and the desired application
performance, leading to inconsistency. We take a different approach: % based on
multivariate prediction games (MPGs): adversarially approximate training data
while optimizing the exact F-score or AER. Structured predictions under this
formulation result from solving zero-sum games between a predictor seeking the
best performance and an adversary seeking the worst while required to
(approximately) match certain structured properties of the training data. We
explore this approach for %two key NLP problems: word alignment (AER
evaluation) and named entity recognition (F-score evaluation) with linear-chain
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Ashkan Rezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ziebart_B/0/1/0/all/0/1&quot;&gt;Brian D. Ziebart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07454">
<title>Fast kNN mode seeking clustering applied to active learning. (arXiv:1712.07454v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07454</link>
<description rdf:parseType="Literal">&lt;p&gt;A significantly faster algorithm is presented for the original kNN mode
seeking procedure. It has the advantages over the well-known mean shift
algorithm that it is feasible in high-dimensional vector spaces and results in
uniquely, well defined modes. Moreover, without any additional computational
effort it may yield a multi-scale hierarchy of clusterings. The time complexity
is just O(n^1.5). resulting computing times range from seconds for 10^4 objects
to minutes for 10^5 objects and to less than an hour for 10^6 objects. The
space complexity is just O(n). The procedure is well suited for finding large
sets of small clusters and is thereby a candidate to analyze thousands of
clusters in millions of objects.
&lt;/p&gt;
&lt;p&gt;The kNN mode seeking procedure can be used for active learning by assigning
the clusters to the class of the modal objects of the clusters. Its feasibility
is shown by some examples with up to 1.5 million handwritten digits. The
obtained classification results based on the clusterings are compared with
those obtained by the nearest neighbor rule and the support vector classifier
based on the same labeled objects for training. It can be concluded that using
the clustering structure for classification can be significantly better than
using the trained classifiers. A drawback of using the clustering for
classification, however, is that no classifier is obtained that may be used for
out-of-sample objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duin_R/0/1/0/all/0/1&quot;&gt;Robert P.W. Duin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verzakov_S/0/1/0/all/0/1&quot;&gt;Sergey Verzakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07495">
<title>A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with the Trace Norm. (arXiv:1712.07495v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1712.07495</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a high-dimensional but low-rank matrix
from a large-scale dataset distributed over several machines, where
low-rankness is enforced by a convex trace norm constraint. We propose
DFW-Trace, a distributed Frank-Wolfe algorithm which leverages the low-rank
structure of its updates to achieve efficiency in time, memory and
communication usage. The step at the heart of DFW-Trace is solved approximately
using a distributed version of the power method. We provide a theoretical
analysis of the convergence of DFW-Trace, showing that we can ensure sublinear
convergence in expectation to an optimal solution with few power iterations per
epoch. We implement DFW-Trace in the Apache Spark distributed programming
framework and validate the usefulness of our approach on synthetic and real
data, including the ImageNet dataset with high-dimensional features extracted
from a deep neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1&quot;&gt;Patrick Gallinari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07519">
<title>Statistical Inference for the Population Landscape via Moment Adjusted Stochastic Gradients. (arXiv:1712.07519v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07519</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern statistical inference tasks often require iterative optimization
methods to approximate the solution. Convergence analysis from optimization
only tells us how well we are approximating the solution deterministically, but
overlooks the sampling nature of the data. However, due to the randomness in
the data, statisticians are keen to provide uncertainty quantification, or
confidence, for the answer obtained after certain steps of optimization.
Therefore, it is important yet challenging to understand the sampling
distribution of the iterative optimization methods.
&lt;/p&gt;
&lt;p&gt;This paper makes some progress along this direction by introducing a new
stochastic optimization method for statistical inference, the moment adjusted
stochastic gradient descent. We establish non-asymptotic theory that
characterizes the statistical distribution of the iterative methods, with good
optimization guarantee. On the statistical front, the theory allows for model
misspecification, with very mild conditions on the data. For optimization, the
theory is flexible for both the convex and non-convex cases. Remarkably, the
moment adjusting idea motivated from &quot;error standardization&quot; in statistics
achieves similar effect as Nesterov&apos;s acceleration in optimization, for certain
convex problems as in fitting generalized linear models. We also demonstrate
this acceleration effect in the non-convex setting through experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_T/0/1/0/all/0/1&quot;&gt;Tengyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07581">
<title>Riemann-Theta Boltzmann Machine. (arXiv:1712.07581v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07581</link>
<description rdf:parseType="Literal">&lt;p&gt;A general Boltzmann machine with continuous visible and discrete integer
valued hidden states is introduced. Under mild assumptions about the connection
matrices, the probability density function of the visible units can be solved
for analytically, yielding a novel parametric density function involving a
ratio of Riemann-Theta functions. The conditional expectation of a hidden state
for given visible states can also be calculated analytically, yielding a
derivative of the logarithmic Riemann-Theta function. The conditional
expectation can be used as activation function in a feedforward neural network,
thereby increasing the modelling capacity of the network. Both the Boltzmann
machine and the derived feedforward neural network can be successfully trained
via standard gradient- and non-gradient-based optimization techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krefl_D/0/1/0/all/0/1&quot;&gt;Daniel Krefl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carrazza_S/0/1/0/all/0/1&quot;&gt;Stefano Carrazza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haghighat_B/0/1/0/all/0/1&quot;&gt;Babak Haghighat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kahlen_J/0/1/0/all/0/1&quot;&gt;Jens Kahlen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07639">
<title>Image Segmentation to Distinguish Between Overlapping Human Chromosomes. (arXiv:1712.07639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.07639</link>
<description rdf:parseType="Literal">&lt;p&gt;In medicine, visualizing chromosomes is important for medical diagnostics,
drug development, and biomedical research. Unfortunately, chromosomes often
overlap and it is necessary to identify and distinguish between the overlapping
chromosomes. A segmentation solution that is fast and automated will enable
scaling of cost effective medicine and biomedical research. We apply neural
network-based image segmentation to the problem of distinguishing between
partially overlapping DNA chromosomes. A convolutional neural network is
customized for this problem. The results achieved intersection over union (IOU)
scores of 94.7% for the overlapping region and 88-94% on the non-overlapping
chromosome regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;R. Lily Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnowski_J/0/1/0/all/0/1&quot;&gt;Jeremy Karnowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadely_R/0/1/0/all/0/1&quot;&gt;Ross Fadely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pommier_J/0/1/0/all/0/1&quot;&gt;Jean-Patrick Pommier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.02068">
<title>On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models. (arXiv:1601.02068v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1601.02068</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive computationally tractable methods to select a small subset of
experiment settings from a large pool of given design points. The primary focus
is on linear regression models, while the technique extends to generalized
linear models and Delta&apos;s method (estimating functions of linear regression
models) as well. The algorithms are based on a continuous relaxation of an
otherwise intractable combinatorial optimization problem, with sampling or
greedy procedures as post-processing steps. Formal approximation guarantees are
established for both algorithms, and numerical results on both synthetic and
real-world data confirm the effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_A/0/1/0/all/0/1&quot;&gt;Adams Wei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.03063">
<title>Neural computation from first principles: Using the maximum entropy method to obtain an optimal bits-per-joule neuron. (arXiv:1606.03063v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1606.03063</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization results are one method for understanding neural computation from
Nature&apos;s perspective and for defining the physical limits on neuron-like
engineering. Earlier work looks at individual properties or performance
criteria and occasionally a combination of two, such as energy and information.
Here we make use of Jaynes&apos; maximum entropy method and combine a larger set of
constraints, possibly dimensionally distinct, each expressible as an
expectation. The method identifies a likelihood-function and a sufficient
statistic arising from each such optimization. This likelihood is a
first-hitting time distribution in the exponential class. Particular constraint
sets are identified that, from an optimal inference perspective, justify
earlier neurocomputational models. Interactions between constraints, mediated
through the inferred likelihood, restrict constraint-set parameterizations,
e.g., the energy-budget limits estimation performance which, in turn, matches
an axonal communication constraint. Such linkages are, for biologists,
experimental predictions of the method. In addition to the related likelihood,
at least one type of constraint set implies marginal distributions, and in this
case, a Shannon bits/joule statement arises.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Levy_W/0/1/0/all/0/1&quot;&gt;William B Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Berger_T/0/1/0/all/0/1&quot;&gt;Toby Berger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sungkar_M/0/1/0/all/0/1&quot;&gt;Mustafa Sungkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.00661">
<title>Localization by Fusing a Group of Fingerprints via Multiple Antennas in Indoor Environment. (arXiv:1609.00661v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.00661</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing fingerprints-based indoor localization approaches are based on
some single fingerprints, such as received signal strength (RSS), channel
impulse response (CIR), and signal subspace. However, the localization accuracy
obtained by the single fingerprint approach is rather susceptible to the
changing environment, multi-path, and non-line-of-sight (NLOS) propagation.
Furthermore, building the fingerprints is a very time consuming process. In
this paper, we propose a novel localization framework by Fusing A Group Of
fingerprinTs (FAGOT) via multiple antennas for the indoor environment. We first
build a GrOup Of Fingerprints (GOOF), which includes five different
fingerprints, namely, RSS, covariance matrix, signal subspace, fractional low
order moment, and fourth-order cumulant, which are obtained by different
transformations of the received signals from multiple antennas in the offline
stage. Then, we design a parallel GOOF multiple classifiers based on AdaBoost
(GOOF-AdaBoost) to train each of these fingerprints in parallel as five strong
multiple classifiers. In the online stage, we input the corresponding
transformations of the real measurements into these strong classifiers to
obtain independent decisions. Finally, we propose an efficient combination
fusion algorithm, namely, MUltiple Classifiers mUltiple Samples (MUCUS) fusion
algorithm to improve the accuracy of localization by combining the predictions
of multiple classifiers with different samples. As compared with the single
fingerprint approaches, the prediction probability of our proposed approach is
improved significantly. The process for building fingerprints can also be
reduced drastically. We demonstrate the feasibility and performance of the
proposed algorithm through extensive simulations as well as via real
experimental data using a Universal Software Radio Peripheral (USRP) platform
with four antennas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiansheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ansari_N/0/1/0/all/0/1&quot;&gt;Nirwan Ansari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.07056">
<title>The Recycling Gibbs Sampler for Efficient Learning. (arXiv:1611.07056v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1611.07056</link>
<description rdf:parseType="Literal">&lt;p&gt;Monte Carlo methods are essential tools for Bayesian inference. Gibbs
sampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively
used in signal processing, machine learning, and statistics, employed to draw
samples from complicated high-dimensional posterior distributions. The key
point for the successful application of the Gibbs sampler is the ability to
draw efficiently samples from the full-conditional probability density
functions. Since in the general case this is not possible, in order to speed up
the convergence of the chain, it is required to generate auxiliary samples
whose information is eventually disregarded. In this work, we show that these
auxiliary samples can be recycled within the Gibbs estimators, improving their
efficiency with no extra cost. This novel scheme arises naturally after
pointing out the relationship between the standard Gibbs sampler and the chain
rule used for sampling purposes. Numerical simulations involving simple and
real inference problems confirm the excellent performance of the proposed
scheme in terms of accuracy and computational efficiency. In particular we give
empirical evidence of performance in a toy example, inference of Gaussian
processes hyperparameters, and learning dependence graphs through regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1&quot;&gt;Luca Martino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1&quot;&gt;Victor Elvira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Camps_Valls_G/0/1/0/all/0/1&quot;&gt;Gustau Camps-Valls&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02184">
<title>Indoor Localization Using Visible Light Via Fusion Of Multiple Classifiers. (arXiv:1703.02184v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02184</link>
<description rdf:parseType="Literal">&lt;p&gt;A multiple classifiers fusion localization technique using received signal
strengths (RSSs) of visible light is proposed, in which the proposed system
transmits different intensity modulated sinusoidal signals by LEDs and the
signals received by a Photo Diode (PD) placed at various grid points. First, we
obtain some {\emph{approximate}} received signal strengths (RSSs) fingerprints
by capturing the peaks of power spectral density (PSD) of the received signals
at each given grid point. Unlike the existing RSSs based algorithms, several
representative machine learning approaches are adopted to train multiple
classifiers based on these RSSs fingerprints. The multiple classifiers
localization estimators outperform the classical RSS-based LED localization
approaches in accuracy and robustness. To further improve the localization
performance, two robust fusion localization algorithms, namely, grid
independent least square (GI-LS) and grid dependent least square (GD-LS), are
proposed to combine the outputs of these classifiers. We also use a singular
value decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical
stability problem when the prediction matrix is singular. Experiments conducted
on intensity modulated direct detection (IM/DD) systems have demonstrated the
effectiveness of the proposed algorithms. The experimental results show that
the probability of having mean square positioning error (MSPE) of less than 5cm
achieved by GD-LS is improved by 93.03\% and 93.15\%, respectively, as compared
to those by the RSS ratio (RSSR) and RSS matching methods with the FFT length
of 2000.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiansheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shao_S/0/1/0/all/0/1&quot;&gt;Sihua Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ansari_N/0/1/0/all/0/1&quot;&gt;Nirwan Ansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khreishah_A/0/1/0/all/0/1&quot;&gt;Abdallah Khreishah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07019">
<title>Model-Robust Counterfactual Prediction Method. (arXiv:1705.07019v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07019</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a method for assessing counterfactual predictions with multiple
groups. It is tuning-free and operational in high-dimensional covariate
scenarios, with a runtime that scales linearly in the number of datapoints. The
computational efficiency is leveraged to produce valid confidence intervals
using the conformal prediction approach. The method is model-robust in that it
enables inferences from observational data even when the data model is
misspecified. The approach is illustrated using both real and synthetic
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zachariah_D/0/1/0/all/0/1&quot;&gt;Dave Zachariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stoica_P/0/1/0/all/0/1&quot;&gt;Petre Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08826">
<title>Learning with Average Top-k Loss. (arXiv:1705.08826v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08826</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce the {\em average top-$k$} (\atk) loss as a new
aggregate loss for supervised learning, which is the average over the $k$
largest individual losses over a training dataset. We show that the \atk loss
is a natural generalization of the two widely used aggregate losses, namely the
average loss and the maximum loss, but can combine their advantages and
mitigate their drawbacks to better adapt to different data distributions.
Furthermore, it remains a convex function over all individual losses, which can
lead to convex optimization problems that can be solved effectively with
conventional gradient-based methods. We provide an intuitive interpretation of
the \atk loss based on its equivalent effect on the continuous individual loss
functions, suggesting that it can reduce the penalty on correctly classified
data. We further give a learning theory analysis of \matk learning on the
classification calibration of the \atk loss and the error bounds of \atk-SVM.
We demonstrate the applicability of minimum average top-$k$ learning for binary
classification and regression using synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yanbo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1&quot;&gt;Yiming Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bao-Gang Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.02691">
<title>Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations. (arXiv:1708.02691v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.02691</link>
<description rdf:parseType="Literal">&lt;p&gt;This article concerns the expressive power of depth in neural nets with ReLU
activations and bounded width. We are particularly interested in the following
questions: what is the minimal width $w_{\text{min}}(d)$ so that ReLU nets of
width $w_{\text{min}}(d)$ (and arbitrary depth) can approximate any continuous
function on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this
minimal width, what can one say about the depth necessary to approximate a
given function? Our approach to this paper is based on the observation that,
due to the convexity of the ReLU activation, ReLU nets are particularly
well-suited for representing convex functions. In particular, we prove that
ReLU nets with width $d+1$ can approximate any continuous convex function of
$d$ variables arbitrarily well. These results then give quantitative depth
estimates for the rate of approximation of any continuous scalar function on
the $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07581">
<title>Accurate parameter estimation for Bayesian Network Classifiers using Hierarchical Dirichlet Processes. (arXiv:1708.07581v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07581</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel parameter estimation method for the probability
tables of Bayesian network classifiers (BNCs), using hierarchical Dirichlet
processes (HDPs). The main result of this paper is to show that improved
parameter estimation allows BNCs to outperform leading learning methods such as
Random Forest for both 0-1 loss and RMSE, albeit just on categorical datasets.
&lt;/p&gt;
&lt;p&gt;As data assets become larger, entering the hyped world of &quot;big&quot;, efficient
accurate classification requires three main elements: (1) classifiers with
low-bias that can capture the fine-detail of large datasets (2) out-of-core
learners that can learn from data without having to hold it all in main memory
and (3) models that can classify new data very efficiently.
&lt;/p&gt;
&lt;p&gt;The latest Bayesian network classifiers (BNCs) satisfy these requirements.
Their bias can be controlled easily by increasing the number of parents of the
nodes in the graph. Their structure can be learned out of core with a limited
number of passes over the data. However, as the bias is made lower to
accurately model classification tasks, so is the accuracy of their parameters&apos;
estimates, as each parameter is estimated from ever decreasing quantities of
data. In this paper, we introduce the use of Hierarchical Dirichlet Processes
for accurate BNC parameter estimation.
&lt;/p&gt;
&lt;p&gt;We conduct an extensive set of experiments on 68 standard datasets and
demonstrate that our resulting classifiers perform very competitively with
Random Forest in terms of prediction, while keeping the out-of-core capability
and superior classification time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petitjean_F/0/1/0/all/0/1&quot;&gt;Francois Petitjean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1&quot;&gt;Wray Buntine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaidi_N/0/1/0/all/0/1&quot;&gt;Nayyar Zaidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06012">
<title>VAMPnets: Deep learning of molecular kinetics. (arXiv:1710.06012v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06012</link>
<description rdf:parseType="Literal">&lt;p&gt;There is an increasing demand for computing the relevant structures,
equilibria and long-timescale kinetics of biomolecular processes, such as
protein-drug binding, from high-throughput molecular dynamics simulations.
Current methods employ transformation of simulated coordinates into structural
features, dimension reduction, clustering the dimension-reduced data, and
estimation of a Markov state model or related model of the interconversion
rates between molecular structures. This handcrafted approach demands a
substantial amount of modeling expertise, as poor decisions at any step will
lead to large modeling errors. Here we employ the variational approach for
Markov processes (VAMP) to develop a deep learning framework for molecular
kinetics using neural networks, dubbed VAMPnets. A VAMPnet encodes the entire
mapping from molecular coordinates to Markov states, thus combining the whole
data processing pipeline in a single end-to-end framework. Our method performs
equally or better than state-of-the art Markov modeling methods and provides
easily interpretable few-state kinetic models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mardt_A/0/1/0/all/0/1&quot;&gt;Andreas Mardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pasquali_L/0/1/0/all/0/1&quot;&gt;Luca Pasquali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Noe_F/0/1/0/all/0/1&quot;&gt;Frank No&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10663">
<title>Predicting readmission risk from doctors&apos; notes. (arXiv:1711.10663v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10663</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a model using deep learning techniques and natural language
processing on unstructured text from medical records to predict hospital-wide
$30$-day unplanned readmission, with c-statistic $.70$. Our model is
constructed to allow physicians to interpret the significant features for
prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Craig_E/0/1/0/all/0/1&quot;&gt;Erin Craig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arias_C/0/1/0/all/0/1&quot;&gt;Carlos Arias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gillman_D/0/1/0/all/0/1&quot;&gt;David Gillman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11293">
<title>Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks. (arXiv:1711.11293v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11293</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a parallel-data-free voice-conversion (VC) method that can learn a
mapping from source to target speech without relying on parallel data. The
proposed method is general purpose, high quality, and parallel-data free and
works without any extra data, modules, or alignment procedure. It also avoids
over-smoothing, which occurs in many conventional statistical model-based VC
methods. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial
network (CycleGAN) with gated convolutional neural networks (CNNs) and an
identity-mapping loss. A CycleGAN learns forward and inverse mappings
simultaneously using adversarial and cycle-consistency losses. This makes it
possible to find an optimal pseudo pair from unpaired data. Furthermore, the
adversarial loss contributes to reducing over-smoothing of the converted
feature sequence. We configure a CycleGAN with gated CNNs and train it with an
identity-mapping loss. This allows the mapping function to capture sequential
and hierarchical structures while preserving linguistic information. We
evaluated our method on a parallel-data-free VC task. An objective evaluation
showed that the converted feature sequence was near natural in terms of global
variance and modulation spectra. A subjective evaluation showed that the
quality of the converted speech was comparable to that obtained with a Gaussian
mixture model-based method under advantageous conditions with parallel and
twice the amount of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaneko_T/0/1/0/all/0/1&quot;&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07106">
<title>Exploring High-Dimensional Structure via Axis-Aligned Decomposition of Linear Projections. (arXiv:1712.07106v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07106</link>
<description rdf:parseType="Literal">&lt;p&gt;Two-dimensional embeddings remain the dominant approach to visualize high
dimensional data. The choice of embeddings ranges from highly non-linear ones,
which can capture complex relationships but are difficult to interpret
quantitatively, to axis-aligned projections, which are easy to interpret but
are limited to bivariate relationships. Linear project can be considered as a
compromise between complexity and interpretability, as they allow explicit axes
labels, yet provide significantly more degrees of freedom compared to
axis-aligned projections. Nevertheless, interpreting the axes directions, which
are linear combinations often with many non-trivial components, remains
difficult. To address this problem we introduce a structure aware decomposition
of (multiple) linear projections into sparse sets of axis aligned projections,
which jointly capture all information of the original linear ones. In
particular, we use tools from Dempster-Shafer theory to formally define how
relevant a given axis aligned project is to explain the neighborhood relations
displayed in some linear projection. Furthermore, we introduce a new approach
to discover a diverse set of high quality linear projections and show that in
practice the information of $k$ linear projections is often jointly encoded in
$\sim k$ axis aligned plots. We have integrated these ideas into an interactive
visualization system that allows users to jointly browse both linear
projections and their axis aligned representatives. Using a number of case
studies we show how the resulting plots lead to more intuitive visualizations
and new insight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shusen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramamurthy_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bremer_P/0/1/0/all/0/1&quot;&gt;Peer-Timo Bremer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01145">
<title>Learning Fast and Slow: PROPEDEUTICA for Real-time Malware Detection. (arXiv:1712.01145v1 [cs.CR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.01145</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce and evaluate PROPEDEUTICA, a novel methodology
and framework for efficient and effective real-time malware detection,
leveraging the best of conventional machine learning (ML) and deep learning
(DL) algorithms. In PROPEDEUTICA, all software processes in the system start
execution subjected to a conventional ML detector for fast classification. If a
piece of software receives a borderline classification, it is subjected to
further analysis via more performance expensive and more accurate DL methods,
via our newly proposed DL algorithm DEEPMALWARE. Further, we introduce delays
to the execution of software subjected to deep learning analysis as a way to
&quot;buy time&quot; for DL analysis and to rate-limit the impact of possible malware in
the system. We evaluated PROPEDEUTICA with a set of 9,115 malware samples and
877 commonly used benign software samples from various categories for the
Windows OS. Our results show that the false positive rate for conventional ML
methods can reach 20%, and for modern DL methods it is usually below 6%.
However, the classification time for DL can be 100X longer than conventional ML
methods. PROPEDEUTICA improved the detection F1-score from 77.54% (conventional
ML method) to 90.25%, and reduced the detection time by 54.86%. Further, the
percentage of software subjected to DL analysis was approximately 40% on
average. Further, the application of delays in software subjected to ML reduced
the detection time by approximately 10%. Finally, we found and discussed a
discrepancy between the detection accuracy offline (analysis after all traces
are collected) and on-the-fly (analysis in tandem with trace collection). Our
insights show that conventional ML and modern DL-based malware detectors in
isolation cannot meet the needs of efficient and effective malware detection:
high accuracy, low false positive rate, and short classification time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruimin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qile Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Aokun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregio_A/0/1/0/all/0/1&quot;&gt;Andre Gregio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1&quot;&gt;Daniela Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolin Li&lt;/a&gt;</dc:creator>
</item></rdf:RDF>