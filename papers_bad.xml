<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07362"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07404"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07099"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07132"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07149"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07217"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07506"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07540"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1402.5481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.03764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01677"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00867"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.07362">
<title>Speeding up the Hyperparameter Optimization of Deep Convolutional Neural Networks. (arXiv:1807.07362v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.07362</link>
<description rdf:parseType="Literal">&lt;p&gt;Most learning algorithms require the practitioner to manually set the values
of many hyperparameters before the learning process can begin. However, with
modern algorithms, the evaluation of a given hyperparameter setting can take a
considerable amount of time and the search space is often very
high-dimensional. We suggest using a lower-dimensional representation of the
original data to quickly identify promising areas in the hyperparameter space.
This information can then be used to initialize the optimization algorithm for
the original, higher-dimensional data. We compare this approach with the
standard procedure of optimizing the hyperparameters only on the original
input.
&lt;/p&gt;
&lt;p&gt;We perform experiments with various state-of-the-art hyperparameter
optimization algorithms such as random search, the tree of parzen estimators
(TPEs), sequential model-based algorithm configuration (SMAC), and a genetic
algorithm (GA). Our experiments indicate that it is possible to speed up the
optimization process by using lower-dimensional data representations at the
beginning, while increasing the dimensionality of the input later in the
optimization process. This is independent of the underlying optimization
procedure, making the approach promising for many existing hyperparameter
optimization algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinz_T/0/1/0/all/0/1&quot;&gt;Tobias Hinz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarro_Guerrero_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Navarro-Guerrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magg_S/0/1/0/all/0/1&quot;&gt;Sven Magg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07404">
<title>Analyzing Hypersensitive AI: Instability in Corporate-Scale Machine Learning. (arXiv:1807.07404v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07404</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive geometric models deliver excellent results for many Machine
Learning use cases. Despite their undoubted performance, neural predictive
algorithms can show unexpected degrees of instability and variance,
particularly when applied to large datasets. We present an approach to measure
changes in geometric models with respect to both output consistency and
topological stability. Considering the example of a recommender system using
word2vec, we analyze the influence of single data points, approximation methods
and parameter settings. Our findings can help to stabilize models where needed
and to detect differences in informational value of data points on a large
scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regneri_M/0/1/0/all/0/1&quot;&gt;Michaela Regneri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1&quot;&gt;Malte Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kost_J/0/1/0/all/0/1&quot;&gt;Jurij Kost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietsch_N/0/1/0/all/0/1&quot;&gt;Niklas Pietsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_T/0/1/0/all/0/1&quot;&gt;Timo Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamm_S/0/1/0/all/0/1&quot;&gt;Sabine Stamm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07560">
<title>Compositional GAN: Learning Conditional Image Composition. (arXiv:1807.07560v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.07560</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) can produce images of surprising
complexity and realism, but are generally modeled to sample from a single
latent source ignoring the explicit spatial interaction between multiple
entities that could be present in a scene. Capturing such complex interactions
between different objects in the world, including their relative scaling,
spatial layout, occlusion, or viewpoint transformation is a challenging
problem. In this work, we propose to model object composition in a GAN
framework as a self-consistent composition-decomposition network. Our model is
conditioned on the object images from their marginal distributions to generate
a realistic image from their joint distribution by explicitly learning the
possible interactions. We evaluate our model through qualitative experiments
and user evaluations in both the scenarios when either paired or unpaired
examples for the individual object images and the joint scenes are given during
training. Our results reveal that the learned model captures potential
interactions between the two object domains given as input to output new
instances of composed scene at test time in a reasonable fashion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azadi_S/0/1/0/all/0/1&quot;&gt;Samaneh Azadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1&quot;&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07099">
<title>Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants. (arXiv:1807.07099v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1807.07099</link>
<description rdf:parseType="Literal">&lt;p&gt;Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species
were used to explore the influence of preprocessing and feature extraction on
efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and
Discrete Wavelet Transforms (DWT) were compared as feature extraction
techniques for FTIR data of medicinal plants. Various combinations of signal
processing steps showed different behavior when applied to classification and
clustering tasks. Best results for WTT and DWT found through grid search were
similar, significantly improving quality of clustering as well as
classification accuracy for tuned logistic regression in comparison to original
spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a
more versatile and easier to use as a data processing tool in various signal
processing applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kharyuk_P/0/1/0/all/0/1&quot;&gt;Pavel Kharyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nazarenko_D/0/1/0/all/0/1&quot;&gt;Dmitry Nazarenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07132">
<title>Distributed Second-order Convex Optimization. (arXiv:1807.07132v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07132</link>
<description rdf:parseType="Literal">&lt;p&gt;Convex optimization problems arise frequently in diverse machine learning
(ML) applications. First-order methods, i.e., those that solely rely on the
gradient information, are most commonly used to solve these problems. This
choice is motivated by their simplicity and low per-iteration cost.
Second-order methods that rely on curvature information through the dense
Hessian matrix have, thus far, proven to be prohibitively expensive at scale,
both in terms of computational and memory requirements. We present a novel
multi-GPU distributed formulation of a second order (Newton-type) solver for
convex finite sum minimization problems for multi-class classification. Our
distributed formulation relies on the Alternating Direction of Multipliers
Method (ADMM), which requires only one round of communication per-iteration --
significantly reducing communication overheads, while incurring minimal
convergence overhead. By leveraging the computational capabilities of GPUs, we
demonstrate that per-iteration costs of Newton-type methods can be
significantly reduced to be on-par with, if not better than, state-of-the-art
first-order alternatives. Given their significantly faster convergence rates,
we demonstrate that our methods can process large data-sets in much shorter
time (orders of magnitude in many cases) compared to existing first and second
order methods, while yielding similar test-accuracy results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chih-Hao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kylasa_S/0/1/0/all/0/1&quot;&gt;Sudhir B Kylasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roosta_Khorasani_F/0/1/0/all/0/1&quot;&gt;Farbod Roosta-Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grama_A/0/1/0/all/0/1&quot;&gt;Ananth Grama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07149">
<title>A Hand-Held Multimedia Translation and Interpretation System with Application to Diet Management. (arXiv:1807.07149v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.07149</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a network independent, hand-held system to translate and
disambiguate foreign restaurant menu items in real-time. The system is based on
the use of a portable multimedia device, such as a smartphones or a PDA. An
accurate and fast translation is obtained using a Machine Translation engine
and a context-specific corpora to which we apply two pre-processing steps,
called translation standardization and $n$-gram consolidation. The phrase-table
generated is orders of magnitude lighter than the ones commonly used in market
applications, thus making translations computationally less expensive, and
decreasing the battery usage. Translation ambiguities are mitigated using
multimedia information including images of dishes and ingredients, along with
ingredient lists. We implemented a prototype of our system on an iPod Touch
Second Generation for English speakers traveling in Spain. Our tests indicate
that our translation method yields higher accuracy than translation engines
such as Google Translate, and does so almost instantaneously. The memory
requirements of the application, including the database of images, are also
well within the limits of the device. By combining it with a database of
nutritional information, our proposed system can be used to help individuals
who follow a medical diet maintain this diet while traveling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parra_A/0/1/0/all/0/1&quot;&gt;Albert Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haddad_A/0/1/0/all/0/1&quot;&gt;Andrew W. Haddad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutin_M/0/1/0/all/0/1&quot;&gt;Mireille Boutin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1&quot;&gt;Edward J. Delp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07207">
<title>A Projection Pursuit Forest Algorithm for Supervised Classification. (arXiv:1807.07207v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07207</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new ensemble learning method for classification
problems called projection pursuit random forest (PPF). PPF uses the PPtree
algorithm introduced in Lee et al. (2013). In PPF, trees are constructed by
splitting on linear combinations of randomly chosen variables. Projection
pursuit is used to choose a projection of the variables that best separates the
classes. Utilizing linear combinations of variables to separate classes takes
the correlation between variables into account which allows PPF to outperform a
traditional random forest when separations between groups occurs in
combinations of variables.
&lt;/p&gt;
&lt;p&gt;The method presented here can be used in multi-class problems and is
implemented into an R (R Core Team, 2018) package, PPforest, which is available
on CRAN, with development versions at https://github.com/natydasilva/PPforest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silva_N/0/1/0/all/0/1&quot;&gt;Natalia da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cook_D/0/1/0/all/0/1&quot;&gt;Dianne Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_E/0/1/0/all/0/1&quot;&gt;Eun-Kyung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07217">
<title>Isolating effects of age with fair representation learning when assessing dementia. (arXiv:1807.07217v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07217</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most prevalent symptoms among the elderly population, dementia,
can be detected using linguistic features extracted from narrative transcripts.
However, these linguistic features are impacted in a similar but different
fashion by normal aging process. It has been hard for machine learning
classifiers to isolate the effects of confounding factors (e.g., age). We show
that deep neural network (DNN) classifiers can infer ages from linguistic
features. They could make classifications based on the bias given age, which
entangles unfairness across age groups. In this paper, we address this problem
with fair representation learning. We build neural network classifiers that
learn low-dimensional representations reflecting the impacts of dementia but do
not contain age-related information. To evaluate these classifiers, we specify
a model-agnostic score $\Delta_{eo}^{(N)}$ measuring how classifier results are
disentangled from age. Our best models are better than baseline DNN
classifiers, in both accuracy and disentanglement, while compromising
accuracies by as little as 2.56% and 2.25% on DementiaBank and Famous People
dataset respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zining Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1&quot;&gt;Jekaterina Novikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1&quot;&gt;Frank Rudzicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07306">
<title>Bounded Information Rate Variational Autoencoders. (arXiv:1807.07306v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07306</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new member of the family of Variational Autoencoders
(VAE) that constrains the rate of information transferred by the latent layer.
The latent layer is interpreted as a communication channel, the information
rate of which is bound by imposing a pre-set signal-to-noise ratio. The new
constraint subsumes the mutual information between the input and latent
variables, combining naturally with the likelihood objective of the observed
data as used in a conventional VAE. The resulting Bounded-Information-Rate
Variational Autoencoder (BIR-VAE) provides a meaningful latent representation
with an information resolution that can be specified directly in bits by the
system designer. The rate constraint can be used to prevent overtraining, and
the method naturally facilitates quantisation of the latent variables at the
set rate. Our experiments confirm that the BIR-VAE has a meaningful latent
representation and that its performance is at least as good as state-of-the-art
competing algorithms, but with lower computational complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braithwaite_D/0/1/0/all/0/1&quot;&gt;D. T. Braithwaite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. B. Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07468">
<title>Latent Dirichlet Allocation (LDA) for Topic Modeling of the CFPB Consumer Complaints. (arXiv:1807.07468v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.07468</link>
<description rdf:parseType="Literal">&lt;p&gt;A text mining approach is proposed based on latent Dirichlet allocation (LDA)
to analyze the Consumer Financial Protection Bureau (CFPB) consumer complaints.
The proposed approach aims to extract latent topics in the CFPB complaint
narratives, and explores their associated trends over time. The time trends
will then be used to evaluate the effectiveness of the CFPB regulations and
expectations on financial institutions in creating a consumer oriented culture
that treats consumers fairly and prioritizes consumer protection in their
decision making processes. The proposed approach can be easily operationalized
as a decision support system to automate detection of emerging topics in
consumer complaints. Hence, the technology-human partnership between the
proposed approach and the CFPB team could certainly improve consumer
protections from unfair, deceptive or abusive practices in the financial
markets by providing more efficient and effective investigations of consumer
complaint narratives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastani_K/0/1/0/all/0/1&quot;&gt;Kaveh Bastani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namavari_H/0/1/0/all/0/1&quot;&gt;Hamed Namavari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaffer_J/0/1/0/all/0/1&quot;&gt;Jeffry Shaffer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07506">
<title>Improving Simple Models with Confidence Profiles. (arXiv:1807.07506v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07506</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new method called ProfWeight for transferring
information from a pre-trained deep neural network that has a high test
accuracy to a simpler interpretable model or a very shallow network of low
complexity and a priori low test accuracy. We are motivated by applications in
interpretability and model deployment in severely memory constrained
environments (like sensors). Our method uses linear probes to generate
confidence scores through flattened intermediate representations. Our transfer
method involves a theoretically justified weighting of samples during the
training of the simple model using confidence scores of these intermediate
layers. The value of our method is first demonstrated on CIFAR-10, where our
weighting method significantly improves (3-4%) networks with only a fraction of
the number of Resnet blocks of a complex Resnet model. We further demonstrate
operationally significant results on a real manufacturing problem, where we
dramatically increase the test accuracy of a CART model (the domain standard)
by roughly 13%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1&quot;&gt;Amit Dhurandhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1&quot;&gt;Ronny Luss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olsen_P/0/1/0/all/0/1&quot;&gt;Peder Olsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07525">
<title>Emulating malware authors for proactive protection using GANs over a distributed image visualization of the dynamic file behavior. (arXiv:1807.07525v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07525</link>
<description rdf:parseType="Literal">&lt;p&gt;Malware authors have always been at an advantage of being able to
adversarially test and augment the malicious code, before deploying their
payload, against anti-malware products at their disposal. The anti-malware
developers and threat experts, on the other hand, do not have such a privilege
of tuning anti-malware products against zero-day attacks pro-actively. This
allows the malware authors to being a step ahead of the anti-malware products,
fundamentally biasing the cat and mouse game played by the two parties. In this
paper, we propose a way that would enable machine learning based threat
prevention models to bridge that gap by being able to tune against a deep
generative adversarial network (GAN), which takes up the role of a malware
author and generates new types of malware. The GAN is trained over a reversible
distributed RGB image representation of known malware behaviors, encoding the
sequence of API call ngrams and the corresponding term frequencies. The
generated images represent synthetic malware that can be decoded back to the
underlying API call sequence information. The image representation is not only
demonstrated as a general technique of incorporating necessary priors for
exploiting convolutional neural network architectures for generative or
discriminative modeling, but also as a visualization method for easy manual
software or malware categorization, by having individual API ngram information
distributed across the image space. In addition, we also propose using
smart-definitions for detecting malwares based on perceptual hashing of these
images. Such hashes are potentially more effective than cryptographic hashes
that do not carry any meaningful similarity metric, and hence, do not
generalize well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhaskara_V/0/1/0/all/0/1&quot;&gt;Vineeth S. Bhaskara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhattacharyya_D/0/1/0/all/0/1&quot;&gt;Debanjan Bhattacharyya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07540">
<title>A unified theory of adaptive stochastic gradient descent as Bayesian filtering. (arXiv:1807.07540v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07540</link>
<description rdf:parseType="Literal">&lt;p&gt;There are a diverse array of schemes for adaptive stochastic gradient descent
for optimizing neural networks, from fully factorised methods with and without
momentum (e.g.\ RMSProp and ADAM), to Kronecker factored methods that consider
the Hessian for a full weight matrix. However, these schemes have been derived
and justified using a wide variety of mathematical approaches, and as such,
there is no unified theory of adaptive stochastic gradients descent methods.
Here, we provide such a theory by showing that many successful adaptive
stochastic gradient descent schemes emerge by considering a filtering-based
inference in a Bayesian optimization problem. In particular, we use
backpropagated gradients to compute a Gaussian posterior over the optimal
neural network parameters, given the data minibatches seen so far. Our unified
theory is able to give some guidance to practitioners on how to choose between
the large number of available optimization methods. In the fully factorised
setting, we recover RMSProp and ADAM under different priors, along with
additional improvements such as Nesterov acceleration and AdamW. Moreover, we
obtain new recommendations, including the possibility of combining RMSProp and
ADAM updates. In the Kronecker factored setting, we obtain a adaptive natural
gradient adaptation scheme that is derived specifically for the minibatch
setting. Furthermore, under a modified prior, we obtain a Kronecker factored
analogue of RMSProp or ADAM, that preconditions the gradient by whitening (i.e.
by multiplying by the square root of the Hessian, as in RMSProp/ADAM). Our work
raises the hope that it is possible to achieve unified theoretical
understanding of empirically successful adaptive gradient descent schemes for
neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1&quot;&gt;Laurence Aitchison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07543">
<title>Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer. (arXiv:1807.07543v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07543</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoencoders provide a powerful framework for learning compressed
representations by encoding all of the information needed to reconstruct a data
point in a latent code. In some cases, autoencoders can &quot;interpolate&quot;: By
decoding the convex combination of the latent codes for two datapoints, the
autoencoder can produce an output which semantically mixes characteristics from
the datapoints. In this paper, we propose a regularization procedure which
encourages interpolated outputs to appear more realistic by fooling a critic
network which has been trained to recover the mixing coefficient from
interpolated data. We then develop a simple benchmark task where we can
quantitatively measure the extent to which various autoencoders can interpolate
and show that our regularizer dramatically improves interpolation in this
setting. We also demonstrate empirically that our regularizer produces latent
codes which are more effective on downstream tasks, suggesting a possible link
between interpolation abilities and learning useful representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthelot_D/0/1/0/all/0/1&quot;&gt;David Berthelot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Aurko Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1402.5481">
<title>From Predictive to Prescriptive Analytics. (arXiv:1402.5481v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1402.5481</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we combine ideas from machine learning (ML) and operations
research and management science (OR/MS) in developing a framework, along with
specific methods, for using data to prescribe optimal decisions in OR/MS
problems. In a departure from other work on data-driven optimization and
reflecting our practical experience with the data available in applications of
OR/MS, we consider data consisting, not only of observations of quantities with
direct effect on costs/revenues, such as demand or returns, but predominantly
of observations of associated auxiliary quantities. The main problem of
interest is a conditional stochastic optimization problem, given imperfect
observations, where the joint probability distributions that specify the
problem are unknown. We demonstrate that our proposed solution methods, which
are inspired by ML methods such as local regression, CART, and random forests,
are generally applicable to a wide range of decision problems. We prove that
they are tractable and asymptotically optimal even when data is not iid and may
be censored. We extend this to the case where decision variables may directly
affect uncertainty in unknown ways, such as pricing&apos;s effect on demand. As an
analogue to R^2, we develop a metric P termed the coefficient of
prescriptiveness to measure the prescriptive content of data and the efficacy
of a policy from an operations perspective. To demonstrate the power of our
approach in a real-world setting we study an inventory management problem faced
by the distribution arm of an international media conglomerate, which ships an
average of 1bil units per year. We leverage internal data and public online
data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational
decisions that outperform baseline measures. Specifically, the data we collect,
leveraged by our methods, accounts for an 88\% improvement as measured by our
P.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bertsimas_D/0/1/0/all/0/1&quot;&gt;Dimitris Bertsimas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kallus_N/0/1/0/all/0/1&quot;&gt;Nathan Kallus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.03764">
<title>Linear Algebraic Structure of Word Senses, with Applications to Polysemy. (arXiv:1601.03764v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1601.03764</link>
<description rdf:parseType="Literal">&lt;p&gt;Word embeddings are ubiquitous in NLP and information retrieval, but it is
unclear what they represent when the word is polysemous. Here it is shown that
multiple word senses reside in linear superposition within the word embedding
and simple sparse coding can recover vectors that approximately capture the
senses. The success of our approach, which applies to several embedding
methods, is mathematically explained using a variant of the random walk on
discourses model (Arora et al., 2016). A novel aspect of our technique is that
each extracted word sense is accompanied by one of about 2000 &quot;discourse atoms&quot;
that gives a succinct description of which other words co-occur with that word
sense. Discourse atoms can be of independent interest, and make the method
potentially more useful. Empirical tests are used to verify and support the
theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1&quot;&gt;Sanjeev Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1&quot;&gt;Andrej Risteski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01677">
<title>A network approach to topic models. (arXiv:1708.01677v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01677</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main computational and scientific challenges in the modern age is
to extract useful information from unstructured texts. Topic models are one
popular machine-learning approach which infers the latent topical structure of
a collection of documents. Despite their success --- in particular of its most
widely used variant called Latent Dirichlet Allocation (LDA) --- and numerous
applications in sociology, history, and linguistics, topic models are known to
suffer from severe conceptual and practical problems, e.g. a lack of
justification for the Bayesian priors, discrepancies with statistical
properties of real texts, and the inability to properly choose the number of
topics. Here we obtain a fresh view on the problem of identifying topical
structures by relating it to the problem of finding communities in complex
networks. This is achieved by representing text corpora as bipartite networks
of documents and words. By adapting existing community-detection methods --
using a stochastic block model (SBM) with non-parametric priors -- we obtain a
more versatile and principled framework for topic modeling (e.g., it
automatically detects the number of topics and hierarchically clusters both the
words and documents). The analysis of artificial and real corpora demonstrates
that our SBM approach leads to better topic models than LDA in terms of
statistical model selection. More importantly, our work shows how to formally
relate methods from community detection and topic modeling, opening the
possibility of cross-fertilization between these two fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gerlach_M/0/1/0/all/0/1&quot;&gt;Martin Gerlach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Altmann_E/0/1/0/all/0/1&quot;&gt;Eduardo G. Altmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04153">
<title>Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v6 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then prove convergence rates for the method in the well-specified and
misspecified cases, and demonstrate its efficiency in the context of
multi-fidelity models for complex engineering systems and a problem of global
illumination in computer graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05852">
<title>Network Representation Learning: A Survey. (arXiv:1801.05852v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05852</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread use of information technologies, information networks are
becoming increasingly popular to capture complex relationships across various
disciplines, such as social networks, citation networks, telecommunication
networks, and biological networks. Analyzing these networks sheds light on
different aspects of social life such as the structure of societies,
information diffusion, and communication patterns. In reality, however, the
large scale of information networks often makes network analytic tasks
computationally expensive or intractable. Network representation learning has
been recently proposed as a new learning paradigm to embed network vertices
into a low-dimensional vector space, by preserving network topology structure,
vertex content, and other side information. This facilitates the original
network to be easily handled in the new vector space for further analysis. In
this survey, we perform a comprehensive review of the current literature on
network representation learning in the data mining and machine learning field.
We propose new taxonomies to categorize and summarize the state-of-the-art
network representation learning techniques according to the underlying learning
mechanisms, the network information intended to preserve, as well as the
algorithmic designs and methodologies. We summarize evaluation protocols used
for validating network representation learning including published benchmark
datasets, evaluation methods, and open source algorithms. We also perform
empirical studies to compare the performance of representative algorithms on
common datasets, and analyze their computational complexity. Finally, we
suggest promising research directions to facilitate future study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daokun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jie Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xingquan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00795">
<title>Estimation of Markov Chain via Rank-Constrained Likelihood. (arXiv:1804.00795v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00795</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the estimation of low-rank Markov chains from empirical
trajectories. We propose a non-convex estimator based on rank-constrained
likelihood maximization. Statistical upper bounds are provided for the
Kullback-Leiber divergence and the $\ell_2$ risk between the estimator and the
true transition matrix. The estimator reveals a compressed state space of the
Markov chain. We also develop a novel DC (difference of convex function)
programming algorithm to tackle the rank-constrained non-smooth optimization
problem. Convergence results are established. Experiments show that the
proposed estimator achieves better empirical performance than other popular
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xudong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10206">
<title>On the Estimation of Entropy in the FastICA Algorithm. (arXiv:1805.10206v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10206</link>
<description rdf:parseType="Literal">&lt;p&gt;The fastICA algorithm is a popular dimension reduction technique used to
reveal patterns in data. Here we show that the approximations used in fastICA
can result in patterns not being successfully recognised. We demonstrate this
problem using a two-dimensional example where a clear structure is immediately
visible to the naked eye, but where the projection chosen by fastICA fails to
reveal this structure. This implies that care is needed when applying fastICA.
We discuss how the problem arises and how it is intrinsically connected to the
approximations that form the basis of the computational efficiency of fastICA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_P/0/1/0/all/0/1&quot;&gt;Paul Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Voss_J/0/1/0/all/0/1&quot;&gt;Jochen Voss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Issoglio_E/0/1/0/all/0/1&quot;&gt;Elena Issoglio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00867">
<title>Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access. (arXiv:1807.00867v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00867</link>
<description rdf:parseType="Literal">&lt;p&gt;A stochastic multi-user multi-armed bandit framework is used to develop
algorithms for uncoordinated spectrum access. In contrast to prior work, the
number of users is assumed to be unknown to each user and can possibly exceed
the number of channels. Also, in contrast to prior work, it is assumed that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
i.e., even when the number of users is less than the number of channels. The
algorithm is extended to the dynamic case where the number of users in the
system evolves over time and our algorithm leads to sub-linear regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bande_M/0/1/0/all/0/1&quot;&gt;Meghana Bande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeravalli_V/0/1/0/all/0/1&quot;&gt;Venugopal V. Veeravalli&lt;/a&gt;</dc:creator>
</item></rdf:RDF>