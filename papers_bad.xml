<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00052"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00152"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00338"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.00369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01548"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00091"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00116"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00162"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00219"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00259"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00297"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.00725"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01625"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.04806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.08722"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06516"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00067"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00149"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00183"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00218"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00250"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00310"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00422"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00491"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.03537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07076"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07557"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08334"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.00052">
<title>Evolving Chaos: Identifying New Attractors of the Generalised Lorenz Family. (arXiv:1803.00052v1 [nlin.CD])</title>
<link>http://arxiv.org/abs/1803.00052</link>
<description rdf:parseType="Literal">&lt;p&gt;In a recent paper, we presented an intelligent evolutionary search technique
through genetic programming (GP) for finding new analytical expressions of
nonlinear dynamical systems, similar to the classical Lorenz attractor&apos;s which
also exhibit chaotic behaviour in the phase space. In this paper, we extend our
previous finding to explore yet another gallery of new chaotic attractors which
are derived from the original Lorenz system of equations. Compared to the
previous exploration with sinusoidal type transcendental nonlinearity, here we
focus on only cross-product and higher-power type nonlinearities in the three
state equations. We here report over 150 different structures of chaotic
attractors along with their one set of parameter values, phase space dynamics
and the Largest Lyapunov Exponents (LLE). The expressions of these new
Lorenz-like nonlinear dynamical systems have been automatically evolved through
multi-gene genetic programming (MGGP). In the past two decades, there have been
many claims of designing new chaotic attractors as an incremental extension of
the Lorenz family. We provide here a large family of chaotic systems whose
structure closely resemble the original Lorenz system but with drastically
different phase space dynamics. This advances the state of the art knowledge of
discovering new chaotic systems which can find application in many real-world
problems. This work may also find its archival value in future in the domain of
new chaotic system discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Pan_I/0/1/0/all/0/1&quot;&gt;Indranil Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Saptarshi Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00152">
<title>A Global Information Based Adaptive Threshold for Grouping Large Scale Global Optimization Problems. (arXiv:1803.00152v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.00152</link>
<description rdf:parseType="Literal">&lt;p&gt;By taking the idea of divide-and-conquer, cooperative coevolution (CC)
provides a powerful architecture for large scale global optimization (LSGO)
problems, but its efficiency relies highly on the decomposition strategy. It
has been shown that differential grouping (DG) performs well on decomposing
LSGO problems by effectively detecting the interaction among decision
variables. However, its decomposition accuracy depends highly on the threshold.
To improve the decomposition accuracy of DG, a global information based
adaptive threshold setting algorithm (GIAT) is proposed in this paper. On the
one hand, by reducing the sensitivity of the indicator in DG to the roundoff
error and the magnitude of contribution weight of subcomponent, we proposed a
new indicator for two variables which is much more sensitive to their
interaction. On the other hand, instead of setting the threshold only based on
one pair of variables, the threshold is generated from the interaction
information for all pair of variables. By conducting the experiments on two
sets of LSGO benchmark functions, the correctness and robustness of this new
indicator and GIAT were verified.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;An Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhigang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1&quot;&gt;Bei Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00338">
<title>Synthesizing realistic neural population activity patterns using Generative Adversarial Networks. (arXiv:1803.00338v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1803.00338</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to synthesize realistic patterns of neural activity is crucial
for studying neural information processing. Here we used the Generative
Adversarial Networks (GANs) framework to simulate the concerted activity of a
population of neurons. We adapted the Wasserstein-GAN variant to facilitate the
generation of unconstrained neural population activity patterns while still
benefiting from parameter sharing in the temporal domain. We demonstrate that
our proposed GAN, which we termed Spike-GAN, generates spike trains that match
accurately the first- and second-order statistics of datasets of tens of
neurons and also approximates well their higher-order statistics. We applied
Spike-GAN to a real dataset recorded from salamander retina and showed that it
performs as well as state-of-the-art approaches based on the maximum entropy
and the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not
require to specify a priori the statistics to be matched by the model, and so
constitutes a more flexible method than these alternative approaches.Finally,
we show how to exploit a trained Spike-GAN to construct &apos;importance maps&apos; to
detect the most relevant statistical structures present in a spike train.
Spike-GAN provides a powerful, easy-to-use technique for generating realistic
spiking neural activity and for describing the most relevant features of the
large-scale neural population recordings studied in modern systems
neuroscience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Molano_Mazon_M/0/1/0/all/0/1&quot;&gt;Manuel Molano-Mazon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1&quot;&gt;Arno Onken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Piasini_E/0/1/0/all/0/1&quot;&gt;Eugenio Piasini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Panzeri_S/0/1/0/all/0/1&quot;&gt;Stefano Panzeri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00370">
<title>Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search. (arXiv:1803.00370v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.00370</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers have applied deep neural networks to image restoration tasks, in
which they proposed various network architectures, loss functions, and training
methods. In particular, adversarial training, which is employed in recent
studies, seems to be a key ingredient to success. In this paper, we show that
simple convolutional autoencoders (CAEs) built upon only standard network
components, i.e., convolutional layers and skip connections, can outperform the
state-of-the-art methods which employ adversarial training and sophisticated
loss functions. The secret is to employ an evolutionary algorithm to
automatically search for good architectures. Training optimized CAEs by
minimizing the $\ell_2$ loss between reconstructed images and their ground
truths using the ADAM optimizer is all we need. Our experimental results show
that this approach achieves 27.8 dB peak signal to noise ratio (PSNR) on the
CelebA dataset and 40.4 dB on the SVHN dataset, compared to 22.8 dB and 33.0 dB
provided by the former state-of-the-art methods, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1&quot;&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1&quot;&gt;Mete Ozay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00412">
<title>A theory of sequence indexing and working memory in recurrent neural networks. (arXiv:1803.00412v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.00412</link>
<description rdf:parseType="Literal">&lt;p&gt;To accommodate structured approaches of neural computation, we propose a
class of recurrent neural networks for indexing and storing sequences of
symbols or analog data vectors. These networks with randomized input weights
and orthogonal recurrent weights implement coding principles previously
described in vector symbolic architectures (VSA), and leverage properties of
reservoir computing. In general, the storage in reservoir computing is lossy
and crosstalk noise limits the retrieval accuracy and information capacity. A
novel theory to optimize memory performance in such networks is presented and
compared with simulation experiments. The theory describes linear readout of
analog data, and readout with winner-take-all error correction of symbolic data
as proposed in VSA models. We find that diverse VSA models from the literature
have universal performance properties, which are superior to what previous
analyses predicted. Further, we propose novel VSA models with the statistically
optimal Wiener filter in the readout that exhibit much higher information
capacity, in particular for storing analog data.
&lt;/p&gt;
&lt;p&gt;The presented theory also applies to memory buffers, networks with gradual
forgetting, which can operate on infinite data streams without memory overflow.
Interestingly, we find that different forgetting mechanisms, such as
attenuating recurrent weights or neural nonlinearities, produce very similar
behavior if the forgetting time constants are aligned. Such models exhibit
extensive capacity when their forgetting time constant is optimized for given
noise conditions and network size. These results enable the design of new types
of VSA models for the online processing of data streams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frady_E/0/1/0/all/0/1&quot;&gt;E. Paxon Frady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1&quot;&gt;Denis Kleyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1&quot;&gt;Friedrich T. Sommer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00447">
<title>Optimal localist and distributed coding of spatiotemporal spike patterns through STDP and coincidence detection. (arXiv:1803.00447v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.00447</link>
<description rdf:parseType="Literal">&lt;p&gt;Repeating spatiotemporal spike patterns exist and carry information. Here we
investigate how a single neuron can optimally signal the presence of one given
pattern (localist coding), or of either one of several patterns (distributed
coding, i.e. the neuron&apos;s response is ambiguous but the identity of the pattern
could be inferred from the response of multiple neurons). Intuitively, we
should connect the detector neuron to the neurons that fire during the
patterns, or during subsections of them. Using a threshold-free leaky
integrate-and-fire (LIF) neuron with time constant $\tau$, non-plastic unitary
synapses and homogeneous Poisson inputs, we derived analytically the
signal-to-noise ratio (SNR) of the resulting pattern detector, even in the
presence of jitter. In most cases, this SNR turned out to be optimal for
relatively short $\tau$ (at most a few tens of ms). Thus long patterns are
optimally detected by coincidence detectors working at a shorter timescale,
although these ignore most of the patterns. When increasing the number of
patterns, the SNR decreases slowly, and remains acceptable for tens of
independent patterns.
&lt;/p&gt;
&lt;p&gt;Next, we wondered if spike-timing-dependent plasticity (STDP) could enable a
neuron to reach the theoretical optimum. We simulated a LIF equipped with STDP,
and repeatedly exposed it to multiple input spike patterns. The LIF
progressively became selective to every repeating pattern with no supervision,
even when the patterns were embedded in Poisson activity. Furthermore, using
certain STDP parameters, the resulting pattern detectors were optimal. Tens of
independent patterns could be learned by a single neuron with a low adaptive
threshold, in contrast with previous studies, in which higher thresholds led to
localist coding only.
&lt;/p&gt;
&lt;p&gt;Taken together these results suggest that coincidence detection and STDP are
powerful mechanisms, compatible with distributed coding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Masquelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1&quot;&gt;Saeed Reza Kheradpisheh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.00369">
<title>New Ideas for Brain Modelling 3. (arXiv:1612.00369v7 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1612.00369</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers a process for the creation and subsequent firing of
sequences of neuronal patterns, as might be found in the human brain. The scale
is one of larger patterns emerging from an ensemble mass, possibly through some
type of energy equation and a reduction procedure. The links between the
patterns can be formed naturally, as a residual effect of the pattern creation
itself. This paper follows-on closely from the earlier research, including two
earlier papers in the series and uses the ideas of entropy and cohesion. With a
small addition, it is possible to show how the inter-pattern links can be
determined. A new compact Grid form of an earlier Counting Mechanism is also
demonstrated. It is possible to explain how a very basic repeating structure
can form the arbitrary patterns and activation sequences between them, and a
key question of how nodes synchronise may even be answerable. The paper
finishes with an implementation architecture, for the realisation and storage
of knowledge and memory, as part of a general design, based on distributed
neural components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06434">
<title>EffNet: An Efficient Structure for Convolutional Neural Networks. (arXiv:1801.06434v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06434</link>
<description rdf:parseType="Literal">&lt;p&gt;With the ever increasing application of Convolutional Neural Networks to
costumer products the need emerges for models which can efficiently run on
embedded, mobile hardware. Slimmer models have therefore become a hot research
topic with multiple different approaches which vary from binary networks to
revised convolution layers. We offer our contribution to the latter and propose
a novel convolution block which significantly reduces the computational burden
while surpassing the current state-of-the-art. Our model, dubbed EffNet, is
optimised for models which are slim to begin with and is created to tackle
issues in existing models such as MobileNet and ShuffleNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1&quot;&gt;Ido Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roese_Koerner_L/0/1/0/all/0/1&quot;&gt;Lutz Roese-Koerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1&quot;&gt;Anton Kummert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01548">
<title>Regularized Evolution for Image Classifier Architecture Search. (arXiv:1802.01548v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01548</link>
<description rdf:parseType="Literal">&lt;p&gt;The effort devoted to hand-crafting image classifiers has motivated the use
of architecture search to discover them automatically. Reinforcement learning
and evolution have both shown promise for this purpose. This study employs a
regularized version of a popular asynchronous evolutionary algorithm. We
rigorously compare it to the non-regularized form and to a highly-successful
reinforcement learning baseline. Using the same hardware, compute effort and
neural network training code, we conduct repeated experiments side-by-side,
exploring different datasets, search spaces and scales. We show regularized
evolution consistently produces models with similar or higher accuracy, across
a variety of contexts without need for re-tuning parameters. In addition,
evolution exhibits considerably better performance than reinforcement learning
at early search stages, suggesting it may be the better choice when fewer
compute resources are available. This constitutes the first controlled
comparison of the two search algorithms in this context. Finally, we present
new architectures discovered with evolution that we nickname AmoebaNets. These
models achieve state-of-the-art results for CIFAR-10 (mean test error = 2.13%),
mobile-size ImageNet (top-1 accuracy = 75.1% with 5.1 M parameters) and
ImageNet (top-1 accuracy = 83.1%). This is the first time evolutionary
algorithms produce state-of-the-art image classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Real_E/0/1/0/all/0/1&quot;&gt;Esteban Real&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1&quot;&gt;Alok Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanping Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02203">
<title>Automatic construction of Chinese herbal prescription from tongue image via CNNs and auxiliary latent therapy topics. (arXiv:1802.02203v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02203</link>
<description rdf:parseType="Literal">&lt;p&gt;The tongue image is an important physical information of human, it is of
great importance to the diagnosis and treatment in clinical medicine. Herbal
prescriptions are simple, noninvasive and low side effects, and are widely
applied in China. Researches on automatic construction technology of herbal
prescription based on tongue image have great significance for deep learning to
explore the relevance from tongue image to herbal prescription, and can be
applied to healthcare services in mobile medical system. In order to adapt to
the tongue image in a variety of photographing environments and construct the
herbal prescriptions, a neural network framework for prescriptions construction
is designed, which includes single / double convolution channels and fully
connected layers, and propose the mechanism of auxiliary therapy topic loss to
model the therapy of Chinese doctors then alleviate the interference of sparse
output labels to the diversity of results. The experimental data include the
patient tongue images and their corresponding prescriptions from real world
outpatient clinic, and the experiment results can generate the prescriptions
that are close to the real samples, which verifies the feasibility of the
proposed method for automatic construction of herbal prescription from tongue
image. Also, provides a reference for automatic herbal prescription
construction from more physical information (or integrated body information).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1&quot;&gt;Guihua Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Huiqiang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00091">
<title>Verification of Markov Decision Processes with Risk-Sensitive Measures. (arXiv:1803.00091v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00091</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a method for computing policies in Markov decision processes with
risk-sensitive measures subject to temporal logic constraints. Specifically, we
use a particular risk-sensitive measure from cumulative prospect theory, which
has been previously adopted in psychology and economics. The nonlinear
transformation of the probabilities and utility functions yields a nonlinear
programming problem, which makes computation of optimal policies typically
challenging. We show that this nonlinear weighting function can be accurately
approximated by the difference of two convex functions. This observation
enables efficient policy computation using convex-concave programming. We
demonstrate the effectiveness of the approach on several scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubuktepe_M/0/1/0/all/0/1&quot;&gt;Murat Cubuktepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00101">
<title>Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning. (arXiv:1803.00101v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00101</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent model-free reinforcement learning algorithms have proposed
incorporating learned dynamics models as a source of additional data with the
intention of reducing sample complexity. Such methods hold the promise of
incorporating imagined data coupled with a notion of model uncertainty to
accelerate the learning of continuous control tasks. Unfortunately, they rely
on heuristics that limit usage of the dynamics model. We present model-based
value expansion, which controls for uncertainty in the model by only allowing
imagination to fixed depth. By enabling wider use of learned dynamics models
within a model-free reinforcement learning algorithm, we improve value
estimation, which, in turn, reduces the sample complexity of learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feinberg_V/0/1/0/all/0/1&quot;&gt;Vladimir Feinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1&quot;&gt;Alvin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00116">
<title>Separators and Adjustment Sets in Causal Graphs: Complete Criteria and an Algorithmic Framework. (arXiv:1803.00116v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00116</link>
<description rdf:parseType="Literal">&lt;p&gt;Principled reasoning about the identifiability of causal effects from
non-experimental data is an important application of graphical causal models.
We present an algorithmic framework for efficiently testing, constructing, and
enumerating $m$-separators in ancestral graphs (AGs), a class of graphical
causal models that can represent uncertainty about the presence of latent
confounders. Furthermore, we prove a reduction from causal effect
identification by covariate adjustment to $m$-separation in a subgraph for
directed acyclic graphs (DAGs) and maximal ancestral graphs (MAGs). Jointly,
these results yield constructive criteria that characterize all adjustment sets
as well as all minimal and minimum adjustment sets for identification of a
desired causal effect with multivariate exposures and outcomes in the presence
of latent confounding. Our results extend several existing solutions for
special cases of these problems. Our efficient algorithms allowed us to
empirically quantify the identifiability gap between covariate adjustment and
the do-calculus in random DAGs, covering a wide range of scenarios.
Implementations of our algorithms are provided in the R package dagitty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zander_B/0/1/0/all/0/1&quot;&gt;Benito van der Zander&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1&quot;&gt;Maciej Li&amp;#x15b;kiewicz&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Textor_J/0/1/0/all/0/1&quot;&gt;Johannes Textor&lt;/a&gt; (2) ((1) Institute for Theoretical Computer Science, Universit&amp;#xe4;t zu L&amp;#xfc;beck, Germany, (2) Institute for Computing and Information Sciences, Radboud University Nijmegen, Nijmegen, The Netherlands)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00119">
<title>Integrating Human-Provided Information Into Belief State Representation Using Dynamic Factorization. (arXiv:1803.00119v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00119</link>
<description rdf:parseType="Literal">&lt;p&gt;In partially observed environments, it can be useful for a human to provide
the robot with declarative information that augments its direct sensory
observations. For instance, given a robot on a search-and-rescue mission, a
human operator might suggest locations of interest. We provide a representation
for the robot&apos;s internal knowledge that supports efficient combination of raw
sensory information with high-level declarative information presented in a
formal language. Computational efficiency is achieved by dynamically selecting
an appropriate factoring of the belief state, combining aspects of the belief
when they are correlated through information and separating them when they are
not. This strategy works in open domains, in which the set of possible objects
is not known in advance, and provides significant improvements in inference
time, leading to more efficient planning for complex partially observable
tasks. We validate our approach experimentally in two open-domain planning
problems: a 2D discrete gridworld task and a 3D continuous cooking task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1&quot;&gt;Rohan Chitnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00162">
<title>Towards Cooperation in Sequential Prisoner&apos;s Dilemmas: a Deep Multiagent Reinforcement Learning Approach. (arXiv:1803.00162v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00162</link>
<description rdf:parseType="Literal">&lt;p&gt;The Iterated Prisoner&apos;s Dilemma has guided research on social dilemmas for
decades. However, it distinguishes between only two atomic actions: cooperate
and defect. In real-world prisoner&apos;s dilemmas, these choices are temporally
extended and different strategies may correspond to sequences of actions,
reflecting grades of cooperation. We introduce a Sequential Prisoner&apos;s Dilemma
(SPD) game to better capture the aforementioned characteristics. In this work,
we propose a deep multiagent reinforcement learning approach that investigates
the evolution of mutual cooperation in SPD games. Our approach consists of two
phases. The first phase is offline: it synthesizes policies with different
cooperation degrees and then trains a cooperation degree detection network. The
second phase is online: an agent adaptively selects its policy based on the
detected degree of opponent cooperation. The effectiveness of our approach is
demonstrated in two representative SPD 2D games: the Apple-Pear game and the
Fruit Gathering game. Experimental results show that our strategy can avoid
being exploited by exploitative opponents and achieve cooperation with
cooperative opponents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weixun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00185">
<title>Facial Expression Recognition Based on Complexity Perception Classification Algorithm. (arXiv:1803.00185v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.00185</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) has always been a challenging issue in
computer vision. The different expressions of emotion and uncontrolled
environmental factors lead to inconsistencies in the complexity of FER and
variability of between expression categories, which is often overlooked in most
facial expression recognition systems. In order to solve this problem
effectively, we presented a simple and efficient CNN model to extract facial
features, and proposed a complexity perception classification (CPC) algorithm
for FER. The CPC algorithm divided the dataset into an easy classification
sample subspace and a complex classification sample subspace by evaluating the
complexity of facial features that are suitable for classification. The
experimental results of our proposed algorithm on Fer2013 and CK-plus datasets
demonstrated the algorithm&apos;s effectiveness and superiority over other
state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1&quot;&gt;Guihua Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;JiaJiong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00196">
<title>Learning Flexible and Reusable Locomotion Primitives for a Microrobot. (arXiv:1803.00196v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.00196</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of gaits for robot locomotion can be a daunting process which
requires significant expert knowledge and engineering. This process is even
more challenging for robots that do not have an accurate physical model, such
as compliant or micro-scale robots. Data-driven gait optimization provides an
automated alternative to analytical gait design. In this paper, we propose a
novel approach to efficiently learn a wide range of locomotion tasks with
walking robots. This approach formalizes locomotion as a contextual policy
search task to collect data, and subsequently uses that data to learn
multi-objective locomotion primitives that can be used for planning. As a
proof-of-concept we consider a simulated hexapod modeled after a recently
developed microrobot, and we thoroughly evaluate the performance of this
microrobot on different tasks and gaits. Our results validate the proposed
controller and learning scheme on single and multi-objective locomotion tasks.
Moreover, the experimental simulations show that without any prior knowledge
about the robot used (e.g., dynamics model), our approach is capable of
learning locomotion primitives within 250 trials and subsequently using them to
successfully navigate through a maze.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Brian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Grant Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1&quot;&gt;Roberto Calandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Contreras_D/0/1/0/all/0/1&quot;&gt;Daniel Contreras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pister_K/0/1/0/all/0/1&quot;&gt;Kristofer Pister&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00204">
<title>Vector Quantization as Sparse Least Square Optimization. (arXiv:1803.00204v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00204</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector quantization aims to form new vectors/matrices with shared values
close to the original. It could compress data with acceptable information loss,
and could be of great usefulness in areas like Image Processing, Pattern
Recognition and Machine Learning. In recent years, the importance of
quantization has been soaring as it has been discovered huge potentials in
deploying practical neural networks, which is among one of the most popular
research topics. Conventional vector quantization methods usually suffer from
their own flaws: hand-coding domain rules quantization could produce poor
results when encountering complex data, and clustering-based algorithms have
the problem of inexact solution and high time consumption. In this paper, we
explored vector quantization problem from a new perspective of sparse least
square optimization and designed multiple algorithms with their program
implementations. Specifically, deriving from a sparse form of coefficient
matrix, three types of sparse least squares, with $l_0$, $l_1$, and generalized
$l_1 + l_2$ penalizations, are designed and implemented respectively. In
addition, to produce quantization results with given amount of quantized
values(instead of penalization coefficient $\lambda$), this paper proposed a
cluster-based least square quantization method, which could also be regarded as
an improvement of information preservation of conventional clustering
algorithm. The algorithms were tested on various data and tasks and their
computational properties were analyzed. The paper offers a new perspective to
probe the area of vector quantization, while the algorithms proposed could
provide more appropriate options for quantization tasks under different
circumstances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Ruisen Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00219">
<title>Tongue image constitution recognition based on Complexity Perception method. (arXiv:1803.00219v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.00219</link>
<description rdf:parseType="Literal">&lt;p&gt;Background and Object: In China, body constitution is highly related to
physiological and pathological functions of human body and determines the
tendency of the disease, which is of great importance for treatment in clinical
medicine. Tongue diagnosis, as a key part of Traditional Chinese Medicine
inspection, is an important way to recognize the type of constitution.In order
to deploy tongue image constitution recognition system on non-invasive mobile
device to achieve fast, efficient and accurate constitution recognition, an
efficient method is required to deal with the challenge of this kind of complex
environment. Methods: In this work, we perform the tongue area detection,
tongue area calibration and constitution classification using methods which are
based on deep convolutional neural network. Subject to the variation of
inconstant environmental condition, the distribution of the picture is uneven,
which has a bad effect on classification performance. To solve this problem, we
propose a method based on the complexity of individual instances to divide
dataset into two subsets and classify them separately, which is capable of
improving classification accuracy. To evaluate the performance of our proposed
method, we conduct experiments on three sizes of tongue datasets, in which deep
convolutional neural network method and traditional digital image analysis
method are respectively applied to extract features for tongue images. The
proposed method is combined with the base classifier Softmax, SVM, and
DecisionTree respectively. Results: As the experiments results shown, our
proposed method improves the classification accuracy by 1.135% on average and
achieves 59.99% constitution classification accuracy. Conclusions: Experimental
results on three datasets show that our proposed method can effectively improve
the classification accuracy of tongue constitution recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiajiong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1&quot;&gt;Guihua Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Haibin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lijun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jianzeng Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00259">
<title>Deep Reinforcement Learning for Sponsored Search Real-time Bidding. (arXiv:1803.00259v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00259</link>
<description rdf:parseType="Literal">&lt;p&gt;Bidding optimization is one of the most critical problems in online
advertising. Sponsored search (SS) auction, due to the randomness of user query
behavior and platform nature, usually adopts keyword-level bidding strategies.
In contrast, the display advertising (DA), as a relatively simpler scenario for
auction, has taken advantage of real-time bidding (RTB) to boost the
performance for advertisers. In this paper, we consider the RTB problem in
sponsored search auction, named SS-RTB. SS-RTB has a much more complex dynamic
environment, due to stochastic user query behavior and more complex bidding
policies based on multiple keywords of an ad. Most previous methods for DA
cannot be applied. We propose a reinforcement learning (RL) solution for
handling the complex dynamic environment. Although some RL methods have been
proposed for online advertising, they all fail to address the &quot;environment
changing&quot; problem: the state transition probabilities vary between two days.
Motivated by the observation that auction sequences of two days share similar
transition patterns at a proper aggregation level, we formulate a robust MDP
model at hour-aggregation level of the auction data and propose a
control-by-model framework for SS-RTB. Rather than generating bid prices
directly, we decide a bidding model for impressions of each hour and perform
real-time bidding accordingly. We also extend the method to handle the
multi-agent problem. We deployed the SS-RTB system in the e-commerce search
auction platform of Alibaba. Empirical experiments of offline evaluation and
online A/B test demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1&quot;&gt;Guang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaofei He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00268">
<title>Representation Learning in Partially Observable Environments using Sensorimotor Prediction. (arXiv:1803.00268v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00268</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to explore and act autonomously in an environment, an agent needs to
learn from the sensorimotor information that is captured while acting. By
extracting the regularities in this sensorimotor stream, it can learn a model
of the world, which in turn can be used as a basis for action and exploration.
&lt;/p&gt;
&lt;p&gt;This requires the acquisition of compact representations from a possibly high
dimensional raw observation, which is noisy and ambiguous. In this paper, we
learn sensory representations from sensorimotor prediction. We propose a model
which integrates sensorimotor information over time, and project it in a
sensory representation which is useful for prediction. We emphasize on a simple
example the role of motor and memory for learning sensory representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulak_T/0/1/0/all/0/1&quot;&gt;Thibaut Kulak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_M/0/1/0/all/0/1&quot;&gt;Michael Garcia Ortiz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00297">
<title>Q-CP: Learning Action Values for Cooperative Planning. (arXiv:1803.00297v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.00297</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on multi-robot systems has demonstrated promising results in
manifold applications and domains. Still, efficiently learning an effective
robot behaviors is very difficult, due to unstructured scenarios, high
uncertainties, and large state dimensionality (e.g. hyper-redundant and groups
of robot). To alleviate this problem, we present Q-CP a cooperative model-based
reinforcement learning algorithm, which exploits action values to both (1)
guide the exploration of the state space and (2) generate effective policies.
Specifically, we exploit Q-learning to attack the curse-of-dimensionality in
the iterations of a Monte-Carlo Tree Search. We implement and evaluate Q-CP on
different stochastic cooperative (general-sum) games: (1) a simple cooperative
navigation problem among 3 robots, (2) a cooperation scenario between a pair of
KUKA YouBots performing hand-overs, and (3) a coordination task between two
mobile robots entering a door. The obtained results show the effectiveness of
Q-CP in the chosen applications, where action values drive the exploration and
reduce the computational demand of the planning process while achieving good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riccio_F/0/1/0/all/0/1&quot;&gt;Francesco Riccio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capobianco_R/0/1/0/all/0/1&quot;&gt;Roberto Capobianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nardi_D/0/1/0/all/0/1&quot;&gt;Daniele Nardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00444">
<title>Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling. (arXiv:1803.00444v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00444</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in the field of inverse reinforcement learning (IRL) have
yielded sophisticated frameworks which relax the original modeling assumption
that the behavior of an observed agent reflects only a single intention.
Instead, the demonstration data is typically divided into parts, to account for
the fact that different trajectories may correspond to different intentions,
e.g., because they were generated by different domain experts. In this work, we
go one step further: using the intuitive concept of subgoals, we build upon the
premise that even a single trajectory can be explained more efficiently locally
within a certain context than globally, enabling a more compact representation
of the observed behavior. Based on this assumption, we build an implicit
intentional model of the agent&apos;s goals to forecast its behavior in unobserved
situations. The result is an integrated Bayesian prediction framework which
provides smooth policy estimates that are consistent with the expert&apos;s plan and
significantly outperform existing IRL solutions. Most notably, our framework
naturally handles situations where the intentions of the agent change with time
and classical IRL algorithms fail. In addition, due to its probabilistic
nature, the model can be straightforwardly applied in an active learning
setting to guide the demonstration process of the expert.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sosic_A/0/1/0/all/0/1&quot;&gt;Adrian &amp;#x160;o&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_E/0/1/0/all/0/1&quot;&gt;Elmar Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoubir_A/0/1/0/all/0/1&quot;&gt;Abdelhak M. Zoubir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1&quot;&gt;Heinz Koeppl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00546">
<title>Semi-Supervised Online Structure Learning for Composite Event Recognition. (arXiv:1803.00546v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00546</link>
<description rdf:parseType="Literal">&lt;p&gt;Online structure learning approaches, such as those stemming from Statistical
Relational Learning, enable the discovery of complex relations in noisy data
streams. However, these methods assume the existence of fully-labelled training
data, which is unrealistic for most real-world applications. We present a novel
approach for completing the supervision of a semi-supervised structure learning
task. We incorporate graph cut minimisation, a technique that derives labels
for unlabelled data, based on their distance to their labelled counterparts. In
order to adapt graph cut minimisation to first order logic, we employ a
suitable structural distance for measuring the distance between sets of logical
atoms. The labelling process is achieved online (single-pass) by means of a
caching mechanism and the Hoeffding bound, a statistical tool to approximate
globally-optimal decisions from locally-optimal ones. We evaluate our approach
on the task of composite event recognition by using a benchmark dataset for
human activity recognition, as well as a real dataset for maritime monitoring.
The evaluation suggests that our approach can effectively complete the missing
labels and eventually, improve the accuracy of the underlying structure
learning system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michelioudakis_E/0/1/0/all/0/1&quot;&gt;Evangelos Michelioudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artikis_A/0/1/0/all/0/1&quot;&gt;Alexander Artikis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1&quot;&gt;Georgios Paliouras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1511.00725">
<title>Toward an Efficient Multi-class Classification in an Open Universe. (arXiv:1511.00725v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1511.00725</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification is a fundamental task in machine learning and data mining.
Existing classification methods are designed to classify unknown instances
within a set of previously known training classes. Such a classification takes
the form of a prediction within a closed-set of classes. However, a more
realistic scenario that fits real-world applications is to consider the
possibility of encountering instances that do not belong to any of the training
classes, $i.e.$, an open-set classification. In such situation, existing
closed-set classifiers will assign a training label to these instances
resulting in a misclassification. In this paper, we introduce Galaxy-X, a novel
multi-class classification approach for open-set recognition problems. For each
class of the training set, Galaxy-X creates a minimum bounding hyper-sphere
that encompasses the distribution of the class by enclosing all of its
instances. In such manner, our method is able to distinguish instances
resembling previously seen classes from those that are of unknown ones. To
adequately evaluate open-set classification, we introduce a novel evaluation
procedure. Experimental results on benchmark datasets show the efficiency of
our approach in classifying novel instances from known as well as unknown
classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhifli_W/0/1/0/all/0/1&quot;&gt;Wajdi Dhifli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1&quot;&gt;Abdoulaye Banir&amp;#xe9; Diallo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01625">
<title>Optimal Vehicle Dispatching Schemes via Dynamic Pricing. (arXiv:1707.01625v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01625</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, ride-sharing has emerged as an effective way to
relieve traffic congestion. A key problem for these platforms is to come up
with a revenue-optimal (or GMV-optimal) pricing scheme and an induced vehicle
dispatching policy that incorporate geographic and temporal information. In
this paper, we aim to tackle this problem via an economic approach.
&lt;/p&gt;
&lt;p&gt;Modeled naively, the underlying optimization problem may be non-convex and
thus hard to compute. To this end, we use a so-called &quot;ironing&quot; technique to
convert the problem into an equivalent convex optimization one via a clean
Markov decision process (MDP) formulation, where the states are the driver
distributions and the decision variables are the prices for each pair of
locations. Our main finding is an efficient algorithm that computes the exact
revenue-optimal (or GMV-optimal) randomized pricing schemes. We characterize
the optimal solution of the MDP by a primal-dual analysis of a corresponding
convex program. We also conduct empirical evaluations of our solution through
real data of a major ride-sharing platform and show its advantages over fixed
pricing schemes as well as several prevalent surge-based pricing schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mengjing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Weiran Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Pingzhong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1&quot;&gt;Song Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.04806">
<title>New Ideas for Brain Modelling 4. (arXiv:1708.04806v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.04806</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper continues the research that considers a new cognitive model. In
particular, it considers the neural binding structure of an earlier paper. To
help with this, the paper describes some new methods in the areas of image
processing and high-level behaviour simulation. The work is all based on
earlier research by the author and the new additions are intended to fit in
with the overall design. For image processing, a grid-like structure is used
with full linking&apos;. Each cell in the classifier grid stores a list of all other
cells it gets associated with and this is used as the learned image that new
input is compared to. For the behaviour metric, a new prediction equation is
suggested, as part of a simulation, that uses feedback and history to
dynamically determine its course of action. While the new methods are from
widely different topics, both can be compared with the binary-analog type of
interface that is the main focus of the paper. It is suggested that the
simplest of linking between a tree and ensemble can explain neural binding and
variable signal strengths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.08722">
<title>Unifying DAGs and UGs. (arXiv:1708.08722v8 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.08722</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new class of graphical models that generalizes
Lauritzen-Wermuth-Frydenberg chain graphs by relaxing the semi-directed
acyclity constraint so that only directed cycles are forbidden. Moreover, up to
two edges are allowed between any pair of nodes. Specifically, we present
local, pairwise and global Markov properties for the new graphical models and
prove their equivalence. We also present an equivalent factorization property.
Finally, we present a causal interpretation of the new models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1&quot;&gt;Jose M. Pe&amp;#xf1;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04794">
<title>Fast semi-supervised discriminant analysis for binary classification of large data-sets. (arXiv:1709.04794v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04794</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional data requires scalable algorithms. We propose and analyze
three scalable and related algorithms for semi-supervised discriminant analysis
(SDA). These methods are based on Krylov subspace methods which exploit the
data sparsity and the shift-invariance of Krylov subspaces. In addition, the
problem definition was improved by adding centralization to the semi-supervised
setting. The proposed methods are evaluated on a industry-scale data set from a
pharmaceutical company to predict compound activity on target proteins. The
results show that SDA achieves good predictive performance and our methods only
require a few seconds, significantly improving computation time on previous
state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavernier_J/0/1/0/all/0/1&quot;&gt;Joris Tavernier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simm_J/0/1/0/all/0/1&quot;&gt;Jaak Simm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meerbergen_K/0/1/0/all/0/1&quot;&gt;Karl Meerbergen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1&quot;&gt;Joerg Kurt Wegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceulemans_H/0/1/0/all/0/1&quot;&gt;Hugo Ceulemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreau_Y/0/1/0/all/0/1&quot;&gt;Yves Moreau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06516">
<title>Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases. (arXiv:1802.06516v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06516</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decade a wide spectrum of machine learning models have been
developed to model the neurodegenerative diseases, associating biomarkers,
especially non-intrusive neuroimaging markers, with key clinical scores
measuring the cognitive status of patients. Multi-task learning (MTL) has been
commonly utilized by these studies to address high dimensionality and small
cohort size challenges. However, most existing MTL approaches are based on
linear models and suffer from two major limitations: 1) they cannot explicitly
consider upper/lower bounds in these clinical scores; 2) they lack the
capability to capture complicated non-linear interactions among the variables.
In this paper, we propose Subspace Network, an efficient deep modeling approach
for non-linear multi-task censored regression. Each layer of the subspace
network performs a multi-task censored regression to improve upon the
predictions from the last layer via sketching a low-dimensional subspace to
perform knowledge transfer among learning tasks. Under mild assumptions, for
each layer the parametric subspace can be recovered using only one pass of
training data. Empirical results demonstrate that the proposed subspace network
quickly picks up the correct parameter subspaces, and outperforms
state-of-the-arts in predicting neurodegenerative clinical scores using
information in brain imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mengying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baytas_I/0/1/0/all/0/1&quot;&gt;Inci M. Baytas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1&quot;&gt;Liang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10503">
<title>Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction. (arXiv:1802.10503v1 [cs.HC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.10503</link>
<description rdf:parseType="Literal">&lt;p&gt;Close human-robot cooperation is a key enabler for new developments in
advanced manufacturing and assistive applications. Close cooperation require
robots that can predict human actions and intent, and understand human
non-verbal cues. Recent approaches based on neural networks have led to
encouraging results in the human action prediction problem both in continuous
and discrete spaces. Our approach extends the research in this direction. Our
contributions are three-fold. First, we validate the use of gaze and body pose
cues as a means of predicting human action through a feature selection method.
Next, we address two shortcomings of existing literature: predicting multiple
and variable-length action sequences. This is achieved by introducing an
encoder-decoder recurrent neural network topology in the discrete action
prediction problem. In addition, we theoretically demonstrate the importance of
predicting multiple action sequences as a means of estimating the stochastic
reward in a human robot cooperation scenario. Finally, we show the ability to
effectively train the prediction model on a action prediction dataset,
involving human motion data, and explore the influence of the model&apos;s
parameters on its performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schydlo_P/0/1/0/all/0/1&quot;&gt;Paul Schydlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakovic_M/0/1/0/all/0/1&quot;&gt;Mirko Rakovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamone_L/0/1/0/all/0/1&quot;&gt;Lorenzo Jamone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Santos-Victor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00067">
<title>Constrained Classification and Ranking via Quantiles. (arXiv:1803.00067v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00067</link>
<description rdf:parseType="Literal">&lt;p&gt;In most machine learning applications, classification accuracy is not the
primary metric of interest. Binary classifiers which face class imbalance are
often evaluated by the $F_\beta$ score, area under the precision-recall curve,
Precision at K, and more. The maximization of many of these metrics can be
expressed as a constrained optimization problem, where the constraint is a
function of the classifier&apos;s predictions.
&lt;/p&gt;
&lt;p&gt;In this paper we propose a novel framework for learning with constraints that
can be expressed as a predicted positive rate (or negative rate) on a subset of
the training data. We explicitly model the threshold at which a classifier must
operate to satisfy the constraint, yielding a surrogate loss function which
avoids the complexity of constrained optimization. The method is model-agnostic
and only marginally more expensive than minimization of the unconstrained loss.
Experiments on a variety of benchmarks show competitive performance relative to
existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_A/0/1/0/all/0/1&quot;&gt;Alan Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eban_E/0/1/0/all/0/1&quot;&gt;Elad Eban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00113">
<title>Approximate Inference for Constructing Astronomical Catalogs from Images. (arXiv:1803.00113v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1803.00113</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new, fully generative model for constructing astronomical
catalogs from optical telescope image sets. Each pixel intensity is treated as
a Poisson random variable with a rate parameter that depends on the latent
properties of stars and galaxies. These latent properties are themselves
random, with scientific prior distributions constructed from large ancillary
datasets. We compare two procedures for posterior inference: Markov chain Monte
Carlo (MCMC) and variational inference (VI). MCMC excels at quantifying
uncertainty while VI is 1000x faster. Both procedures outperform the current
state-of-the-art method for measuring celestial bodies&apos; colors, shapes, and
morphologies. On a supercomputer, the VI procedure efficiently uses 665,000 CPU
cores (1.3 million hardware threads) to construct an astronomical catalog from
50 terabytes of images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Regier_J/0/1/0/all/0/1&quot;&gt;Jeffrey Regier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miller_A/0/1/0/all/0/1&quot;&gt;Andrew C. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schlegel_D/0/1/0/all/0/1&quot;&gt;David Schlegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adams_R/0/1/0/all/0/1&quot;&gt;Ryan P. Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McAuliffe_J/0/1/0/all/0/1&quot;&gt;Jon D. McAuliffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prabhat/0/1/0/all/0/1&quot;&gt;Prabhat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00114">
<title>SQL-Rank: A Listwise Approach to Collaborative Ranking. (arXiv:1803.00114v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00114</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a listwise approach for constructing user-specific
rankings in recommendation systems in a collaborative fashion. We contrast the
listwise approach to previous pointwise and pairwise approaches, which are
based on treating either each rating or each pairwise comparison as an
independent instance respectively. By extending the work of (Cao et al. 2007),
we cast listwise collaborative ranking as maximum likelihood under a
permutation model which applies probability mass to permutations based on a low
rank latent score matrix. We present a novel algorithm called SQL-Rank, which
can accommodate ties and missing data and can run in linear time. We develop a
theoretical framework for analyzing listwise ranking methods based on a novel
representation theory for the permutation model. Applying this framework to
collaborative ranking, we derive asymptotic statistical rates as the number of
users and items grow together. We conclude by demonstrating that our SQL-Rank
method often outperforms current state-of-the-art algorithms for implicit
feedback such as Weighted-MF and BPR and achieve favorable results when
compared to explicit feedback algorithms such as matrix factorization and
collaborative ranking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1&quot;&gt;James Sharpnack&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00149">
<title>Deep Learning for Causal Inference. (arXiv:1803.00149v1 [econ.EM])</title>
<link>http://arxiv.org/abs/1803.00149</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose deep learning techniques for econometrics,
specifically for causal inference and for estimating individual as well as
average treatment effects. The contribution of this paper is twofold: 1. For
generalized neighbor matching to estimate individual and average treatment
effects, we analyze the use of autoencoders for dimensionality reduction while
maintaining the local neighborhood structure among the data points in the
embedding space. This deep learning based technique is shown to perform better
than simple k nearest neighbor matching for estimating treatment effects,
especially when the data points have several features/covariates but reside in
a low dimensional manifold in high dimensional space. We also observe better
performance than manifold learning methods for neighbor matching. 2. Propensity
score matching is one specific and popular way to perform matching in order to
estimate average and individual treatment effects. We propose the use of deep
neural networks (DNNs) for propensity score matching, and present a network
called PropensityNet for this. This is a generalization of the logistic
regression technique traditionally used to estimate propensity scores and we
show empirically that DNNs perform better than logistic regression at
propensity score matching. Code for both methods will be made available shortly
on Github at: https://github.com/vikas84bf
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Ramachandra_V/0/1/0/all/0/1&quot;&gt;Vikas Ramachandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00183">
<title>Learning with Correntripy-induced Losses for Regression with Mixture of Symmetric Stable Noise. (arXiv:1803.00183v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00183</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, correntropy and its applications in machine learning have
been drawing continuous attention owing to its merits in dealing with
non-Gaussian noise and outliers. However, theoretical understanding of
correntropy, especially in the statistical learning context, is still limited.
In this study, within the statistical learning framework, we investigate
correntropy based regression in the presence of non-Gaussian noise or outliers.
To this purpose, we first introduce mixture of symmetric stable noise, which
include Gaussian noise, Cauchy noise, and the mixture of Gaussian noise as
special cases, to model non-Gaussian noise and outliers. We demonstrate that
under the mixture of symmetric stable noise assumption, correntropy based
regression can learn the conditional mean function or the conditional median
function well without requiring the finite variance assumption of the noise. In
particular, we establish learning rates for correntropy based regression
estimators that are asymptotically of type $\mathcal{O}(n^{-1})$. We believe
that the present study completes our understanding towards correntropy based
regression from a statistical learning viewpoint, and may also shed some light
on robust statistical learning for regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunlong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1&quot;&gt;Yiming Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00186">
<title>Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form. (arXiv:1803.00186v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00186</link>
<description rdf:parseType="Literal">&lt;p&gt;Semidefinite programs (SDP) are important in learning and combinatorial
optimization with numerous applications. In pursuit of low-rank solutions and
low complexity algorithms, we consider the Burer--Monteiro factorization
approach for solving SDPs. We show that all approximate local optima are global
optima for the penalty formulation of appropriately rank-constrained SDPs as
long as the number of constraints scales sub-quadratically with the desired
rank of the optimal solution. Our result is based on a simple penalty function
formulation of the rank-constrained SDP along with a smoothed analysis to avoid
worst-case cost matrices. We particularize our results to two applications,
namely, Max-Cut and matrix completion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhojanapalli_S/0/1/0/all/0/1&quot;&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boumal_N/0/1/0/all/0/1&quot;&gt;Nicolas Boumal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_P/0/1/0/all/0/1&quot;&gt;Prateek Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Netrapalli_P/0/1/0/all/0/1&quot;&gt;Praneeth Netrapalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00212">
<title>prDeep: Robust Phase Retrieval with Flexible Deep Neural Networks. (arXiv:1803.00212v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00212</link>
<description rdf:parseType="Literal">&lt;p&gt;Phase retrieval (PR) algorithms have become an important component in many
modern computational imaging systems. For instance, in the context of
ptychography and speckle correlation imaging PR algorithms enable imaging past
the diffraction limit and through scattering media, respectively.
Unfortunately, traditional PR algorithms struggle in the presence of noise.
Recently PR algorithms have been developed that use priors to make themselves
more robust. However, these algorithms often require unrealistic (Gaussian or
coded diffraction pattern) measurement models and offer slow computation times.
These drawbacks have hindered widespread adoption. In this work we use
convolutional neural networks, a powerful tool from machine learning, to
regularize phase retrieval problems and improve recovery performance. We test
our new algorithm, prDeep, in simulation and demonstrate that it is robust to
noise, can handle a variety system models, and operates fast enough for
high-resolution applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Metzler_C/0/1/0/all/0/1&quot;&gt;Christopher A. Metzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schniter_P/0/1/0/all/0/1&quot;&gt;Philip Schniter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veeraraghavan_A/0/1/0/all/0/1&quot;&gt;Ashok Veeraraghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard G. Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00218">
<title>Interval-based Prediction Uncertainty Bound Computation in Learning with Missing Values. (arXiv:1803.00218v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00218</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of machine learning with missing values is common in many areas.
A simple approach is to first construct a dataset without missing values simply
by discarding instances with missing entries or by imputing a fixed value for
each missing entry, and then train a prediction model with the new dataset. A
drawback of this naive approach is that the uncertainty in the missing entries
is not properly incorporated in the prediction. In order to evaluate prediction
uncertainty, the multiple imputation (MI) approach has been studied, but the
performance of MI is sensitive to the choice of the probabilistic model of the
true values in the missing entries, and the computational cost of MI is high
because multiple models must be trained. In this paper, we propose an
alternative approach called the Interval-based Prediction Uncertainty Bounding
(IPUB) method. The IPUB method represents the uncertainties due to missing
entries as intervals, and efficiently computes the lower and upper bounds of
the prediction results when all possible training sets constructed by imputing
arbitrary values in the intervals are considered. The IPUB method can be
applied to a wide class of convex learning algorithms including penalized
least-squares regression, support vector machine (SVM), and logistic
regression. We demonstrate the advantages of the IPUB method by comparing it
with an existing method in numerical experiment with benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanada_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Hanada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takada_T/0/1/0/all/0/1&quot;&gt;Toshiyuki Takada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sakuma_J/0/1/0/all/0/1&quot;&gt;Jun Sakuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00225">
<title>Block Coordinate Descent for Deep Learning: Unified Convergence Guarantees. (arXiv:1803.00225v1 [math.OC])</title>
<link>http://arxiv.org/abs/1803.00225</link>
<description rdf:parseType="Literal">&lt;p&gt;Training deep neural networks (DNNs) efficiently is a challenge due to the
associated highly nonconvex optimization. Recently, the efficiency of the block
coordinate descent (BCD) type methods has been empirically illustrated for DNN
training. The main idea of BCD is to decompose the highly composite and
nonconvex DNN training problem into several almost separable simple
subproblems. However, their convergence property has not been thoroughly
studied. In this paper, we establish some unified global convergence guarantees
of BCD type methods for a wide range of DNN training models, including but not
limited to multilayer perceptrons (MLPs), convolutional neural networks (CNNs)
and residual networks (ResNets). This paper nontrivially extends the existing
convergence results of nonconvex BCD from the smooth case to the nonsmooth
case. Our convergence analysis is built upon the powerful
Kurdyka-{\L}ojasiewicz (KL) framework but some new techniques are introduced,
including the establishment of the KL property of the objective functions of
many commonly used DNNs, where the loss function can be taken as squared, hinge
and logistic losses, and the activation function can be taken as rectified
linear units (ReLUs), sigmoid and linear link functions. The efficiency of BCD
method is also demonstrated by a series of exploratory numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jinshan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lau_T/0/1/0/all/0/1&quot;&gt;Tim Tsz-Kit Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shaobo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00250">
<title>Wasserstein Distance Measure Machines. (arXiv:1803.00250v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00250</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a distance-based discriminative framework for learning
with probability distributions. Instead of using kernel mean embeddings or
generalized radial basis kernels, we introduce embeddings based on
dissimilarity of distributions to some reference distributions denoted as
templates. Our framework extends the theory of similarity of
\citet{balcan2008theory} to the population distribution case and we prove that,
for some learning problems, Wasserstein distance achieves low-error linear
decision functions with high probability. Our key result is to prove that the
theory also holds for empirical distributions. Algorithmically, the proposed
approach is very simple as it consists in computing a mapping based on pairwise
Wasserstein distances and then learning a linear decision function. Our
experimental results show that this Wasserstein distance embedding performs
better than kernel mean embeddings and computing Wasserstein distance is far
more tractable than estimating pairwise Kullback-Leibler divergence of
empirical distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1&quot;&gt;Alain Rakotomamonjy&lt;/a&gt; (LITIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traore_A/0/1/0/all/0/1&quot;&gt;Abraham Traore&lt;/a&gt; (LITIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1&quot;&gt;Maxime Berar&lt;/a&gt; (LITIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Flamary&lt;/a&gt; (OCA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1&quot;&gt;Nicolas Courty&lt;/a&gt; (OBELIX)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00276">
<title>Model-Based Clustering and Classification of Functional Data. (arXiv:1803.00276v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00276</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of complex data analysis is a central topic of modern statistical
science and learning systems and is becoming of broader interest with the
increasing prevalence of high-dimensional data. The challenge is to develop
statistical models and autonomous algorithms that are able to acquire knowledge
from raw data for exploratory analysis, which can be achieved through
clustering techniques or to make predictions of future data via classification
(i.e., discriminant analysis) techniques. Latent data models, including mixture
model-based approaches are one of the most popular and successful approaches in
both the unsupervised context (i.e., clustering) and the supervised one (i.e,
classification or discrimination). Although traditionally tools of multivariate
analysis, they are growing in popularity when considered in the framework of
functional data analysis (FDA). FDA is the data analysis paradigm in which the
individual data units are functions (e.g., curves, surfaces), rather than
simple vectors. In many areas of application, the analyzed data are indeed
often available in the form of discretized values of functions or curves (e.g.,
time series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data).
This functional aspect of the data adds additional difficulties compared to the
case of a classical multivariate (non-functional) data analysis. We review and
present approaches for model-based clustering and classification of functional
data. We derive well-established statistical models along with efficient
algorithmic tools to address problems regarding the clustering and the
classification of these high-dimensional data, including their heterogeneity,
missing information, and dynamical hidden structure. The presented models and
algorithms are illustrated on real-world functional data analysis problems from
several application area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chamroukhi_F/0/1/0/all/0/1&quot;&gt;Faicel Chamroukhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien D. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00310">
<title>Minimax rates for cost-sensitive learning on manifolds with approximate nearest neighbours. (arXiv:1803.00310v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00310</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the approximate nearest neighbour method for cost-sensitive
classification on low-dimensional manifolds embedded within a high-dimensional
feature space. We determine the minimax learning rates for distributions on a
smooth manifold, in a cost-sensitive setting. This generalises a classic result
of Audibert and Tsybakov. Building upon recent work of Chaudhuri and Dasgupta
we prove that these minimax rates are attained by the approximate nearest
neighbour algorithm, where neighbours are computed in a randomly projected
low-dimensional space. In addition, we give a bound on the number of dimensions
required for the projection which depends solely upon the reach and dimension
of the manifold, combined with the regularity of the marginal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reeve_H/0/1/0/all/0/1&quot;&gt;Henry WJ Reeve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1&quot;&gt;Gavin Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00316">
<title>The K-Nearest Neighbour UCB algorithm for multi-armed bandits with covariates. (arXiv:1803.00316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00316</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose and explore the k-Nearest Neighbour UCB algorithm
for multi-armed bandits with covariates. We focus on a setting where the
covariates are supported on a metric space of low intrinsic dimension, such as
a manifold embedded within a high dimensional ambient feature space. The
algorithm is conceptually simple and straightforward to implement. The
k-Nearest Neighbour UCB algorithm does not require prior knowledge of the
either the intrinsic dimension of the marginal distribution or the time
horizon. We prove a regret bound for the k-Nearest Neighbour UCB algorithm
which is minimax optimal up to logarithmic factors. In particular, the
algorithm automatically takes advantage of both low intrinsic dimensionality of
the marginal distribution over the covariates and low noise in the data,
expressed as a margin condition. In addition, focusing on the case of bounded
rewards, we give corresponding regret bounds for the k-Nearest Neighbour KL-UCB
algorithm, which is an analogue of the KL-UCB algorithm adapted to the setting
of multi-armed bandits with covariates. Finally, we present empirical results
which demonstrate the ability of both the k-Nearest Neighbour UCB and k-Nearest
Neighbour KL-UCB to take advantage of situations where the data is supported on
an unknown sub-manifold of a high-dimensional feature space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reeve_H/0/1/0/all/0/1&quot;&gt;Henry WJ Reeve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1&quot;&gt;Joe Mellor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1&quot;&gt;Gavin Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00420">
<title>Tractable and Scalable Schatten Quasi-Norm Approximations for Rank Minimization. (arXiv:1803.00420v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00420</link>
<description rdf:parseType="Literal">&lt;p&gt;The Schatten quasi-norm was introduced to bridge the gap between the trace
norm and rank function. However, existing algorithms are too slow or even
impractical for large-scale problems. Motivated by the equivalence relation
between the trace norm and its bilinear spectral penalty, we define two
tractable Schatten norms, i.e.\ the bi-trace and tri-trace norms, and prove
that they are in essence the Schatten-$1/2$ and $1/3$ quasi-norms,
respectively. By applying the two defined Schatten quasi-norms to various rank
minimization problems such as MC and RPCA, we only need to solve much smaller
factor matrices. We design two efficient linearized alternating minimization
algorithms to solve our problems and establish that each bounded sequence
generated by our algorithms converges to a critical point. We also provide the
restricted strong convexity (RSC) based and MC error bounds for our algorithms.
Our experimental results verified both the efficiency and effectiveness of our
algorithms compared with the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fanhua Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;James Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00422">
<title>Distributed multivariable modeling for signature development under data protection constraints. (arXiv:1803.00422v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00422</link>
<description rdf:parseType="Literal">&lt;p&gt;Data protection constraints frequently require distributed analysis of data,
i.e. individual-level data remains at many different sites, but analysis
nevertheless has to be performed jointly. The data exchange is often handled
manually, requiring explicit permission before transfer, i.e. the number of
data calls and the amount of data should be limited. Thus, only simple summary
statistics are typically transferred and aggregated with just a single call,
but this does not allow for complex statistical techniques, e.g., automatic
variable selection for prognostic signature development. We propose a
multivariable regression approach for building a prognostic signature by
automatic variable selection that is based on aggregated data from different
locations in iterative calls. To minimize the amount of transferred data and
the number of calls, we also provide a heuristic variant of the approach. To
further strengthen data protection, the approach can also be combined with a
trusted third party architecture. We evaluate our proposed method in a
simulation study comparing our results to the results obtained with the pooled
individual data. The proposed method is seen to be able to detect covariates
with true effect to a comparable extent as a method based on individual data,
although the performance is moderately decreased if the number of sites is
large. In a typical scenario, the heuristic decreases the number of data calls
from more than 10 to 3. To make our approach widely available for application,
we provide an implementation on top of the DataSHIELD framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zoller_D/0/1/0/all/0/1&quot;&gt;Daniela Z&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lenz_S/0/1/0/all/0/1&quot;&gt;Stefan Lenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Binder_H/0/1/0/all/0/1&quot;&gt;Harald Binder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00446">
<title>Inferring Missing Categorical Information in Noisy and Sparse Web Markup. (arXiv:1803.00446v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00446</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedded markup of Web pages has seen widespread adoption throughout the past
years driven by standards such as RDFa and Microdata and initiatives such as
schema.org, where recent studies show an adoption by 39% of all Web pages
already in 2016. While this constitutes an important information source for
tasks such as Web search, Web page classification or knowledge graph
augmentation, individual markup nodes are usually sparsely described and often
lack essential information. For instance, from 26 million nodes describing
events within the Common Crawl in 2016, 59% of nodes provide less than six
statements and only 257,000 nodes (0.96%) are typed with more specific event
subtypes. Nevertheless, given the scale and diversity of Web markup data, nodes
that provide missing information can be obtained from the Web in large
quantities, in particular for categorical properties. Such data constitutes
potential training data for inferring missing information to significantly
augment sparsely described nodes. In this work, we introduce a supervised
approach for inferring missing categorical properties in Web markup. Our
experiments, conducted on properties of events and movies, show a performance
of 79% and 83% F1 score correspondingly, significantly outperforming existing
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tempelmeier_N/0/1/0/all/0/1&quot;&gt;Nicolas Tempelmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1&quot;&gt;Elena Demidova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1&quot;&gt;Stefan Dietze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00491">
<title>The Power Mean Laplacian for Multilayer Graph Clustering. (arXiv:1803.00491v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00491</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilayer graphs encode different kind of interactions between the same set
of entities. When one wants to cluster such a multilayer graph, the natural
question arises how one should merge the information different layers. We
introduce in this paper a one-parameter family of matrix power means for
merging the Laplacians from different layers and analyze it in expectation in
the stochastic block model. We show that this family allows to recover ground
truth clusters under different settings and verify this in real world data.
While computing the matrix power mean can be very expensive for large graphs,
we introduce a numerical scheme to efficiently compute its eigenvectors for the
case of large sparse graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mercado_P/0/1/0/all/0/1&quot;&gt;Pedro Mercado&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gautier_A/0/1/0/all/0/1&quot;&gt;Antoine Gautier&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tudisco_F/0/1/0/all/0/1&quot;&gt;Francesco Tudisco&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt; (1) ((1) Saarland University, (2) University of Strathclyde)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00500">
<title>Natural data structure extracted from neighborhood-similarity graphs. (arXiv:1803.00500v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00500</link>
<description rdf:parseType="Literal">&lt;p&gt;&apos;Big&apos; high-dimensional data are commonly analyzed in low-dimensions, after
performing a dimensionality-reduction step that inherently distorts the data
structure. For the same purpose, clustering methods are also often used. These
methods also introduce a bias, either by starting from the assumption of a
particular geometric form of the clusters, or by using iterative schemes to
enhance cluster contours, with uncontrollable consequences. The goal of data
analysis should, however, be to encode and detect structural data features at
all scales and densities simultaneously, without assuming a parametric form of
data point distances, or modifying them. We propose a novel approach that
directly encodes data point neighborhood similarities as a sparse graph. Our
non-iterative framework permits a transparent interpretation of data, without
altering the original data dimension and metric. Several natural and synthetic
data applications demonstrate the efficacy of our novel approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lorimer_T/0/1/0/all/0/1&quot;&gt;Tom Lorimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kanders_K/0/1/0/all/0/1&quot;&gt;Karlis Kanders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stoop_R/0/1/0/all/0/1&quot;&gt;Ruedi Stoop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.03537">
<title>An Efficient, Expressive and Local Minima-free Method for Learning Controlled Dynamical Systems. (arXiv:1702.03537v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.03537</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a framework for modeling and estimating the state of controlled
dynamical systems, where an agent can affect the system through actions and
receives partial observations. Based on this framework, we propose the
Predictive State Representation with Random Fourier Features (RFFPSR). A key
property in RFF-PSRs is that the state estimate is represented by a conditional
distribution of future observations given future actions. RFF-PSRs combine this
representation with moment-matching, kernel embedding and local optimization to
achieve a method that enjoys several favorable qualities: It can represent
controlled environments which can be affected by actions; it has an efficient
and theoretically justified learning algorithm; it uses a non-parametric
representation that has expressive power to represent continuous non-linear
dynamics. We provide a detailed formulation, a theoretical analysis and an
experimental evaluation that demonstrates the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hefny_A/0/1/0/all/0/1&quot;&gt;Ahmed Hefny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Downey_C/0/1/0/all/0/1&quot;&gt;Carlton Downey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gordon_G/0/1/0/all/0/1&quot;&gt;Geoffrey J. Gordon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00514">
<title>Selective Inference for Change Point Detection in Multi-dimensional Sequences. (arXiv:1706.00514v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00514</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of detecting change points (CPs) that are characterized
by a subset of dimensions in a multi-dimensional sequence. A method for
detecting those CPs can be formulated as a two-stage method: one for selecting
relevant dimensions, and another for selecting CPs. It has been difficult to
properly control the false detection probability of these CP detection methods
because selection bias in each stage must be properly corrected. Our main
contribution in this paper is to formulate a CP detection problem as a
selective inference problem, and show that exact (non-asymptotic) inference is
possible for a class of CP detection methods. We demonstrate the performances
of the proposed selective inference framework through numerical simulations and
its application to our motivating medical data analysis problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Umezu_Y/0/1/0/all/0/1&quot;&gt;Yuta Umezu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10568">
<title>Stochastic Training of Graph Convolutional Networks with Variance Reduction. (arXiv:1710.10568v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10568</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks (GCNs) are powerful deep neural networks for
graph-structured data. However, GCN computes the representation of a node
recursively from its neighbors, making the receptive field size grow
exponentially with the number of layers. Previous attempts on reducing the
receptive field size by subsampling neighbors do not have a convergence
guarantee, and their receptive field size per node is still in the order of
hundreds. In this paper, we develop control variate based algorithms which
allow sampling an arbitrarily small neighbor size. Furthermore, we prove new
theoretical guarantee for our algorithms to converge to a local optimum of GCN.
Empirical results show that our algorithms enjoy a similar convergence with the
exact algorithm using only two neighbors per node. The runtime of our
algorithms on a large Reddit dataset is only one seventh of previous neighbor
sampling algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianfei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07076">
<title>Does mitigating ML&apos;s impact disparity require treatment disparity?. (arXiv:1711.07076v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07076</link>
<description rdf:parseType="Literal">&lt;p&gt;Following related work in law and policy, two notions of disparity have come
to shape the study of fairness in algorithmic decision-making. Algorithms
exhibit treatment disparity if they formally treat members of protected
subgroups differently; algorithms exhibit impact disparity when outcomes differ
across subgroups, even if the correlation arises unintentionally. Naturally, we
can achieve impact parity through purposeful treatment disparity. In one thread
of technical work, papers aim to reconcile the two forms of parity proposing
disparate learning processes (DLPs). Here, the learning algorithm can see group
membership during training but produce a classifier that is group-blind at test
time. In this paper, we show theoretically that: (i) When other features
correlate to group membership, DLPs will (indirectly) implement treatment
disparity, undermining the policy desiderata they are designed to address; (ii)
When group membership is partly revealed by other features, DLPs induce
within-class discrimination; and (iii) In general, DLPs provide a suboptimal
trade-off between accuracy and impact parity. Based on our technical analysis,
we argue that transparent treatment disparity is preferable to occluded methods
for achieving impact parity. Experimental results on several real-world
datasets highlight the practical consequences of applying DLPs vs. per-group
thresholds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chouldechova_A/0/1/0/all/0/1&quot;&gt;Alexandra Chouldechova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McAuley_J/0/1/0/all/0/1&quot;&gt;Julian McAuley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07557">
<title>Differentially Private Federated Learning: A Client Level Perspective. (arXiv:1712.07557v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07557</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a recent advance in privacy protection. In this
context, a trusted curator aggregates parameters optimized in decentralized
fashion by multiple clients. The resulting model is then distributed back to
all clients, ultimately converging to a joint representative model without
explicitly having to share the data. However, the protocol is vulnerable to
differential attacks, which could originate from any party contributing during
federated optimization. In such an attack, a client&apos;s contribution during
training and information about their data set is revealed through analyzing the
distributed model. We tackle this problem and propose an algorithm for client
sided differential privacy preserving federated optimization. The aim is to
hide clients&apos; contributions during training, balancing the trade-off between
privacy loss and model performance. Empirical studies suggest that given a
sufficiently large number of participating clients, our proposed procedure can
maintain client-level differential privacy at only a minor cost in model
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_R/0/1/0/all/0/1&quot;&gt;Robin C. Geyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1&quot;&gt;Tassilo Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1&quot;&gt;Moin Nabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04374">
<title>Tempered Adversarial Networks. (arXiv:1802.04374v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04374</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) have been shown to produce realistic
samples from high-dimensional distributions, but training them is considered
hard. A possible explanation for training instabilities is the inherent
imbalance between the networks: While the discriminator is trained directly on
both real and fake samples, the generator only has control over the fake
samples it produces since the real data distribution is fixed by the choice of
a given dataset. We propose a simple modification that gives the generator
control over the real samples which leads to a tempered learning process for
both generator and discriminator. The real data distribution passes through a
lens before being revealed to the discriminator, balancing the generator and
discriminator by gradually revealing more detailed features necessary to
produce high-quality results. The proposed module automatically adjusts the
learning process to the current strength of the networks, yet is generic and
easy to add to any GAN variant. In a number of experiments, we show that this
can improve quality, stability and/or convergence speed across a range of
different GAN architectures (DCGAN, LSGAN, WGAN-GP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08334">
<title>Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification. (arXiv:1802.08334v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08334</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove that the ordinary least-squares (OLS) estimator attains nearly
minimax optimal performance for the identification of linear dynamical systems
from a single observed trajectory. Our upper bound relies on a generalization
of Mendelson&apos;s small-ball method to dependent data, eschewing the use of
standard mixing-time arguments. Our lower bounds reveal that these upper bounds
match up to logarithmic factors. In particular, we capture the correct
signal-to-noise behavior of the problem, showing that more unstable linear
systems are easier to estimate. This behavior is qualitatively different from
arguments which rely on mixing-time calculations that suggest that unstable
systems are more difficult to estimate. We generalize our technique to provide
bounds for a more general class of linear response time-series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mania_H/0/1/0/all/0/1&quot;&gt;Horia Mania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1&quot;&gt;Stephen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1&quot;&gt;Benjamin Recht&lt;/a&gt;</dc:creator>
</item></rdf:RDF>