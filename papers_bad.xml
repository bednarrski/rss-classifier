<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03304"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03395"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03441"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03282"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03437"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03599"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03611"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03154"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03184"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03194"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03201"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03236"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03273"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03346"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03523"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03565"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.08232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.07256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07079"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01526"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.03294">
<title>A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers. (arXiv:1804.03294v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.03294</link>
<description rdf:parseType="Literal">&lt;p&gt;Weight pruning methods for deep neural networks (DNNs) have been investigated
recently, but prior work in this area is mainly heuristic, iterative pruning,
thereby lacking guarantees on the weight reduction ratio and convergence time.
To mitigate these limitations, we present a systematic weight pruning framework
of DNNs using the alternating direction method of multipliers (ADMM). We first
formulate the weight pruning problem of DNNs as a nonconvex optimization
problem with combinatorial constraints specifying the sparsity requirements,
and then adopt the ADMM framework for systematic weight pruning. By using ADMM,
the original nonconvex optimization problem is decomposed into two subproblems
that are solved iteratively. One of these subproblems can be solved using
stochastic gradient descent, while the other can be solved analytically. The
proposed ADMM weight pruning method incurs no additional suboptimality besides
that resulting from the nonconvex nature of the original optimization problem.
Furthermore, our approach achieves a fast convergence rate.
&lt;/p&gt;
&lt;p&gt;The weight pruning results are very promising and consistently outperform
prior work. On the LeNet-5 model for the MNIST data set, we achieve 40.2 times
weight reduction without accuracy loss. On the AlexNet model for the ImageNet
data set, we achieve 20 times weight reduction without accuracy loss. When we
focus on the convolutional layer pruning for computation reductions, we can
reduce the total computation by five times compared with prior work (achieving
a total of 13.4 times weight reduction in convolutional layers). A significant
acceleration for DNN training is observed as well, in that we can finish the
whole training process on AlexNet around 80 hours. Our models are released at
https://drive.google.com/drive/folders/1_O9PLIFiNHIaQIuOIJjq0AyQ7UpotlNl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Shaokai Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wujie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fardad_M/0/1/0/all/0/1&quot;&gt;Makan Fardad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03304">
<title>Seeking Open-Ended Evolution in Swarm Chemistry II: Analyzing Long-Term Dynamics via Automated Object Harvesting. (arXiv:1804.03304v1 [nlin.AO])</title>
<link>http://arxiv.org/abs/1804.03304</link>
<description rdf:parseType="Literal">&lt;p&gt;We studied the long-term dynamics of evolutionary Swarm Chemistry by
extending the simulation length ten-fold compared to earlier work and by
developing and using a new automated object harvesting method. Both macroscopic
dynamics and microscopic object features were characterized and tracked using
several measures. Results showed that the evolutionary dynamics tended to
settle down into a stable state after the initial transient period, and that
the extent of environmental perturbations also affected the evolutionary trends
substantially. In the meantime, the automated harvesting method successfully
produced a huge collection of spontaneously evolved objects, revealing the
system&apos;s autonomous creativity at an unprecedented scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Sayama_H/0/1/0/all/0/1&quot;&gt;Hiroki Sayama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03395">
<title>Effects of Higher Order and Long-Range Synchronizations for Classification and Computing in Oscillator-Based Spiking Neural Networks. (arXiv:1804.03395v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.03395</link>
<description rdf:parseType="Literal">&lt;p&gt;Development of artificial oscillator-based spiking neural networks (SNN),
which are able to solve effectively various cybernetics problems including
image recognition and adaptive control, is a key line of research. We have
thoroughly explored the scheme of two thermally coupled $VO_2$ oscillators and
found its effect of high order synchronization (HOS), which may be used to
increase SNN classification capacity $N_s$. Phase-locking estimation method has
been developed to determine values of subharmonic ratio SHR and synchronization
effectiveness {\eta}. The experimental scheme has $N_s=12$ and SHR
distributions are shaped as Arnold&apos;s tongues. In a model $N_s$ may reach
maximum of $N_s&amp;gt;150$ at certain levels of coupling strength and noise. We
demonstrate the long-range synchronization effect in a one-dimensional chain of
oscillators and the phenomenon of synchronization transfer even at low values
of {\eta} for intermediate links. The paper demonstrates realization of
analogue operation of &quot;multiplication&quot;, binary logic, and possibility of
development of the interface between SNN and computer. The described effects
increasing classification capacity of oscillator schemes and calculation
principles based on the universal physical effect - HOS may be applied for any
spiking type oscillators with any coupling type therefore enhancing practical
value of the presented results to expand SNN capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1&quot;&gt;Andrey Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putrolaynen_V/0/1/0/all/0/1&quot;&gt;Vadim Putrolaynen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1&quot;&gt;Maksim Belyaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03441">
<title>The Brain on Low Power Architectures - Efficient Simulation of Cortical Slow Waves and Asynchronous States. (arXiv:1804.03441v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1804.03441</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient brain simulation is a scientific grand challenge, a
parallel/distributed coding challenge and a source of requirements and
suggestions for future computing architectures. Indeed, the human brain
includes about 10^15 synapses and 10^11 neurons activated at a mean rate of
several Hz. Full brain simulation poses Exascale challenges even if simulated
at the highest abstraction level. The WaveScalES experiment in the Human Brain
Project (HBP) has the goal of matching experimental measures and simulations of
slow waves during deep-sleep and anesthesia and the transition to other brain
states. The focus is the development of dedicated large-scale
parallel/distributed simulation technologies. The ExaNeSt project designs an
ARM-based, low-power HPC architecture scalable to million of cores, developing
a dedicated scalable interconnect system, and SWA/AW simulations are included
among the driving benchmarks. At the joint between both projects is the INFN
proprietary Distributed and Plastic Spiking Neural Networks (DPSNN) simulation
engine. DPSNN can be configured to stress either the networking or the
computation features available on the execution platforms. The simulation
stresses the networking component when the neural net - composed by a
relatively low number of neurons, each one projecting thousands of synapses -
is distributed over a large number of hardware cores. When growing the number
of neurons per core, the computation starts to be the dominating component for
short range connections. This paper reports about preliminary performance
results obtained on an ARM-based HPC prototype developed in the framework of
the ExaNeSt project. Furthermore, a comparison is given of instantaneous power,
total energy consumption, execution time and energetic cost per synaptic event
of SWA/AW DPSNN simulations when executed on either ARM- or Intel-based server
platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammendola_R/0/1/0/all/0/1&quot;&gt;Roberto Ammendola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biagioni_A/0/1/0/all/0/1&quot;&gt;Andrea Biagioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capuani_F/0/1/0/all/0/1&quot;&gt;Fabrizio Capuani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cretaro_P/0/1/0/all/0/1&quot;&gt;Paolo Cretaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonis_G/0/1/0/all/0/1&quot;&gt;Giulia De Bonis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cicero_F/0/1/0/all/0/1&quot;&gt;Francesca Lo Cicero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lonardo_A/0/1/0/all/0/1&quot;&gt;Alessandro Lonardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinelli_M/0/1/0/all/0/1&quot;&gt;Michele Martinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paolucci_P/0/1/0/all/0/1&quot;&gt;Pier Stanislao Paolucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pastorelli_E/0/1/0/all/0/1&quot;&gt;Elena Pastorelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pontisso_L/0/1/0/all/0/1&quot;&gt;Luca Pontisso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simula_F/0/1/0/all/0/1&quot;&gt;Francesco Simula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicini_P/0/1/0/all/0/1&quot;&gt;Piero Vicini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02884">
<title>Policy Gradient With Value Function Approximation For Collective Multiagent Planning. (arXiv:1804.02884v1 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1804.02884</link>
<description rdf:parseType="Literal">&lt;p&gt;Decentralized (PO)MDPs provide an expressive framework for sequential
decision making in a multiagent system. Given their computational complexity,
recent research has focused on tractable yet practical subclasses of
Dec-POMDPs. We address such a subclass called CDEC-POMDP where the collective
behavior of a population of agents affects the joint-reward and environment
dynamics. Our main contribution is an actor-critic (AC) reinforcement learning
method for optimizing CDEC-POMDP policies. Vanilla AC has slow convergence for
larger problems. To address this, we show how a particular decomposition of the
approximate action-value function over agents leads to effective updates, and
also derive a new way to train the critic based on local reward signals.
Comparisons on a synthetic benchmark and a real-world taxi fleet optimization
problem show that our new AC approach provides better quality solutions than
previous best approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc Thien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Akshat Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_H/0/1/0/all/0/1&quot;&gt;Hoong Chuin Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03282">
<title>Segmentation of Multiple Sclerosis lesion in brain MR images using Fuzzy C-Means. (arXiv:1804.03282v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1804.03282</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance images (MRI) play an important role in supporting and
substituting clinical information in the diagnosis of multiple sclerosis (MS)
disease by presenting lesion in brain MR images. In this paper, an algorithm
for MS lesion segmentation from Brain MR Images has been presented. We revisit
the modification of properties of fuzzy -c means algorithms and the canny edge
detection. By changing and reformed fuzzy c-means clustering algorithms, and
applying canny contraction principle, a relationship between MS lesions and
edge detection is established. For the special case of FCM, we derive a
sufficient condition and clustering parameters, allowing identification of them
as (local) minima of the objective function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gheshlaghi_S/0/1/0/all/0/1&quot;&gt;Saba Heidari Gheshlaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Madani_A/0/1/0/all/0/1&quot;&gt;Abolfazl Madani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Suratgar_A/0/1/0/all/0/1&quot;&gt;AmirAbolfazl Suratgar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Faraji_F/0/1/0/all/0/1&quot;&gt;Fardin Faraji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03437">
<title>The AGINAO Self-Programming Engine. (arXiv:1804.03437v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.03437</link>
<description rdf:parseType="Literal">&lt;p&gt;The AGINAO is a project to create a human-level artificial general
intelligence system (HL AGI) embodied in the Aldebaran Robotics&apos; NAO humanoid
robot. The dynamical and open-ended cognitive engine of the robot is
represented by an embedded and multi-threaded control program, that is
self-crafted rather than hand-crafted, and is executed on a simulated Universal
Turing Machine (UTM). The actual structure of the cognitive engine emerges as a
result of placing the robot in a natural preschool-like environment and running
a core start-up system that executes self-programming of the cognitive layer on
top of the core layer. The data from the robot&apos;s sensory devices supplies the
training samples for the machine learning methods, while the commands sent to
actuators enable testing hypotheses and getting a feedback. The individual
self-created subroutines are supposed to reflect the patterns and concepts of
the real world, while the overall program structure reflects the spatial and
temporal hierarchy of the world dependencies. This paper focuses on the details
of the self-programming approach, limiting the discussion of the applied
cognitive architecture to a necessary minimum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skaba_W/0/1/0/all/0/1&quot;&gt;Wojciech Skaba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03439">
<title>Evaluating Actuators in a Purely Information-Theory Based Reward Model. (arXiv:1804.03439v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.03439</link>
<description rdf:parseType="Literal">&lt;p&gt;AGINAO builds its cognitive engine by applying self-programming techniques to
create a hierarchy of interconnected codelets - the tiny pieces of code
executed on a virtual machine. These basic processing units are evaluated for
their applicability and fitness with a notion of reward calculated from
self-information gain of binary partitioning of the codelet&apos;s input
state-space. This approach, however, is useless for the evaluation of
actuators. Instead, a model is proposed in which actuators are evaluated by
measuring the impact that an activation of an effector, and consequently the
feedback from the robot sensors, has on average reward received by the
processing units.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skaba_W/0/1/0/all/0/1&quot;&gt;Wojciech Skaba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03578">
<title>Towards Training Probabilistic Topic Models on Neuromorphic Multi-chip Systems. (arXiv:1804.03578v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03578</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic topic models are popular unsupervised learning methods,
including probabilistic latent semantic indexing (pLSI) and latent Dirichlet
allocation (LDA). By now, their training is implemented on general purpose
computers (GPCs), which are flexible in programming but energy-consuming.
Towards low-energy implementations, this paper investigates their training on
an emerging hardware technology called the neuromorphic multi-chip systems
(NMSs). NMSs are very effective for a family of algorithms called spiking
neural networks (SNNs). We present three SNNs to train topic models. The first
SNN is a batch algorithm combining the conventional collapsed Gibbs sampling
(CGS) algorithm and an inference SNN to train LDA. The other two SNNs are
online algorithms targeting at both energy- and storage-limited environments.
The two online algorithms are equivalent with training LDA by using
maximum-a-posterior estimation and maximizing the semi-collapsed likelihood,
respectively. They use novel, tailored ordinary differential equations for
stochastic optimization. We simulate the new algorithms and show that they are
comparable with the GPC algorithms, while being suitable for NMS
implementation. We also propose an extension to train pLSI and a method to
prune the network to obey the limited fan-in of some NMSs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zihao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianfei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03599">
<title>Understanding disentangling in $\beta$-VAE. (arXiv:1804.03599v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03599</link>
<description rdf:parseType="Literal">&lt;p&gt;We present new intuitions and theoretical assessments of the emergence of
disentangled representation in variational autoencoders. Taking a
rate-distortion theory perspective, we show the circumstances under which
representations aligned with the underlying generative factors of variation of
data emerge when optimising the modified ELBO bound in $\beta$-VAE, as training
progresses. From these insights, we propose a modification to the training
regime of $\beta$-VAE, that progressively increases the information capacity of
the latent code during training. This modification facilitates the robust
learning of disentangled representations in $\beta$-VAE, without the previous
trade-off in reconstruction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burgess_C/0/1/0/all/0/1&quot;&gt;Christopher P. Burgess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Higgins_I/0/1/0/all/0/1&quot;&gt;Irina Higgins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Arka Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matthey_L/0/1/0/all/0/1&quot;&gt;Loic Matthey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Watters_N/0/1/0/all/0/1&quot;&gt;Nick Watters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Desjardins_G/0/1/0/all/0/1&quot;&gt;Guillaume Desjardins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lerchner_A/0/1/0/all/0/1&quot;&gt;Alexander Lerchner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03611">
<title>Binary Space Partitioning as Intrinsic Reward. (arXiv:1804.03611v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.03611</link>
<description rdf:parseType="Literal">&lt;p&gt;An autonomous agent embodied in a humanoid robot, in order to learn from the
overwhelming flow of raw and noisy sensory, has to effectively reduce the high
spatial-temporal data dimensionality. In this paper we propose a novel method
of unsupervised feature extraction and selection with binary space
partitioning, followed by a computation of information gain that is interpreted
as intrinsic reward, then applied as immediate-reward signal for the
reinforcement-learning. The space partitioning is executed by tiny codelets
running on a simulated Turing Machine. The features are represented by concept
nodes arranged in a hierarchy, in which those of a lower level become the input
vectors of a higher level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skaba_W/0/1/0/all/0/1&quot;&gt;Wojciech Skaba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06202">
<title>A Robust Genetic Algorithm for Learning Temporal Specifications from Data. (arXiv:1711.06202v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06202</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of mining signal temporal logical requirements from a
dataset of regular (good) and anomalous (bad) trajectories of a dynamical
system. We assume the training set to be labeled by human experts and that we
have access only to a limited amount of data, typically noisy. We provide a
systematic approach to synthesize both the syntactical structure and the
parameters of the temporal logic formula using a two-steps procedure: first, we
leverage a novel evolutionary algorithm for learning the structure of the
formula; second, we perform the parameter synthesis operating on the
statistical emulation of the average robustness for a candidate formula w.r.t.
its parameters. We compare our results with our previous work [{BufoBSBLB14]
and with a recently proposed decision-tree [bombara_decision_2016] based
method. We present experimental results on two case studies: an anomalous
trajectory detection problem of a naval surveillance system and the
characterization of an Ineffective Respiratory effort, showing the usefulness
of our work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silvetti_S/0/1/0/all/0/1&quot;&gt;Simone Silvetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenzi_L/0/1/0/all/0/1&quot;&gt;Laura Nenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1&quot;&gt;Ezio Bartocci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1&quot;&gt;Luca Bortolussi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01508">
<title>The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. (arXiv:1804.01508v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01508</link>
<description rdf:parseType="Literal">&lt;p&gt;Although simple individually, artificial neurons provide state-of-the-art
performance when interconnected in deep networks. Unknown to many, there exists
an arguably even simpler and more versatile learning mechanism, namely, the
Tsetlin Automaton. Merely by means of a single integer as memory, it learns the
optimal action in stochastic environments. In this paper, we introduce the
Tsetlin Machine, which solves complex pattern recognition problems with
easy-to-interpret propositional formulas, composed by a collective of Tsetlin
Automata. To eliminate the longstanding problem of vanishing signal-to-noise
ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our
theoretical analysis establishes that the Nash equilibria of the game are
aligned with the propositional formulas that provide optimal pattern
recognition accuracy. This translates to learning without local optima, only
global ones. We argue that the Tsetlin Machine finds the propositional formula
that provides optimal accuracy, with probability arbitrarily close to unity. In
four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks,
SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It
further turns out that the accuracy advantage of the Tsetlin Machine increases
with lack of data. The Tsetlin Machine has a significant computational
performance advantage since both inputs, patterns, and outputs are expressed as
bits, while recognition of patterns relies on bit manipulation. The combination
of accuracy, interpretability, and computational simplicity makes the Tsetlin
Machine a promising tool for a wide range of domains, including safety-critical
medicine. Being the first of its kind, we believe the Tsetlin Machine will
kick-start completely new paths of research, with a potentially significant
impact on the AI field and the applications of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02969">
<title>A review of possible effects of cognitive biases on interpretation of rule-based machine learning models. (arXiv:1804.02969v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02969</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates to what extent do cognitive biases affect human
understanding of interpretable machine learning models, in particular of rules
discovered from data. Twenty cognitive biases (illusions, effects) are covered,
as are possibly effective debiasing techniques that can be adopted by designers
of machine learning algorithms and software. While there seems no universal
approach for eliminating all the identified cognitive biases, it follows from
our analysis that the effect of most biases can be ameliorated by making
rule-based models more concise. Due to lack of previous research, our review
transfers general results obtained in cognitive psychology to the domain of
machine learning. It needs to be succeeded by empirical studies specifically
aimed at the machine learning domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kliegr_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Kliegr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bahnik_S/0/1/0/all/0/1&quot;&gt;&amp;#x160;t&amp;#x11b;p&amp;#xe1;n Bahn&amp;#xed;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Furnkranz_J/0/1/0/all/0/1&quot;&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03154">
<title>Cauchy noise loss for stochastic optimization of random matrix models via free deterministic equivalents. (arXiv:1804.03154v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03154</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on free probability theory and stochastic optimization, we introduce a
new parameter estimation method of random matrix models. Our method is inspired
by free deterministic equivalents and iterative methods for computing Cauchy
transforms. Moreover, we study an asymptotic property of a generalization gap
and show numerical experiments of the optimization. We treat two random matrix
models; the compound Wishart model and the information-plus-noise model. In
addition, we propose a new rank recovery method for the information-plus-noise
model, and experimentally demonstrate that it recovers the true rank even if
the rank is not low.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hayase_T/0/1/0/all/0/1&quot;&gt;Tomohiro Hayase&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03184">
<title>Adversarial Time-to-Event Modeling. (arXiv:1804.03184v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03184</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern health data science applications leverage abundant molecular and
electronic health data, providing opportunities for machine learning to build
statistical models to support clinical practice. Time-to-event analysis, also
called survival analysis, stands as one of the most representative examples of
such statistical models. We present a novel deep-network-based approach that
leverages adversarial learning to address a key challenge in modern
time-to-event modeling: nonparametric estimation of event-time distributions.
We also introduce a principled cost function to exploit information from
censored events (events that occur subsequent to the observation window).
Unlike most time-to-event models, we focus on the estimation of time-to-event
distributions, rather than time ordering. We validate our model on both
benchmark and real datasets, demonstrating that the proposed formulation yields
significant performance gains relative to a parametric alternative, which we
also propose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chapfuwa_P/0/1/0/all/0/1&quot;&gt;Paidamoyo Chapfuwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1&quot;&gt;Chenyang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Page_C/0/1/0/all/0/1&quot;&gt;Courtney Page&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goldstein_B/0/1/0/all/0/1&quot;&gt;Benjamin Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Henao_R/0/1/0/all/0/1&quot;&gt;Ricardo Henao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03194">
<title>Human-Guided Data Exploration. (arXiv:1804.03194v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03194</link>
<description rdf:parseType="Literal">&lt;p&gt;The outcome of the explorative data analysis (EDA) phase is vital for
successful data analysis. EDA is more effective when the user interacts with
the system used to carry out the exploration. In the recently proposed paradigm
of iterative data mining the user controls the exploration by inputting
knowledge in the form of patterns observed during the process. The system then
shows the user views of the data that are maximally informative given the
user&apos;s current knowledge. Although this scheme is good at showing surprising
views of the data to the user, there is a clear shortcoming: the user cannot
steer the process. In many real cases we want to focus on investigating
specific questions concerning the data. This paper presents the Human Guided
Data Exploration framework, generalising previous research. This framework
allows the user to incorporate existing knowledge into the exploration process,
focus on exploring a subset of the data, and compare different complex
hypotheses concerning relations in the data. The framework utilises a
computationally efficient constrained randomisation scheme. To showcase the
framework, we developed a free open-source tool, using which the empirical
evaluation on real-world datasets was carried out. Our evaluation shows that
the ability to focus on particular subsets and being able to compare hypotheses
are important additions to the interactive iterative data mining process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Henelius_A/0/1/0/all/0/1&quot;&gt;Andreas Henelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oikarinen_E/0/1/0/all/0/1&quot;&gt;Emilia Oikarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Puolamaki_K/0/1/0/all/0/1&quot;&gt;Kai Puolam&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03201">
<title>Scalable Factorized Hierarchical Variational Autoencoder Training. (arXiv:1804.03201v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03201</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have achieved great success in unsupervised learning
with the ability to capture complex nonlinear relationships between latent
generating factors and observations. Among them, a factorized hierarchical
variational autoencoder (FHVAE) is a variational inference-based model that
formulates a hierarchical generative process for sequential data. Specifically,
an FHVAE model can learn disentangled and interpretable representations, which
have been proven useful for numerous speech applications, such as speaker
verification, robust speech recognition, and voice conversion. However, as we
will elaborate in this paper, the training algorithm proposed in the original
paper is not scalable to datasets of thousands of hours, which makes this model
less applicable on a larger scale. After identifying limitations in terms of
runtime, memory, and hyperparameter optimization, we propose a hierarchical
sampling training algorithm to address all three issues. Our proposed method is
evaluated comprehensively on a wide variety of datasets, ranging from 3 to
1,000 hours and involving different types of generating factors, such as
recording conditions and noise types. In addition, we also present a new
visualization method for qualitatively evaluating the performance with respect
to interpretability and disentanglement. Models trained with our proposed
algorithm demonstrate the desired characteristics on all the datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03236">
<title>Building Function Approximators on top of Haar Scattering Networks. (arXiv:1804.03236v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03236</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we propose building general-purpose function approximators on
top of Haar Scattering Networks. We advocate that this architecture enables a
better comprehension of feature extraction, in addition to its implementation
simplicity and low computational costs. We show its approximation and feature
extraction capabilities in a wide range of different problems, which can be
applied on several phenomena in signal processing, system identification,
econometrics and other potential fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neto_F/0/1/0/all/0/1&quot;&gt;Fernando Fernandes Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03273">
<title>On the Supermodularity of Active Graph-based Semi-supervised Learning with Stieltjes Matrix Regularization. (arXiv:1804.03273v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03273</link>
<description rdf:parseType="Literal">&lt;p&gt;Active graph-based semi-supervised learning (AG-SSL) aims to select a small
set of labeled examples and utilize their graph-based relation to other
unlabeled examples to aid in machine learning tasks. It is also closely related
to the sampling theory in graph signal processing. In this paper, we revisit
the original formulation of graph-based SSL and prove the supermodularity of an
AG-SSL objective function under a broad class of regularization functions
parameterized by Stieltjes matrices. Under this setting, supermodularity yields
a novel greedy label sampling algorithm with guaranteed performance relative to
the optimal sampling set. Compared to three state-of-the-art graph signal
sampling and recovery methods on two real-life community detection datasets,
the proposed AG-SSL method attains superior classification accuracy given
limited sample budgets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dennis Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03280">
<title>A Deep Active Survival Analysis Approach for Precision Treatment Recommendations: Application of Prostate Cancer. (arXiv:1804.03280v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03280</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis has been developed and applied in the number of areas
including manufacturing, finance, economics and healthcare. In healthcare
domain, usually clinical data are high-dimensional, sparse and complex and
sometimes there exists few amount of time-to-event (labeled) instances.
Therefore building an accurate survival model from electronic health records is
challenging. With this motivation, we address this issue and provide a new
survival analysis framework using deep learning and active learning with a
novel sampling strategy. First, our approach provides better representation
with lower dimensions from clinical features using labeled (time-to-event) and
unlabeled (censored) instances and then actively trains the survival model by
labeling the censored data using an oracle. As a clinical assistive tool, we
introduce a simple effective treatment recommendation approach based on our
survival model. In the experimental study, we apply our approach on
SEER-Medicare data related to prostate cancer among African-Americans and white
patients. The results indicate that our approach outperforms significantly than
baseline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nezhad_M/0/1/0/all/0/1&quot;&gt;Milad Zafar Nezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadati_N/0/1/0/all/0/1&quot;&gt;Najibesadat Sadati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03329">
<title>Representation Tradeoffs for Hyperbolic Embeddings. (arXiv:1804.03329v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperbolic embeddings offer excellent quality with few dimensions when
embedding hierarchical data structures like synonym or type hierarchies. Given
a tree, we give a combinatorial construction that embeds the tree in hyperbolic
space with arbitrarily low distortion without using optimization. On WordNet,
our combinatorial embedding obtains a mean-average-precision of 0.989 with only
two dimensions, while Nickel et al.&apos;s recent construction obtains 0.87 using
200 dimensions. We provide upper and lower bounds that allow us to characterize
the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To
embed general metric spaces, we propose a hyperbolic generalization of
multidimensional scaling (h-MDS). We show how to perform exact recovery of
hyperbolic points from distances, provide a perturbation analysis, and give a
recovery result that allows us to reduce dimensionality. The h-MDS approach
offers consistently low distortion even with few dimensions across several
datasets. Finally, we extract lessons from the algorithms and theory above to
design a PyTorch-based implementation that can handle incomplete information
and is scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1&quot;&gt;Albert Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03346">
<title>Learning Latent Events from Network Message Logs: A Decomposition Based Approach. (arXiv:1804.03346v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03346</link>
<description rdf:parseType="Literal">&lt;p&gt;In this communication, we describe a novel technique for event mining using a
decomposition based approach that combines non-parametric change-point
detection with LDA. We prove theoretical guarantees about sample-complexity and
consistency of the approach. In a companion paper, we will perform a thorough
evaluation of our approach with detailed experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1&quot;&gt;Siddhartha Satpathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_S/0/1/0/all/0/1&quot;&gt;Supratim Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1&quot;&gt;R Srikant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;He Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03523">
<title>Discontinuous Hamiltonian Monte Carlo for Probabilistic Programs. (arXiv:1804.03523v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1804.03523</link>
<description rdf:parseType="Literal">&lt;p&gt;Hamiltonian Monte Carlo (HMC) is the dominant statistical inference algorithm
used in most popular first-order differentiable probabilistic programming
languages. HMC requires that the joint density be differentiable with respect
to all latent variables. This complicates expressing some models in such
languages and prohibits others. A recently proposed new integrator for HMC
yielded a new Discontinuous HMC (DHMC) algorithm that can be used for inference
in models with joint densities that have discontinuities. In this paper we show
how to use DHMC for inference in probabilistic programs. To do this we
introduce a sufficient set of language restrictions, a corresponding
mathematical formalism that ensures that any joint density denoted in such a
language has a suitably low measure of discontinuous points, and a recipe for
how to apply DHMC in the more general probabilistic-programming context. Our
experimental findings demonstrate the correctness of this approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gram_Hansen_B/0/1/0/all/0/1&quot;&gt;Bradley Gram-Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohn_T/0/1/0/all/0/1&quot;&gt;Tobias Kohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongseok Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03565">
<title>Predicting Gross Movie Revenue. (arXiv:1804.03565v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1804.03565</link>
<description rdf:parseType="Literal">&lt;p&gt;&apos;There is no terror in the bang, only is the anticipation of it&apos; - Alfred
Hitchcock.
&lt;/p&gt;
&lt;p&gt;Yet there is everything in correctly anticipating the bang a movie would make
in the box-office. Movies make a high profile, billion dollar industry and
prediction of movie revenue can be very lucrative. Predicted revenues can be
used for planning both the production and distribution stages. For example,
projected gross revenue can be used to plan the remuneration of the actors and
crew members as well as other parts of the budget [1].
&lt;/p&gt;
&lt;p&gt;Success or failure of a movie can depend on many factors: star-power, release
date, budget, MPAA (Motion Picture Association of America) rating, plot and the
highly unpredictable human reactions. The enormity of the number of exogenous
variables makes manual revenue prediction process extremely difficult. However,
in the era of computer and data sciences, volumes of data can be efficiently
processed and modelled. Hence the tough job of predicting gross revenue of a
movie can be simplified with the help of modern computing power and the
historical data available as movie databases [2].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1&quot;&gt;Sharmistha Dey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03615">
<title>Subsampled Optimization: Statistical Guarantees, Mean Squared Error Approximation, and Sampling Method. (arXiv:1804.03615v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03615</link>
<description rdf:parseType="Literal">&lt;p&gt;For optimization on large-scale data, exactly calculating its solution may be
computationally difficulty because of the large size of the data. In this paper
we consider subsampled optimization for fast approximating the exact solution.
In this approach, one gets a surrogate dataset by sampling from the full data,
and then obtains an approximate solution by solving the subsampled optimization
based on the surrogate. One main theoretical contributions are to provide the
asymptotic properties of the approximate solution with respect to the exact
solution as statistical guarantees, and to rigorously derive an accurate
approximation of the mean squared error (MSE) and an approximately unbiased MSE
estimator. These results help us better diagnose the subsampled optimization in
the context that a confidence region on the exact solution is provided using
the approximate solution. The other consequence of our results is to propose an
optimal sampling method, Hessian-based sampling, whose probabilities are
proportional to the norms of Newton directions. Numerical experiments with
least-squares and logistic regression show promising performance, in line with
our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Rong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiming Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.08232">
<title>The block-Poisson estimator for optimally tuned exact subsampling MCMC. (arXiv:1603.08232v5 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1603.08232</link>
<description rdf:parseType="Literal">&lt;p&gt;Speeding up Markov Chain Monte Carlo (MCMC) for datasets with many
observations by data subsampling has recently received considerable attention
in the literature. The currently available methods are either approximate,
highly inefficient or limited to small dimensional models. We propose a
pseudo-marginal MCMC method that estimates the likelihood by data subsampling
using a block-Poisson estimator. The estimator is a product of Poisson
estimators, each based on an independent subset of the observations. The
construction allows us to update a subset of the blocks in each MCMC iteration,
thereby inducing a controllable correlation between the estimates at the
current and proposed draw in the Metropolis-Hastings ratio. This makes it
possible to use highly variable likelihood estimators without adversely
affecting the sampling efficiency. Poisson estimators are unbiased but not
necessarily positive. We therefore follow Lyne et al. (2015) and run the MCMC
on the absolute value of the estimator and use an importance sampling
correction for occasionally negative likelihood estimates to estimate
expectations of any function of the parameters. We provide analytically derived
guidelines to select the optimal tuning parameters for the algorithm by
minimizing the variance of the importance sampling corrected estimator per unit
of computing time. The guidelines are derived under idealized conditions, but
are demonstrated to be quite accurate in empirical experiments. The guidelines
apply to any pseudo-marginal algorithm if the likelihood is estimated by the
block-Poisson estimator, including the class of doubly intractable problems in
Lyne et al. (2015). We illustrate the method in a logistic regression example
and find dramatic improvements compared to regular MCMC without subsampling and
a popular exact subsampling approach recently proposed in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Quiroz_M/0/1/0/all/0/1&quot;&gt;Matias Quiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Ngoc Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villani_M/0/1/0/all/0/1&quot;&gt;Mattias Villani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohn_R/0/1/0/all/0/1&quot;&gt;Robert Kohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Khue-Dung Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.07256">
<title>Adaptive Design of Experiments for Conservative Estimation of Excursion Sets. (arXiv:1611.07256v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1611.07256</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a Gaussian process model trained on few evaluations of an
expensive-to-evaluate deterministic function and we study the problem of
estimating a fixed excursion set of this function. We focus on conservative
estimates as they allow control on false positives while minimizing false
negatives. We introduce adaptive strategies that sequentially selects new
evaluations of the function by reducing the uncertainty on conservative
estimates. Following the Stepwise Uncertainty Reduction approach we obtain new
evaluations by minimizing adapted criteria. We provide tractable formulae for
the conservative criteria and we benchmark the method on random functions
generated under the model assumptions in two and five dimensions. Finally the
method is applied to a reliability engineering test case. Overall, the proposed
strategy of minimizing false negatives in conservative estimation achieves
competitive performance both in terms of model based and a-posteriori
indicators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Azzimonti_D/0/1/0/all/0/1&quot;&gt;Dario Azzimonti&lt;/a&gt; (IDSIA), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ginsbourger_D/0/1/0/all/0/1&quot;&gt;David Ginsbourger&lt;/a&gt; (Idiap, IMSV), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chevalier_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Chevalier&lt;/a&gt; (UNINE), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bect_J/0/1/0/all/0/1&quot;&gt;Julien Bect&lt;/a&gt; (L2S, GdR MASCOT-NUM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Richet_Y/0/1/0/all/0/1&quot;&gt;Yann Richet&lt;/a&gt; (IRSN, GdR MASCOT-NUM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07079">
<title>Scalable Variational Inference for Dynamical Systems. (arXiv:1705.07079v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07079</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient matching is a promising tool for learning parameters and state
dynamics of ordinary differential equations. It is a grid free inference
approach, which, for fully observable systems is at times competitive with
numerical integration. However, for many real-world applications, only sparse
observations are available or even unobserved variables are included in the
model description. In these cases most gradient matching methods are difficult
to apply or simply do not provide satisfactory results. That is why, despite
the high computational cost, numerical integration is still the gold standard
in many applications. Using an existing gradient matching approach, we propose
a scalable variational inference framework which can infer states and
parameters simultaneously, offers computational speedups, improved accuracy and
works well even under model misspecifications in a partially observable system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorbach_N/0/1/0/all/0/1&quot;&gt;Nico S. Gorbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bauer_S/0/1/0/all/0/1&quot;&gt;Stefan Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buhmann_J/0/1/0/all/0/1&quot;&gt;Joachim M. Buhmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08621">
<title>Nonparametric Preference Completion. (arXiv:1705.08621v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08621</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of collaborative preference completion: given a pool of
items, a pool of users and a partially observed item-user rating matrix, the
goal is to recover the \emph{personalized ranking} of each user over all of the
items. Our approach is nonparametric: we assume that each item $i$ and each
user $u$ have unobserved features $x_i$ and $y_u$, and that the associated
rating is given by $g_u(f(x_i,y_u))$ where $f$ is Lipschitz and $g_u$ is a
monotonic transformation that depends on the user. We propose a $k$-nearest
neighbors-like algorithm and prove that it is consistent. To the best of our
knowledge, this is the first consistency result for the collaborative
preference completion problem in a nonparametric setting. Finally, we
demonstrate the performance of our algorithm with experiments on the Netflix
and Movielens datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Katz_Samuels_J/0/1/0/all/0/1&quot;&gt;Julian Katz-Samuels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scott_C/0/1/0/all/0/1&quot;&gt;Clayton Scott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00032">
<title>Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification. (arXiv:1712.00032v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00032</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new Urban Point Cloud Dataset for Automatic
Segmentation and Classification acquired by Mobile Laser Scanning (MLS). We
describe how the dataset is obtained from acquisition to post-processing and
labeling. This dataset can be used to learn classification algorithm, however,
given that a great attention has been paid to the split between the different
objects, this dataset can also be used to learn the segmentation. The dataset
consists of around 2km of MLS point cloud acquired in two cities. The number of
points and range of classes make us consider that it can be used to train
Deep-Learning methods. Besides we show some results of automatic segmentation
and classification. The dataset is available at:
&lt;a href=&quot;http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roynard_X/0/1/0/all/0/1&quot;&gt;Xavier Roynard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1&quot;&gt;Jean-Emmanuel Deschaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Goulette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01526">
<title>End-to-End DNN Training with Block Floating Point Arithmetic. (arXiv:1804.01526v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01526</link>
<description rdf:parseType="Literal">&lt;p&gt;DNNs are ubiquitous datacenter workloads, requiring orders of magnitude more
computing power from servers than traditional workloads. As such, datacenter
operators are forced to adopt domain-specific accelerators that employ
half-precision floating-point (FP) numeric representations to improve
arithmetic density. Unfortunately, even these representations are not dense
enough, and are, therefore, sub-optimal for DNNs. We propose a hybrid approach
that employs dense block floating-point (BFP) arithmetic on dot product
computations and FP arithmetic elsewhere. While using BFP improves the
performance of dot product operations, that compose most of DNN computations,
allowing values to freely float between dot product operations leads to a
better choice of tensor exponents when converting values to back BFP. We show
that models trained with hybrid BFP-FP arithmetic either match or outperform
their FP32 counterparts, leading to more compact models and denser arithmetic
in computing platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drumond_M/0/1/0/all/0/1&quot;&gt;Mario Drumond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falsafi_B/0/1/0/all/0/1&quot;&gt;Babak Falsafi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>