<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07647"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07739"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07220"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.05756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11139"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06656"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04680"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11027"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.04487">
<title>Better Runtime Guarantees Via Stochastic Domination. (arXiv:1801.04487v5 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04487</link>
<description rdf:parseType="Literal">&lt;p&gt;Apart from few exceptions, the mathematical runtime analysis of evolutionary
algorithms is mostly concerned with expected runtimes. In this work, we argue
that stochastic domination is a notion that should be used more frequently in
this area. Stochastic domination allows to formulate much more informative
performance guarantees, it allows to decouple the algorithm analysis into the
true algorithmic part of detecting a domination statement and the
probability-theoretical part of deriving the desired probabilistic guarantees
from this statement, and it helps finding simpler and more natural proofs.
&lt;/p&gt;
&lt;p&gt;As particular results, we prove a fitness level theorem which shows that the
runtime is dominated by a sum of independent geometric random variables, we
prove the first tail bounds for several classic runtime problems, and we give a
short and natural proof for Witt&apos;s result that the runtime of any $(\mu,p)$
mutation-based algorithm on any function with unique optimum is subdominated by
the runtime of a variant of the \oea on the \onemax function.
&lt;/p&gt;
&lt;p&gt;As side-products, we determine the fastest unbiased (1+1) algorithm for the
\leadingones benchmark problem, both in the general case and when restricted to
static mutation operators, and we prove a Chernoff-type tail bound for sums of
independent coupon collector distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07621">
<title>Latent Dirichlet Allocation for Internet Price War. (arXiv:1808.07621v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07621</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet market makers are always facing intense competitive environment,
where personalized price reductions or discounted coupons are provided for
attracting more customers. Participants in such a price war scenario have to
invest a lot to catch up with other competitors. However, such a huge cost of
money may not always lead to an improvement of market share. This is mainly due
to a lack of information about others&apos; strategies or customers&apos; willingness
when participants develop their strategies.
&lt;/p&gt;
&lt;p&gt;In order to obtain this hidden information through observable data, we study
the relationship between companies and customers in the Internet price war.
Theoretically, we provide a formalization of the problem as a stochastic game
with imperfect and incomplete information. Then we develop a variant of Latent
Dirichlet Allocation (LDA) to infer latent variables under the current market
environment, which represents the preferences of customers and strategies of
competitors. To our best knowledge, it is the first time that LDA is applied to
game scenario.
&lt;/p&gt;
&lt;p&gt;We conduct simulated experiments where our LDA model exhibits a significant
improvement on finding strategies in the Internet price war by including all
available market information of the market maker&apos;s competitors. And the model
is applied to an open dataset for real business. Through comparisons on the
likelihood of prediction for users&apos; behavior and distribution distance between
inferred opponent&apos;s strategy and the real one, our model is shown to be able to
provide a better understanding for the market environment.
&lt;/p&gt;
&lt;p&gt;Our work marks a successful learning method to infer latent information in
the environment of price war by the LDA modeling, and sets an example for
related competitive applications to follow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiaotie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wei Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1&quot;&gt;Junlong Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jianshan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Junwu Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07647">
<title>Machine Learning at the Edge: A Data-Driven Architecture with Applications to 5G Cellular Networks. (arXiv:1808.07647v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1808.07647</link>
<description rdf:parseType="Literal">&lt;p&gt;The fifth generation of cellular networks (5G) will rely on edge cloud
deployments to satisfy the ultra-low latency demand of future applications. In
this paper, we argue that an edge-based deployment can also be used as an
enabler of advanced Machine Learning (ML) applications in cellular networks,
thanks to the balance it strikes between a completely distributed and a
centralized approach. First, we will present an edge-controller-based
architecture for cellular networks. Second, by using real data from hundreds of
base stations of a major U.S. national operator, we will provide insights on
how to dynamically cluster the base stations under the domain of each
controller. Third, we will describe how these controllers can be used to run ML
algorithms to predict the number of users, and a use case in which these
predictions are used by a higher-layer application to route vehicular traffic
according to network Key Performance Indicators (KPIs). We show that prediction
accuracy improves when based on machine learning algorithms that exploit the
controllers&apos; view with respect to when it is based only on the local data of
each single base station.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polese_M/0/1/0/all/0/1&quot;&gt;Michele Polese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jana_R/0/1/0/all/0/1&quot;&gt;Rittwik Jana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kounev_V/0/1/0/all/0/1&quot;&gt;Velin Kounev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Ke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_S/0/1/0/all/0/1&quot;&gt;Supratim Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1&quot;&gt;Michele Zorzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07712">
<title>Predicting Action Tubes. (arXiv:1808.07712v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.07712</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a method to predict an entire `action tube&apos; (a set
of temporally linked bounding boxes) in a trimmed video just by observing a
smaller subset of it. Predicting where an action is going to take place in the
near future is essential to many computer vision based applications such as
autonomous driving or surgical robotics. Importantly, it has to be done in
real-time and in an online fashion. We propose a Tube Prediction network
(TPnet) which jointly predicts the past, present and future bounding boxes
along with their action classification scores. At test time TPnet is used in a
(temporal) sliding window setting, and its predictions are put into a tube
estimation framework to construct/predict the video long action tubes not only
for the observed part of the video but also for the unobserved part.
Additionally, the proposed action tube predictor helps in completing action
tubes for unobserved segments of the video. We quantitatively demonstrate the
latter ability, and the fact that TPnet improves state-of-the-art detection
performance, on one of the standard action detection benchmarks - J-HMDB-21
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gurkirt Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Suman Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07739">
<title>Diversity-Driven Selection of Exploration Strategies in Multi-Armed Bandits. (arXiv:1808.07739v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07739</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a scenario where an agent has multiple available strategies to
explore an unknown environment. For each new interaction with the environment,
the agent must select which exploration strategy to use. We provide a new
strategy-agnostic method that treat the situation as a Multi-Armed Bandits
problem where the reward signal is the diversity of effects that each strategy
produces. We test the method empirically on a simulated planar robotic arm, and
establish that the method is both able discriminate between strategies of
dissimilar quality, even when the differences are tenuous, and that the
resulting performance is competitive with the best fixed mixture of strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benureau_F/0/1/0/all/0/1&quot;&gt;Fabien C. Y. Benureau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08924">
<title>Learning-Based Mean-Payoff Optimization in an Unknown MDP under Omega-Regular Constraints. (arXiv:1804.08924v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08924</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the problem of maximizing the mean-payoff value with high
probability while satisfying a parity objective in a Markov decision process
(MDP) with unknown probabilistic transition function and unknown reward
function. Assuming the support of the unknown transition function and a lower
bound on the minimal transition probability are known in advance, we show that
in MDPs consisting of a single end component, two combinations of guarantees on
the parity and mean-payoff objectives can be achieved depending on how much
memory one is willing to use. (i) For all $\epsilon$ and $\gamma$ we can
construct an online-learning finite-memory strategy that almost-surely
satisfies the parity objective and which achieves an $\epsilon$-optimal mean
payoff with probability at least $1 - \gamma$. (ii) Alternatively, for all
$\epsilon$ and $\gamma$ there exists an online-learning infinite-memory
strategy that satisfies the parity objective surely and which achieves an
$\epsilon$-optimal mean payoff with probability at least $1 - \gamma$. We
extend the above results to MDPs consisting of more than one end component in a
natural way. Finally, we show that the aforementioned guarantees are tight,
i.e. there are MDPs for which stronger combinations of the guarantees cannot be
ensured.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretinsky_J/0/1/0/all/0/1&quot;&gt;Jan K&amp;#x159;et&amp;#xed;nsk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Raskin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07560">
<title>Compositional GAN: Learning Conditional Image Composition. (arXiv:1807.07560v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07560</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) can produce images of surprising
complexity and realism, but are generally modeled to sample from a single
latent source ignoring the explicit spatial interaction between multiple
entities that could be present in a scene. Capturing such complex interactions
between different objects in the world, including their relative scaling,
spatial layout, occlusion, or viewpoint transformation is a challenging
problem. In this work, we propose to model object composition in a GAN
framework as a self-consistent composition-decomposition network. Our model is
conditioned on the object images from their marginal distributions to generate
a realistic image from their joint distribution by explicitly learning the
possible interactions. We evaluate our model through qualitative experiments
and user evaluations in both the scenarios when either paired or unpaired
examples for the individual object images and the joint scenes are given during
training. Our results reveal that the learned model captures potential
interactions between the two object domains given as input to output new
instances of composed scene at test time in a reasonable fashion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azadi_S/0/1/0/all/0/1&quot;&gt;Samaneh Azadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1&quot;&gt;Sayna Ebrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07220">
<title>Approximating Poker Probabilities with Deep Learning. (arXiv:1808.07220v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07220</link>
<description rdf:parseType="Literal">&lt;p&gt;Many poker systems, whether created with heuristics or machine learning, rely
on the probability of winning as a key input. However calculating the precise
probability using combinatorics is an intractable problem, so instead we
approximate it. Monte Carlo simulation is an effective technique that can be
used to approximate the probability that a player will win and/or tie a hand.
However, without the use of a memory-intensive lookup table or a supercomputer,
it becomes infeasible to run millions of times when training an agent with
self-play. To combat the space-time tradeoff, we use deep learning to
approximate the probabilities obtained from the Monte Carlo simulation with
high accuracy. The learned model proves to be a lightweight alternative to
Monte Carlo simulation, which ultimately allows us to use the probabilities as
inputs during self-play efficiently. The source code and optimized neural
network can be found at
https://github.com/brandinho/Poker-Probability-Approximation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Brandon Da Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07510">
<title>XPCA: Extending PCA for a Combination of Discrete and Continuous Variables. (arXiv:1808.07510v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;Principal component analysis (PCA) is arguably the most popular tool in
multivariate exploratory data analysis. In this paper, we consider the question
of how to handle heterogeneous variables that include continuous, binary, and
ordinal. In the probabilistic interpretation of low-rank PCA, the data has a
normal multivariate distribution and, therefore, normal marginal distributions
for each column. If some marginals are continuous but not normal, the
semiparametric copula-based principal component analysis (COCA) method is an
alternative to PCA that combines a Gaussian copula with nonparametric
marginals. If some marginals are discrete or semi-continuous, we propose a new
extended PCA (XPCA) method that also uses a Gaussian copula and nonparametric
marginals and accounts for discrete variables in the likelihood calculation by
integrating over appropriate intervals. Like PCA, the factors produced by XPCA
can be used to find latent structure in data, build predictive models, and
perform dimensionality reduction. We present the new model, its induced
likelihood function, and a fitting algorithm which can be applied in the
presence of missing data. We demonstrate how to use XPCA to produce an
estimated full conditional distribution for each data point, and use this to
produce to provide estimates for missing data that are automatically range
respecting. We compare the methods as applied to simulated and real-world data
sets that have a mixture of discrete and continuous variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anderson_Bergman_C/0/1/0/all/0/1&quot;&gt;Clifford Anderson-Bergman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kolda_T/0/1/0/all/0/1&quot;&gt;Tamara G. Kolda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kincher_Winoto_K/0/1/0/all/0/1&quot;&gt;Kina Kincher-Winoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07573">
<title>Approximation Trees: Statistical Stability in Model Distillation. (arXiv:1808.07573v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07573</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the stability of learned explanations for black-box
predictions via model distillation with decision trees. One approach to
intelligibility in machine learning is to use an understandable `student&apos; model
to mimic the output of an accurate `teacher&apos;. Here, we consider the use of
regression trees as a student model, in which nodes of the tree can be used as
`explanations&apos; for particular predictions, and the whole structure of the tree
can be used as a global representation of the resulting function. However,
individual trees are sensitive to the particular data sets used to train them,
and an interpretation of a student model may be suspect if small changes in the
training data have a large effect on it. In this context, access to outcomes
from a teacher helps to stabilize the greedy splitting strategy by generating a
much larger corpus of training examples than was originally available. We
develop tests to ensure that enough examples are generated at each split so
that the same splitting rule would be chosen with high probability were the
tree to be re trained. Further, we develop a stopping rule to indicate how deep
the tree should be built based on recent results on the variability of Random
Forests when these are used as the teacher. We provide concrete examples of
these procedures on the CAD-MDD and COMPAS data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhengze Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1&quot;&gt;Giles Hooker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07576">
<title>Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms. (arXiv:1808.07576v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07576</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art distributed machine learning suffers from significant delays
due to frequent communication and synchronizing between worker nodes. Emerging
communication-efficient SGD algorithms that limit synchronization between
locally trained models have been shown to be effective in speeding-up
distributed SGD. However, a rigorous convergence analysis and comparative study
of different communication-reduction strategies remains a largely open problem.
This paper presents a new framework called Coooperative SGD that subsumes
existing communication-efficient SGD algorithms such as federated-averaging,
elastic-averaging and decentralized SGD. By analyzing Cooperative SGD, we
provide novel convergence guarantees for existing algorithms. Moreover this
framework enables us to design new communication-efficient SGD algorithms that
strike the best balance between reducing communication overhead and achieving
fast error convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07632">
<title>DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN. (arXiv:1808.07632v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07632</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the introduction of the generative adversarial network (GAN) and
its variants has enabled the generation of realistic synthetic samples, which
has been used for enlarging training sets. Previous work primarily focused on
data augmentation for semi-supervised and supervised tasks. In this paper, we
instead focus on unsupervised anomaly detection and propose a novel generative
data augmentation framework optimized for this task. In particular, we propose
to oversample infrequent normal samples - normal samples that occur with small
probability, e.g., rare normal events. We show that these samples are
responsible for false positives in anomaly detection. However, oversampling of
infrequent normal samples is challenging for real-world high-dimensional data
with multimodal distributions. To address this challenge, we propose to use a
GAN variant known as the adversarial autoencoder (AAE) to transform the
high-dimensional multimodal data distributions into low-dimensional unimodal
latent distributions with well-defined tail probability. Then, we
systematically oversample at the `edge&apos; of the latent distributions to increase
the density of infrequent normal samples. We show that our oversampling
pipeline is a unified one: it is generally applicable to datasets with
different complex data distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Swee Kiat Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loo_Y/0/1/0/all/0/1&quot;&gt;Yi Loo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Ngoc-Trung Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1&quot;&gt;Gemma Roig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07784">
<title>Time-Agnostic Prediction: Predicting Predictable Video Frames. (arXiv:1808.07784v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.07784</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction is arguably one of the most basic functions of an intelligent
system. In general, the problem of predicting events in the future or between
two waypoints is exceedingly difficult. However, most phenomena naturally pass
through relatively predictable bottlenecks---while we cannot predict the
precise trajectory of a robot arm between being at rest and holding an object
up, we can be certain that it must have picked the object up. To exploit this,
we decouple visual prediction from a rigid notion of time. While conventional
approaches predict frames at regularly spaced temporal intervals, our
time-agnostic predictors (TAP) are not tied to specific times so that they may
instead discover predictable &quot;bottleneck&quot; frames no matter when they occur. We
evaluate our approach for future and intermediate frame prediction across three
robotic manipulation tasks. Our predictions are not only of higher visual
quality, but also correspond to coherent semantic subgoals in temporally
extended tasks. Project website: goo.gl/tL6Jgr.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1&quot;&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebert_F/0/1/0/all/0/1&quot;&gt;Frederik Ebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07801">
<title>On a &apos;Two Truths&apos; Phenomenon in Spectral Graph Clustering. (arXiv:1808.07801v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07801</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is concerned with coherently grouping observations without any
explicit concept of true groupings. Spectral graph clustering - clustering the
vertices of a graph based on their spectral embedding - is commonly approached
via K-means (or, more generally, Gaussian mixture model) clustering composed
with either Laplacian or Adjacency spectral embedding (LSE or ASE). Recent
theoretical results provide new understanding of the problem and solutions, and
lead us to a &apos;Two Truths&apos; LSE vs. ASE spectral graph clustering phenomenon
convincingly illustrated here via a diffusion MRI connectome data set: the
different embedding methods yield different clustering results, with LSE
capturing left hemisphere/right hemisphere affinity structure and ASE capturing
gray matter/white matter core-periphery structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Youngser Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Conroy_J/0/1/0/all/0/1&quot;&gt;John M. Conroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyzinskic_V/0/1/0/all/0/1&quot;&gt;Vince Lyzinskic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Minh Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athreya_A/0/1/0/all/0/1&quot;&gt;Avanti Athreya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cape_J/0/1/0/all/0/1&quot;&gt;Joshua Cape&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bridgeford_E/0/1/0/all/0/1&quot;&gt;Eric Bridgeford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07840">
<title>Learning to Importance Sample in Primary Sample Space. (arXiv:1808.07840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07840</link>
<description rdf:parseType="Literal">&lt;p&gt;Importance sampling is one of the most widely used variance reduction
strategies in Monte Carlo rendering. In this paper, we propose a novel
importance sampling technique that uses a neural network to learn how to sample
from a desired density represented by a set of samples. Our approach considers
an existing Monte Carlo rendering algorithm as a black box. During a
scene-dependent training phase, we learn to generate samples with a desired
density in the primary sample space of the rendering algorithm using maximum
likelihood estimation. We leverage a recent neural network architecture that
was designed to represent real-valued non-volume preserving (&apos;Real NVP&apos;)
transformations in high dimensional spaces. We use Real NVP to non-linearly
warp primary sample space and obtain desired densities. In addition, Real NVP
efficiently computes the determinant of the Jacobian of the warp, which is
required to implement the change of integration variables implied by the warp.
A main advantage of our approach is that it is agnostic of underlying light
transport effects, and can be combined with many existing rendering techniques
by treating them as a black box. We show that our approach leads to effective
variance reduction in several practical scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Quan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1&quot;&gt;Matthias Zwicker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.05756">
<title>Modeling community structure and topics in dynamic text networks. (arXiv:1610.05756v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1610.05756</link>
<description rdf:parseType="Literal">&lt;p&gt;The last decade has seen great progress in both dynamic network modeling and
topic modeling. This paper draws upon both areas to create a Bayesian method
that allows topic discovery to inform the latent network model and the network
structure to facilitate topic identification. We apply this method to the 467
top political blogs of 2012. Our results find complex community structure
within this set of blogs, where community membership depends strongly upon the
set of topics in which the blogger is interested.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1&quot;&gt;Teague Henry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banks_D/0/1/0/all/0/1&quot;&gt;David Banks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_C/0/1/0/all/0/1&quot;&gt;Christine Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owens_Oas_D/0/1/0/all/0/1&quot;&gt;Derek Owens-Oas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11139">
<title>Easy High-Dimensional Likelihood-Free Inference. (arXiv:1711.11139v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11139</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a framework using Generative Adversarial Networks (GANs) for
likelihood--free inference (LFI) and Approximate Bayesian Computation (ABC)
where we replace the black-box simulator model with an approximator network and
generate a rich set of summary features in a data driven fashion. On benchmark
data sets, our approach improves on others with respect to scalability, ability
to handle high dimensional data and complex probability distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jethava_V/0/1/0/all/0/1&quot;&gt;Vinay Jethava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubhashi_D/0/1/0/all/0/1&quot;&gt;Devdatt Dubhashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06234">
<title>Clustering Analysis on Locally Asymptotically Self-similar Processes with Known Number of Clusters. (arXiv:1804.06234v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06234</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problems of clustering locally asymptotically self-similar
stochastic processes, when the true number of clusters is priorly known. A new
covariance-based dissimilarity measure is introduced, from which the so-called
approximately asymptotically consistent clustering algorithms are obtained. In
a simulation study, clustering data sampled from multifractional Brownian
motions is performed to illustrate the approximated asymptotic consistency of
the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peng_Q/0/1/0/all/0/1&quot;&gt;Qidi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_N/0/1/0/all/0/1&quot;&gt;Nan Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ran Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00373">
<title>New Heuristics for Parallel and Scalable Bayesian Optimization. (arXiv:1807.00373v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00373</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization has emerged as a strong candidate tool for global
optimization of functions with expensive evaluation costs. However, due to the
dynamic nature of research in Bayesian approaches, and the evolution of
computing technology, using Bayesian optimization in a parallel computing
environment remains a challenge for the non-expert. In this report, I review
the state-of-the-art in parallel and scalable Bayesian optimization methods. In
addition, I propose practical ways to avoid a few of the pitfalls of Bayesian
optimization, such as oversampling of edge parameters and over-exploitation of
high performance parameters. Finally, I provide relatively simple, heuristic
algorithms, along with their open source software implementations, that can be
immediately and easily deployed in any computing environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rubin_R/0/1/0/all/0/1&quot;&gt;Ran Rubin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06638">
<title>Supervised Kernel PCA For Longitudinal Data. (arXiv:1808.06638v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06638</link>
<description rdf:parseType="Literal">&lt;p&gt;In statistical learning, high covariate dimensionality poses challenges for
robust prediction and inference. To address this challenge, supervised
dimension reduction is often performed, where dependence on the outcome is
maximized for a selected covariate subspace with smaller dimensionality.
Prevalent dimension reduction techniques assume data are $i.i.d.$, which is not
appropriate for longitudinal data comprising multiple subjects with repeated
measurements over time. In this paper, we derive a decomposition of the
Hilbert-Schmidt Independence Criterion as a supervised loss function for
longitudinal data, enabling dimension reduction between and within clusters
separately, and propose a dimensionality-reduction technique, $sklPCA$, that
performs this decomposed dimension reduction. We also show that this technique
yields superior model accuracy compared to the model it extends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Staples_P/0/1/0/all/0/1&quot;&gt;Patrick Staples&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ouyang_M/0/1/0/all/0/1&quot;&gt;Min Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dougherty_R/0/1/0/all/0/1&quot;&gt;Robert F. Dougherty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ryslik_G/0/1/0/all/0/1&quot;&gt;Gregory A. Ryslik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dagum_P/0/1/0/all/0/1&quot;&gt;Paul Dagum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06656">
<title>Causally Regularized Learning with Agnostic Data Selection Bias. (arXiv:1708.06656v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1708.06656</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of previous machine learning algorithms are proposed based on the i.i.d.
hypothesis. However, this ideal assumption is often violated in real
applications, where selection bias may arise between training and testing
process. Moreover, in many scenarios, the testing data is not even available
during the training process, which makes the traditional methods like transfer
learning infeasible due to their need on prior of test distribution. Therefore,
how to address the agnostic selection bias for robust model learning is of
paramount importance for both academic research and real applications. In this
paper, under the assumption that causal relationships among variables are
robust across domains, we incorporate causal technique into predictive modeling
and propose a novel Causally Regularized Logistic Regression (CRLR) algorithm
by jointly optimize global confounder balancing and weighted logistic
regression. Global confounder balancing helps to identify causal features,
whose causal effect on outcome are stable across domains, then performing
logistic regression on those causal features constructs a robust predictive
model against the agnostic bias. To validate the effectiveness of our CRLR
algorithm, we conduct comprehensive experiments on both synthetic and real
world datasets. Experimental results clearly demonstrate that our CRLR
algorithm outperforms the state-of-the-art methods, and the interpretability of
our method can be fully depicted by the feature visualization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zheyan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peixuan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04680">
<title>Unseeded low-rank graph matching by transform-based unsupervised point registration. (arXiv:1807.04680v1 [stat.ME] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.04680</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of learning a correspondence relationship between nodes of two
networks has drawn much attention of the computer science community and
recently that of statisticians. The unseeded version of this problem, in which
we do not know any part of the true correspondence, is a long-standing
challenge. For low-rank networks, the problem can be translated into an
unsupervised point registration problem, in which two point sets generated from
the same distribution are matchable by an unknown orthonormal transformation.
Conventional methods generally lack consistency guarantee and are usually
computationally costly.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel approach to this problem. Instead of
simultaneously estimating the unknown correspondence and orthonormal
transformation to match up the two point sets, we match their distributions via
minimizing our designed loss function capturing the discrepancy between their
Laplace transforms, thus avoiding the optimization over all possible
correspondences. This dramatically reduces the dimension of the optimization
problem from $\Omega(n^2)$ parameters to $O(d^2)$ parameters, where $d$ is the
fixed rank, and enables convenient theoretical analysis. In this paper, we
provide arguably the first consistency guarantee and explicit error rate for
general low-rank models. Our method provides control over the computational
complexity ranging from $\omega(n)$ (any growth rate faster than $n$) to
$O(n^2)$ while pertaining consistency. We demonstrate the effectiveness of our
method through several numerical examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11027">
<title>Consistent polynomial-time unseeded graph matching for Lipschitz graphons. (arXiv:1807.11027v1 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.11027</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a consistent polynomial-time method for the unseeded node matching
problem for networks with smooth underlying structures. Despite widely
conjectured by the research community that the structured graph matching
problem to be significantly easier than its worst case counterpart, well-known
to be NP-hard, the statistical version of the problem has stood a challenge
that resisted any solution both provable and polynomial-time. The closest
existing work requires quasi-polynomial time. Our method is based on the latest
advances in graphon estimation techniques and analysis on the concentration of
empirical Wasserstein distances. Its core is a simple yet unconventional
sampling-and-matching scheme that reduces the problem from unseeded to seeded.
Our method allows flexible efficiencies, is convenient to analyze and
potentially can be extended to more general settings. Our work enables a rich
variety of subsequent estimations and inferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;</dc:creator>
</item></rdf:RDF>