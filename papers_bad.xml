<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03826"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03906"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03782"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03973"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03980"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03728"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03761"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03797"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03836"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04118"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.06386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10353"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11279"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02101"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.03826">
<title>AFA-PredNet: The action modulation within predictive coding. (arXiv:1804.03826v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.03826</link>
<description rdf:parseType="Literal">&lt;p&gt;The predictive processing (PP) hypothesizes that the predictive inference of
our sensorimotor system is encoded implicitly in the regularities between
perception and action. We propose a neural architecture in which such
regularities of active inference are encoded hierarchically. We further suggest
that this encoding emerges during the embodied learning process when the
appropriate action is selected to minimize the prediction error in perception.
Therefore, this predictive stream in the sensorimotor loop is generated in a
top-down manner. Specifically, it is constantly modulated by the motor actions
and is updated by the bottom-up prediction error signals. In this way, the
top-down prediction originally comes from the prior experience from both
perception and action representing the higher levels of this hierarchical
cognition. In our proposed embodied model, we extend the PredNet Network, a
hierarchical predictive coding network, with the motor action units implemented
by a multi-layer perceptron network (MLP) to modulate the network top-down
prediction. Two experiments, a minimalistic world experiment, and a mobile
robot experiment are conducted to evaluate the proposed model in a qualitative
way. In the neural representation, it can be observed that the causal inference
of predictive percept from motor actions can be also observed while the agent
is interacting with the environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Junpei Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1&quot;&gt;Angelo Cangelosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1&quot;&gt;Tetsuya Ogata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03906">
<title>Discovering the Elite Hypervolume by Leveraging Interspecies Correlation. (arXiv:1804.03906v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.03906</link>
<description rdf:parseType="Literal">&lt;p&gt;Evolution has produced an astonishing diversity of species, each filling a
different niche. Algorithms like MAP-Elites mimic this divergent evolutionary
process to find a set of behaviorally diverse but high-performing solutions,
called the elites. Our key insight is that species in nature often share a
surprisingly large part of their genome, in spite of occupying very different
niches; similarly, the elites are likely to be concentrated in a specific
&quot;elite hypervolume&quot; whose shape is defined by their common features. In this
paper, we first introduce the elite hypervolume concept and propose two metrics
to characterize it: the genotypic spread and the genotypic similarity. We then
introduce a new variation operator, called &quot;directional variation&quot;, that
exploits interspecies (or inter-elites) correlations to accelerate the
MAP-Elites algorithm. We demonstrate the effectiveness of this operator in
three problems (a toy function, a redundant robotic arm, and a hexapod robot).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassiliades_V/0/1/0/all/0/1&quot;&gt;Vassilis Vassiliades&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00970">
<title>A Classification-Based Perspective on GAN Distributions. (arXiv:1711.00970v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00970</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental, and still largely unanswered, question in the context of
Generative Adversarial Networks (GANs) is whether GANs are actually able to
capture the key characteristics of the datasets they are trained on. The
current approaches to examining this issue require significant human
supervision, such as visual inspection of sampled images, and often offer only
fairly limited scalability. In this paper, we propose new techniques that
employ a classification-based perspective to evaluate synthetic GAN
distributions and their capability to accurately reflect the essential
properties of the training data. These techniques require only minimal human
supervision and can easily be scaled and adapted to evaluate a variety of
state-of-the-art GANs on large, popular datasets. Our analysis indicates that
GANs have significant problems in reproducing the more distributional
properties of the training dataset. In particular, when seen through the lens
of classification, the diversity of GAN data is orders of magnitude less than
that of the original data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1&quot;&gt;Shibani Santurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander M&amp;#x105;dry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03782">
<title>CoT: Cooperative Training for Generative Modeling. (arXiv:1804.03782v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03782</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Cooperative Training (CoT) for training generative models that
measure a tractable density function for target data. CoT coordinately trains a
generator $G$ and an auxiliary predictive mediator $M$. The training target of
$M$ is to estimate a mixture density of the learned distribution $G$ and the
target distribution $P$, and that of $G$ is to minimize the Jensen-Shannon
divergence estimated through $M$. CoT achieves independent success without the
necessity of pre-training via Maximum Likelihood Estimation or involving
high-variance algorithms like REINFORCE. This low-variance algorithm is
theoretically proved to be unbiased for both generative and predictive tasks.
We also theoretically and empirically show the superiority of CoT over most
previous algorithms, in terms of generative quality and diversity, predictive
generalization ability and computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Sidi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lantao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03824">
<title>Reference-less Measure of Faithfulness for Grammatical Error Correction. (arXiv:1804.03824v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.03824</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose {\sc USim}, a semantic measure for Grammatical Error Correction
(GEC) that measures the semantic faithfulness of the output to the source,
thereby complementing existing reference-less measures (RLMs) for measuring the
output&apos;s grammaticality. {\sc USim} operates by comparing the semantic symbolic
structure of the source and the correction, without relying on manually-curated
references. Our experiments establish the validity of {\sc USim}, by showing
that (1) semantic annotation can be consistently applied to ungrammatical text;
(2) valid corrections obtain a high {\sc USim} similarity score to the source;
and (3) invalid corrections obtain a lower score.\footnote{Our code is
available in \url{https://github.com/borgr/USim}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1&quot;&gt;Leshem Choshen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1&quot;&gt;Omri Abend&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03954">
<title>A Variable Neighborhood Search for Flying Sidekick Traveling Salesman Problem. (arXiv:1804.03954v1 [math.OC])</title>
<link>http://arxiv.org/abs/1804.03954</link>
<description rdf:parseType="Literal">&lt;p&gt;An innovative model of parcel distribution is emerging from the accelerated
evolution of drones and the effort of logistic companies to proceed faster
deliveries at a reduced cost. This new modality originated the Flying Sidekick
Traveling Salesman Problem (FSTSP) in which customers are served either by a
truck or a drone. Additionally, this variant of the Traveling Salesman Problem
(TSP) presents several new restrictions concerning the drone such as endurance
and payload capacity. This work proposes a hybrid heuristic that the initial
solution is created from the optimal TSP solution reached by a Mixed-Integer
Programming (MIP) solver. Next, an implementation of the General Variable
Neighborhood Search is used to obtain the delivery routes of truck and drone.
Computational experiments show the potential of the algorithm to improve the
total delivery time up to 67.79%. New best-known solutions (BKS) are
established for all FSTSP instances that results are reported in the
literature. Furthermore, a new set of instances based on well-known TSPLIB
instances is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Freitas_J/0/1/0/all/0/1&quot;&gt;Julia C. Freitas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Penna_P/0/1/0/all/0/1&quot;&gt;Puca Huachi V. Penna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03967">
<title>Incremental Predictive Process Monitoring: How to Deal with the Variability of Real Environments. (arXiv:1804.03967v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.03967</link>
<description rdf:parseType="Literal">&lt;p&gt;A characteristic of existing predictive process monitoring techniques is to
first construct a predictive model based on past process executions, and then
use it to predict the future of new ongoing cases, without the possibility of
updating it with new cases when they complete their execution. This can make
predictive process monitoring too rigid to deal with the variability of
processes working in real environments that continuously evolve and/or exhibit
new variant behaviors over time. As a solution to this problem, we propose the
use of algorithms that allow the incremental construction of the predictive
model. These incremental learning algorithms update the model whenever new
cases become available so that the predictive model evolves over time to fit
the current circumstances. The algorithms have been implemented using different
case encoding strategies and evaluated on a number of real and synthetic
datasets. The results provide a first evidence of the potential of incremental
learning strategies for predicting process monitoring in real environments, and
of the impact of different case encoding strategies in this setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francescomarino_C/0/1/0/all/0/1&quot;&gt;Chiara Di Francescomarino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1&quot;&gt;Chiara Ghidini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizzi_W/0/1/0/all/0/1&quot;&gt;Williams Rizzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Persia_C/0/1/0/all/0/1&quot;&gt;Cosimo Damiano Persia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03973">
<title>Reasoning about Safety of Learning-Enabled Components in Autonomous Cyber-physical Systems. (arXiv:1804.03973v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1804.03973</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simulation-based approach for generating barrier certificate
functions for safety verification of cyber-physical systems (CPS) that contain
neural network-based controllers. A linear programming solver is utilized to
find a candidate generator function from a set of simulation traces obtained by
randomly selecting initial states for the CPS model. A level set of the
generator function is then selected to act as a barrier certificate for the
system, meaning it demonstrates that no unsafe system states are reachable from
a given set of initial states. The barrier certificate properties are verified
with an SMT solver. This approach is demonstrated on a case study in which a
Dubins car model of an autonomous vehicle is controlled by a neural network to
follow a given path.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuncali_C/0/1/0/all/0/1&quot;&gt;Cumhur Erkan Tuncali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapinski_J/0/1/0/all/0/1&quot;&gt;James Kapinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1&quot;&gt;Hisahiro Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1&quot;&gt;Jyotirmoy V. Deshmukh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03980">
<title>Emergent Communication through Negotiation. (arXiv:1804.03980v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.03980</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent reinforcement learning offers a way to study how communication
could emerge in communities of agents needing to solve specific problems. In
this paper, we study the emergence of communication in the negotiation
environment, a semi-cooperative model of agent interaction. We introduce two
communication protocols -- one grounded in the semantics of the game, and one
which is \textit{a priori} ungrounded and is a form of cheap talk. We show that
self-interested agents can use the pre-grounded communication channel to
negotiate fairly, but are unable to effectively use the ungrounded channel.
However, prosocial agents do learn to use cheap talk to find an optimal
negotiating strategy, suggesting that cooperation is necessary for language to
emerge. We also study communication behaviour in a setting where one agent
interacts with agents in a community with different levels of prosociality and
show how agent identifiability can aid negotiation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1&quot;&gt;Kris Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1&quot;&gt;Angeliki Lazaridou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1&quot;&gt;Marc Lanctot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1&quot;&gt;Joel Z Leibo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1&quot;&gt;Karl Tuyls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1&quot;&gt;Stephen Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04012">
<title>DORA The Explorer: Directed Outreaching Reinforcement Action-Selection. (arXiv:1804.04012v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04012</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploration is a fundamental aspect of Reinforcement Learning, typically
implemented using stochastic action-selection. Exploration, however, can be
more efficient if directed toward gaining new world knowledge. Visit-counters
have been proven useful both in practice and in theory for directed
exploration. However, a major limitation of counters is their locality. While
there are a few model-based solutions to this shortcoming, a model-free
approach is still missing. We propose $E$-values, a generalization of counters
that can be used to evaluate the propagating exploratory value over
state-action trajectories. We compare our approach to commonly used RL
techniques, and show that using $E$-values improves learning and performance
over traditional counters. We also show how our method can be implemented with
function approximation to efficiently learn continuous MDPs. We demonstrate
this by showing that our approach surpasses state of the art performance in the
Freeway Atari 2600 game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1&quot;&gt;Leshem Choshen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_L/0/1/0/all/0/1&quot;&gt;Lior Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loewenstein_Y/0/1/0/all/0/1&quot;&gt;Yonatan Loewenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06562">
<title>An Iterative Closest Points Approach to Neural Generative Models. (arXiv:1711.06562v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06562</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple way to learn a transformation that maps samples of one
distribution to the samples of another distribution. Our algorithm comprises an
iteration of 1) drawing samples from some simple distribution and transforming
them using a neural network, 2) determining pairwise correspondences between
the transformed samples and training data (or a minibatch), and 3) optimizing
the weights of the neural network being trained to minimize the distances
between the corresponding vectors. This can be considered as a variant of the
Iterative Closest Points (ICP) algorithm, common in geometric computer vision,
although ICP typically operates on sensor point clouds and linear transforms
instead of random sample sets and neural nonlinear transforms. We demonstrate
the algorithm on simple synthetic data and MNIST data. We furthermore
demonstrate that the algorithm is capable of handling distributions with both
continuous and discrete variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajamaki_J/0/1/0/all/0/1&quot;&gt;Joose Rajam&amp;#xe4;ki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1&quot;&gt;Perttu H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01508">
<title>The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. (arXiv:1804.01508v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01508</link>
<description rdf:parseType="Literal">&lt;p&gt;Although simple individually, artificial neurons provide state-of-the-art
performance when interconnected in deep networks. Unknown to many, there exists
an arguably even simpler and more versatile learning mechanism, namely, the
Tsetlin Automaton. Merely by means of a single integer as memory, it learns the
optimal action in stochastic environments. In this paper, we introduce the
Tsetlin Machine, which solves complex pattern recognition problems with
easy-to-interpret propositional formulas, composed by a collective of Tsetlin
Automata. To eliminate the longstanding problem of vanishing signal-to-noise
ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our
theoretical analysis establishes that the Nash equilibria of the game are
aligned with the propositional formulas that provide optimal pattern
recognition accuracy. This translates to learning without local optima, only
global ones. We argue that the Tsetlin Machine finds the propositional formula
that provides optimal accuracy, with probability arbitrarily close to unity. In
four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks,
SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It
further turns out that the accuracy advantage of the Tsetlin Machine increases
with lack of data. The Tsetlin Machine has a significant computational
performance advantage since both inputs, patterns, and outputs are expressed as
bits, while recognition of patterns relies on bit manipulation. The combination
of accuracy, interpretability, and computational simplicity makes the Tsetlin
Machine a promising tool for a wide range of domains, including safety-critical
medicine. Being the first of its kind, we believe the Tsetlin Machine will
kick-start completely new paths of research, with a potentially significant
impact on the AI field and the applications of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02276">
<title>End-to-End Learning of Communications Systems Without a Channel Model. (arXiv:1804.02276v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02276</link>
<description rdf:parseType="Literal">&lt;p&gt;The idea of end-to-end learning of communications systems through neural
network -based autoencoders has the shortcoming that it requires a
differentiable channel model. We present in this paper a novel learning
algorithm which alleviates this problem. The algorithm iterates between
supervised training of the receiver and reinforcement learning -based training
of the transmitter. We demonstrate that this approach works as well as fully
supervised methods on additive white Gaussian noise (AWGN) and Rayleigh
block-fading (RBF) channels. Surprisingly, while our method converges slower on
AWGN channels than supervised training, it converges faster on RBF channels.
Our results are a first step towards learning of communications systems over
any type of channel without prior assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1&quot;&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1&quot;&gt;Jakob Hoydis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03707">
<title>A Tamper-Free Semi-Universal Communication System for Deletion Channels. (arXiv:1804.03707v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03707</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of reliable communication between two legitimate
parties over deletion channels under an active eavesdropping (aka jamming)
adversarial model. To this goal, we develop a theoretical framework based on
probabilistic finite-state automata to define novel encoding and decoding
schemes that ensure small error probability in both message decoding as well as
tamper detecting. We then experimentally verify the reliability and
tamper-detection property of our scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Asoodeh_S/0/1/0/all/0/1&quot;&gt;Shahab Asoodeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chattopadhyay_I/0/1/0/all/0/1&quot;&gt;Ishanu Chattopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03728">
<title>Tensor Robust Principal Component Analysis with A New Tensor Nuclear Norm. (arXiv:1804.03728v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03728</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the Tensor Robust Principal Component Analysis
(TRPCA) problem, which aims to exactly recover the low-rank and sparse
components from their sum. Our model is based on the recently proposed
tensor-tensor product (or t-product) [13]. Induced by the t-product, we first
rigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor
average rank, and show that the tensor nuclear norm is the convex envelope of
the tensor average rank within the unit ball of the tensor spectral norm. These
definitions, their relationships and properties are consistent with matrix
cases. Equipped with the new tensor nuclear norm, we then solve the TRPCA
problem by solving a convex program and provide the theoretical guarantee for
the exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA
as a special case. Numerical experiments verify our results, and the
applications to image recovery and background modeling problems demonstrate the
effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Canyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhouchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03740">
<title>Multimodal Sparse Bayesian Dictionary Learning. (arXiv:1804.03740v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03740</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of this paper is to address the problem of learning dictionaries
for multimodal datasets, i.e. datasets collected from multiple data sources. We
present an algorithm called multimodal sparse Bayesian dictionary learning
(MSBDL). The MSBDL algorithm is able to leverage information from all available
data modalities through a joint sparsity constraint on each modality&apos;s sparse
codes without restricting the coefficients themselves to be equal. Our
framework offers a considerable amount of flexibility to practitioners and
addresses many of the shortcomings of existing multimodal dictionary learning
approaches. Unlike existing approaches, MSBDL allows the dictionaries for each
data modality to have different cardinality. In addition, MSBDL can be used in
numerous scenarios, from small datasets to extensive datasets with large
dimensionality. MSBDL can also be used in supervised settings and allows for
learning multimodal dictionaries concurrently with classifiers for each
modality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedorov_I/0/1/0/all/0/1&quot;&gt;Igor Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_B/0/1/0/all/0/1&quot;&gt;Bhaskar D. Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03761">
<title>Derivative free optimization via repeated classification. (arXiv:1804.03761v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03761</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop an algorithm for minimizing a function using $n$ batched function
value measurements at each of $T$ rounds by using classifiers to identify a
function&apos;s sublevel set. We show that sufficiently accurate classifiers can
achieve linear convergence rates, and show that the convergence rate is tied to
the difficulty of active learning sublevel sets. Further, we show that the
bootstrap is a computationally efficient approximation to the necessary
classification scheme.
&lt;/p&gt;
&lt;p&gt;The end result is a computationally efficient derivative-free algorithm
requiring no tuning that consistently outperforms other approaches on
simulations, standard benchmarks, real-world DNA binding optimization, and
airfoil design problems whenever batched function queries are natural.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yadlowsky_S/0/1/0/all/0/1&quot;&gt;Steve Yadlowsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duchi_J/0/1/0/all/0/1&quot;&gt;John C. Duchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03794">
<title>Differentially Private Confidence Intervals for Empirical Risk Minimization. (arXiv:1804.03794v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03794</link>
<description rdf:parseType="Literal">&lt;p&gt;The process of data mining with differential privacy produces results that
are affected by two types of noise: sampling noise due to data collection and
privacy noise that is designed to prevent the reconstruction of sensitive
information. In this paper, we consider the problem of designing confidence
intervals for the parameters of a variety of differentially private machine
learning models. The algorithms can provide confidence intervals that satisfy
differential privacy (as well as the more recently proposed concentrated
differential privacy) and can be used with existing differentially private
mechanisms that train models using objective perturbation and output
perturbation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1&quot;&gt;Daniel Kifer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewoo Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03797">
<title>Dynamic Multivariate Functional Data Modeling via Sparse Subspace Learning. (arXiv:1804.03797v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03797</link>
<description rdf:parseType="Literal">&lt;p&gt;Multivariate functional data from a complex system are naturally
high-dimensional and have complex cross-correlation structure. The complexity
of data structure can be observed as that (1) some functions are strongly
correlated with similar features, while some others may have almost no
cross-correlations with quite diverse features; and (2) the cross-correlation
structure may also change over time due to the system evolution. With this
regard, this paper presents a dynamic subspace learning method for multivariate
functional data modeling. In particular, we consider different functions come
from different subspaces, and only functions of the same subspace have
cross-correlations with each other. The subspaces can be automatically
formulated and learned by reformatting the problem as a sparse regression. By
allowing but regularizing the regression change over time, we can describe the
cross-correlation dynamics. The model can be efficiently estimated by the fast
iterative shrinkage-thresholding algorithm (FISTA), and the features of every
subspace can be extracted using the smooth multi-channel functional PCA.
Numerical studies together with case studies demonstrate the efficiency and
applicability of the proposed methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jianjun Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03811">
<title>Estimating Time-Varying Graphical Models. (arXiv:1804.03811v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.03811</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study time-varying graphical models based on data measured
over a temporal grid. Such models are motivated by the needs to describe and
understand evolving interacting relationships among a set of random variables
in many real applications, for instance the study of how stocks interact with
each other and how such interactions change over time.
&lt;/p&gt;
&lt;p&gt;We propose a new model, LOcal Group Graphical Lasso Estimation (loggle),
under the assumption that the graph topology changes gradually over time.
Specifically, loggle uses a novel local group-lasso type penalty to efficiently
incorporate information from neighboring time points and to impose structural
smoothness of the graphs. We implement an ADMM based algorithm to fit the
loggle model. This algorithm utilizes blockwise fast computation and
pseudo-likelihood approximation to improve computational efficiency. An R
package loggle has also been developed.
&lt;/p&gt;
&lt;p&gt;We evaluate the performance of loggle by simulation experiments. We also
apply loggle to S&amp;amp;P 500 stock price data and demonstrate that loggle is able to
reveal the interacting relationships among stocks and among industrial sectors
in a time period that covers the recent global financial crisis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jilei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jie Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03836">
<title>Bayesian Semi-Supervised Tensor Decomposition using Natural Gradients for Anomaly Detection. (arXiv:1804.03836v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03836</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly Detection has several important applications. In this paper, our
focus is on detecting anomalies in seller-reviewer data using tensor
decomposition. While tensor-decomposition is mostly unsupervised, we formulate
Bayesian semi-supervised tensor decomposition to take advantage of sparse
labeled data. In addition, we use Polya-Gamma data augmentation for the
semi-supervised Bayesian tensor decomposition. Finally, we show that the
Polya-Gamma formulation simplifies calculation of the Fisher information matrix
for partial natural gradient learning. Our experimental results show that our
semi-supervised approach outperforms state of the art unsupervised baselines.
And that the partial natural gradient learning outperforms stochastic gradient
learning and Online-EM with sufficient statistics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yelundur_A/0/1/0/all/0/1&quot;&gt;Anil R. Yelundur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1&quot;&gt;Srinivasan H. Sengamedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03981">
<title>Compressive Regularized Discriminant Analysis of High-Dimensional Data with Applications to Microarray Studies. (arXiv:1804.03981v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1804.03981</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a modification of linear discriminant analysis, referred to as
compressive regularized discriminant analysis (CRDA), for analysis of
high-dimensional datasets. CRDA is specially designed for feature elimination
purpose and can be used as gene selection method in microarray studies. CRDA
lends ideas from $\ell_{q,1}$ norm minimization algorithms in the multiple
measurement vectors (MMV) model and utilizes joint-sparsity promoting hard
thresholding for feature elimination. A regularization of the sample covariance
matrix is also needed as we consider the challenging scenario where the number
of features (variables) is comparable or exceeding the sample size of the
training dataset. A simulation study and four examples of real-life microarray
datasets evaluate the performances of CRDA based classifiers. Overall, the
proposed method gives fewer misclassification errors than its competitors,
while at the same time achieving accurate feature selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tabassum_M/0/1/0/all/0/1&quot;&gt;Muhammad Naveed Tabassum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ollila_E/0/1/0/all/0/1&quot;&gt;Esa Ollila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04048">
<title>Cost-Aware Learning and Optimization for Opportunistic Spectrum Access. (arXiv:1804.04048v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1804.04048</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate cost-aware joint learning and optimization for
multi-channel opportunistic spectrum access in a cognitive radio system. We
investigate a discrete time model where the time axis is partitioned into
frames. Each frame consists of a sensing phase, followed by a transmission
phase. During the sensing phase, the user is able to sense a subset of channels
sequentially before it decides to use one of them in the following transmission
phase. We assume the channel states alternate between busy and idle according
to independent Bernoulli random processes from frame to frame. To capture the
inherent uncertainty in channel sensing, we assume the reward of each
transmission when the channel is idle is a random variable. We also associate
random costs with sensing and transmission actions. Our objective is to
understand how the costs and reward of the actions would affect the optimal
behavior of the user in both offline and online settings, and design the
corresponding opportunistic spectrum access strategies to maximize the expected
cumulative net reward (i.e., reward-minus-cost). We start with an offline
setting where the statistics of the channel status, costs and reward are known
beforehand. We show that the the optimal policy exhibits a recursive double
threshold structure, and the user needs to compare the channel statistics with
those thresholds sequentially in order to decide its actions. With such
insights, we then study the online setting, where the statistical information
of the channels, costs and reward are unknown a priori. We judiciously balance
exploration and exploitation, and show that the cumulative regret scales in
O(log T). We also establish a matched lower bound, which implies that our
online algorithm is order-optimal. Simulation results corroborate our
theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chao Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Ruida Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cong Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04097">
<title>End-to-end Deep Learning of Optical Fiber Communications. (arXiv:1804.04097v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1804.04097</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we implement an optical fiber communication system as an
end-to-end deep neural network, including the complete chain of transmitter,
channel model, and receiver. This approach enables the optimization of the
transceiver in a single end-to-end process. We illustrate the benefits of this
method by applying it to intensity modulation/direct detection (IM/DD) systems
and show that we can achieve bit error rates below the 6.7\% hard-decision
forward error correction (HD-FEC) threshold. We model all componentry of the
transmitter and receiver, as well as the fiber channel, and apply deep learning
to find transmitter and receiver configurations minimizing the symbol error
rate. We propose and verify in simulations a training method that yields robust
and flexible transceivers that allow---without reconfiguration---reliable
transmission over a large range of link dispersions. The results from
end-to-end deep learning are successfully verified for the first time in an
experiment. In particular, we achieve information rates of 42\,Gb/s below the
HD-FEC threshold at distances beyond 40\,km. We find that our results
outperform conventional IM/DD solutions based on 2 and 4 level pulse amplitude
modulation (PAM2/PAM4) with feedforward equalization (FFE) at the receiver. Our
study is the first step towards end-to-end deep learning-based optimization of
optical fiber communication systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanov_B/0/1/0/all/0/1&quot;&gt;Boris Karanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chagnon_M/0/1/0/all/0/1&quot;&gt;Mathieu Chagnon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thouin_F/0/1/0/all/0/1&quot;&gt;F&amp;#xe9;lix Thouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eriksson_T/0/1/0/all/0/1&quot;&gt;Tobias A. Eriksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulow_H/0/1/0/all/0/1&quot;&gt;Henning B&amp;#xfc;low&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavery_D/0/1/0/all/0/1&quot;&gt;Domani&amp;#xe7; Lavery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayvel_P/0/1/0/all/0/1&quot;&gt;Polina Bayvel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalen_L/0/1/0/all/0/1&quot;&gt;Laurent Schmalen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04112">
<title>Beamformed Fingerprint Learning for Accurate Millimeter Wave Positioning. (arXiv:1804.04112v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1804.04112</link>
<description rdf:parseType="Literal">&lt;p&gt;With millimeter wave wireless communications, the resulting radiation
reflects on most visible objects, creating rich multipath environments, namely
in urban scenarios. The radiation captured by a listening device is thus shaped
by the obstacles encountered, which carry latent information regarding their
relative positions. In this paper, a system to convert the received millimeter
wave radiation into the device&apos;s position is proposed, making use of the
aforementioned hidden information. Using deep learning techniques and a
pre-established codebook of beamforming patterns transmitted by a base station,
the simulations show that average estimation errors below 10 meters are
achievable in realistic outdoors scenarios that contain mostly
non-line-of-sight positions, paving the way for new positioning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gante_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Gante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Falcao_G/0/1/0/all/0/1&quot;&gt;Gabriel Falc&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sousa_L/0/1/0/all/0/1&quot;&gt;Leonel Sousa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04118">
<title>Personalized Dynamics Models for Adaptive Assistive Navigation Interfaces. (arXiv:1804.04118v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04118</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the role of personalization for assistive navigational systems
(e.g., service robot, wearable system or smartphone app) that guide visually
impaired users through speech, sound and haptic-based instructional guidance.
Based on our analysis of real-world users, we show that the dynamics of blind
users cannot be accounted for by a single universal model but instead must be
learned on an individual basis. To learn personalized instructional interfaces,
we propose PING (Personalized INstruction Generation agent), a model-based
reinforcement learning framework which aims to quickly adapt its state
transition dynamics model to match the reactions of the user using a novel
end-to-end learned weighted majority-based regression algorithm. In our
experiments, we show that PING learns dynamics models significantly faster
compared to baseline transfer learning approaches on real-world data. We find
that through better reasoning over personal mobility nuances, interaction with
surrounding obstacles, and the current navigation task, PING is able to improve
the performance of instructional assistive navigation at the most crucial
junctions such as turns or veering paths. To enable sufficient planning time
over user responses, we emphasize prediction of human motion for long horizons.
Specifically, the learned dynamics models are shown to consistently improve
long-term position prediction by over 1 meter on average (nearly the width of a
hallway) compared to baseline approaches even when considering a prediction
horizon of 20 seconds into the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1&quot;&gt;Eshed Ohn-Bar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris Kitani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asakawa_C/0/1/0/all/0/1&quot;&gt;Chieko Asakawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.06386">
<title>Bridging the Gap between Constant Step Size Stochastic Gradient Descent and Markov Chains. (arXiv:1707.06386v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.06386</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the minimization of an objective function given access to
unbiased estimates of its gradient through stochastic gradient descent (SGD)
with constant step-size. While the detailed analysis was only performed for
quadratic functions, we provide an explicit asymptotic expansion of the moments
of the averaged SGD iterates that outlines the dependence on initial
conditions, the effect of noise and the step-size, as well as the lack of
convergence in the general (non-quadratic) case. For this analysis, we bring
tools from Markov chain theory into the analysis of stochastic gradient. We
then show that Richardson-Romberg extrapolation may be used to get closer to
the global optimum and we show empirical improvements of the new extrapolation
scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dieuleveut_A/0/1/0/all/0/1&quot;&gt;Aymeric Dieuleveut&lt;/a&gt; (SIERRA, LIENS), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1&quot;&gt;Alain Durmus&lt;/a&gt; (CMLA), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt; (SIERRA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10353">
<title>Kernel-based Inference of Functions over Graphs. (arXiv:1711.10353v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10353</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of networks has witnessed an explosive growth over the past decades
with several ground-breaking methods introduced. A particularly interesting --
and prevalent in several fields of study -- problem is that of inferring a
function defined over the nodes of a network. This work presents a versatile
kernel-based framework for tackling this inference problem that naturally
subsumes and generalizes the reconstruction approaches put forth recently by
the signal processing on graphs community. Both the static and the dynamic
settings are considered along with effective modeling approaches for addressing
real-world problems. The herein analytical discussion is complemented by a set
of numerical examples, which showcase the effectiveness of the presented
techniques, as well as their merits related to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ioannidis_V/0/1/0/all/0/1&quot;&gt;Vassilis N. Ioannidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Meng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nikolakopoulos_A/0/1/0/all/0/1&quot;&gt;Athanasios N. Nikolakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Romero_D/0/1/0/all/0/1&quot;&gt;Daniel Romero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11279">
<title>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV). (arXiv:1711.11279v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11279</link>
<description rdf:parseType="Literal">&lt;p&gt;The interpretation of deep learning models is a challenge due to their size,
complexity, and often opaque internal state. In addition, many systems, such as
image classifiers, operate on low-level features rather than high-level
concepts. To address these challenges, we introduce Concept Activation Vectors
(CAVs), which provide an interpretation of a neural net&apos;s internal state in
terms of human-friendly concepts. The key idea is to view the high-dimensional
internal state of a neural net as an aid, not an obstacle. We show how to use
CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional
derivatives to quantify the degree to which a user-defined concept is important
to a classification result--for example, how sensitive a prediction of &quot;zebra&quot;
is to the presence of stripes. Using the domain of image classification as a
testing ground, we describe how CAVs may be used to explore hypotheses and
generate insights for a standard image classification network as well as a
medical application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wattenberg_M/0/1/0/all/0/1&quot;&gt;Martin Wattenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gilmer_J/0/1/0/all/0/1&quot;&gt;Justin Gilmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Carrie Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wexler_J/0/1/0/all/0/1&quot;&gt;James Wexler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Viegas_F/0/1/0/all/0/1&quot;&gt;Fernanda Viegas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sayres_R/0/1/0/all/0/1&quot;&gt;Rory Sayres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02101">
<title>Modeling Popularity in Asynchronous Social Media Streams with Recurrent Neural Networks. (arXiv:1804.02101v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02101</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding and predicting the popularity of online items is an important
open problem in social media analysis. Considerable progress has been made
recently in data-driven predictions, and in linking popularity to external
promotions. However, the existing methods typically focus on a single source of
external influence, whereas for many types of online content such as YouTube
videos or news articles, attention is driven by multiple heterogeneous sources
simultaneously - e.g. microblogs or traditional media coverage. Here, we
propose RNN-MAS, a recurrent neural network for modeling asynchronous streams.
It is a sequence generator that connects multiple streams of different
granularity via joint inference. We show RNN-MAS not only to outperform the
current state-of-the-art Youtube popularity prediction system by 17%, but also
to capture complex dynamics, such as seasonal trends of unseen influence. We
define two new metrics: promotion score quantifies the gain in popularity from
one unit of promotion for a Youtube video; the loudness level captures the
effects of a particular user tweeting about the video. We use the loudness
level to compare the effects of a video being promoted by a single
highly-followed user (in the top 1% most followed users) against being promoted
by a group of mid-followed users. We find that results depend on the type of
content being promoted: superusers are more successful in promoting Howto and
Gaming videos, whereas the cohort of regular users are more influential for
Activism videos. This work provides more accurate and explainable popularity
predictions, as well as computational tools for content producers and marketers
to allocate resources for promotion campaigns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Swapnil Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1&quot;&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lexing Xie&lt;/a&gt;</dc:creator>
</item></rdf:RDF>