<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1408.0848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02301"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01842"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01901"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02018"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02100"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02310"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01905"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01980"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02030"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02042"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02312"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.01793"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04072"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07831"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06292"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01454"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.02043">
<title>Online Deep Learning: Growing RBM on the fly. (arXiv:1803.02043v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.02043</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel online learning algorithm for Restricted Boltzmann
Machines (RBM), namely, the Online Generative Discriminative Restricted
Boltzmann Machine (OGD-RBM), that provides the ability to build and adapt the
network architecture of RBM according to the statistics of streaming data. The
OGD-RBM is trained in two phases: (1) an online generative phase for
unsupervised feature representation at the hidden layer and (2) a
discriminative phase for classification. The online generative training begins
with zero neurons in the hidden layer, adds and updates the neurons to adapt to
statistics of streaming data in a single pass unsupervised manner, resulting in
a feature representation best suited to the data. The discriminative phase is
based on stochastic gradient descent and associates the represented features to
the class labels. We demonstrate the OGD-RBM on a set of multi-category and
binary classification problems for data sets having varying degrees of
class-imbalance. We first apply the OGD-RBM algorithm on the multi-class MNIST
dataset to characterize the network evolution. We demonstrate that the online
generative phase converges to a stable, concise network architecture, wherein
individual neurons are inherently discriminative to the class labels despite
unsupervised training. We then benchmark OGD-RBM performance to other machine
learning, neural network and ClassRBM techniques for credit scoring
applications using 3 public non-stationary two-class credit datasets with
varying degrees of class-imbalance. We report that OGD-RBM improves accuracy by
2.5-3% over batch learning techniques while requiring at least 24%-70% fewer
neurons and fewer training samples. This online generative training approach
can be extended greedily to multiple layers for training Deep Belief Networks
in non-stationary data mining applications without the need for a priori fixed
architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasamy_S/0/1/0/all/0/1&quot;&gt;Savitha Ramasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajaraman_K/0/1/0/all/0/1&quot;&gt;Kanagasabai Rajaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1&quot;&gt;Pavitra Krishnaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekhar_V/0/1/0/all/0/1&quot;&gt;Vijay Chandrasekhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1408.0848">
<title>Multilayer bootstrap networks. (arXiv:1408.0848v8 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1408.0848</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear
network from bottom up for unsupervised nonlinear dimensionality reduction.
Each layer of the network is a nonparametric density estimator. It consists of
a group of k-centroids clusterings. Each clustering randomly selects data
points with randomly selected features as its centroids, and learns a one-hot
encoder by one-nearest-neighbor optimization. Geometrically, the nonparametric
density estimator at each layer projects the input data space to a
uniformly-distributed discrete feature space, where the similarity of two data
points in the discrete feature space is measured by the number of the nearest
centroids they share in common. The multilayer network gradually reduces the
nonlinear variations of data from bottom up by building a vast number of
hierarchical trees implicitly on the original data space. Theoretically, the
estimation error caused by the nonparametric density estimator is proportional
to the correlation between the clusterings, both of which are reduced by the
randomization steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao-Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01406">
<title>NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps. (arXiv:1706.01406v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01406</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have become the dominant neural network
architecture for solving many state-of-the-art (SOA) visual processing tasks.
Even though Graphical Processing Units (GPUs) are most often used in training
and deploying CNNs, their power efficiency is less than 10 GOp/s/W for
single-frame runtime inference. We propose a flexible and efficient CNN
accelerator architecture called NullHop that implements SOA CNNs useful for
low-power and low-latency application scenarios. NullHop exploits the sparsity
of neuron activations in CNNs to accelerate the computation and reduce memory
requirements. The flexible architecture allows high utilization of available
computing resources across kernel sizes ranging from 1x1 to 7x7. NullHop can
process up to 128 input and 128 output feature maps per layer in a single pass.
We implemented the proposed architecture on a Xilinx Zynq FPGA platform and
present results showing how our implementation reduces external memory
transfers and compute time in five different CNNs ranging from small ones up to
the widely known large VGG16 and VGG19 CNNs. Post-synthesis simulations using
Mentor Modelsim in a 28nm process with a clock frequency of 500 MHz show that
the VGG19 network achieves over 450 GOp/s. By exploiting sparsity, NullHop
achieves an efficiency of 368%, maintains over 98% utilization of the MAC
units, and achieves a power efficiency of over 3TOp/s/W in a core area of
6.3mm$^2$. As further proof of NullHop&apos;s usability, we interfaced its FPGA
implementation with a neuromorphic event camera for real time interactive
demonstrations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aimar_A/0/1/0/all/0/1&quot;&gt;Alessandro Aimar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mostafa_H/0/1/0/all/0/1&quot;&gt;Hesham Mostafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calabrese_E/0/1/0/all/0/1&quot;&gt;Enrico Calabrese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rios_Navarro_A/0/1/0/all/0/1&quot;&gt;Antonio Rios-Navarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tapiador_Morales_R/0/1/0/all/0/1&quot;&gt;Ricardo Tapiador-Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungu_I/0/1/0/all/0/1&quot;&gt;Iulia-Alexandra Lungu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milde_M/0/1/0/all/0/1&quot;&gt;Moritz B. Milde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corradi_F/0/1/0/all/0/1&quot;&gt;Federico Corradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linares_Barranco_A/0/1/0/all/0/1&quot;&gt;Alejandro Linares-Barranco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shih-Chii Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1&quot;&gt;Tobi Delbruck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02301">
<title>Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02301</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has achieved many recent successes, but our
understanding of its strengths and limitations is hampered by the lack of rich
environments in which we can fully characterize optimal behavior, and
correspondingly diagnose individual actions against such a characterization.
Here we consider a family of combinatorial games, arising from work of Erdos,
Selfridge, and Spencer, and we propose their use as environments for evaluating
and comparing different approaches to reinforcement learning. These games have
a number of appealing features: they are challenging for current learning
approaches, but they form (i) a low-dimensional, simply parametrized
environment where (ii) there is a linear closed form solution for optimal
behavior from any state, and (iii) the difficulty of the game can be tuned by
changing environment parameters in an interpretable way. We use these
Erdos-Selfridge-Spencer games not only to compare different algorithms, but
test for generalization, make comparisons to supervised learning, analyse
multiagent play, and even develop a self play algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1&quot;&gt;Maithra Raghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1&quot;&gt;Alex Irpan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_R/0/1/0/all/0/1&quot;&gt;Robert Kleinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01842">
<title>Towards Automatic &amp; Personalised Mobile Health Interventions: An Interactive Machine Learning Perspective. (arXiv:1803.01842v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1803.01842</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) is the fastest growing field in computer science and
healthcare, providing future benefits in improved medical diagnoses, disease
analyses and prevention. In this paper, we introduce an application of
interactive machine learning (iML) in a telemedicine system, to enable
automatic and personalised interventions for lifestyle promotion. We first
present the high level architecture of the system and the components forming
the overall architecture. We then illustrate the interactive machine learning
process design. Prediction models are expected to be trained through the
participants&apos; profiles, activity performance, and feedback from the caregiver.
Finally, we show some preliminary results during the system implementation and
discuss future directions. We envisage the proposed system to be digitally
implemented, and behaviourally designed to promote healthy lifestyle and
activities, and hence prevent users from the risk of chronic diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadhil_A/0/1/0/all/0/1&quot;&gt;Ahmed Fadhil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01901">
<title>On Discrimination Discovery and Removal in Ranked Data using Causal Graph. (arXiv:1803.01901v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01901</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive models learned from historical data are widely used to help
companies and organizations make decisions. However, they may digitally
unfairly treat unwanted groups, raising concerns about fairness and
discrimination. In this paper, we study the fairness-aware ranking problem
which aims to discover discrimination in ranked datasets and reconstruct the
fair ranking. Existing methods in fairness-aware ranking are mainly based on
statistical parity that cannot measure the true discriminatory effect since
discrimination is causal. On the other hand, existing methods in causal-based
anti-discrimination learning focus on classification problems and cannot be
directly applied to handle the ranked data. To address these limitations, we
propose to map the rank position to a continuous score variable that represents
the qualification of the candidates. Then, we build a causal graph that
consists of both the discrete profile attributes and the continuous score. The
path-specific effect technique is extended to the mixed-variable causal graph
to identify both direct and indirect discrimination. The relationship between
the path-specific effects for the ranked data and those for the binary decision
is theoretically analyzed. Finally, algorithms for discovering and removing
discrimination from a ranked dataset are developed. Experiments using the real
dataset show the effectiveness of our approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongkai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02018">
<title>Intent-aware Multi-agent Reinforcement Learning. (arXiv:1803.02018v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02018</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes an intent-aware multi-agent planning framework as well as
a learning algorithm. Under this framework, an agent plans in the goal space to
maximize the expected utility. The planning process takes the belief of other
agents&apos; intents into consideration. Instead of formulating the learning problem
as a partially observable Markov decision process (POMDP), we propose a simple
but effective linear function approximation of the utility function. It is
based on the observation that for humans, other people&apos;s intents will pose an
influence on our utility for a goal. The proposed framework has several major
advantages: i) it is computationally feasible and guaranteed to converge. ii)
It can easily integrate existing intent prediction and low-level planning
algorithms. iii) It does not suffer from sparse feedbacks in the action space.
We experiment our algorithm in a real-world problem that is non-episodic, and
the number of agents and goals can vary over time. Our algorithm is trained in
a scene in which aerial robots and humans interact, and tested in a novel scene
with a different environment. Experimental results show that our algorithm
achieves the best performance and human-like behaviors emerge during the
dynamic process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02096">
<title>Cooperative Tracking of Cyclists Based on Smart Devices and Infrastructure. (arXiv:1803.02096v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1803.02096</link>
<description rdf:parseType="Literal">&lt;p&gt;In future traffic scenarios, vehicles and other traffic participants will be
interconnected and equipped with various types of sensors, allowing for
cooperation based on data or information exchange. This article presents an
approach to cooperative tracking of cyclists using smart devices and
infrastructure-based sensors. A smart device is carried by the cyclists and an
intersection is equipped with a wide angle stereo camera system. Two tracking
models are presented and compared. The first model is based on the stereo
camera system detections only, whereas the second model cooperatively combines
the camera based detections with velocity and yaw rate data provided by the
smart device. Our aim is to overcome limitations of tracking approaches based
on single data sources. We show in numerical evaluations on scenes where
cyclists are starting or turning right that the cooperation leads to an
improvement in both the ability to keep track of a cyclist and the accuracy of
the track particularly when it comes to occlusions in the visual system. We,
therefore, contribute to the safety of vulnerable road users in future traffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reitberger_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nther Reitberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1&quot;&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bieshaar_M/0/1/0/all/0/1&quot;&gt;Maarten Bieshaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1&quot;&gt;Konrad Doll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_E/0/1/0/all/0/1&quot;&gt;Erich Fuchs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02100">
<title>The ORCA Hub: Explainable Offshore Robotics through Intelligent Interfaces. (arXiv:1803.02100v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02100</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the UK Robotics and Artificial Intelligence Hub for Offshore
Robotics for Certification of Assets (ORCA Hub), a 3.5 year EPSRC funded,
multi-site project. The ORCA Hub vision is to use teams of robots and
autonomous intelligent systems (AIS) to work on offshore energy platforms to
enable cheaper, safer and more efficient working practices. The ORCA Hub will
research, integrate, validate and deploy remote AIS solutions that can operate
with existing and future offshore energy assets and sensors, interacting safely
in autonomous or semi-autonomous modes in complex and cluttered environments,
co-operating with remote operators. The goal is that through the use of such
robotic systems offshore, the need for personnel will decrease. To enable this
to happen, the remote operator will need a high level of situation awareness
and key to this is the transparency of what the autonomous systems are doing
and why. This increased transparency will facilitate a trusting relationship,
which is particularly key in high-stakes, hazardous situations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1&quot;&gt;Helen Hastie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohan_K/0/1/0/all/0/1&quot;&gt;Katrin Lohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chantler_M/0/1/0/all/0/1&quot;&gt;Mike Chantler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robb_D/0/1/0/all/0/1&quot;&gt;David A. Robb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1&quot;&gt;Subramanian Ramamoorthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrick_R/0/1/0/all/0/1&quot;&gt;Ron Petrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1&quot;&gt;Sethu Vijayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lane_D/0/1/0/all/0/1&quot;&gt;David Lane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02124">
<title>MIRIAM: A Multimodal Chat-Based Interface for Autonomous Systems. (arXiv:1803.02124v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02124</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MIRIAM (Multimodal Intelligent inteRactIon for Autonomous
systeMs), a multimodal interface to support situation awareness of autonomous
vehicles through chat-based interaction. The user is able to chat about the
vehicle&apos;s plan, objectives, previous activities and mission progress. The
system is mixed initiative in that it pro-actively sends messages about key
events, such as fault warnings. We will demonstrate MIRIAM using SeeByte&apos;s
SeeTrack command and control interface and Neptune autonomy simulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1&quot;&gt;Helen Hastie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_F/0/1/0/all/0/1&quot;&gt;Francisco J. Chiyah Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robb_D/0/1/0/all/0/1&quot;&gt;David A. Robb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patron_P/0/1/0/all/0/1&quot;&gt;Pedro Patron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laskov_A/0/1/0/all/0/1&quot;&gt;Atanas Laskov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02232">
<title>Optimal Stochastic Package Delivery Planning with Deadline: A Cardinality Minimization in Routing. (arXiv:1803.02232v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02232</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle Routing Problem with Private fleet and common Carrier (VRPPC) has
been proposed to help a supplier manage package delivery services from a single
depot to multiple customers. Most of the existing VRPPC works consider
deterministic parameters which may not be practical and uncertainty has to be
taken into account. In this paper, we propose the Optimal Stochastic Delivery
Planning with Deadline (ODPD) to help a supplier plan and optimize the package
delivery. The aim of ODPD is to service all customers within a given deadline
while considering the randomness in customer demands and traveling time. We
formulate the ODPD as a stochastic integer programming, and use the cardinality
minimization approach for calculating the deadline violation probability. To
accelerate computation, the L-shaped decomposition method is adopted. We
conduct extensive performance evaluation based on real customer locations and
traveling time from Google Map.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawadsitang_S/0/1/0/all/0/1&quot;&gt;Suttinee Sawadsitang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Siwei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02242">
<title>Early Start Intention Detection of Cyclists Using Motion History Images and a Deep Residual Network. (arXiv:1803.02242v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.02242</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we present a novel approach to detect starting motions of
cyclists in real world traffic scenarios based on Motion History Images (MHIs).
The method uses a deep Convolutional Neural Network (CNN) with a residual
network architecture (ResNet), which is commonly used in image classification
and detection tasks. By combining MHIs with a ResNet classifier and performing
a frame by frame classification of the MHIs, we are able to detect starting
motions in image sequences. The detection is performed using a wide angle
stereo camera system at an urban intersection. We compare our algorithm to an
existing method to detect movement transitions of pedestrians that uses MHIs in
combination with a Histograms of Oriented Gradients (HOG) like descriptor and a
Support Vector Machine (SVM), which we adapted to cyclists. To train and
evaluate the methods a dataset containing MHIs of 394 cyclist starting motions
was created. The results show that both methods can be used to detect starting
motions of cyclists. Using the SVM approach, we were able to safely detect
starting motions 0.506 s on average after the bicycle starts moving with an
F1-score of 97.7%. The ResNet approach achieved an F1-score of 100% at an
average detection time of 0.144 s. The ResNet approach outperformed the SVM
approach in both robustness against false positive detections and detection
time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1&quot;&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1&quot;&gt;Viktor Kress&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1&quot;&gt;Konrad Doll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02310">
<title>Deep Thermal Imaging: Proximate Material Type Recognition in the Wild through Deep Learning of Spatial Surface Temperature Patterns. (arXiv:1803.02310v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.02310</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Deep Thermal Imaging, a new approach for close-range automatic
recognition of materials to enhance the understanding of people and ubiquitous
technologies of their proximal environment. Our approach uses a low-cost mobile
thermal camera integrated into a smartphone to capture thermal textures. A deep
neural network classifies these textures into material types. This approach
works effectively without the need for ambient light sources or direct contact
with materials. Furthermore, the use of a deep learning network removes the
need to handcraft the set of features for different materials. We evaluated the
performance of the system by training it to recognise 32 material types in both
indoor and outdoor environments. Our approach produced recognition accuracies
above 98% in 14,860 images of 15 indoor materials and above 89% in 26,584
images of 17 outdoor materials. We conclude by discussing its potentials for
real-time use in HCI applications and future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Youngjun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchi_Berthouze_N/0/1/0/all/0/1&quot;&gt;Nadia Bianchi-Berthouze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marquardt_N/0/1/0/all/0/1&quot;&gt;Nicolai Marquardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1&quot;&gt;Simon J. Julier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00781">
<title>Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration. (arXiv:1803.00781v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00781</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsically motivated goal exploration algorithms enable machines to
discover repertoires of policies that produce a diversity of effects in complex
environments. These exploration algorithms have been shown to allow real world
robots to acquire skills such as tool use in high-dimensional continuous state
and action spaces. However, they have so far assumed that self-generated goals
are sampled in a specifically engineered feature space, limiting their
autonomy. In this work, we propose to use deep representation learning
algorithms to learn an adequate goal space. This is a developmental 2-stage
approach: first, in a perceptual learning stage, deep learning algorithms use
passive raw sensor observations of world changes to learn a corresponding
latent space; then goal exploration happens in a second stage by sampling goals
in this latent space. We present experiments where a simulated robot arm
interacts with an object, and we show that exploration algorithms using such
learned representations can match the performance obtained using engineered
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pere_A/0/1/0/all/0/1&quot;&gt;Alexandre P&amp;#xe9;r&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forestier_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Forestier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1&quot;&gt;Olivier Sigaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01905">
<title>Convergence of Gradient Descent on Separable Data. (arXiv:1803.01905v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01905</link>
<description rdf:parseType="Literal">&lt;p&gt;The implicit bias of gradient descent is not fully understood even in simple
linear classification tasks (e.g., logistic regression). Soudry et al. (2018)
studied this bias on separable data, where there are multiple solutions that
correctly classify the data. It was found that, when optimizing monotonically
decreasing loss functions with exponential tails using gradient descent, the
linear classifier specified by the gradient descent iterates converge to the
$L_2$ max margin separator. However, the convergence rate to the maximum margin
solution with fixed step size was found to be extremely slow: $1/\log(t)$.
&lt;/p&gt;
&lt;p&gt;Here we examine how the convergence is influenced by using different loss
functions and by using variable step sizes. First, we calculate the convergence
rate for loss functions with poly-exponential tails near $\exp(-u^{\nu})$. We
prove that $\nu=1$ yields the optimal convergence rate in the range $\nu&amp;gt;0.25$.
Based on further analysis we conjecture that this remains the optimal rate for
$\nu \leq 0.25$, and even for sub-poly-exponential tails --- until loss
functions with polynomial tails no longer converge to the max margin. Second,
we prove the convergence rate could be improved to $(\log t) /\sqrt{t}$ for the
exponential loss, by using aggressive step sizes which compensate for the
rapidly vanishing gradients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nacson_M/0/1/0/all/0/1&quot;&gt;Mor Shpigel Nacson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gunasekar_S/0/1/0/all/0/1&quot;&gt;Suriya Gunasekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srebro_N/0/1/0/all/0/1&quot;&gt;Nathan Srebro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01927">
<title>Energy-entropy competition and the effectiveness of stochastic gradient descent in machine learning. (arXiv:1803.01927v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.01927</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding parameters that minimise a loss function is at the core of many
machine learning methods. The Stochastic Gradient Descent algorithm is widely
used and delivers state of the art results for many problems. Nonetheless,
Stochastic Gradient Descent typically cannot find the global minimum, thus its
empirical effectiveness is hitherto mysterious. We derive a correspondence
between parameter inference and free energy minimisation in statistical
physics. The degree of undersampling plays the role of temperature. Analogous
to the energy-entropy competition in statistical physics, wide but shallow
minima can be optimal if the system is undersampled, as is typical in many
applications. Moreover, we show that the stochasticity in the algorithm has a
non-trivial correlation structure which systematically biases it towards wide
minima. We illustrate our argument with two prototypical models: image
classification using deep learning, and a linear neural network where we can
analytically reveal the relationship between entropy and out-of-sample error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1&quot;&gt;Andrew M. Saxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Advani_M/0/1/0/all/0/1&quot;&gt;Madhu S. Advani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Alpha A. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01980">
<title>Learning Filter Bank Sparsifying Transforms. (arXiv:1803.01980v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.01980</link>
<description rdf:parseType="Literal">&lt;p&gt;Data is said to follow the transform (or analysis) sparsity model if it
becomes sparse when acted on by a linear operator called a sparsifying
transform. Several algorithms have been designed to learn such a transform
directly from data, and data-adaptive sparsifying transforms have demonstrated
excellent performance in signal restoration tasks. Sparsifying transforms are
typically learned using small sub-regions of data called patches, but these
algorithms often ignore redundant information shared between neighboring
patches.
&lt;/p&gt;
&lt;p&gt;We show that many existing transform and analysis sparse representations can
be viewed as filter banks, thus linking the local properties of patch-based
model to the global properties of a convolutional model. We propose a new
transform learning framework where the sparsifying transform is an undecimated
perfect reconstruction filter bank. Unlike previous transform learning
algorithms, the filter length can be chosen independently of the number of
filter bank channels. Numerical results indicate filter bank sparsifying
transforms outperform existing patch-based transform learning for image
denoising while benefiting from additional flexibility in the design process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pfister_L/0/1/0/all/0/1&quot;&gt;Luke Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bresler_Y/0/1/0/all/0/1&quot;&gt;Yoram Bresler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02030">
<title>Exact partial information decompositions for Gaussian systems based on dependency constraints. (arXiv:1803.02030v1 [cond-mat.stat-mech])</title>
<link>http://arxiv.org/abs/1803.02030</link>
<description rdf:parseType="Literal">&lt;p&gt;The Partial Information Decomposition (PID) [&lt;a href=&quot;/abs/1004.2515&quot;&gt;arXiv:1004.2515&lt;/a&gt;] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[&lt;a href=&quot;/abs/1709.06653&quot;&gt;arXiv:1709.06653&lt;/a&gt;] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [&lt;a href=&quot;/abs/1411.2832&quot;&gt;arXiv:1411.2832&lt;/a&gt;] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kay_J/0/1/0/all/0/1&quot;&gt;James W. Kay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ince_R/0/1/0/all/0/1&quot;&gt;Robin A. A. Ince&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02032">
<title>John&apos;s Walk. (arXiv:1803.02032v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.02032</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an affine-invariant random walk for drawing uniform random samples
from a convex body $\mathcal{K} \subset \mathbb{R}^n$ for which the maximum
volume inscribed ellipsoid, known as John&apos;s ellipsoid, may be computed. We
consider a polytope $\mathcal{P} = \{x \in \mathbb{R}^n \mid Ax \leq 1\}$ where
$A \in \mathbb{R}^{m \times n}$ as a special case. Our algorithm makes steps
using uniform sampling from the John&apos;s ellipsoid of the symmetrization of
$\mathcal{K}$ at the current point. We show that from a warm start, the random
walk mixes in $\widetilde{O}(n^7)$ steps where the log factors depend only on
constants determined by the warm start and error parameters (and not on the
dimension or number of constraints defining the body). This sampling algorithm
thus offers improvement over the affine-invariant Dikin Walk for polytopes
(which mixes in $\widetilde{O}(mn)$ steps from a warm start) for applications
in which $m \gg n$. Furthermore, we describe an $\widetilde{O}(mn^{\omega+1} +
n^{2\omega+2})$ algorithm for finding a suitably approximate John&apos;s ellipsoid
for a symmetric polytope based on Vaidya&apos;s algorithm, and show the mixing time
is retained using these approximate ellipsoids (where $\omega &amp;lt; 2.373$ is the
current value of the fast matrix multiplication constant).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gustafson_A/0/1/0/all/0/1&quot;&gt;Adam Gustafson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narayanan_H/0/1/0/all/0/1&quot;&gt;Hariharan Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02042">
<title>Accelerated Gradient Boosting. (arXiv:1803.02042v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.02042</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient tree boosting is a prediction algorithm that sequentially produces a
model in the form of linear combinations of decision trees, by solving an
infinite-dimensional optimization problem. We combine gradient boosting and
Nesterov&apos;s accelerated descent to design a new algorithm, which we call AGB
(for Accelerated Gradient Boosting). Substantial numerical evidence is provided
on both synthetic and real-life data sets to assess the excellent performance
of the method in a large variety of prediction problems. It is empirically
shown that AGB is much less sensitive to the shrinkage parameter and outputs
predictors that are considerably more sparse in the number of trees, while
retaining the exceptional performance of gradient boosting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Biau_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe9;rard Biau&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cadre_B/0/1/0/all/0/1&quot;&gt;Beno&amp;#xee;t Cadre&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rouviere_L/0/1/0/all/0/1&quot;&gt;Laurent Rouv&amp;#xec;&amp;#xe8;re&lt;/a&gt; (2) ((1) LPSM UMR 8001, (2) IRMAR)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02222">
<title>Multi-class Active Learning: A Hybrid Informative and Representative Criterion Inspired Approach. (arXiv:1803.02222v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.02222</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeling each instance in a large dataset is extremely labor- and time-
consuming . One way to alleviate this problem is active learning, which aims to
which discover the most valuable instances for labeling to construct a powerful
classifier. Considering both informativeness and representativeness provides a
promising way to design a practical active learning. However, most existing
active learning methods select instances favoring either informativeness or
representativeness. Meanwhile, many are designed based on the binary class, so
that they may present suboptimal solutions on the datasets with multiple
classes. In this paper, a hybrid informative and representative criterion based
multi-class active learning approach is proposed. We combine the informative
informativeness and representativeness into one formula, which can be solved
under a unified framework. The informativeness is measured by the margin
minimum while the representative information is measured by the maximum mean
discrepancy. By minimizing the upper bound for the true risk, we generalize the
empirical risk minimization principle to the active learning setting.
Simultaneously, our proposed method makes full use of the label information,
and the proposed active learning is designed based on multiple classes. So the
proposed method is not suitable to the binary class but also the multiple
classes. We conduct our experiments on twelve benchmark UCI data sets, and the
experimental results demonstrate that the proposed method performs better than
some state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zengmao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xinyao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chen Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02312">
<title>Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization. (arXiv:1803.02312v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.02312</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic optimization naturally arises in machine learning. Efficient
algorithms with provable guarantees, however, are still largely missing, when
the objective function is nonconvex and the data points are dependent. This
paper studies this fundamental challenge through a streaming PCA problem for
stationary time series data. Specifically, our goal is to estimate the
principle component of time series data with respect to the covariance matrix
of the stationary distribution. Computationally, we propose a variant of Oja&apos;s
algorithm combined with downsampling to control the bias of the stochastic
gradient caused by the data dependency. Theoretically, we quantify the
uncertainty of our proposed stochastic algorithm based on diffusion
approximations. This allows us to prove the global convergence in terms of the
continuous time limiting solution trajectory and further implies near optimal
sample complexity. Numerical experiments are provided to support our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minshuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.01793">
<title>Low-rank Optimization with Convex Constraints. (arXiv:1606.01793v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1606.01793</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of low-rank approximation with convex constraints, which appears
in data analysis, system identification, model order reduction, low-order
controller design and low-complexity modelling is considered. Given a matrix,
the objective is to find a low-rank approximation that meets rank and convex
constraints, while minimizing the distance to the matrix in the squared
Frobenius norm. In many situations, this non-convex problem is convexified by
nuclear norm regularization. However, we will see that the approximations
obtained by this method may be far from optimal. In this paper, we propose an
alternative convex relaxation that uses the convex envelope of the squared
Frobenius norm and the rank constraint. With this approach, easily verifiable
conditions are obtained under which the solutions to the convex relaxation and
the original non-convex problem coincide. An SDP representation of the convex
envelope is derived, which allows us to apply this approach to several known
problems. Our example on optimal low-rank Hankel approximation/model reduction
illustrates that the proposed convex relaxation performs consistently better
than nuclear norm regularization and may outperform balanced truncation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grussler_C/0/1/0/all/0/1&quot;&gt;Christian Grussler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rantzer_A/0/1/0/all/0/1&quot;&gt;Anders Rantzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Giselsson_P/0/1/0/all/0/1&quot;&gt;Pontus Giselsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04072">
<title>A convergence frame for inexact nonconvex and nonsmooth algorithms and its applications to several iterations. (arXiv:1709.04072v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04072</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the convergence of an abstract inexact nonconvex
and nonsmooth algorithm. We promise a pseudo sufficient descent condition and a
pseudo relative error condition, which both are related to an auxiliary
sequence, for the algorithm; and a continuity condition is assumed to hold. In
fact, a wide of classical inexact nonconvex and nonsmooth algorithms allow
these three conditions. Under the finite energy assumption on the auxiliary
sequence, we prove the sequence generated by the general algorithm converges to
a critical point of the objective function if being assumed Kurdyka-
Lojasiewicz property. The core of the proofs lies on building a new Lyapunov
function, whose successive difference provides a bound for the successive
difference of the points generated by the algorithm. And then, we apply our
findings to several classical nonconvex iterative algorithms and derive
corresponding convergence results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lizhi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07831">
<title>On Breast Cancer Detection: An Application of Machine Learning Algorithms on the Wisconsin Diagnostic Dataset. (arXiv:1711.07831v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07831</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comparison of six machine learning (ML) algorithms:
GRU-SVM (Agarap, 2017), Linear Regression, Multilayer Perceptron (MLP), Nearest
Neighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) on
the Wisconsin Diagnostic Breast Cancer (WDBC) dataset (Wolberg, Street, &amp;amp;
Mangasarian, 1992) by measuring their classification test accuracy and their
sensitivity and specificity values. The said dataset consists of features which
were computed from digitized images of FNA tests on a breast mass (Wolberg,
Street, &amp;amp; Mangasarian, 1992). For the implementation of the ML algorithms, the
dataset was partitioned in the following fashion: 70% for training phase, and
30% for the testing phase. The hyper-parameters used for all the classifiers
were manually assigned. Results show that all the presented ML algorithms
performed well (all exceeded 90% test accuracy) on the classification task. The
MLP algorithm stands out among the implemented algorithms with a test accuracy
of ~99.04%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1&quot;&gt;Abien Fred Agarap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00181">
<title>Personalized Gaussian Processes for Future Prediction of Alzheimer&apos;s Disease Progression. (arXiv:1712.00181v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00181</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the use of a personalized Gaussian Process model
(pGP) to predict the key metrics of Alzheimer&apos;s Disease progression (MMSE,
ADAS-Cog13, CDRSB and CS) based on each patient&apos;s previous visits. We start by
learning a population-level model using multi-modal data from previously seen
patients using the base Gaussian Process (GP) regression. Then, this model is
adapted sequentially over time to a new patient using domain adaptive GPs to
form the patient&apos;s pGP. We show that this new approach, together with an
auto-regressive formulation, leads to significant improvements in forecasting
future clinical status and cognitive scores for target patients when compared
to modeling the population with traditional GPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peterson_K/0/1/0/all/0/1&quot;&gt;Kelly Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudovic_O/0/1/0/all/0/1&quot;&gt;Ognjen Rudovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1&quot;&gt;Ricardo Guerrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_R/0/1/0/all/0/1&quot;&gt;Rosalind W. Picard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06292">
<title>Nonparametric Estimation of Low Rank Matrix Valued Function. (arXiv:1802.06292v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06292</link>
<description rdf:parseType="Literal">&lt;p&gt;Let $A:[0,1]\rightarrow\mathbb{H}_m$ (the space of Hermitian matrices) be a
matrix valued function which is low rank with entries in H\&quot;{o}lder class
$\Sigma(\beta,L)$. The goal of this paper is to study statistical estimation of
$A$ based on the regression model $\mathbb{E}(Y_j|\tau_j,X_j) = \langle
A(\tau_j), X_j \rangle,$ where $\tau_j$ are i.i.d. uniformly distributed in
$[0,1]$, $X_j$ are i.i.d. matrix completion sampling matrices, $Y_j$ are
independent bounded responses. We propose an innovative nuclear norm penalized
local polynomial estimator and establish an upper bound on its point-wise risk
measured by Frobenius norm. Then we extend this estimator globally and prove an
upper bound on its integrated risk measured by $L_2$-norm. We also propose
another new estimator based on bias-reducing kernels to study the case when $A$
is not necessarily low rank and establish an upper bound on its risk measured
by $L_{\infty}$-norm. We show that the obtained rates are all optimal up to
some logarithmic factor in minimax sense. Finally, we propose an adaptive
estimation procedure based on Lepski&apos;s method and the penalized data splitting
technique which is computationally efficient and can be easily implemented and
parallelized.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01454">
<title>Fast Best Subset Selection: Coordinate Descent and Local Combinatorial Optimization Algorithms. (arXiv:1803.01454v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01454</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the canonical $L_0$-regularized least squares problem (aka best
subsets) which is generally perceived as a `gold-standard&apos; for many sparse
learning regimes. In spite of worst-case computational intractability results,
recent work has shown that advances in mixed integer optimization can be used
to obtain near-optimal solutions to this problem for instances where the number
of features $p \approx 10^3$. While these methods lead to estimators with
excellent statistical properties, often there is a price to pay in terms of a
steep increase in computation times, especially when compared to highly
efficient popular algorithms for sparse learning (e.g., based on
$L_1$-regularization) that scale to much larger problem sizes. Bridging this
gap is a main goal of this paper. We study the computational aspects of a
family of $L_0$-regularized least squares problems with additional convex
penalties. We propose a hierarchy of necessary optimality conditions for these
problems. We develop new algorithms, based on coordinate descent and local
combinatorial optimization schemes, and study their convergence properties. We
demonstrate that the choice of an algorithm determines the quality of solutions
obtained; and local combinatorial optimization-based algorithms generally
result in solutions of superior quality. We show empirically that our proposed
framework is relatively fast for problem instances with $p\approx 10^6$ and
works well, in terms of both optimization and statistical properties (e.g.,
prediction, estimation, and variable selection), compared to simpler heuristic
algorithms. A version of our algorithm reaches up to a three-fold speedup (with
$p$ up to $10^6$) when compared to state-of-the-art schemes for sparse learning
such as glmnet and ncvreg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hazimeh_H/0/1/0/all/0/1&quot;&gt;Hussein Hazimeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1&quot;&gt;Rahul Mazumder&lt;/a&gt;</dc:creator>
</item></rdf:RDF>