<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03346"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03361"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03392"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03710"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05443"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05236"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01035"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03633"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03765"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04327"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03039"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03299"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03312"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03469"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03545"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03708"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03723"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03746"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.08082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08352"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03607"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11659"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01085"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.03346">
<title>Using Swarm Optimization To Enhance Autoencoders Images. (arXiv:1807.03346v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03346</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoencoders learn data representations through reconstruction. Robust
training is the key factor affecting the quality of the learned representations
and, consequently, the accuracy of the application that use them. Previous
works suggested methods for deciding the optimal autoencoder configuration
which allows for robust training. Nevertheless, improving the accuracy of a
trained autoencoder has got limited, if no, attention. We propose a new
approach that improves the accuracy of a trained autoencoders results and
answers the following question, Given a trained autoencoder, a test image, and
using a real-parameter optimizer, can we generate better quality reconstructed
image version than the one generated by the autoencoder?. Our proposed approach
combines both the decoder part of a trained Resitricted Boltman Machine-based
autoencoder with the Competitive Swarm Optimization algorithm. Experiments show
that it is possible to reconstruct images using trained decoder from randomly
initialized representations. Results also show that our approach reconstructed
better quality images than the autoencoder in most of the test cases.
Indicating that, we can use the approach for improving the performance of a
pre-trained autoencoder if it does not give satisfactory results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doaud_M/0/1/0/all/0/1&quot;&gt;Maisa Doaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayo_M/0/1/0/all/0/1&quot;&gt;Michael Mayo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03361">
<title>Weakly-Supervised Convolutional Neural Networks for Multimodal Image Registration. (arXiv:1807.03361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03361</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the fundamental challenges in supervised learning for multimodal image
registration is the lack of ground-truth for voxel-level spatial
correspondence. This work describes a method to infer voxel-level
transformation from higher-level correspondence information contained in
anatomical labels. We argue that such labels are more reliable and practical to
obtain for reference sets of image pairs than voxel-level correspondence.
Typical anatomical labels of interest may include solid organs, vessels, ducts,
structure boundaries and other subject-specific ad hoc landmarks. The proposed
end-to-end convolutional neural network approach aims to predict displacement
fields to align multiple labelled corresponding structures for individual image
pairs during the training, while only unlabelled image pairs are used as the
network input for inference. We highlight the versatility of the proposed
strategy, for training, utilising diverse types of anatomical labels, which
need not to be identifiable over all training image pairs. At inference, the
resulting 3D deformable image registration algorithm runs in real-time and is
fully-automated without requiring any anatomical labels or initialisation.
Several network architecture variants are compared for registering T2-weighted
magnetic resonance images and 3D transrectal ultrasound images from prostate
cancer patients. A median target registration error of 3.6 mm on landmark
centroids and a median Dice of 0.87 on prostate glands are achieved from
cross-validation experiments, in which 108 pairs of multimodal images from 76
patients were tested with high-quality anatomical labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modat_M/0/1/0/all/0/1&quot;&gt;Marc Modat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1&quot;&gt;Eli Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavami_N/0/1/0/all/0/1&quot;&gt;Nooshin Ghavami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonmati_E/0/1/0/all/0/1&quot;&gt;Ester Bonmati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guotai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandula_S/0/1/0/all/0/1&quot;&gt;Steven Bandula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_C/0/1/0/all/0/1&quot;&gt;Caroline M. Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emberton_M/0/1/0/all/0/1&quot;&gt;Mark Emberton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1&quot;&gt;J. Alison Noble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1&quot;&gt;Dean C. Barratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03392">
<title>Evolving Multimodal Robot Behavior via Many Stepping Stones with the Combinatorial Multi-Objective Evolutionary Algorithm. (arXiv:1807.03392v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03392</link>
<description rdf:parseType="Literal">&lt;p&gt;An important challenge in reinforcement learning, including evolutionary
robotics, is to solve multimodal problems, where agents have to act in
qualitatively different ways depending on the circumstances. Because multimodal
problems are often too difficult to solve directly, it is helpful to take
advantage of staging, where a difficult task is divided into simpler subtasks
that can serve as stepping stones for solving the overall problem.
Unfortunately, choosing an effective ordering for these subtasks is difficult,
and a poor ordering can reduce the speed and performance of the learning
process. Here, we provide a thorough introduction and investigation of the
Combinatorial Multi-Objective Evolutionary Algorithm (CMOEA), which avoids
ordering subtasks by allowing all combinations of subtasks to be explored
simultaneously. We compare CMOEA against two algorithms that can similarly
optimize on multiple subtasks simultaneously: NSGA-II and Lexicase Selection.
The algorithms are tested on a multimodal robotics problem with six subtasks as
well as a maze navigation problem with a hundred subtasks. On these problems,
CMOEA either outperforms or is competitive with the controls. Separately, we
show that adding a linear combination over all objectives can improve the
ability of NSGA-II to solve these multimodal problems. Lastly, we show that, in
contrast to NSGA-II and Lexicase Selection, CMOEA can effectively leverage
secondary objectives to achieve state-of-the-art results on the robotics task.
In general, our experiments suggest that CMOEA is a promising, state-of-the-art
algorithm for solving multimodal problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huizinga_J/0/1/0/all/0/1&quot;&gt;Joost Huizinga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03403">
<title>Optimal Parameter Choices via Precise Black-Box Analysis. (arXiv:1807.03403v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03403</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been observed that some working principles of evolutionary algorithms,
in particular, the influence of the parameters, cannot be understood from
results on the asymptotic order of the runtime, but only from more precise
results. In this work, we complement the emerging topic of precise runtime
analysis with a first precise complexity theoretic result. Our vision is that
the interplay between algorithm analysis and complexity theory becomes a
fruitful tool also for analyses more precise than asymptotic orders of
magnitude.
&lt;/p&gt;
&lt;p&gt;As particular result, we prove that the unary unbiased black-box complexity
of the OneMax benchmark function class is $n \ln(n) - cn \pm o(n)$ for a
constant $c$ which is between $0.2539$ and $0.2665$. This runtime can be
achieved with a simple (1+1)-type algorithm using a fitness-dependent mutation
strength. When translated into the fixed-budget perspective, our algorithm
finds solutions which are roughly 13\% closer to the optimum than those of the
best previously known algorithms. To prove our results, we formulate several
new versions of the variable drift theorems which might be of independent
interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1&quot;&gt;Carola Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03495">
<title>Significance-based Estimation-of-Distribution Algorithms. (arXiv:1807.03495v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03495</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimation-of-distribution algorithms (EDAs) are randomized search heuristics
that maintain a stochastic model of the solution space. This model is updated
from iteration to iteration based on the quality of the solutions sampled
according to the model. As previous works show, this short-term perspective can
lead to erratic updates of the model, in particular, to bit-frequencies
approaching a random boundary value. This can lead to significant performance
losses.
&lt;/p&gt;
&lt;p&gt;In order to overcome this problem, we propose a new EDA that takes into
account a longer history of samples and updates its model only with respect to
information which it classifies as statistically significant. We prove that
this significance-based compact genetic algorithm (sig-cGA) optimizes the
common benchmark functions OneMax and LeadingOnes both in $O(n \log n)$ time, a
result shown for no other EDA or evolutionary algorithm so far. For the
recently proposed scGA -- an EDA that tries to prevent erratic model updates by
imposing a bias to the uniformly distributed model -- we prove that it
optimizes OneMax only in a time exponential in the hypothetical population size
$1/\rho$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krejca_M/0/1/0/all/0/1&quot;&gt;Martin Krejca&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03710">
<title>Recurrent Auto-Encoder Model for Large-Scale Industrial Sensor Signal Analysis. (arXiv:1807.03710v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03710</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent auto-encoder model summarises sequential data through an encoder
structure into a fixed-length vector and then reconstructs the original
sequence through the decoder structure. The summarised vector can be used to
represent time series features. In this paper, we propose relaxing the
dimensionality of the decoder output so that it performs partial
reconstruction. The fixed-length vector therefore represents features in the
selected dimensions only. In addition, we propose using rolling fixed window
approach to generate training samples from unbounded time series data. The
change of time series features over time can be summarised as a smooth
trajectory path. The fixed-length vectors are further analysed using additional
visualisation and unsupervised clustering techniques. The proposed method can
be applied in large-scale industrial processes for sensors signal analysis
purpose, where clusters of the vector representations can reflect the operating
states of the industrial system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1&quot;&gt;Timothy Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02851">
<title>Whale swarm algorithm with the mechanism of identifying and escaping from extreme point for multimodal function optimization. (arXiv:1804.02851v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02851</link>
<description rdf:parseType="Literal">&lt;p&gt;Most real-world optimization problems often come with multiple global optima
or local optima. Therefore, increasing niching metaheuristic algorithms, which
devote to finding multiple optima in a single run, are developed to solve these
multimodal optimization problems. However, there are two difficulties urgently
to be solved for most existing niching metaheuristic algorithms: how to set the
optimal values of niching parameters for different optimization problems, and
how to jump out of the local optima efficiently. These two difficulties limited
their practicality largely. Based on Whale Swarm Algorithm (WSA) we proposed
previously, this paper presents a new multimodal optimizer named WSA with
Iterative Counter (WSA-IC) to address these two difficulties. In the one hand,
WSA-IC improves the iteration rule of the original WSA for multimodal
optimization, which removes the need of specifying different values of
attenuation coefficient for different problems to form multiple subpopulations,
without introducing any niching parameter. In the other hand, WSA-IC enables
the identification of extreme point during iterations relying on two new
parameters (i.e., stability threshold Ts and fitness threshold Tf), to jump out
of the located extreme point. Moreover, the convergence of WSA-IC is proved.
Finally, the proposed WSA-IC is compared with several niching metaheuristic
algorithms on CEC2015 niching benchmark test functions and five additional
classical multimodal functions with high dimensions. The experimental results
demonstrate that WSA-IC statistically outperforms other niching metaheuristic
algorithms on most test functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Bing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Liang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Haozhen Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05443">
<title>Better Fixed-Arity Unbiased Black-Box Algorithms. (arXiv:1804.05443v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05443</link>
<description rdf:parseType="Literal">&lt;p&gt;In their GECCO&apos;12 paper, Doerr and Doerr proved that the $k$-ary unbiased
black-box complexity of OneMax on $n$ bits is $O(n/k)$ for $2\le k\le O(\log
n)$. We propose an alternative strategy for achieving this unbiased black-box
complexity when $3\le k\le\log_2 n$. While it is based on the same idea of
block-wise optimization, it uses $k$-ary unbiased operators in a different way.
&lt;/p&gt;
&lt;p&gt;For each block of size $2^{k-1}-1$ we set up, in $O(k)$ queries, a virtual
coordinate system, which enables us to use an arbitrary unrestricted algorithm
to optimize this block. This is possible because this coordinate system
introduces a bijection between unrestricted queries and a subset of $k$-ary
unbiased operators. We note that this technique does not depend on OneMax being
solved and can be used in more general contexts.
&lt;/p&gt;
&lt;p&gt;This together constitutes an algorithm which is conceptually simpler than the
one by Doerr and Doerr, and at the same time achieves better constant factors
in the asymptotic notation. Our algorithm works in $(2+o(1))\cdot n/(k-1)$,
where $o(1)$ relates to $k$. Our experimental evaluation of this algorithm
shows its efficiency already for $3\le k\le6$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulanova_N/0/1/0/all/0/1&quot;&gt;Nina Bulanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buzdalov_M/0/1/0/all/0/1&quot;&gt;Maxim Buzdalov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05236">
<title>Manifold Mixup: Encouraging Meaningful On-Manifold Interpolation as a Regularizer. (arXiv:1806.05236v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05236</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep networks often perform well on the data manifold on which they are
trained, yet give incorrect (and often very confident) answers when evaluated
on points from off of the training distribution. This is exemplified by the
adversarial examples phenomenon but can also be seen in terms of model
generalization and domain shift. We propose Manifold Mixup which encourages the
network to produce more reasonable and less confident predictions at points
with combinations of attributes not seen in the training set. This is
accomplished by training on convex combinations of the hidden state
representations of data samples. Using this method, we demonstrate improved
semi-supervised learning, learning with limited labeled data, and robustness to
adversarial examples. Manifold Mixup requires no (significant) additional
computation. Analytical experiments on both real data and synthetic data
directly support our hypothesis for why the Manifold Mixup method improves
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verma_V/0/1/0/all/0/1&quot;&gt;Vikas Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lamb_A/0/1/0/all/0/1&quot;&gt;Alex Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beckham_C/0/1/0/all/0/1&quot;&gt;Christopher Beckham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mitliagkas_I/0/1/0/all/0/1&quot;&gt;Ioannis Mitliagkas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01035">
<title>Deep Neural Object Analysis by Interactive Auditory Exploration with a Humanoid Robot. (arXiv:1807.01035v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01035</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach for interactive auditory object analysis with a
humanoid robot. The robot elicits sensory information by physically shaking
visually indistinguishable plastic capsules. It gathers the resulting audio
signals from microphones that are embedded into the robotic ears. A neural
network architecture learns from these signals to analyze properties of the
contents of the containers. Specifically, we evaluate the material
classification and weight prediction accuracy and demonstrate that the
framework is fairly robust to acoustic real-world noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eppe_M/0/1/0/all/0/1&quot;&gt;Manfred Eppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1&quot;&gt;Matthias Kerzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strahl_E/0/1/0/all/0/1&quot;&gt;Erik Strahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03341">
<title>Troubling Trends in Machine Learning Scholarship. (arXiv:1807.03341v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03341</link>
<description rdf:parseType="Literal">&lt;p&gt;Collectively, machine learning (ML) researchers are engaged in the creation
and dissemination of knowledge about data-driven algorithms. In a given paper,
researchers might aspire to any subset of the following goals, among others: to
theoretically characterize what is learnable, to obtain understanding through
empirically rigorous experiments, or to build a working system that has high
predictive accuracy. While determining which knowledge warrants inquiry may be
subjective, once the topic is fixed, papers are most valuable to the community
when they act in service of the reader, creating foundational knowledge and
communicating as clearly as possible.
&lt;/p&gt;
&lt;p&gt;Recent progress in machine learning comes despite frequent departures from
these ideals. In this paper, we focus on the following four patterns that
appear to us to be trending in ML scholarship: (i) failure to distinguish
between explanation and speculation; (ii) failure to identify the sources of
empirical gains, e.g., emphasizing unnecessary modifications to neural
architectures when gains actually stem from hyper-parameter tuning; (iii)
mathiness: the use of mathematics that obfuscates or impresses rather than
clarifies, e.g., by confusing technical and non-technical concepts; and (iv)
misuse of language, e.g., by choosing terms of art with colloquial connotations
or by overloading established technical terms.
&lt;/p&gt;
&lt;p&gt;While the causes behind these patterns are uncertain, possibilities include
the rapid expansion of the community, the consequent thinness of the reviewer
pool, and the often-misaligned incentives between scholarship and short-term
measures of success (e.g., bibliometrics, attention, and entrepreneurial
opportunity). While each pattern offers a corresponding remedy (don&apos;t do it),
we also discuss some speculative suggestions for how the community might combat
these trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Steinhardt_J/0/1/0/all/0/1&quot;&gt;Jacob Steinhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03440">
<title>Developing Brain Atlas through Deep Learning. (arXiv:1807.03440v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.03440</link>
<description rdf:parseType="Literal">&lt;p&gt;To uncover the organizational principles governing the human brain,
neuroscientists are in need of developing high-throughput methods that can
explore the structure and function of distinct brain regions using animal
models. The first step towards this goal is to accurately register the regions
of interest in a mouse brain, against a standard reference atlas, with minimum
human supervision. The second step is to scale this approach to different
animal ages, so as to also allow insights into normal and pathological brain
development and aging. We introduce here a fully automated convolutional neural
network-based method (SeBRe) for registration through Segmenting Brain Regions
of interest in mice at different ages. We demonstrate the validity of our
method on different mouse brain post-natal (P) developmental time points,
across a range of neuronal markers. Our method outperforms the existing brain
registration methods, and provides the minimum mean squared error (MSE) score
on a mouse brain dataset. We propose that our deep learning-based registration
method can (i) accelerate brain-wide exploration of region-specific changes in
brain development and (ii) replace the existing complex brain registration
methodology, by simply segmenting brain regions of interest for high-throughput
brain-wide analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1&quot;&gt;Asim Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1&quot;&gt;Romesa Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karayannis_T/0/1/0/all/0/1&quot;&gt;Theofanis Karayannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03527">
<title>Algebraic Equivalence of Linear Structural Equation Models. (arXiv:1807.03527v1 [math.ST])</title>
<link>http://arxiv.org/abs/1807.03527</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their popularity, many questions about the algebraic constraints
imposed by linear structural equation models remain open problems. For causal
discovery, two of these problems are especially important: the enumeration of
the constraints imposed by a model, and deciding whether two graphs define the
same statistical model. We show how the half-trek criterion can be used to make
progress in both of these problems. We apply our theoretical results to a
small-scale model selection problem, and find that taking the additional
algebraic constraints into account may lead to significant improvements in
model selection accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ommen_T/0/1/0/all/0/1&quot;&gt;Thijs van Ommen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mooij_J/0/1/0/all/0/1&quot;&gt;Joris M. Mooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03571">
<title>A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees. (arXiv:1807.03571v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03571</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the improved accuracy of deep neural networks, the discovery of
adversarial examples has raised serious safety concerns. In this paper, we
study two variants of pointwise robustness, the maximum safe radius problem,
which for a given input sample computes the minimum distance to an adversarial
example, and the feature robustness problem, which aims to quantify the
robustness of individual features to adversarial perturbations. We demonstrate
that, under the assumption of Lipschitz continuity, both problems can be
approximated using finite optimisation by discretising the input space, and the
approximation has provable guarantees, i.e., the error is bounded. We then show
that the resulting optimisation problems can be reduced to the solution of
two-player turn-based games, where the first player selects features and the
second perturbs the image within the feature. While the second player aims to
minimise the distance to an adversarial example, depending on the optimisation
objective the first player can be cooperative or competitive. We employ an
anytime approach to solve the games, in the sense of approximating the value of
a game by monotonically improving its upper and lower bounds. The Monte Carlo
tree search algorithm is applied to compute upper bounds for both games, and
the Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used
to compute lower bounds for the maximum safety radius and feature robustness
games. When working on the upper bound of the maximum safe radius problem, our
tool demonstrates competitive performance against existing adversarial example
crafting algorithms. Furthermore, we show how our framework can be deployed to
evaluate pointwise robustness of neural networks in safety-critical
applications such as traffic sign recognition in self-driving cars.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1&quot;&gt;Matthew Wicker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1&quot;&gt;Marta Kwiatkowska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03633">
<title>Interpretable Patient Mortality Prediction with Multi-value Rule Sets. (arXiv:1807.03633v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.03633</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Multi-vAlue Rule Set (MRS) model for in-hospital predicting
patient mortality. Compared to rule sets built from single-valued rules, MRS
adopts a more generalized form of association rules that allows multiple values
in a condition. Rules of this form are more concise than classical
single-valued rules in capturing and describing patterns in data. Our
formulation also pursues a higher efficiency of feature utilization, which
reduces possible cost in data collection and storage. We propose a Bayesian
framework for formulating a MRS model and propose an efficient inference method
for learning a maximum \emph{a posteriori}, incorporating theoretically
grounded bounds to iteratively reduce the search space and improve the search
efficiency. Experiments show that our model was able to achieve better
performance than baseline method including the current system used by the
hospital.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allareddy_V/0/1/0/all/0/1&quot;&gt;Veerajalandhar Allareddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampa_S/0/1/0/all/0/1&quot;&gt;Sankeerth Rampa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allareddy_V/0/1/0/all/0/1&quot;&gt;Veerasathpurush Allareddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03760">
<title>SimArch: A Multi-agent System For Human Path Simulation In Architecture Design. (arXiv:1807.03760v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.03760</link>
<description rdf:parseType="Literal">&lt;p&gt;Human moving path is an important feature in architecture design. By studying
the path, architects know where to arrange the basic elements (e.g. structures,
glasses, furniture, etc.) in the space. This paper presents SimArch, a
multi-agent system for human moving path simulation. It involves a behavior
model built by using a Markov Decision Process. The model simulates human
mental states, target range detection, and collision prediction when agents are
on the floor, in a particular small gallery, looking at an exhibit, or leaving
the floor. It also models different kinds of human characteristics by assigning
different transition probabilities. A modified weighted A* search algorithm
quickly plans the sub-optimal path of the agents. In an experiment, SimArch
takes a series of preprocessed floorplans as inputs, simulates the moving path,
and outputs a density map for evaluation. The density map provides the
prediction that how likely a person will occur in a location. A following
discussion illustrates how architects can use the density map to improve their
floorplan design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yen-Chia Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03765">
<title>Is Q-learning Provably Efficient?. (arXiv:1807.03765v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03765</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-free reinforcement learning (RL) algorithms, such as Q-learning,
directly parameterize and update value functions or policies without explicitly
modeling the environment. They are typically simpler, more flexible to use, and
thus more prevalent in modern deep RL than model-based approaches. However,
empirical work has suggested that model-free algorithms may require more
samples to learn [Deisenroth and Rasmussen 2011, Schulman et al. 2015]. The
theoretical question of &quot;whether model-free algorithms can be made sample
efficient&quot; is one of the most fundamental questions in RL, and remains unsolved
even in the basic scenario with finitely many states and actions.
&lt;/p&gt;
&lt;p&gt;We prove that, in an episodic MDP setting, Q-learning with UCB exploration
achieves regret $\tilde{O}(\sqrt{H^3 SAT})$, where $S$ and $A$ are the numbers
of states and actions, $H$ is the number of steps per episode, and $T$ is the
total number of steps. This sample efficiency matches the optimal regret that
can be achieved by any model-based approach, up to a single $\sqrt{H}$ factor.
To the best of our knowledge, this is the first analysis in the model-free
setting that establishes $\sqrt{T}$ regret without requiring access to a
&quot;simulator.&quot;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1&quot;&gt;Sebastien Bubeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03769">
<title>Kernel-Based Learning for Smart Inverter Control. (arXiv:1807.03769v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.03769</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution grids are currently challenged by frequent voltage excursions
induced by intermittent solar generation. Smart inverters have been advocated
as a fast-responding means to regulate voltage and minimize ohmic losses. Since
optimal inverter coordination may be computationally challenging and preset
local control rules are subpar, the approach of customized control rules
designed in a quasi-static fashion features as a golden middle. Departing from
affine control rules, this work puts forth non-linear inverter control
policies. Drawing analogies to multi-task learning, reactive control is posed
as a kernel-based regression task. Leveraging a linearized grid model and given
anticipated data scenarios, inverter rules are jointly designed at the feeder
level to minimize a convex combination of voltage deviations and ohmic losses
via a linearly-constrained quadratic program. Numerical tests using real-world
data on a benchmark feeder demonstrate that nonlinear control rules driven also
by a few non-local readings can attain near-optimal performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Aditie Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jalali_M/0/1/0/all/0/1&quot;&gt;Mana Jalali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kekatos_V/0/1/0/all/0/1&quot;&gt;Vassilis Kekatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gatsis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Gatsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09010">
<title>Datasheets for Datasets. (arXiv:1803.09010v3 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09010</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently there is no standard way to identify how a dataset was created, and
what characteristics, motivations, and potential skews it represents. To begin
to address this issue, we propose the concept of a datasheet for datasets, a
short document to accompany public datasets, commercial APIs, and pretrained
models. The goal of this proposal is to enable better communication between
dataset creators and users, and help the AI community move toward greater
transparency and accountability. By analogy, in computer hardware, it has
become industry standard to accompany everything from the simplest components
(e.g., resistors), to the most complex microprocessor chips, with datasheets
detailing standard operating characteristics, test results, recommended usage,
and other information. We outline some of the questions a datasheet for
datasets should answer. These questions focus on when, where, and how the
training data was gathered, its recommended use cases, and, in the case of
human-centric datasets, information regarding the subjects&apos; demographics and
consent as applicable. We develop prototypes of datasheets for two well-known
datasets: Labeled Faces in The Wild and the Pang \&amp;amp; Lee Polarity Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebru_T/0/1/0/all/0/1&quot;&gt;Timnit Gebru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgenstern_J/0/1/0/all/0/1&quot;&gt;Jamie Morgenstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vecchione_B/0/1/0/all/0/1&quot;&gt;Briana Vecchione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1&quot;&gt;Jennifer Wortman Vaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallach_H/0/1/0/all/0/1&quot;&gt;Hanna Wallach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daumee_H/0/1/0/all/0/1&quot;&gt;Hal Daume&amp;#xe9; III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crawford_K/0/1/0/all/0/1&quot;&gt;Kate Crawford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04327">
<title>Attention-based Group Recommendation. (arXiv:1804.04327v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04327</link>
<description rdf:parseType="Literal">&lt;p&gt;Group recommendation aims to recommend items for a group of users, e.g.,
recommending a restaurant for a group of colleagues. The group recommendation
problem is challenging, in that a good model should understand the group
decision making process appropriately: users are likely to follow decisions of
only a few users, who are group&apos;s leaders or experts. To address this
challenge, we propose using an attention mechanism to capture the impact of
each user in a group. Specifically, our model learns the influence weight of
each user in a group and recommends items to the group based on its members&apos;
weighted preferences. Moreover, our model can dynamically adjust the weight of
each user across the groups; thus, the model provides a new and flexible method
to model the complicated group decision making process, which differentiates us
from other existing solutions. Through extensive experiments, it has
demonstrated that our model significantly outperforms baseline methods for the
group recommendation problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinh_T/0/1/0/all/0/1&quot;&gt;Tran Dang Quang Vinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Nguyen Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1&quot;&gt;Gao Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiao-Li Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06760">
<title>Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components. (arXiv:1804.06760v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06760</link>
<description rdf:parseType="Literal">&lt;p&gt;Many organizations are developing autonomous driving systems, which are
expected to be deployed at a large scale in the near future. Despite this,
there is a lack of agreement on appropriate methods to test, debug, and certify
the performance of these systems. One of the main challenges is that many
autonomous driving systems have machine learning components, such as deep
neural networks, for which formal properties are difficult to characterize. We
present a testing framework that is compatible with test case generation and
automatic falsification methods, which are used to evaluate cyber-physical
systems. We demonstrate how the framework can be used to evaluate closed-loop
properties of an autonomous driving system model that includes the ML
components, all within a virtual environment. We demonstrate how to use test
case generation methods, such as covering arrays, as well as requirement
falsification methods to automatically identify problematic test scenarios. The
resulting framework can be used to increase the reliability of autonomous
driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuncali_C/0/1/0/all/0/1&quot;&gt;Cumhur Erkan Tuncali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fainekos_G/0/1/0/all/0/1&quot;&gt;Georgios Fainekos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1&quot;&gt;Hisahiro Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapinski_J/0/1/0/all/0/1&quot;&gt;James Kapinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07199">
<title>Agent-Mediated Social Choice. (arXiv:1806.07199v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07199</link>
<description rdf:parseType="Literal">&lt;p&gt;Direct democracy is often proposed as a possible solution to the 21st-century
problems of democracy. However, this suggestion clashes with the size and
complexity of 21st-century societies, entailing an excessive cognitive burden
on voters, who would have to submit informed opinions on an excessive number of
issues. In this paper I argue for the development of voting avatars, autonomous
agents debating and voting on behalf of each citizen. Theoretical research from
artificial intelligence, and in particular multiagent systems and computational
social choice, proposes 21st-century techniques for this purpose, from the
compact representation of a voter&apos;s preferences and values, to the development
of voting procedures for autonomous agents use only.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grandi_U/0/1/0/all/0/1&quot;&gt;Umberto Grandi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03039">
<title>Glow: Generative Flow with Invertible 1x1 Convolutions. (arXiv:1807.03039v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03039</link>
<description rdf:parseType="Literal">&lt;p&gt;Flow-based generative models (Dinh et al., 2014) are conceptually attractive
due to tractability of the exact log-likelihood, tractability of exact
latent-variable inference, and parallelizability of both training and
synthesis. In this paper we propose Glow, a simple type of generative flow
using an invertible 1x1 convolution. Using our method we demonstrate a
significant improvement in log-likelihood on standard benchmarks. Perhaps most
strikingly, we demonstrate that a generative model optimized towards the plain
log-likelihood objective is capable of efficient realistic-looking synthesis
and manipulation of large images. The code for our model is available at
https://github.com/openai/glow
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kingma_D/0/1/0/all/0/1&quot;&gt;Diederik P. Kingma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhariwal_P/0/1/0/all/0/1&quot;&gt;Prafulla Dhariwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03299">
<title>Optimization of a SSP&apos;s Header Bidding Strategy using Thompson Sampling. (arXiv:1807.03299v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03299</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decade, digital media (web or app publishers) generalized the
use of real time ad auctions to sell their ad spaces. Multiple auction
platforms, also called Supply-Side Platforms (SSP), were created. Because of
this multiplicity, publishers started to create competition between SSPs. In
this setting, there are two successive auctions: a second price auction in each
SSP and a secondary, first price auction, called header bidding auction,
between SSPs.In this paper, we consider an SSP competing with other SSPs for ad
spaces. The SSP acts as an intermediary between an advertiser wanting to buy ad
spaces and a web publisher wanting to sell its ad spaces, and needs to define a
bidding strategy to be able to deliver to the advertisers as many ads as
possible while spending as little as possible. The revenue optimization of this
SSP can be written as a contextual bandit problem, where the context consists
of the information available about the ad opportunity, such as properties of
the internet user or of the ad placement.Using classical multi-armed bandit
strategies (such as the original versions of UCB and EXP3) is inefficient in
this setting and yields a low convergence speed, as the arms are very
correlated. In this paper we design and experiment a version of the Thompson
Sampling algorithm that easily takes this correlation into account. We combine
this bayesian algorithm with a particle filter, which permits to handle
non-stationarity by sequentially estimating the distribution of the highest bid
to beat in order to win an auction. We apply this methodology on two real
auction datasets, and show that it significantly outperforms more classical
approaches.The strategy defined in this paper is being developed to be deployed
on thousands of publishers worldwide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jauvion_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Jauvion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grislain_N/0/1/0/all/0/1&quot;&gt;Nicolas Grislain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dkengne_P/0/1/0/all/0/1&quot;&gt;Pascal Sielenou Dkengne&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garivier_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Garivier&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Gerchinovitz&lt;/a&gt; (IMT)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03312">
<title>Spectral Analysis of Jet Substructure with Neural Network: Boosted Higgs Case. (arXiv:1807.03312v1 [hep-ph])</title>
<link>http://arxiv.org/abs/1807.03312</link>
<description rdf:parseType="Literal">&lt;p&gt;Jets from boosted heavy particles have a typical angular scale which can be
used to distinguish it from QCD jets. We introduce a machine learning strategy
for jet substructure analysis using a spectral function on the angular scale.
The angular spectrum allows us to scan energy deposits over the angle between a
pair of particles in a highly visual way. We set up an artificial neural
network (ANN) to find out characteristic shapes of the spectra of the jets from
heavy particle decays. By taking the discrimination of Higgs jets from QCD jets
as an example, we show that the ANN based on the angular spectrum has similar
performance to existing taggers. In addition, some improvement is seen in the
case that additional extra radiations occur. Notably, the new algorithm
automatically combines the information of the multi-point correlations in the
jet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Sung Hak Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Nojiri_M/0/1/0/all/0/1&quot;&gt;Mihoko M. Nojiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03369">
<title>Ensemble Kalman Filtering for Online Gaussian Process Regression and Learning. (arXiv:1807.03369v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03369</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian process regression is a machine learning approach which has been
shown its power for estimation of unknown functions. However, Gaussian
processes suffer from high computational complexity, as in a basic form they
scale cubically with the number of observations. Several approaches based on
inducing points were proposed to handle this problem in a static context. These
methods though face challenges with real-time tasks and when the data is
received sequentially over time. In this paper, a novel online algorithm for
training sparse Gaussian process models is presented. It treats the mean and
hyperparameters of the Gaussian process as the state and parameters of the
ensemble Kalman filter, respectively. The online evaluation of the parameters
and the state is performed on new upcoming samples of data. This procedure
iteratively improves the accuracy of parameter estimates. The ensemble Kalman
filter reduces the computational complexity required to obtain predictions with
Gaussian processes preserving the accuracy level of these predictions. The
performance of the proposed method is demonstrated on the synthetic dataset and
real large dataset of UK house prices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuzin_D/0/1/0/all/0/1&quot;&gt;Danil Kuzin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Le Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Isupova_O/0/1/0/all/0/1&quot;&gt;Olga Isupova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mihaylova_L/0/1/0/all/0/1&quot;&gt;Lyudmila Mihaylova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03379">
<title>Online Scoring with Delayed Information: A Convex Optimization Viewpoint. (arXiv:1807.03379v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03379</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a system where agents enter in an online fashion and are
evaluated based on their attributes or context vectors. There can be practical
situations where this context is partially observed, and the unobserved part
comes after some delay. We assume that an agent, once left, cannot re-enter the
system. Therefore, the job of the system is to provide an estimated score for
the agent based on her instantaneous score and possibly some inference of the
instantaneous score over the delayed score. In this paper, we estimate the
delayed context via an online convex game between the agent and the system. We
argue that the error in the score estimate accumulated over $T$ iterations is
small if the regret of the online convex game is small. Further, we leverage
side information about the delayed context in the form of a correlation
function with the known context. We consider the settings where the delay is
fixed or arbitrarily chosen by an adversary. Furthermore, we extend the
formulation to the setting where the contexts are drawn from some Banach space.
Overall, we show that the average penalty for not knowing the delayed context
while making a decision scales with $\mathcal{O}(\frac{1}{\sqrt{T}})$, where
this can be improved to $\mathcal{O}(\frac{\log T}{T})$ under special setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramchandran_K/0/1/0/all/0/1&quot;&gt;Kannan Ramchandran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03387">
<title>Process Monitoring Using Maximum Sequence Divergence. (arXiv:1807.03387v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03387</link>
<description rdf:parseType="Literal">&lt;p&gt;Process Monitoring involves tracking a system&apos;s behaviors, evaluating the
current state of the system, and discovering interesting events that require
immediate actions. In this paper, we consider monitoring temporal system state
sequences to help detect the changes of dynamic systems, check the divergence
of the system development, and evaluate the significance of the deviation. We
begin with discussions of data reduction, symbolic data representation, and the
anomaly detection in temporal discrete sequences. Time-series representation
methods are also discussed and used in this paper to discretize raw data into
sequences of system states. Markov Chains and stationary state distributions
are continuously generated from temporal sequences to represent snapshots of
the system dynamics in different time frames. We use generalized Jensen-Shannon
Divergence as the measure to monitor changes of the stationary symbol
probability distributions and evaluate the significance of system deviations.
We prove that the proposed approach is able to detect deviations of the systems
we monitor and assess the deviation significance in probabilistic manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yihuang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zadorozhny_V/0/1/0/all/0/1&quot;&gt;Vladimir Zadorozhny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03431">
<title>A New Variational Model for Binary Classification in the Supervised Learning Context. (arXiv:1807.03431v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03431</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the supervised learning problem in its continuous setting and give
a general optimality condition through techniques of functional analysis and
the calculus of variations. This enables us to solve the optimality condition
for the desired function u numerically and make several comparisons with other
widely utilized su- pervised learning models. We employ the accuracy and area
under the receiver operating characteristic curve as metrics of the
performance. Finally 3 analyses are conducted based on these two mentioned
metrics where we compare the models and make conclusions to determine whether
or not our method is competitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacheco_C/0/1/0/all/0/1&quot;&gt;Carlos David Brito Pacheco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loeza_C/0/1/0/all/0/1&quot;&gt;Carlos Francisco Brito Loeza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03456">
<title>Predicting property damage from tornadoes with deep learning. (arXiv:1807.03456v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03456</link>
<description rdf:parseType="Literal">&lt;p&gt;Tornadoes are the most violent of all atmospheric storms. In a typical year,
the United States experiences hundreds of tornadoes with associated damages on
the order of one billion dollars. Community preparation and resilience would
benefit from accurate predictions of these economic losses, particularly as
populations in tornado-prone areas continue to increase in density and extent.
Here, we use artificial neural networks to predict tornado-induced property
damage using publicly available data. We find that the large number of
tornadoes which cause zero property damage (30.6% of the data) poses a
challenge for predictive models. We developed a model that predicts whether a
tornado will cause property damage to a high degree of accuracy (out of sample
accuracy = 0.829 and AUROC = 0.873). Conditional on a tornado causing damage,
another model predicts the amount of damage. When combined, these two models
yield an expected value for the amount of property damage caused by a tornado
event. From the best-performing models (out of sample mean squared error =
0.089 and R2 = 0.473), we provide an interactive, gridded map of monthly
expected values for the year 2018. One major weakness is that the model
predictive power is optimized with log-transformed, mean-normalized property
damages, however this leads to large natural-scale residuals for the most
destructive tornadoes. The predictive capacity of this model along with an
interactive interface may provide an opportunity for science-informed tornado
disaster planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Diaz_J/0/1/0/all/0/1&quot;&gt;Jeremy Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joseph_M/0/1/0/all/0/1&quot;&gt;Maxwell Joseph&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03469">
<title>Pairwise Covariates-adjusted Block Model for Community Detection. (arXiv:1807.03469v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1807.03469</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most fundamental problems in network study is community detection.
The stochastic block model (SBM) is one widely used model for network data with
different estimation methods developed with their community detection
consistency results unveiled. However, the SBM is restricted by the strong
assumption that all nodes in the same community are stochastically equivalent,
which may not be suitable for practical applications. We introduce pairwise
covariates-adjusted stochastic block model (PCABM), a generalization of SBM
that incorporates pairwise covariate information. We study the maximum
likelihood estimates of the coefficients for the covariates as well as the
community assignments. It is shown that both the coefficient estimates of the
covariates and the community assignments are consistent under suitable sparsity
conditions. Spectral clustering with adjustment (SCWA) is introduced to
efficiently solve PCABM. Under certain conditions, we derive the error bound of
community estimation under SCWA and show that it is community detection
consistent. PCABM compares favorably with the SBM or degree-corrected
stochastic block model (DCBM) under a wide range of simulated and real networks
when covariate information is accessible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sihan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03513">
<title>Automatic trajectory recognition in Active Target Time Projection Chambers data by means of hierarchical clustering. (arXiv:1807.03513v1 [physics.ins-det])</title>
<link>http://arxiv.org/abs/1807.03513</link>
<description rdf:parseType="Literal">&lt;p&gt;The automatic reconstruction of three-dimensional particle tracks from Active
Target Time Projection Chambers data can be a challenging task, especially in
the presence of noise. In this article, we propose a non-parametric algorithm
that is based on the idea of clustering point triplets instead of the original
points. We define an appropriate distance measure on point triplets and then
apply a single-link hierarchical clustering on the triplets. Compared to
parametric approaches like RANSAC or the Hough transform, the new algorithm has
the advantage of potentially finding trajectories even of shapes that are not
known beforehand. This feature is particularly important in low-energy nuclear
physics experiments with AT operating inside a magnetic field. The algorithm
has been validated using data from experiments performed with the Active Target
Time Projection Chamber (AT-TPC) at the National Superconducting Cyclotron
Laboratory (NSCL).The results demonstrate the capability of the algorithm to
identify and isolate particle tracks that describe non-analytical trajectories.
For curved tracks, the vertex detection recall was 86\% and the precision 94\%.
For straight tracks, the vertex detection recall was 96\% and the precision
98\%. In the case of a test set containing only straight linear tracks, the
algorithm performed better than an iterative Hough transform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dalitz_C/0/1/0/all/0/1&quot;&gt;Christoph Dalitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ayyad_Y/0/1/0/all/0/1&quot;&gt;Yassid Ayyad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wilberg_J/0/1/0/all/0/1&quot;&gt;Jens Wilberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Aymans_L/0/1/0/all/0/1&quot;&gt;Lukas Aymans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bazin_D/0/1/0/all/0/1&quot;&gt;Daniel Bazin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mittig_W/0/1/0/all/0/1&quot;&gt;Wolfgang Mittig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03521">
<title>Privacy-Adversarial User Representations in Recommender Systems. (arXiv:1807.03521v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.03521</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent factor models for recommender systems represent users and items as low
dimensional vectors. Privacy risks have been previously studied mostly in the
context of recovery of personal information in the form of usage records from
the training data. However, the user representations themselves may be used
together with external data to recover private user information such as gender
and age. In this paper we show that user vectors calculated by a common
recommender system can be exploited in this way. We propose the
privacy-adversarial framework to eliminate such leakage, and study the
trade-off between recommender performance and leakage both theoretically and
empirically using a benchmark dataset. We briefly discuss further applications
of this method towards the generation of deeper and more insightful
recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resheff_Y/0/1/0/all/0/1&quot;&gt;Yehezkel S. Resheff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1&quot;&gt;Yanai Elazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahar_M/0/1/0/all/0/1&quot;&gt;Moni Shahar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalom_O/0/1/0/all/0/1&quot;&gt;Oren Sar Shalom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03545">
<title>Dual optimization for convex constrained objectives without the gradient-Lipschitz assumption. (arXiv:1807.03545v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03545</link>
<description rdf:parseType="Literal">&lt;p&gt;The minimization of convex objectives coming from linear supervised learning
problems, such as penalized generalized linear models, can be formulated as
finite sums of convex functions. For such problems, a large set of stochastic
first-order solvers based on the idea of variance reduction are available and
combine both computational efficiency and sound theoretical guarantees (linear
convergence rates). Such rates are obtained under both gradient-Lipschitz and
strong convexity assumptions. Motivated by learning problems that do not meet
the gradient-Lipschitz assumption, such as linear Poisson regression, we work
under another smoothness assumption, and obtain a linear convergence rate for a
shifted version of Stochastic Dual Coordinate Ascent (SDCA) that improves the
current state-of-the-art. Our motivation for considering a solver working on
the Fenchel-dual problem comes from the fact that such objectives include many
linear constraints, that are easier to deal with in the dual. Our approach and
theoretical findings are validated on several datasets, for Poisson regression
and another objective coming from the negative log-likelihood of the Hawkes
process, which is a family of models which proves extremely useful for the
modeling of information propagation in social networks and causality inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bompaire_M/0/1/0/all/0/1&quot;&gt;Martin Bompaire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bacry_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bacry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03555">
<title>An Empirical Approach For Probing the Definiteness of Kernels. (arXiv:1807.03555v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03555</link>
<description rdf:parseType="Literal">&lt;p&gt;Models like support vector machines or Gaussian process regression often
require positive semi-definite kernels. These kernels may be based on distance
functions. While definiteness is proven for common distances and kernels, a
proof for a new kernel may require too much time and effort for users who
simply aim at practical usage. Furthermore, designing definite distances or
kernels may be equally intricate. Finally, models can be enabled to use
indefinite kernels. This may deteriorate the accuracy or computational cost of
the model. Hence, an efficient method to determine definiteness is required. We
propose an empirical approach. We show that sampling as well as optimization
with an evolutionary algorithm may be employed to determine definiteness. We
provide a proof-of-concept with 16 different distance measures for
permutations. Our approach allows to disprove definiteness if a respective
counter-example is found. It can also provide an estimate of how likely it is
to obtain indefinite kernel matrices. This provides a simple, efficient tool to
decide whether additional effort should be spent on designing/selecting a more
suitable kernel or algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaefferer_M/0/1/0/all/0/1&quot;&gt;Martin Zaefferer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1&quot;&gt;Thomas Bartz-Beielstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nter Rudolph&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03558">
<title>Bandits with Side Observations: Bounded vs. Logarithmic Regret. (arXiv:1807.03558v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03558</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the classical stochastic multi-armed bandit but where, from time
to time and roughly with frequency $\epsilon$, an extra observation is gathered
by the agent for free. We prove that, no matter how small $\epsilon$ is the
agent can ensure a regret uniformly bounded in time.
&lt;/p&gt;
&lt;p&gt;More precisely, we construct an algorithm with a regret smaller than $\sum_i
\frac{\log(1/\epsilon)}{\Delta_i}$, up to multiplicative constant and loglog
terms. We also prove a matching lower-bound, stating that no reasonable
algorithm can outperform this quantity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Degenne_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Degenne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcelon_E/0/1/0/all/0/1&quot;&gt;Evrard Garcelon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1&quot;&gt;Vianney Perchet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03570">
<title>Small-Variance Asymptotics for Nonparametric Bayesian Overlapping Stochastic Blockmodels. (arXiv:1807.03570v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03570</link>
<description rdf:parseType="Literal">&lt;p&gt;The latent feature relational model (LFRM) is a generative model for
graph-structured data to learn a binary vector representation for each node in
the graph. The binary vector denotes the node&apos;s membership in one or more
communities. At its core, the LFRM miller2009nonparametric is an overlapping
stochastic blockmodel, which defines the link probability between any pair of
nodes as a bilinear function of their community membership vectors. Moreover,
using a nonparametric Bayesian prior (Indian Buffet Process) enables learning
the number of communities automatically from the data. However, despite its
appealing properties, inference in LFRM remains a challenge and is typically
done via MCMC methods. This can be slow and may take a long time to converge.
In this work, we develop a small-variance asymptotics based framework for the
non-parametric Bayesian LFRM. This leads to an objective function that retains
the nonparametric Bayesian flavor of LFRM, while enabling us to design
deterministic inference algorithms for this model, that are easy to implement
(using generic or specialized optimization routines) and are fast in practice.
Our results on several benchmark datasets demonstrate that our algorithm is
competitive to methods such as MCMC, while being much faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arora_G/0/1/0/all/0/1&quot;&gt;Gundeep Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Porwal_A/0/1/0/all/0/1&quot;&gt;Anupreet Porwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agarwal_K/0/1/0/all/0/1&quot;&gt;Kanupriya Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Samdariya_A/0/1/0/all/0/1&quot;&gt;Avani Samdariya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rai_P/0/1/0/all/0/1&quot;&gt;Piyush Rai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03708">
<title>Generalized deterministic policy gradient algorithms. (arXiv:1807.03708v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03708</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a setting of reinforcement learning, where the state transition is a
convex combination of a stochastic continuous function and a deterministic
discontinuous function. Such a setting include as a special case the stochastic
state transition setting, namely the setting of deterministic policy gradient
(DPG).
&lt;/p&gt;
&lt;p&gt;We introduce a theoretical technique to prove the existence of the policy
gradient in this generalized setting. Using this technique, we prove that the
deterministic policy gradient indeed exists for a certain set of discount
factors, and further prove two conditions that guarantee the existence for all
discount factors. We then derive a closed form of the policy gradient whenever
exists. Interestingly, the form of the policy gradient in such setting is
equivalent to that in DPG. Furthermore, to overcome the challenge of high
sample complexity of DPG in this setting, we propose the Generalized
Deterministic Policy Gradient (GDPG) algorithm. The main innovation of the
algorithm is to optimize a weighted objective of the original Markov decision
process (MDP) and an augmented MDP that simplifies the original one, and serves
as its lower bound. To solve the augmented MDP, we make use of the model-based
methods which enable fast convergence. We finally conduct extensive experiments
comparing GDPG with state-of-the-art methods on several standard benchmarks.
Results demonstrate that GDPG substantially outperforms other baselines in
terms of both convergence and long-term rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1&quot;&gt;Qingpeng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Ling Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Pingzhong Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03711">
<title>Geometric Generalization Based Zero-Shot Learning Dataset Infinite World: Simple Yet Powerful. (arXiv:1807.03711v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03711</link>
<description rdf:parseType="Literal">&lt;p&gt;Raven&apos;s Progressive Matrices are one of the widely used tests in evaluating
the human test taker&apos;s fluid intelligence. Analogously, this paper introduces
geometric generalization based zero-shot learning tests to measure the rapid
learning ability and the internal consistency of deep generative models. Our
empirical research analysis on state-of-the-art generative models discern their
ability to generalize concepts across classes. In the process, we introduce
Infinit World, an evaluable, scalable, multi-modal, light-weight dataset and
Zero-Shot Intelligence Metric ZSI. The proposed tests condenses human-level
spatial and numerical reasoning tasks to its simplistic geometric forms. The
dataset is scalable to a theoretical limit of infinity, in numerical features
of the generated geometric figures, image size and in quantity. We
systematically analyze state-of-the-art model&apos;s internal consistency, identify
their bottlenecks and propose a pro-active optimization method for few-shot and
zero-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidambaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Chidambaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1&quot;&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lachmann_T/0/1/0/all/0/1&quot;&gt;Thomas Lachmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03723">
<title>Understanding VAEs in Fisher-Shannon Plane. (arXiv:1807.03723v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03723</link>
<description rdf:parseType="Literal">&lt;p&gt;In information theory, Fisher information and Shannon information (entropy)
are respectively used to measure the ability in parameter estimation and the
uncertainty among variables. The uncertainty principle asserts a fundamental
relationship between Fisher information and Shannon information, i.e., the more
Fisher information we get, the less Shannon information we gain, and vice
versa. This enlightens us about the essence of the encoding/decoding procedure
in \emph{variational auto-encoders} (VAEs) and motivates us to investigate VAEs
in the Fisher-Shannon plane. Our studies show that the performance of the
latent representation learning and the log-likelihood estimation are
intrinsically influenced by the trade-off between Fisher information and
Shannon information. To flexibly adjust the trade-off, we further propose a
variant of VAEs that can explicitly control Fisher information in
encoding/decoding mechanism, termed as Fisher auto-encoder (FAE). Through
qualitative and quantitative experiments, we show the complementary properties
of Fisher information and Shannon information, and give a guide for Fisher
information conditioning to achieve high resolution reconstruction, disentangle
feature learning, over-fitting/over-regularization resistance, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor W. Tsang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03746">
<title>Scalable Sparse Subspace Clustering via Ordered Weighted $\ell_1$ Regression. (arXiv:1807.03746v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03746</link>
<description rdf:parseType="Literal">&lt;p&gt;The main contribution of the paper is a new approach to subspace clustering
that is significantly more computationally efficient and scalable than existing
state-of-the-art methods. The central idea is to modify the regression
technique in sparse subspace clustering (SSC) by replacing the $\ell_1$
minimization with a generalization called Ordered Weighted $\ell_1$ (OWL)
minimization which performs simultaneous regression and clustering of
correlated variables. Using random geometric graph theory, we prove that OWL
regression selects more points within each subspace, resulting in better
clustering results. This allows for accurate subspace clustering based on
regression solutions for only a small subset of the total dataset,
significantly reducing the computational complexity compared to SSC. In
experiments, we find that our OWL approach can achieve a speedup of 20$\times$
to 30$\times$ for synthetic problems and 4$\times$ to 8$\times$ on real data
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oswal_U/0/1/0/all/0/1&quot;&gt;Urvashi Oswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03748">
<title>Representation Learning with Contrastive Predictive Coding. (arXiv:1807.03748v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03748</link>
<description rdf:parseType="Literal">&lt;p&gt;While supervised learning has enabled great progress in many applications,
unsupervised learning has not seen such widespread adoption, and remains an
important and challenging endeavor for artificial intelligence. In this work,
we propose a universal unsupervised learning approach to extract useful
representations from high-dimensional data, which we call Contrastive
Predictive Coding. The key insight of our model is to learn such
representations by predicting the future in latent space by using powerful
autoregressive models. We use a probabilistic contrastive loss which induces
the latent space to capture information that is maximally useful to predict
future samples. It also makes the model tractable by using negative sampling.
While most prior work has focused on evaluating representations for a
particular modality, we demonstrate that our approach is able to learn useful
representations achieving strong performance on four distinct domains: speech,
images, text and reinforcement learning in 3D environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1&quot;&gt;Aaron van den Oord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yazhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.08082">
<title>Constructing Effective Personalized Policies Using Counterfactual Inference from Biased Data Sets with Many Features. (arXiv:1612.08082v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1612.08082</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel approach for constructing effective personalized
policies when the observed data lacks counter-factual information, is biased
and possesses many features. The approach is applicable in a wide variety of
settings from healthcare to advertising to education to finance. These settings
have in common that the decision maker can observe, for each previous instance,
an array of features of the instance, the action taken in that instance, and
the reward realized -- but not the rewards of actions that were not taken: the
counterfactual information. Learning in such settings is made even more
difficult because the observed data is typically biased by the existing policy
(that generated the data) and because the array of features that might affect
the reward in a particular instance -- and hence should be taken into account
in deciding on an action in each particular instance -- is often vast. The
approach presented here estimates propensity scores for the observed data,
infers counterfactuals, identifies a (relatively small) number of features that
are (most) relevant for each possible action and instance, and prescribes a
policy to be followed. Comparison of the proposed algorithm against the
state-of-art algorithm on actual datasets demonstrates that the proposed
algorithm achieves a significant improvement in performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Atan_O/0/1/0/all/0/1&quot;&gt;Onur Atan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zame_W/0/1/0/all/0/1&quot;&gt;William R. Zame&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qiaojun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00789">
<title>WARP: Wavelets with adaptive recursive partitioning for multi-dimensional data. (arXiv:1711.00789v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional statistical wavelet analysis carries out modeling and inference
under a given, predetermined wavelet transform. This approach can quickly lose
efficiency for multi-dimensional data (e.g., observations measured on a
multi-dimensional grid), because a predetermined wavelet transform does not
adaptively exploit the information or energy distribution in a problem-specific
manner. This work aims to overcome this challenge within the multivariate
wavelet analysis framework by incorporating adaptivity into the wavelet
transform itself in a principled manner. By exploiting a connection between
wavelet transforms and permutations on the index space of multi-dimensional
functions, we show that the desired adaptive wavelet transform can be achieved
by adopting a layer of Bayesian hierarchical modeling on the space of such
permutations. In particular, when combined with the Haar basis, exact Bayesian
inference under the model can be achieved analytically through a recursive
message passing algorithm with an efficient computational complexity that is
linear with sample size. We also provide recipe for incorporating block
shrinkage and general wavelet bases into the framework, all while maintaining
such adaptivity. We demonstrate via extensive numerical experiments that with
our framework even simple 1D Haar wavelets can achieve excellent performance in
the context of 2D and 3D image reconstruction, outperforming state-of-the-art
wavelet and non-wavelet methods especially in noisy, low signal-to-noise ratio
settings at a fraction of the computational cost. Furthermore, we investigate
the source of the gain by quantitatively comparing the efficacy of energy
concentration under our adaptive wavelet transform with that of classical fixed
wavelet transforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Li Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08352">
<title>Asymmetric Variational Autoencoders. (arXiv:1711.08352v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08352</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference for latent variable models is prevalent in various
machine learning problems, typically solved by maximizing the Evidence Lower
Bound (ELBO) of the true data likelihood with respect to a variational
distribution. However, freely enriching the family of variational distribution
is challenging since the ELBO requires variational likelihood evaluations of
the latent variables. In this paper, we propose a novel framework to enrich the
variational family by incorporating auxiliary variables to the variational
family. The resulting inference network doesn&apos;t require density evaluations for
the auxiliary variables and thus complex implicit densities over the auxiliary
variables can be constructed by neural networks. It can be shown that the
actual variational posterior of the proposed approach is essentially modeling a
rich probabilistic mixture of simple variational posterior indexed by auxiliary
variables, thus a flexible inference model can be built. Empirical evaluations
on several density estimation tasks demonstrates the effectiveness of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonell_J/0/1/0/all/0/1&quot;&gt;Jaime Carbonell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08577">
<title>Adaptive Stochastic Dual Coordinate Ascent for Conditional Random Fields. (arXiv:1712.08577v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08577</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the training of conditional random fields (CRFs) via
the stochastic dual coordinate ascent (SDCA) algorithm of Shalev-Shwartz and
Zhang (2016). SDCA enjoys a linear convergence rate and a strong empirical
performance for binary classification problems. However, it has never been used
to train CRFs. Yet it benefits from an `exact&apos; line search with a single
marginalization oracle call, unlike previous approaches. In this paper, we
adapt SDCA to train CRFs, and we enhance it with an adaptive non-uniform
sampling strategy based on block duality gaps. We perform experiments on four
standard sequence prediction tasks. SDCA demonstrates performances on par with
the state of the art, and improves over it on three of the four datasets, which
have in common the use of sparse features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priol_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Le Priol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Piche_A/0/1/0/all/0/1&quot;&gt;Alexandre Pich&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacoste_Julien_S/0/1/0/all/0/1&quot;&gt;Simon Lacoste-Julien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04211">
<title>Towards Arbitrary Noise Augmentation - Deep Learning for Sampling from Arbitrary Probability Distributions. (arXiv:1801.04211v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04211</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate noise modelling is important for training of deep learning
reconstruction algorithms. While noise models are well known for traditional
imaging techniques, the noise distribution of a novel sensor may be difficult
to determine a priori. Therefore, we propose learning arbitrary noise
distributions. To do so, this paper proposes a fully connected neural network
model to map samples from a uniform distribution to samples of any explicitly
known probability density function. During the training, the Jensen-Shannon
divergence between the distribution of the model&apos;s output and the target
distribution is minimized. We experimentally demonstrate that our model
converges towards the desired state. It provides an alternative to existing
sampling methods such as inversion sampling, rejection sampling, Gaussian
mixture models and Markov-Chain-Monte-Carlo. Our model has high sampling
efficiency and is easily applied to any probability distribution, without the
need of further analytical or numerical calculations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horger_F/0/1/0/all/0/1&quot;&gt;Felix Horger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wurfl_T/0/1/0/all/0/1&quot;&gt;Tobias W&amp;#xfc;rfl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1&quot;&gt;Vincent Christlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03607">
<title>On Generation of Adversarial Examples using Convex Programming. (arXiv:1803.03607v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03607</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been observed that deep learning architectures tend to make erroneous
decisions with high reliability for particularly designed adversarial
instances. In this work, we show that the perturbation analysis of these
architectures provides a framework for generating adversarial instances by
convex programming which, for classification tasks, is able to recover variants
of existing non-adaptive adversarial methods. The proposed framework can be
used for the design of adversarial noise under various desirable constraints
and different types of networks. Moreover, this framework is capable of
explaining various existing adversarial methods and can be used to derive new
algorithms as well. Furthermore, we make use of these results to obtain novel
algorithms. Experiments show the competitive performance of the obtained
solutions, in terms of fooling ratio, when benchmarked with well-known
adversarial methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balda_E/0/1/0/all/0/1&quot;&gt;Emilio Rafael Balda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behboodi_A/0/1/0/all/0/1&quot;&gt;Arash Behboodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathar_R/0/1/0/all/0/1&quot;&gt;Rudolf Mathar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11659">
<title>A Unified Particle-Optimization Framework for Scalable Bayesian Sampling. (arXiv:1805.11659v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11659</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been recent interest in developing scalable Bayesian sampling
methods such as stochastic gradient MCMC (SG-MCMC) and Stein variational
gradient descent (SVGD) for big-data analysis. A standard SG-MCMC algorithm
simulates samples from a discrete-time Markov chain to approximate a target
distribution, thus samples could be highly correlated, an undesired property
for SG-MCMC. In contrary, SVGD directly optimizes a set of particles to
approximate a target distribution, and thus is able to obtain good
approximations with relatively much fewer samples. In this paper, we propose a
principle particle-optimization framework based on Wasserstein gradient flows
to unify SG-MCMC and SVGD, and to allow new algorithms to be developed. Our
framework interprets SG-MCMC as particle optimization on the space of
probability measures, revealing a strong connection between SG-MCMC and SVGD.
The key component of our framework is several particle-approximate techniques
to efficiently solve the original partial differential equations on the space
of probability measures. Extensive experiments on both synthetic data and deep
neural networks demonstrate the effectiveness and efficiency of our framework
for scalable Bayesian sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liqun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00068">
<title>Fully Nonparametric Bayesian Additive Regression Trees. (arXiv:1807.00068v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00068</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Additive Regression Trees (BART) is a fully Bayesian approach to
modeling with ensembles of trees. BART can uncover complex regression functions
with high dimensional regressors in a fairly automatic way and provide Bayesian
quantification of the uncertainty through the posterior. However, BART assumes
IID normal errors. This strong parametric assumption can lead to misleading
inference and uncertainty quantification. In this paper, we use the classic
Dirichlet process mixture (DPM) mechanism to nonparametrically model the error
distribution. A key strength of BART is that default prior settings work
reasonably well in a variety of problems. The challenge in extending BART is to
choose the parameters of the DPM so that the strengths of the standard BART
approach is not lost when the errors are close to normal, but the DPM has the
ability to adapt to non-normal errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+George_E/0/1/0/all/0/1&quot;&gt;Edward George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laud_P/0/1/0/all/0/1&quot;&gt;Prakash Laud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Logan_B/0/1/0/all/0/1&quot;&gt;Brent Logan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCulloch_R/0/1/0/all/0/1&quot;&gt;Robert McCulloch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sparapani_R/0/1/0/all/0/1&quot;&gt;Rodney Sparapani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00374">
<title>Augmented Cyclic Adversarial Learning for Domain Adaptation. (arXiv:1807.00374v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00374</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a model to perform a task typically requires a large amount of data
from the domains in which the task will be applied. However, it is often the
case that data are abundant in some domains but scarce in others. Domain
adaptation deals with the challenge of adapting a model trained from a
data-rich source domain to perform well in a data-poor target domain. In
general, this requires learning plausible mappings between domains. CycleGAN is
a powerful framework that efficiently learns to map inputs from one domain to
another using adversarial training and a cycle-consistency constraint. However,
the conventional approach of enforcing cycle-consistency via reconstruction may
be overly restrictive in cases where one or more domains have limited training
data. In this paper, we propose an augmented cyclic adversarial learning model
that enforces the cycle-consistency constraint through an external task
specific model, which encourages the preservation of task-relevant content as
opposed to exact reconstruction. This task specific model both relaxes the
cycle-consistency constraint and complements the role of the discriminator
during training, serving as an augmented information source for learning the
mapping. In the experiment, we adopt a speech recognition model from each
domain as the task specific model. Our approach improves absolute performance
of speech recognition by $2\%$ for female speakers in the TIMIT dataset, where
the majority of training samples are from male voices. We also explore digit
classification with MNIST and SVHN in a low-resource setting and show that our
approach improves absolute performance by $14\%$ and $4\%$ when adapting SVHN
to MNIST and vice versa, respectively. Our approach also outperforms
unsupervised domain adaptation methods, which require high-resource unlabeled
target domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_Asl_E/0/1/0/all/0/1&quot;&gt;Ehsan Hosseini-Asl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01066">
<title>Behaviour Policy Estimation in Off-Policy Policy Evaluation: Calibration Matters. (arXiv:1807.01066v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01066</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we consider the problem of estimating a behaviour policy for
use in Off-Policy Policy Evaluation (OPE) when the true behaviour policy is
unknown. Via a series of empirical studies, we demonstrate how accurate OPE is
strongly dependent on the calibration of estimated behaviour policy models: how
precisely the behaviour policy is estimated from data. We show how powerful
parametric models such as neural networks can result in highly uncalibrated
behaviour policy models on a real-world medical dataset, and illustrate how a
simple, non-parametric, k-nearest neighbours model produces better calibrated
behaviour policy estimates and can be used to obtain superior importance
sampling-based OPE estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghu_A/0/1/0/all/0/1&quot;&gt;Aniruddh Raghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottesman_O/0/1/0/all/0/1&quot;&gt;Omer Gottesman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komorowski_M/0/1/0/all/0/1&quot;&gt;Matthieu Komorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1&quot;&gt;Aldo Faisal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma Brunskill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01085">
<title>One-Class Kernel Spectral Regression for Outlier Detection. (arXiv:1807.01085v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01085</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces a new efficient nonlinear one-class classifier
formulated as the Rayleigh quotient criterion. The method, operating in a
reproducing kernel Hilbert subspace, minimises the scatter of target
distribution along an optimal projection direction while at the same time
keeping projections of target observations as distant as possible from the
origin which serves as an artificial outlier with respect to the data. We
provide a graph embedding view of the problem which can then be solved
efficiently using the spectral regression approach. In this sense, unlike
previous similar methods which often require costly eigen-computations of dense
matrices, the proposed approach casts the problem under consideration into a
regression framework which avoids eigen-decomposition computations. In
particular, it is shown that the dominant complexity of the proposed method is
the complexity of computing the kernel matrix. Additional appealing
characteristics of the proposed one-class classifier are: 1-the ability to be
trained in an incremental fashion (allowing for application in streaming data
scenarios while also reducing computational complexity in the non-streaming
operation mode); 2-being unsupervised while also providing the ability for the
user to specify the expected fraction of outliers in the training set in
advance; And last but not least 3-the deployment of the kernel trick allowing
for a large class of functions by nonlinearly mapping the data into a
high-dimensional feature space. Extensive experiments conducted on several
datasets verifies the merits of the proposed approach in comparison with some
other alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_S/0/1/0/all/0/1&quot;&gt;Shervin Rahimzadeh Arashloo an Josef Kittler&lt;/a&gt;</dc:creator>
</item></rdf:RDF>