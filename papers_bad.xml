<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04509"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06919"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04556"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04565"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04585"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04646"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04674"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04837"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04932"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.01581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.04250"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04478"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04479"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04654"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04665"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04929"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.00624"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.06545"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.09773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08235"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03628"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06678"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04209"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.04509">
<title>Sorting by Swaps with Noisy Comparisons. (arXiv:1803.04509v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.04509</link>
<description rdf:parseType="Literal">&lt;p&gt;We study sorting of permutations by random swaps if each comparison gives the
wrong result with some fixed probability $p&amp;lt;1/2$. We use this process as
prototype for the behaviour of randomized, comparison-based optimization
heuristics in the presence of noisy comparisons. As quality measure, we compute
the expected fitness of the stationary distribution. To measure the runtime, we
compute the minimal number of steps after which the average fitness
approximates the expected fitness of the stationary distribution.
&lt;/p&gt;
&lt;p&gt;We study the process where in each round a random pair of elements at
distance at most $r$ are compared. We give theoretical results for the extreme
cases $r=1$ and $r=n$, and experimental results for the intermediate cases. We
find a trade-off between faster convergence (for large $r$) and better quality
of the solution after convergence (for small $r$).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavenciak_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Gaven&amp;#x10d;iak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geissmann_B/0/1/0/all/0/1&quot;&gt;Barbara Geissmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengler_J/0/1/0/all/0/1&quot;&gt;Johannes Lengler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04773">
<title>A case for multiple and parallel RRAMs as synaptic model for training SNNs. (arXiv:1803.04773v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1803.04773</link>
<description rdf:parseType="Literal">&lt;p&gt;To enable a dense integration of model synapses in a spiking neural networks
hardware, various nano-scale devices are being considered. Such a device,
besides exhibiting spike-time dependent plasticity (STDP), needs to be highly
scalable, have a large endurance and require low energy for transitioning
between states. In this work, we first introduce and empirically determine two
new specifications for an synapse in SNNs: number of conductance levels per
synapse and maximum learning-rate. To the best of our knowledge, there are no
RRAMs that meet the latter specification. As a solution, we propose the use of
multiple PCMO-RRAMs in parallel within a synapse. While synaptic reading, all
PCMO-RRAMs are simultaneously read and for each synaptic conductance-change
event, the mechanism for conductance STDP is initiated for only one RRAM,
randomly picked from the set. Second, to validate our solution, we
experimentally demonstrate STDP of conductance of a PCMO-RRAM and then show
that due to a large learning-rate, a single PCMO-RRAM fails to model a synapse
in the training of an SNN. As anticipated, network training improves as more
PCMO-RRAMs are added to the synapse. Fourth, we discuss the
circuit-requirements for implementing such a scheme, to conclude that the
requirements are within bounds. Thus, our work presents specifications for
synaptic devices in trainable SNNs, indicates the shortcomings of state-of-art
synaptic contenders, and provides a solution to extrinsically meet the
specifications and discusses the peripheral circuitry that implements the
solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1&quot;&gt;Aditya Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_S/0/1/0/all/0/1&quot;&gt;Sidharth Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lashkare_S/0/1/0/all/0/1&quot;&gt;Sandip Lashkare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguly_U/0/1/0/all/0/1&quot;&gt;Udayan Ganguly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04813">
<title>Artificial neural network based modelling approach for municipal solid waste gasification in a fluidized bed reactor. (arXiv:1803.04813v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04813</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, multi-layer feed forward neural networks are used to predict
the lower heating value of gas (LHV), lower heating value of gasification
products including tars and entrained char (LHVp) and syngas yield during
gasification of municipal solid waste (MSW) during gasification in a fluidized
bed reactor. These artificial neural networks (ANNs) with different
architectures are trained using the Levenberg-Marquardt (LM) back-propagation
algorithm and a cross validation is also performed to ensure that the results
generalise to other unseen datasets. A rigorous study is carried out on
optimally choosing the number of hidden layers, number of neurons in the hidden
layer and activation function in a network using multiple Monte Carlo runs.
Nine input and three output parameters are used to train and test various
neural network architectures in both multiple output and single output
prediction paradigms using the available experimental datasets. The model
selection procedure is carried out to ascertain the best network architecture
in terms of predictive accuracy. The simulation results show that the ANN based
methodology is a viable alternative which can be used to predict the
performance of a fluidized bed gasifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_D/0/1/0/all/0/1&quot;&gt;Daya Shankar Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Saptarshi Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_I/0/1/0/all/0/1&quot;&gt;Indranil Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leahy_J/0/1/0/all/0/1&quot;&gt;James J. Leahy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwapinski_W/0/1/0/all/0/1&quot;&gt;Witold Kwapinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06917">
<title>Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics. (arXiv:1709.06917v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06917</link>
<description rdf:parseType="Literal">&lt;p&gt;The most data-efficient algorithms for reinforcement learning in robotics are
model-based policy search algorithms, which alternate between learning a
dynamical model of the robot and optimizing a policy to maximize the expected
return given the model and its uncertainties. Among the few proposed
approaches, the recently introduced Black-DROPS algorithm exploits a black-box
optimization algorithm to achieve both high data-efficiency and good
computation times when several cores are used; nevertheless, like all
model-based policy search approaches, Black-DROPS does not scale to high
dimensional state/action spaces. In this paper, we introduce a new model
learning procedure in Black-DROPS that leverages parameterized black-box priors
to (1) scale up to high-dimensional systems, and (2) be robust to large
inaccuracies of the prior information. We demonstrate the effectiveness of our
approach with the &quot;pendubot&quot; swing-up task in simulation and with a physical
hexapod robot (48D state space, 18D action space) that has to walk forward as
fast as possible. The results show that our new algorithm is more
data-efficient than previous model-based policy search algorithms (with and
without priors) and that it can allow a physical 6-legged robot to learn new
gaits in only 16 to 30 seconds of interaction time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzilygeroudis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Chatzilygeroudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06919">
<title>Bayesian Optimization with Automatic Prior Selection for Data-Efficient Direct Policy Search. (arXiv:1709.06919v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06919</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most interesting features of Bayesian optimization for direct
policy search is that it can leverage priors (e.g., from simulation or from
previous tasks) to accelerate learning on a robot. In this paper, we are
interested in situations for which several priors exist but we do not know in
advance which one fits best the current situation. We tackle this problem by
introducing a novel acquisition function, called Most Likely Expected
Improvement (MLEI), that combines the likelihood of the priors and the expected
improvement. We evaluate this new acquisition function on a transfer learning
task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has
to learn to walk on flat ground and on stairs, with priors corresponding to
different stairs and different kinds of damages. Our results show that MLEI
effectively identifies and exploits the priors, even when there is no obvious
match between the current situations and the priors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pautrat_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Pautrat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzilygeroudis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Chatzilygeroudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03806">
<title>ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Neural Network Accelerators. (arXiv:1802.03806v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03806</link>
<description rdf:parseType="Literal">&lt;p&gt;Hardware accelerators are being increasingly deployed to boost the
performance and energy efficiency of deep neural network (DNN) inference. In
this paper we propose Thundervolt, a new framework that enables aggressive
voltage underscaling of high-performance DNN accelerators without compromising
classification accuracy even in the presence of high timing error rates. Using
post-synthesis timing simulations of a DNN accelerator modeled on the Google
TPU, we show that Thundervolt enables between 34%-57% energy savings on
state-of-the-art speech and image recognition benchmarks with less than 1% loss
in classification accuracy and no performance loss. Further, we show that
Thundervolt is synergistic with and can further increase the energy efficiency
of commonly used run-time DNN pruning techniques like Zero-Skip.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jeff Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangineni_K/0/1/0/all/0/1&quot;&gt;Kartheek Rangineni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddharth Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04459">
<title>Clustering with Simultaneous Local and Global View of Data: A message passing based approach. (arXiv:1803.04459v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04459</link>
<description rdf:parseType="Literal">&lt;p&gt;A good clustering algorithm should not only be able to discover clusters of
arbitrary shapes (global view) but also provide additional information, which
can be used to gain more meaningful insights into the internal structure of the
clusters (local view). In this work we use the mathematical framework of factor
graphs and message passing algorithms to optimize a pairwise similarity based
cost function, in the same spirit as was done in Affinity Propagation. Using
this framework we develop two variants of a new clustering algorithm, EAP and
SHAPE. EAP/SHAPE can not only discover clusters of arbitrary shapes but also
provide a rich local view in the form of meaningful local representatives
(exemplars) and connections between these local exemplars. We discuss how this
local information can be used to gain various insights about the clusters
including varying relative cluster densities and indication of local strength
in different regions of a cluster . We also discuss how this can help an
analyst in discovering and resolving potential inconsistencies in the results.
The efficacy of EAP/SHAPE is shown by applying it to various synthetic and real
world benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1&quot;&gt;Rayyan Ahmad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1&quot;&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinsteuber_M/0/1/0/all/0/1&quot;&gt;Martin Kleinsteuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04474">
<title>Predicting Crime Using Spatial Features. (arXiv:1803.04474v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.04474</link>
<description rdf:parseType="Literal">&lt;p&gt;Our study aims to build a machine learning model for crime prediction using
geospatial features for different categories of crime. The reverse geocoding
technique is applied to retrieve open street map (OSM) spatial data. This study
also proposes finding hotpoints extracted from crime hotspots area found by
Hierarchical Density-Based Spatial Clustering of Applications with Noise
(HDBSCAN). A spatial distance feature is then computed based on the position of
different hotpoints for various types of crime and this value is used as a
feature for classifiers. We test the engineered features in crime data from
Royal Canadian Mounted Police of Halifax, NS. We observed a significant
performance improvement in crime prediction using the new generated spatial
features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bappee_F/0/1/0/all/0/1&quot;&gt;Fateha Khanam Bappee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1&quot;&gt;Amilcar Soares Junior&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04551">
<title>Multi-Sensor Conflict Measurement and Information Fusion. (arXiv:1803.04551v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1803.04551</link>
<description rdf:parseType="Literal">&lt;p&gt;In sensing applications where multiple sensors observe the same scene, fusing
sensor outputs can provide improved results. However, if some of the sensors
are providing lower quality outputs, the fused results can be degraded. In this
work, a multi-sensor conflict measure is proposed which estimates multi-sensor
conflict by representing each sensor output as interval-valued information and
examines the sensor output overlaps on all possible n-tuple sensor
combinations. The conflict is based on the sizes of the intervals and how many
sensors output values lie in these intervals. In this work, conflict is defined
in terms of how little the output from multiple sensors overlap. That is, high
degrees of overlap mean low sensor conflict, while low degrees of overlap mean
high conflict. This work is a preliminary step towards a robust conflict and
sensor fusion framework. In addition, a sensor fusion algorithm is proposed
based on a weighted sum of sensor outputs, where the weights for each sensor
diminish as the conflict measure increases. The proposed methods can be
utilized to (1) assess a measure of multi-sensor conflict, and (2) improve
sensor output fusion by lessening weighting for sensors with high conflict.
Using this measure, a simulated example is given to explain the mechanics of
calculating the conflict measure, and stereo camera 3D outputs are analyzed and
fused. In the stereo camera case, the sensor output is corrupted by additive
impulse noise, DC offset, and Gaussian noise. Impulse noise is common in
sensors due to intermittent interference, a DC offset a sensor bias or
registration error, and Gaussian noise represents a sensor output with low SNR.
The results show that sensor output fusion based on the conflict measure shows
improved accuracy over a simple averaging fusion strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Pan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ball_J/0/1/0/all/0/1&quot;&gt;John E. Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Derek T. Anderson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04556">
<title>Measuring Conflict in a Multi-Source Environment as a Normal Measure. (arXiv:1803.04556v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1803.04556</link>
<description rdf:parseType="Literal">&lt;p&gt;In a multi-source environment, each source has its own credibility. If there
is no external knowledge about credibility then we can use the information
provided by the sources to assess their credibility. In this paper, we propose
a way to measure conflict in a multi-source environment as a normal measure. We
examine our algorithm using three simulated examples of increasing conflict and
one experimental example. The results demonstrate that the proposed measure can
represent conflict in a meaningful way similar to what a human might expect and
from it we can identify conflict within our sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Pan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ball_J/0/1/0/all/0/1&quot;&gt;John E. Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Derek T. Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harsh_A/0/1/0/all/0/1&quot;&gt;Archit Harsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Archibald_C/0/1/0/all/0/1&quot;&gt;Christopher Archibald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04565">
<title>Learning to recognize Abnormalities in Chest X-Rays with Location-Aware Dense Networks. (arXiv:1803.04565v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.04565</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-ray is the most common medical imaging exam used to assess multiple
pathologies. Automated algorithms and tools have the potential to support the
reading workflow, improve efficiency, and reduce reading errors. With the
availability of large scale data sets, several methods have been proposed to
classify pathologies on chest X-ray images. However, most methods report
performance based on random image based splitting, ignoring the high
probability of the same patient appearing in both training and test set. In
addition, most methods fail to explicitly incorporate the spatial information
of abnormalities or utilize the high resolution images. We propose a novel
approach based on location aware Dense Networks (DNetLoc), whereby we
incorporate both high-resolution image data and spatial information for
abnormality classification. We evaluate our method on the largest data set
reported in the community, containing a total of 86,876 patients and 297,541
chest X-ray images. We achieve (i) the best average AUC score for published
training and test splits on the single benchmarking data set (ChestX-Ray14),
and (ii) improved AUC scores when the pathology location information is
explicitly used. To foster future research we demonstrate the limitations of
the current benchmarking setup and provide new reference patient-wise splits
for the used data sets. This could support consistent and meaningful
benchmarking of future methods on the largest publicly available data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guendel_S/0/1/0/all/0/1&quot;&gt;Sebastian Guendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grbic_S/0/1/0/all/0/1&quot;&gt;Sasa Grbic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgescu_B/0/1/0/all/0/1&quot;&gt;Bogdan Georgescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kevin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritschl_L/0/1/0/all/0/1&quot;&gt;Ludwig Ritschl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_A/0/1/0/all/0/1&quot;&gt;Andreas Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comaniciu_D/0/1/0/all/0/1&quot;&gt;Dorin Comaniciu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04585">
<title>Categorizing Variants of Goodhart&apos;s Law. (arXiv:1803.04585v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.04585</link>
<description rdf:parseType="Literal">&lt;p&gt;There are several distinct failure modes for overoptimization of systems on
the basis of metrics. This occurs when a metric which can be used to improve a
system is used to an extent that further optimization is ineffective or
harmful, and is sometimes termed Goodhart&apos;s Law. This class of failure is often
poorly understood, partly because terminology for discussing them is ambiguous,
and partly because discussion using this ambiguous terminology ignores
distinctions between different failure modes of this general type. This paper
expands on an earlier discussion by Garrabrant, which notes there are &quot;(at
least) four different mechanisms&quot; that relate to Goodhart&apos;s Law. This paper is
intended to explore these mechanisms further, and specify more clearly how they
occur. This discussion should be helpful in better understanding these types of
failures in economic regulation, in public policy, in machine learning, and in
Artificial Intelligence alignment. The importance of Goodhart effects depends
on the amount of power directed towards optimizing the proxy, and so the
increased optimization power offered by artificial intelligence makes it
especially critical for that field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manheim_D/0/1/0/all/0/1&quot;&gt;David Manheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrabrant_S/0/1/0/all/0/1&quot;&gt;Scott Garrabrant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04646">
<title>On Cryptographic Attacks Using Backdoors for SAT. (arXiv:1803.04646v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.04646</link>
<description rdf:parseType="Literal">&lt;p&gt;Propositional satisfiability (SAT) is at the nucleus of state-of-the-art
approaches to a variety of computationally hard problems, one of which is
cryptanalysis. Moreover, a number of practical applications of SAT can only be
tackled efficiently by identifying and exploiting a subset of formula&apos;s
variables called backdoor set (or simply backdoors). This paper proposes a new
class of backdoor sets for SAT used in the context of cryptographic attacks,
namely guess-and-determine attacks. The idea is to identify the best set of
backdoor variables subject to a statistically estimated hardness of the
guess-and-determine attack using a SAT solver. Experimental results on weakened
variants of the renowned encryption algorithms exhibit advantage of the
proposed approach compared to the state of the art in terms of the estimated
hardness of the resulting guess-and-determine attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semenov_A/0/1/0/all/0/1&quot;&gt;Alexander Semenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaikin_O/0/1/0/all/0/1&quot;&gt;Oleg Zaikin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otpuschennikov_I/0/1/0/all/0/1&quot;&gt;Ilya Otpuschennikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochemazov_S/0/1/0/all/0/1&quot;&gt;Stepan Kochemazov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1&quot;&gt;Alexey Ignatiev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04674">
<title>Hierarchical Reinforcement Learning: Approximating Optimal Discounted TSP Using Local Policies. (arXiv:1803.04674v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04674</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we provide theoretical guarantees for reward decomposition in
deterministic MDPs. Reward decomposition is a special case of Hierarchical
Reinforcement Learning, that allows one to learn many policies in parallel and
combine them into a composite solution. Our approach builds on mapping this
problem into a Reward Discounted Traveling Salesman Problem, and then deriving
approximate solutions for it. In particular, we focus on approximate solutions
that are local, i.e., solutions that only observe information about the current
state. Local policies are easy to implement and do not require substantial
computational resources as they do not perform planning. While local
deterministic policies, like Nearest Neighbor, are being used in practice for
hierarchical reinforcement learning, we propose three stochastic policies that
guarantee better performance than any deterministic policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1&quot;&gt;Tom Zahavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasidim_A/0/1/0/all/0/1&quot;&gt;Avinatan Hasidim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1&quot;&gt;Haim Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1&quot;&gt;Yishay Mansour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04837">
<title>Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction. (arXiv:1803.04837v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of a large amount of electronic health records (EHR)
provides huge opportunities to improve health care service by mining these
data. One important application is clinical endpoint prediction, which aims to
predict whether a disease, a symptom or an abnormal lab test will happen in the
future according to patients&apos; history records. This paper develops deep
learning techniques for clinical endpoint prediction, which are effective in
many practical applications. However, the problem is very challenging since
patients&apos; history records contain multiple heterogeneous temporal events such
as lab tests, diagnosis, and drug administrations. The visiting patterns of
different types of events vary significantly, and there exist complex nonlinear
relationships between different events. In this paper, we propose a novel model
for learning the joint representation of heterogeneous temporal events. The
model adds a new gate to control the visiting rates of different events which
effectively models the irregular patterns of different events and their
nonlinear correlations. Experiment results with real-world clinical data on the
tasks of predicting death and abnormal lab tests prove the effectiveness of our
proposed approach over competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Luchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jianhao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04848">
<title>Soft-Robust Actor-Critic Policy-Gradient. (arXiv:1803.04848v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04848</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust Reinforcement Learning aims to derive an optimal behavior that
accounts for model uncertainty in dynamical systems. However, previous studies
have shown that by considering the worst case scenario, robust policies can be
overly conservative. Our \textit{soft-robust} framework is an attempt to
overcome this issue. In this paper, we present a novel Soft-Robust Actor-Critic
algorithm (SR-AC). It learns an optimal policy with respect to a distribution
over an uncertainty set and stays robust to model uncertainty but avoids the
conservativeness of robust strategies. We show convergence of the SR-AC and
test the efficiency of our approach on different domains by comparing it
against regular learning methods and their robust formulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derman_E/0/1/0/all/0/1&quot;&gt;Esther Derman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1&quot;&gt;Daniel J. Mankowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_T/0/1/0/all/0/1&quot;&gt;Timothy A. Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04927">
<title>An Agent-Based Simulation of Residential Location Choice of Tenants in Tehran, Iran. (arXiv:1803.04927v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1803.04927</link>
<description rdf:parseType="Literal">&lt;p&gt;Residential location choice modeling is one of the substantial components of
land use and transportation models. While numerous aggregated mathematical and
statistical approaches have been developed to model the residence choice
behavior of households, disaggregated approaches such as the agent-based
modeling have shown interesting capabilities. In this article, a novel
agent-based approach is developed to simulate the residential location choice
of tenants in Tehran, the capital of Iran. Tenants are considered as agents who
select their desired residential alternatives according to their
characteristics and preferences for various criteria such as the rent,
accessibility to different services and facilities, environmental pollution,
and distance from their workplace and former residence. The choice set of
agents is limited to their desired residential alternatives by applying a
constrained NSGA-II algorithm. Then, agents compete with each other to select
their final residence among their alternatives. Results of the proposed
approach are validated by comparing simulated and actual residences of a sample
of tenants. Results show that the proposed approach is able to accurately
simulate the residence of 59.3% of tenants at the traffic analysis zone level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babakan_A/0/1/0/all/0/1&quot;&gt;A. Shirzadi Babakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alimohammadi_A/0/1/0/all/0/1&quot;&gt;A. Alimohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04932">
<title>Impacts of transport development on residence choice of renter households: An agent-based evaluation. (arXiv:1803.04932v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1803.04932</link>
<description rdf:parseType="Literal">&lt;p&gt;Because of improving accessibility, transport developments play an important
role in residence choice of renter households. In this paper, an agent-based
model is developed to investigate impacts of different transport developments
on residence choice of renter households in Tehran, the capital of Iran. In the
proposed model, renter households are considered as agents who make a
multi-objective decision and compete with each other to rent a preferred
residential zone. Then, three transport development scenarios including
construction a new highway, subway and bus rapid transit (BRT) line are
simulated and resulting changes in residence choice of agents are evaluated.
Results show that transport development scenarios significantly affect
residence choice behavior of different socio-economic categories of renter
households and lead to considerable changes in the residential demand,
composition of residents, mean income level and mean car ownership in their
vicinities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babakan_A/0/1/0/all/0/1&quot;&gt;A. S. Babakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taleai_M/0/1/0/all/0/1&quot;&gt;M. Taleai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04934">
<title>An agent-based evaluation of impacts of transport developments on the modal shift in Tehran, Iran. (arXiv:1803.04934v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1803.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;Changes in travel modes used by people, particularly reduction of the private
car use, is an important determinant of effectiveness of transportation plans.
Because of dependencies between the choices of residential location and travel
mode, integrated modelling of these choices has been proposed by some
researchers. In this paper, an agent-based microsimulation model has been
developed to evaluate impacts of different transport development plans on
choices of residential location and commuting mode of tenant households in
Tehran, the capital of Iran. In the proposed model, households are considered
as agents who select their desired residential location using a constrained
NSGA-II algorithm and in a competition with other households. In addition, they
choose their commuting mode by applying a multi-criteria decision making
method. Afterwards, effects of development of a new highway, subway and bus
rapid transit (BRT) line on their residential location and commuting mode
choices are evaluated. Results show that despite the residential self-selection
effects, these plans result in considerable changes in the commuting mode of
different socioeconomic categories of households. Development of the new subway
line shows promising results by reducing the private car use among the all
socio-economic categories of households. But the new highway development
unsatisfactorily results in increase in the private car use. In addition,
development of the new BRT line does not show significant effects on the
commuting mode change, particularly on decrease in the private car use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babakan_A/0/1/0/all/0/1&quot;&gt;A. Shirzadi Babakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alimohammadi_A/0/1/0/all/0/1&quot;&gt;A. Alimohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taleai_M/0/1/0/all/0/1&quot;&gt;M. Taleai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.01581">
<title>Causal inference for cloud computing. (arXiv:1603.01581v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1603.01581</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud computing involves complex technical and economical systems and
interactions. This brings about various challenges, two of which are: (1)
debugging and control of computing systems with the help of sandbox
experiments, and (2) prediction of the cost of &quot;spot&quot; resources for decision
making of cloud clients. In this paper, we formalize debugging by
counterfactual probabilities and control by post-(soft-)interventional
probabilities. We prove that counterfactuals can approximately be calculated
from a &quot;stochastic&quot; graphical causal model (while they are originally defined
only for &quot;deterministic&quot; functional causal models), and based on this sketch an
approach to address problem (1). To address problem (2), we formalize bidding
by post-(soft-)interventional probabilities and present a simple mathematical
result on approximate integration of &quot;incomplete&quot; conditional probability
distributions. We show how this can be used by cloud clients to trade off
privacy against predictability of the outcome of their bidding actions in a toy
scenario. We report experiments on simulated and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_P/0/1/0/all/0/1&quot;&gt;Philipp Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carata_L/0/1/0/all/0/1&quot;&gt;Lucian Carata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Schoelkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.04250">
<title>Experimental and causal view on information integration in autonomous agents. (arXiv:1606.04250v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1606.04250</link>
<description rdf:parseType="Literal">&lt;p&gt;The amount of digitally available but heterogeneous information about the
world is remarkable, and new technologies such as self-driving cars, smart
homes, or the internet of things may further increase it. In this paper we
present preliminary ideas about certain aspects of the problem of how such
heterogeneous information can be harnessed by autonomous agents. After
discussing potentials and limitations of some existing approaches, we
investigate how \emph{experiments} can help to obtain a better understanding of
the problem. Specifically, we present a simple agent that integrates video data
from a different agent, and implement and evaluate a version of it on the novel
experimentation platform \emph{Malmo}. The focus of a second investigation is
on how information about the hardware of different agents, the agents&apos; sensory
data, and \emph{causal} information can be utilized for knowledge transfer
between agents and subsequently more data-efficient decision making. Finally,
we discuss potential future steps w.r.t.\ theory and experimentation, and
formulate open questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_P/0/1/0/all/0/1&quot;&gt;Philipp Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1&quot;&gt;Katja Hofmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07822">
<title>Geometrical Insights for Implicit Generative Modeling. (arXiv:1712.07822v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07822</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning algorithms for implicit generative models can optimize a variety of
criteria that measure how the data distribution differs from the implicit model
distribution, including the Wasserstein distance, the Energy distance, and the
Maximum Mean Discrepancy criterion. A careful look at the geometries induced by
these distances on the space of probability measures reveals interesting
differences. In particular, we can establish surprising approximate global
convergence guarantees for the $1$-Wasserstein distance,even when the
parametric generator has a nonconvex parametrization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1&quot;&gt;Leon Bottou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arjovsky_M/0/1/0/all/0/1&quot;&gt;Martin Arjovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopez_Paz_D/0/1/0/all/0/1&quot;&gt;David Lopez-Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oquab_M/0/1/0/all/0/1&quot;&gt;Maxime Oquab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04431">
<title>Scalable Algorithms for Learning High-Dimensional Linear Mixed Models. (arXiv:1803.04431v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04431</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear mixed models (LMMs) are used extensively to model dependecies of
observations in linear regression and are used extensively in many application
areas. Parameter estimation for LMMs can be computationally prohibitive on big
data. State-of-the-art learning algorithms require computational complexity
which depends at least linearly on the dimension $p$ of the covariates, and
often use heuristics that do not offer theoretical guarantees. We present
scalable algorithms for learning high-dimensional LMMs with sublinear
computational complexity dependence on $p$. Key to our approach are novel dual
estimators which use only kernel functions of the data, and fast computational
techniques based on the subsampled randomized Hadamard transform. We provide
theoretical guarantees for our learning algorithms, demonstrating the
robustness of parameter estimation. Finally, we complement the theory with
experiments on large synthetic and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zilong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roche_K/0/1/0/all/0/1&quot;&gt;Kimberly Roche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Sayan Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04464">
<title>False Discovery Rate Control via Debiased Lasso. (arXiv:1803.04464v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1803.04464</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of variable selection in high-dimensional statistical
models where the goal is to report a set of variables, out of many predictors
$X_1, \dotsc, X_p$, that are relevant to a response of interest. For linear
high-dimensional model, where the number of parameters exceeds the number of
samples $(p&amp;gt;n)$, we propose a procedure for variables selection and prove that
it controls the \emph{directional} false discovery rate (FDR) below a
pre-assigned significance level $q\in [0,1]$. We further analyze the
statistical power of our framework and show that for designs with subgaussian
rows and a common precision matrix $\Omega\in\mathbb{R}^{p\times p}$, if the
minimum nonzero parameter $\theta_{\min}$ satisfies $$\sqrt{n} \theta_{\min} -
\sigma \sqrt{2(\max_{i\in [p]}\Omega_{ii})\log\left(\frac{2p}{qs_0}\right)} \to
\infty\,,$$ then this procedure achieves asymptotic power one.
&lt;/p&gt;
&lt;p&gt;Our framework is built upon the debiasing approach and assumes the standard
condition $s_0 = o(\sqrt{n}/(\log p)^2)$, where $s_0$ indicates the number of
true positives among the $p$ features. Notably, this framework achieves exact
directional FDR control without any assumption on the amplitude of unknown
regression parameters, and does not require any knowledge of the distribution
of covariates or the noise level. We test our method in synthetic and real data
experiments to asses its performance and to corroborate our theoretical
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Javanmard_A/0/1/0/all/0/1&quot;&gt;Adel Javanmard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Javadi_H/0/1/0/all/0/1&quot;&gt;Hamid Javadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04475">
<title>Accuracy-Reliability Cost Function for Empirical Variance Estimation. (arXiv:1803.04475v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04475</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we focus on the problem of assigning uncertainties to
single-point predictions. We introduce a cost function that encodes the
trade-off between accuracy and reliability in probabilistic forecast. We derive
analytic formula for the case of forecasts of continuous scalar variables
expressed in terms of Gaussian distributions. The Accuracy-Reliability cost
function can be used to empirically estimate the variance in heteroskedastic
regression problems (input dependent noise), by solving a two-objective
optimization problem. The simple philosophy behind this strategy is that
predictions based on the estimated variances should be both accurate and
reliable (i.e. statistical consistent with observations). We show several
examples with synthetic data, where the underlying hidden noise function can be
accurately recovered, both in one and multi-dimensional problems. The practical
implementation of the method has been done using a Neural Network and, in the
one-dimensional case, with a simple polynomial fit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Camporeale_E/0/1/0/all/0/1&quot;&gt;Enrico Camporeale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04478">
<title>Bridge type classification: supervised learning on a modified NBI dataset. (arXiv:1803.04478v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04478</link>
<description rdf:parseType="Literal">&lt;p&gt;A key phase in the bridge design process is the selection of the structural
system. Due to budget and time constraints, engineers typically rely on
engineering judgment and prior experience when selecting a structural system,
often considering a limited range of design alternatives. The objective of this
study was to explore the suitability of supervised machine learning as a
preliminary design aid that provides guidance to engineers with regards to the
statistically optimal bridge type to choose, ultimately improving the
likelihood of optimized design, design standardization, and reduced maintenance
costs. In order to devise this supervised learning system, data for over
600,000 bridges from the National Bridge Inventory database were analyzed. Key
attributes for determining the bridge structure type were identified through
three feature selection techniques. Potentially useful attributes like seismic
intensity and historic data on the cost of materials (steel and concrete) were
then added from the US Geological Survey (USGS) database and Engineering News
Record. Decision tree, Bayes network and Support Vector Machines were used for
predicting the bridge design type. Due to state-to-state variations in material
availability, material costs, and design codes, supervised learning models
based on the complete data set did not yield favorable results. Supervised
learning models were then trained and tested using 10-fold cross validation on
data for each state. Inclusion of seismic data improved the model performance
noticeably. The data was then resampled to reduce the bias of the models
towards more common design types, and the supervised learning models thus
constructed showed further improvements in performance. The average recall and
precision for the state models was 88.6% and 88.0% using Decision Trees, 84.0%
and 83.7% using Bayesian Networks, and 80.8% and 75.6% using SVM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jootoo_A/0/1/0/all/0/1&quot;&gt;Achyuthan Jootoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lattanzi_D/0/1/0/all/0/1&quot;&gt;David Lattanzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04479">
<title>Machine Learning Harnesses Molecular Dynamics to Discover New $\mu$ Opioid Chemotypes. (arXiv:1803.04479v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/1803.04479</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational chemists typically assay drug candidates by virtually screening
compounds against crystal structures of a protein despite the fact that some
targets, like the $\mu$ Opioid Receptor and other members of the GPCR family,
traverse many non-crystallographic states. We discover new conformational
states of $\mu OR$ with molecular dynamics simulation and then machine learn
ligand-structure relationships to predict opioid ligand function. These
artificial intelligence models identified a novel $\mu$ opioid chemotype.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Feinberg_E/0/1/0/all/0/1&quot;&gt;Evan N. Feinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Farimani_A/0/1/0/all/0/1&quot;&gt;Amir Barati Farimani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Uprety_R/0/1/0/all/0/1&quot;&gt;Rajendra Uprety&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hunkele_A/0/1/0/all/0/1&quot;&gt;Amanda Hunkele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pasternak_G/0/1/0/all/0/1&quot;&gt;Gavril W. Pasternak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Susruta Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay S. Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04494">
<title>Gradient Augmented Information Retrieval with Autoencoders and Semantic Hashing. (arXiv:1803.04494v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1803.04494</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper will explore the use of autoencoders for semantic hashing in the
context of Information Retrieval. This paper will summarize how to efficiently
train an autoencoder in order to create meaningful and low-dimensional
encodings of data. This paper will demonstrate how computing and storing the
closest encodings to an input query can help speed up search time and improve
the quality of our search results. The novel contributions of this paper
involve using the representation of the data learned by an auto-encoder in
order to augment our search query in various ways. I present and evaluate the
new gradient search augmentation (GSA) approach, as well as the more well-known
pseudo-relevance-feedback (PRF) adjustment. I find that GSA helps to improve
the performance of the TF-IDF based information retrieval system, and PRF
combined with GSA works best overall for the systems compared in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Billings_S/0/1/0/all/0/1&quot;&gt;Sean Billings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04497">
<title>Automated software vulnerability detection with machine learning. (arXiv:1803.04497v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1803.04497</link>
<description rdf:parseType="Literal">&lt;p&gt;Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harer_J/0/1/0/all/0/1&quot;&gt;Jacob A. Harer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_L/0/1/0/all/0/1&quot;&gt;Louis Y. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_R/0/1/0/all/0/1&quot;&gt;Rebecca L. Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdemir_O/0/1/0/all/0/1&quot;&gt;Onur Ozdemir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosta_L/0/1/0/all/0/1&quot;&gt;Leonard R. Kosta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangamani_A/0/1/0/all/0/1&quot;&gt;Akshay Rangamani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_L/0/1/0/all/0/1&quot;&gt;Lei H. Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Centeno_G/0/1/0/all/0/1&quot;&gt;Gabriel I. Centeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Key_J/0/1/0/all/0/1&quot;&gt;Jonathan R. Key&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellingwood_P/0/1/0/all/0/1&quot;&gt;Paul M. Ellingwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McConley_M/0/1/0/all/0/1&quot;&gt;Marc W. McConley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Opper_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Opper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1&quot;&gt;Peter Chin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazovich_T/0/1/0/all/0/1&quot;&gt;Tomo Lazovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04547">
<title>Analysis of spectral clustering algorithms for community detection: the general bipartite setting. (arXiv:1803.04547v1 [math.ST])</title>
<link>http://arxiv.org/abs/1803.04547</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the analysis of spectral clustering algorithms for community
detection under a stochastic block model (SBM). A general spectral clustering
algorithm consists of three steps: (1) regularization of an appropriate
adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a
k-means type algorithm in the reduced spectral domain. By varying each step,
one can obtain different spectral algorithms. In light of the recent
developments in refining consistency results for the spectral clustering, we
identify the necessary bounds at each of these three steps, and then derive and
compare consistency results for some existing spectral algorithms as well as a
new variant that we propose. The focus of the paper is on providing a better
understanding of the analysis of spectral methods for community detection, with
an emphasis on the bipartite setting which has received less theoretical
consideration. We show how the variations in the spectral truncation step
reflects in the consistency results under a general SBM. We also investigate
the necessary bounds for the k-means step in some detail, allowing one to
replace this step with any algorithm (k-means type or otherwise) that
guarantees the necessary bound. We discuss some of the neglected aspects of the
bipartite setting, e.g., the role of the mismatch between the communities of
the two sides on the performance of spectral methods. Finally, we show how the
consistency results can be extended beyond SBMs to the problem of clustering
inhomogeneous random graph models that can be approximated by SBMs in a certain
sense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhixin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Amini_A/0/1/0/all/0/1&quot;&gt;Arash A. Amini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04566">
<title>Compact Convolutional Neural Networks for Classification of Asynchronous Steady-state Visual Evoked Potentials. (arXiv:1803.04566v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04566</link>
<description rdf:parseType="Literal">&lt;p&gt;Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from
the parietal and occipital regions of the brain that are evoked from flickering
visual stimuli. SSVEPs are robust signals measurable in the
electroencephalogram (EEG) and are commonly used in brain-computer interfaces
(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require
hand-crafted approaches that leverage domain-specific knowledge of the stimulus
signals, such as specific temporal frequencies in the visual stimuli and their
relative spatial arrangement. When this knowledge is unavailable, such as when
SSVEP signals are acquired asynchronously, such approaches tend to fail. In
this paper, we show how a compact convolutional neural network (Compact-CNN),
which only requires raw EEG signals for automatic feature extraction, can be
used to decode signals from a 12-class SSVEP dataset without the need for any
domain-specific knowledge or calibration data. We report across subject mean
accuracy of approximately 80% (chance being 8.3%) and show this is
substantially better than current state-of-the-art hand-crafted approaches
using canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we
analyze our Compact-CNN to examine the underlying feature representation,
discovering that the deep learner extracts additional phase and amplitude
related features associated with the structure of the dataset. We discuss how
our Compact-CNN shows promise for BCI applications that allow users to freely
gaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as
provides a method for analyzing SSVEP signals in a way that might augment our
understanding about the basic processing in the visual cortex.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Waytowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1&quot;&gt;Vernon Lawhern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1&quot;&gt;Javier O. Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummings_J/0/1/0/all/0/1&quot;&gt;Jennifer Cummings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faller_J/0/1/0/all/0/1&quot;&gt;Josef Faller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajda_P/0/1/0/all/0/1&quot;&gt;Paul Sajda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vettel_J/0/1/0/all/0/1&quot;&gt;Jean M. Vettel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04572">
<title>COPA: Constrained PARAFAC2 for Sparse &amp; Large Datasets. (arXiv:1803.04572v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04572</link>
<description rdf:parseType="Literal">&lt;p&gt;PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is jointly
modeling treatments across a set of patients with varying number of medical
encounters, where the alignment of events in time bears no clinical meaning,
and it may also be impossible to align them due to their varying length.
Despite recent improvements on scaling up unconstrained PARAFAC2, its model
factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a COnstrained PARAFAC2 (COPA)
method, which carefully incorporates optimization constraints such as temporal
smoothness, sparsity, and non-negativity in the resulting factors. To
efficiently support all those constraints, COPA adopts a hybrid optimization
framework using alternating optimization and alternating direction method of
multiplier (AO-ADMM). As evaluated on large electronic health record (EHR)
datasets with hundreds of thousands of patients, COPA achieves significant
speedups (up to 36x faster) over prior PARAFAC2 approaches that only attempt to
handle a subset of the constraints that COPA enables. Overall, our method
outperforms all the baselines attempting to handle a subset of the constraints
in terms of speed, while achieving the same level of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afshar_A/0/1/0/all/0/1&quot;&gt;Ardavan Afshar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perros_I/0/1/0/all/0/1&quot;&gt;Ioakeim Perros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Searles_E/0/1/0/all/0/1&quot;&gt;Elizabeth Searles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1&quot;&gt;Joyce Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04654">
<title>Simulation and Calibration of a Fully Bayesian Marked Multidimensional Hawkes Process with Dissimilar Decays. (arXiv:1803.04654v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04654</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simulation method for multidimensional Hawkes processes based on
superposition theory of point processes. This formulation allows us to design
efficient simulations for Hawkes processes with differing exponentially
decaying intensities. We demonstrate that inter-arrival times can be decomposed
into simpler auxiliary variables that can be sampled directly, giving exact
simulation with no approximation. We establish that the auxiliary variables
provides information on the parent process for each event time. The algorithm
correctness is shown by verifying the simulated intensities with their
theoretical moments. A modular inference procedure consisting of Gibbs samplers
through the auxiliary variable augmentation and adaptive rejection sampling is
presented. Finally, we compare our proposed simulation method against existing
methods, and find significant improvement in terms of algorithm speed. Our
inference algorithm is used to discover the strengths of mutually excitations
in real dark networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lim_K/0/1/0/all/0/1&quot;&gt;Kar Wai Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Young Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanlen_L/0/1/0/all/0/1&quot;&gt;Leif Hanlen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hongbiao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04663">
<title>Binary Matrix Completion Using Unobserved Entries. (arXiv:1803.04663v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04663</link>
<description rdf:parseType="Literal">&lt;p&gt;A matrix completion problem, which aims to recover a complete matrix from its
partial observations, is one of the important problems in the machine learning
field and has been studied actively. However, there is a discrepancy between
the mainstream problem setting, which assumes continuous-valued observations,
and some practical applications such as recommendation systems and SNS link
predictions where observations take discrete or even binary values. To cope
with this problem, Davenport et al. (2014) proposed a binary matrix completion
(BMC) problem, where observations are quantized into binary values. Hsieh et
al. (2015) proposed a PU (Positive and Unlabeled) matrix completion problem,
which is an extension of the BMC problem. This problem targets the setting
where we cannot observe negative values, such as SNS link predictions. In the
construction of their method for this setting, they introduced a methodology of
the classification problem, regarding each matrix entry as a sample. Their
risk, which defines losses over unobserved entries as well, indicates the
possibility of the use of unobserved entries. In this paper, motivated by a
semi-supervised classification method recently proposed by Sakai et al. (2017),
we develop a method for the BMC problem which can use all of positive,
negative, and unobserved entries, by combining the risks of Davenport et al.
(2014) and Hsieh et al. (2015). To the best of our knowledge, this is the first
BMC method which exploits all kinds of matrix entries. We experimentally show
that an appropriate mixture of risks improves the performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hayashi_M/0/1/0/all/0/1&quot;&gt;Masayoshi Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sakai_T/0/1/0/all/0/1&quot;&gt;Tomoya Sakai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04665">
<title>Pure Exploration in Infinitely-Armed Bandit Models with Fixed-Confidence. (arXiv:1803.04665v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04665</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of near-optimal arm identification in the fixed
confidence setting of the infinitely armed bandit problem when nothing is known
about the arm reservoir distribution. We (1) introduce a PAC-like framework
within which to derive and cast results; (2) derive a sample complexity lower
bound for near-optimal arm identification; (3) propose an algorithm that
identifies a nearly-optimal arm with high probability and derive an upper bound
on its sample complexity which is within a log factor of our lower bound; and
(4) discuss whether our log^2(1/delta) dependence is inescapable for
&quot;two-phase&quot; (select arms first, identify the best later) algorithms in the
infinite setting. This work permits the application of bandit models to a
broader class of problems where fewer assumptions hold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aziz_M/0/1/0/all/0/1&quot;&gt;Maryam Aziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anderton_J/0/1/0/all/0/1&quot;&gt;Jesse Anderton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaufmann_E/0/1/0/all/0/1&quot;&gt;Emilie Kaufmann&lt;/a&gt; (SEQUEL, CNRS, CRIStAL), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aslam_J/0/1/0/all/0/1&quot;&gt;Javed Aslam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04779">
<title>Hybrid Forecasting of Chaotic Processes: Using Machine Learning in Conjunction with a Knowledge-Based Model. (arXiv:1803.04779v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04779</link>
<description rdf:parseType="Literal">&lt;p&gt;A model-based approach to forecasting chaotic dynamical systems utilizes
knowledge of the physical processes governing the dynamics to build an
approximate mathematical model of the system. In contrast, machine learning
techniques have demonstrated promising results for forecasting chaotic systems
purely from past time series measurements of system state variables (training
data), without prior knowledge of the system dynamics. The motivation for this
paper is the potential of machine learning for filling in the gaps in our
underlying mechanistic knowledge that cause widely-used knowledge-based models
to be inaccurate. Thus we here propose a general method that leverages the
advantages of these two approaches by combining a knowledge-based model and a
machine learning technique to build a hybrid forecasting scheme. Potential
applications for such an approach are numerous (e.g., improving weather
forecasting). We demonstrate and test the utility of this approach using a
particular illustrative version of a machine learning known as reservoir
computing, and we apply the resulting hybrid forecaster to a low-dimensional
chaotic system, as well as to a high-dimensional spatiotemporal chaotic system.
These tests yield extremely promising results in that our hybrid technique is
able to accurately predict for a much longer period of time than either its
machine-learning component or its model-based component alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1&quot;&gt;Jaideep Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wikner_A/0/1/0/all/0/1&quot;&gt;Alexander Wikner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fussell_R/0/1/0/all/0/1&quot;&gt;Rebeckah Fussell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1&quot;&gt;Sarthak Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunt_B/0/1/0/all/0/1&quot;&gt;Brian Hunt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girvan_M/0/1/0/all/0/1&quot;&gt;Michelle Girvan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_E/0/1/0/all/0/1&quot;&gt;Edward Ott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04825">
<title>Low-Rank Boolean Matrix Approximation by Integer Programming. (arXiv:1803.04825v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04825</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank approximations of data matrices are an important dimensionality
reduction tool in machine learning and regression analysis. We consider the
case of categorical variables, where it can be formulated as the problem of
finding low-rank approximations to Boolean matrices. In this paper we give what
is to the best of our knowledge the first integer programming formulation that
relies on only polynomially many variables and constraints, we discuss how to
solve it computationally and report numerical tests on synthetic and real-world
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovacs_R/0/1/0/all/0/1&quot;&gt;Reka Kovacs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunluk_O/0/1/0/all/0/1&quot;&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauser_R/0/1/0/all/0/1&quot;&gt;Raphael Hauser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04899">
<title>Optimal Transport for Multi-source Domain Adaptation under Target Shift. (arXiv:1803.04899v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04899</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose to tackle the problem of reducing discrepancies
between multiple domains referred to as multi-source domain adaptation and
consider it under the target shift assumption: in all domains we aim to solve a
classification problem with the same output classes, but with labels&apos;
proportions differing across them. We design a method based on optimal
transport, a theory that is gaining momentum to tackle adaptation problems in
machine learning due to its efficiency in aligning probability distributions.
Our method performs multi-source adaptation and target shift correction
simultaneously by learning the class probabilities of the unlabeled target
sample and the coupling allowing to align two (or more) probability
distributions. Experiments on both synthetic and real-world data related to
satellite image segmentation task show the superiority of the proposed method
over the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Redko_I/0/1/0/all/0/1&quot;&gt;Ievgen Redko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Courty_N/0/1/0/all/0/1&quot;&gt;Nicolas Courty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flamary_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tuia_D/0/1/0/all/0/1&quot;&gt;Devis Tuia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04924">
<title>Dense Limit of the Dawid-Skene Model for Crowdsourcing and Regions of Sub-optimality of Message Passing Algorithms. (arXiv:1803.04924v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04924</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowdsourcing is a strategy to categorize data through the contribution of
many individuals. A wide range of theoretical and algorithmic contributions are
based on the model of Dawid and Skene \cite{DawidSkene79}. Recently it was
shown in \cite{ok16,ok2016optimal} that, in certain regimes, belief propagation
is asymptotically optimal for data generated from the Dawid-Skene model. This
paper is motivated by this recent progress. We analyze the dense limit of the
Dawid-Skene model. It is shown that it belongs to a larger class of low-rank
matrix estimation problems for which it is possible to express the asymptotic,
Bayes-optimal, performance in a simple closed form. In the dense limit the
mapping to a low-rank matrix estimation problem provides an approximate message
passing algorithm that solves the problem algorithmically. We identify the
regions where the algorithm efficiently computes the Bayes-optimal estimates.
Our analysis refines the results of \cite{ok16,ok2016optimal} about optimality
of message passing algorithms by characterizing regions of parameters where
these algorithms do not match the Bayes-optimal performance. We further study
numerically the performance of approximate message passing, derived in the
dense limit, on sparse instances and carry out experiments on a real world
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmidt_C/0/1/0/all/0/1&quot;&gt;Christian Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zdeborova_L/0/1/0/all/0/1&quot;&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04926">
<title>Active Reinforcement Learning with Monte-Carlo Tree Search. (arXiv:1803.04926v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04926</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Reinforcement Learning (ARL) is a twist on RL where the agent observes
reward information only if it pays a cost. This subtle change makes exploration
substantially more challenging. Powerful principles in RL like optimism,
Thompson sampling, and random exploration do not help with ARL. We relate ARL
in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm
using Monte-Carlo Tree Search that is asymptotically Bayes optimal.
Experimentally, this algorithm is near-optimal on small Bandit problems and
MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised
heuristics for ARL. By analysing exploration behaviour in detail, we uncover
obstacles to scaling up simulation-based algorithms for ARL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_S/0/1/0/all/0/1&quot;&gt;Sebastian Schulze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1&quot;&gt;Owain Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04929">
<title>SAM: Structural Agnostic Model, Causal Discovery and Penalized Adversarial Learning. (arXiv:1803.04929v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04929</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Structural Agnostic Model (SAM), a framework to estimate
end-to-end non-acyclic causal graphs from observational data. In a nutshell,
SAM implements an adversarial game in which a separate model generates each
variable, given real values from all others. In tandem, a discriminator
attempts to distinguish between the joint distributions of real and generated
samples. Finally, a sparsity penalty forces each generator to consider only a
small subset of the variables, yielding a sparse causal graph. SAM scales
easily to hundreds variables. Our experiments show the state-of-the-art
performance of SAM on discovering causal structures and modeling interventions,
in both acyclic and non-acyclic graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kalainathan_D/0/1/0/all/0/1&quot;&gt;Diviyan Kalainathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goudet_O/0/1/0/all/0/1&quot;&gt;Olivier Goudet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guyon_I/0/1/0/all/0/1&quot;&gt;Isabelle Guyon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopez_Paz_D/0/1/0/all/0/1&quot;&gt;David Lopez-Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sebag_M/0/1/0/all/0/1&quot;&gt;Mich&amp;#xe8;le Sebag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.00624">
<title>Oracle Inequalities for High-dimensional Prediction. (arXiv:1608.00624v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1608.00624</link>
<description rdf:parseType="Literal">&lt;p&gt;The abundance of high-dimensional data in the modern sciences has generated
tremendous interest in penalized estimators such as the lasso, scaled lasso,
square-root lasso, elastic net, and many others. In this paper, we establish a
general oracle inequality for prediction in high-dimensional linear regression
with such methods. Since the proof relies only on convexity and continuity
arguments, the result holds irrespective of the design matrix and applies to a
wide range of penalized estimators. Overall, the bound demonstrates that
generic estimators can provide consistent prediction with any design matrix.
From a practical point of view, the bound can help to identify the potential of
specific estimators, and they can help to get a sense of the prediction
accuracy in a given application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lederer_J/0/1/0/all/0/1&quot;&gt;Johannes Lederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gaynanova_I/0/1/0/all/0/1&quot;&gt;Irina Gaynanova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.06545">
<title>Revisiting Classifier Two-Sample Tests. (arXiv:1610.06545v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.06545</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of two-sample tests is to assess whether two samples, $S_P \sim P^n$
and $S_Q \sim Q^m$, are drawn from the same distribution. Perhaps intriguingly,
one relatively unexplored method to build two-sample tests is the use of binary
classifiers. In particular, construct a dataset by pairing the $n$ examples in
$S_P$ with a positive label, and by pairing the $m$ examples in $S_Q$ with a
negative label. If the null hypothesis &quot;$P = Q$&quot; is true, then the
classification accuracy of a binary classifier on a held-out subset of this
dataset should remain near chance-level. As we will show, such Classifier
Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly,
return test statistics in interpretable units, have a simple null distribution,
and their predictive uncertainty allow to interpret where $P$ and $Q$ differ.
The goal of this paper is to establish the properties, performance, and uses of
C2ST. First, we analyze their main theoretical properties. Second, we compare
their performance against a variety of state-of-the-art alternatives. Third, we
propose their use to evaluate the sample quality of generative models with
intractable likelihoods, such as Generative Adversarial Networks (GANs).
Fourth, we showcase the novel application of GANs together with C2ST for causal
discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopez_Paz_D/0/1/0/all/0/1&quot;&gt;David Lopez-Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oquab_M/0/1/0/all/0/1&quot;&gt;Maxime Oquab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00893">
<title>Being Robust (in High Dimensions) Can Be Practical. (arXiv:1703.00893v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00893</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust estimation is much more challenging in high dimensions than it is in
one dimension: Most techniques either lead to intractable optimization problems
or estimators that can tolerate only a tiny fraction of errors. Recent work in
theoretical computer science has shown that, in appropriate distributional
models, it is possible to robustly estimate the mean and covariance with
polynomial time algorithms that can tolerate a constant fraction of
corruptions, independent of the dimension. However, the sample and time
complexity of these algorithms is prohibitively large for high-dimensional
applications. In this work, we address both of these issues by establishing
sample complexity bounds that are optimal, up to logarithmic factors, as well
as giving various refinements that allow the algorithms to tolerate a much
larger fraction of corruptions. Finally, we show on both synthetic and real
data that our algorithms have state-of-the-art performance and suddenly make
high-dimensional robust estimation a realistic possibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1&quot;&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1&quot;&gt;Gautam Kamath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1&quot;&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jerry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1&quot;&gt;Ankur Moitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_A/0/1/0/all/0/1&quot;&gt;Alistair Stewart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.09773">
<title>Interpretability via Model Extraction. (arXiv:1706.09773v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.09773</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to interpret machine learning models has become increasingly
important now that machine learning is used to inform consequential decisions.
We propose an approach called model extraction for interpreting complex,
blackbox models. Our approach approximates the complex model using a much more
interpretable model; as long as the approximation quality is good, then
statistical properties of the complex model are reflected in the interpretable
model. We show how model extraction can be used to understand and debug random
forests and neural nets trained on several datasets from the UCI Machine
Learning Repository, as well as control policies learned for several classical
reinforcement learning problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1&quot;&gt;Osbert Bastani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Carolyn Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastani_H/0/1/0/all/0/1&quot;&gt;Hamsa Bastani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11303">
<title>Algorithmic learning of probability distributions from random data in the limit. (arXiv:1710.11303v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11303</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of identifying a probability distribution for some given
randomly sampled data in the limit, in the context of algorithmic learning
theory as proposed recently by Vinanyi and Chater. We show that there exists a
computable partial learner for the computable probability measures, while by
Bienvenu, Monin and Shen it is known that there is no computable learner for
the computable probability measures. Our main result is the characterization of
the oracles that compute explanatory learners for the computable (continuous)
probability measures as the high oracles. This provides an analogue of a
well-known result of Adleman and Blum in the context of learning computable
probability distributions. We also discuss related learning notions such as
behaviorally correct learning and orther variations of explanatory learning, in
the context of learning probability distributions from data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barmpalias_G/0/1/0/all/0/1&quot;&gt;George Barmpalias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephan_F/0/1/0/all/0/1&quot;&gt;Frank Stephan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00673">
<title>Fast Information-theoretic Bayesian Optimisation. (arXiv:1711.00673v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00673</link>
<description rdf:parseType="Literal">&lt;p&gt;Information-theoretic Bayesian optimisation techniques have demonstrated
state-of-the-art performance in tackling important global optimisation
problems. However, current information-theoretic approaches require many
approximations in implementation, introduce often-prohibitive computational
overhead and limit the choice of kernels available to model the objective. We
develop a fast information-theoretic Bayesian Optimisation method, FITBO, that
avoids the need for sampling the global minimiser, thus significantly reducing
computational overhead. Moreover, in comparison with existing approaches, our
method faces fewer constraints on kernel choice and enjoys the merits of
dealing with the output space. We demonstrate empirically that FITBO inherits
the performance associated with information-theoretic Bayesian optimisation,
while being even faster than simpler Bayesian optimisation approaches, such as
Expected Improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McLeod_M/0/1/0/all/0/1&quot;&gt;Mark McLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1&quot;&gt;Diego Granziol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael Osborne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01012">
<title>Policy Optimization by Genetic Distillation. (arXiv:1711.01012v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01012</link>
<description rdf:parseType="Literal">&lt;p&gt;Genetic algorithms have been widely used in many practical optimization
problems. Inspired by natural selection, operators, including mutation,
crossover and selection, provide effective heuristics for search and black-box
optimization. However, they have not been shown useful for deep reinforcement
learning, possibly due to the catastrophic consequence of parameter crossovers
of neural networks. Here, we present Genetic Policy Optimization (GPO), a new
genetic algorithm for sample-efficient deep policy optimization. GPO uses
imitation learning for policy crossover in the state space and applies policy
gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as
a genetic algorithm is able to provide superior performance over the
state-of-the-art policy gradient methods and achieves comparable or higher
sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gangwani_T/0/1/0/all/0/1&quot;&gt;Tanmay Gangwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jian Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02317">
<title>Multi-Player Bandits Revisited. (arXiv:1711.02317v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02317</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-player Multi-Armed Bandits (MAB) have been extensively studied in the
literature, motivated by applications to Cognitive Radio systems. Driven by
such applications as well, we motivate the introduction of several levels of
feedback for multi-player MAB algorithms. Most existing work assume that
sensing information is available to the algorithm. Under this assumption, we
improve the state-of-the-art lower bound for the regret of any decentralized
algorithms and introduce two algorithms, RandTopM and MCTopM, that are shown to
empirically outperform existing algorithms. Moreover, we provide strong
theoretical guarantees for these algorithms, including a notion of asymptotic
optimality in terms of the number of selections of bad arms. We then introduce
a promising heuristic, called Selfish, that can operate without sensing
information, which is crucial for emerging applications to Internet of Things
networks. We investigate the empirical performance of this algorithm and
provide some first theoretical elements for the understanding of its behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Besson_L/0/1/0/all/0/1&quot;&gt;Lilian Besson&lt;/a&gt; (IETR, SEQUEL), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaufmann_E/0/1/0/all/0/1&quot;&gt;Emilie Kaufmann&lt;/a&gt; (CRIStAL, SEQUEL)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08235">
<title>Estimating activity cycles with probabilistic methods I. Bayesian Generalised Lomb-Scargle Periodogram with Trend. (arXiv:1712.08235v2 [astro-ph.SR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08235</link>
<description rdf:parseType="Literal">&lt;p&gt;Period estimation is one of the central topics in astronomical time series
analysis, where data is often unevenly sampled. Especially challenging are
studies of stellar magnetic cycles, as there the periods looked for are of the
order of the same length than the datasets themselves. The datasets often
contain trends, the origin of which is either a real long-term cycle or an
instrumental effect, but these effects cannot be reliably separated, while they
can lead to erroneous period determinations if not properly handled. In this
study we aim at developing a method that can handle the trends properly, and by
performing extensive set of testing, we show that this is the optimal procedure
when contrasted with methods that do not include the trend directly to the
model. The effect of the form of the noise (whether constant or
heteroscedastic) on the results is also investigated. We introduce a Bayesian
Generalised Lomb-Scargle Periodogram with Trend (BGLST), which is a
probabilistic linear regression model using Gaussian priors for the
coefficients and uniform prior for the frequency parameter. We show, using
synthetic data, that when there is no prior information on whether and to what
extent the true model of the data contains a linear trend, the introduced BGLST
method is preferable to the methods which either detrend the data or leave the
data untrended before fitting the periodic model. Whether to use noise with
different than constant variance in the model depends on the density of the
data sampling as well as on the true noise type of the process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Olspert_N/0/1/0/all/0/1&quot;&gt;N. Olspert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Pelt_J/0/1/0/all/0/1&quot;&gt;J. Pelt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kapyla_M/0/1/0/all/0/1&quot;&gt;M. J. K&amp;#xe4;pyl&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lehtinen_J/0/1/0/all/0/1&quot;&gt;J. Lehtinen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03335">
<title>Black-box Variational Inference for Stochastic Differential Equations. (arXiv:1802.03335v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03335</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter inference for stochastic differential equations is challenging due
to the presence of a latent diffusion process. Working with an Euler-Maruyama
discretisation for the diffusion, we use variational inference to jointly learn
the parameters and the diffusion paths. We use a standard mean-field
variational approximation of the parameter posterior, and introduce a recurrent
neural network to approximate the posterior for the diffusion paths conditional
on the parameters. This neural network learns how to provide Gaussian state
transitions which bridge between observations in a very similar way to the
conditioned diffusion process. The resulting black-box inference method can be
applied to any SDE system with light tuning requirements. We illustrate the
method on a Lotka-Volterra system and an epidemic model, producing accurate
parameter estimates in a few hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ryder_T/0/1/0/all/0/1&quot;&gt;Thomas Ryder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Golightly_A/0/1/0/all/0/1&quot;&gt;Andrew Golightly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McGough_A/0/1/0/all/0/1&quot;&gt;A. Stephen McGough&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prangle_D/0/1/0/all/0/1&quot;&gt;Dennis Prangle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03628">
<title>Learning Correlation Space for Time Series. (arXiv:1802.03628v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03628</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approximation algorithm for efficient correlation search in
time series data. In our method, we use Fourier transform and neural network to
embed time series into a low-dimensional Euclidean space. The given space is
learned such that time series correlation can be effectively approximated from
Euclidean distance between corresponding embedded vectors. Therefore, search
for correlated time series can be done using an index in the embedding space
for efficient nearest neighbor search. Our theoretical analysis illustrates
that our method&apos;s accuracy can be guaranteed under certain regularity
conditions. We further conduct experiments on real-world datasets and the
results show that our method indeed outperforms the baseline solution. In
particular, for approximation of correlation, our method reduces the
approximation loss by a half in most test cases compared to the baseline
solution. For top-$k$ highest correlation search, our method improves the
precision from 5\% to 20\% while the query time is similar to the baseline
approach query time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1&quot;&gt;Hoang Thanh Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Francesco Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinn_M/0/1/0/all/0/1&quot;&gt;Mathieu Sinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05370">
<title>Covariance Function Pre-Training with m-Kernels for Accelerated Bayesian Optimisation. (arXiv:1802.05370v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05370</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents a novel approach to direct covariance function learning
for Bayesian optimisation, with particular emphasis on experimental design
problems where an existing corpus of condensed knowledge is present. The method
presented borrows techniques from reproducing kernel Banach space theory
(specifically m-kernels) and leverages them to convert (or re-weight) existing
covariance functions into new, problem-specific covariance functions. The key
advantage of this approach is that rather than relying on the user to manually
select (with some hyperparameter tuning and experimentation) an appropriate
covariance function it constructs the covariance function to specifically match
the problem at hand. The technique is demonstrated on two real-world problems -
specifically alloy design and short-polymer fibre manufacturing - as well as a
selected test function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shilton_A/0/1/0/all/0/1&quot;&gt;Alistair Shilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sunil Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rana_S/0/1/0/all/0/1&quot;&gt;Santu Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vellanki_P/0/1/0/all/0/1&quot;&gt;Pratibha Vellanki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_L/0/1/0/all/0/1&quot;&gt;Laurence Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sutti_A/0/1/0/all/0/1&quot;&gt;Alessandra Sutti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rubin_D/0/1/0/all/0/1&quot;&gt;David Rubin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorin_T/0/1/0/all/0/1&quot;&gt;Thomas Dorin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vahid_A/0/1/0/all/0/1&quot;&gt;Alireza Vahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Height_M/0/1/0/all/0/1&quot;&gt;Murray Height&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06678">
<title>Large Scale Automated Forecasting for Monitoring Network Safety and Security. (arXiv:1802.06678v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06678</link>
<description rdf:parseType="Literal">&lt;p&gt;Real time large scale streaming data pose major challenges to forecasting, in
particular defying the presence of human experts to perform the corresponding
analysis. We present here a class of models and methods used to develop an
automated, scalable and versatile system for large scale forecasting oriented
towards safety and security monitoring. Our system provides short and long term
forecasts and uses them to detect safety and security issues in relation with
multiple internet connected devices well in advance they might take place.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Naveiro_R/0/1/0/all/0/1&quot;&gt;Roi Naveiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Sim&amp;#xf3;n Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Insua_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xed;os Insua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07309">
<title>Detection limits in the high-dimensional spiked rectangular model. (arXiv:1802.07309v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07309</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of detecting the presence of a single unknown spike in a
rectangular data matrix, in a high-dimensional regime where the spike has fixed
strength and the aspect ratio of the matrix converges to a finite limit. This
setup includes Johnstone&apos;s spiked covariance model. We analyze the likelihood
ratio of the spiked model against an &quot;all noise&quot; null model of reference, and
show it has asymptotically Gaussian fluctuations in a region below---but in
general not up to---the so-called BBP threshold from random matrix theory. Our
result parallels earlier findings of Onatski et al.\ (2013) and
Johnstone-Onatski (2015) for spherical spikes. We present a probabilistic
approach capable of treating generic product priors. In particular, sparsity in
the spike is allowed. Our approach is based on Talagrand&apos;s interpretation of
the cavity method from spin-glass theory. The question of the maximal parameter
region where asymptotic normality is expected to hold is left open. This region
is shaped by the prior in a non-trivial way. We conjecture that this is the
entire paramagnetic phase of an associated spin-glass model, and is defined by
the vanishing of the replica-symmetric solution of Lesieur et al.\ (2015).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Alaoui_A/0/1/0/all/0/1&quot;&gt;Ahmed El Alaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04209">
<title>High Throughput Synchronous Distributed Stochastic Gradient Descent. (arXiv:1803.04209v1 [cs.DC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.04209</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new, high-throughput, synchronous, distributed, data-parallel,
stochastic-gradient-descent learning algorithm. This algorithm uses amortized
inference in a compute-cluster-specific, deep, generative, dynamical model to
perform joint posterior predictive inference of the mini-batch gradient
computation times of all worker-nodes in a parallel computing cluster. We show
that a synchronous parameter server can, by utilizing such a model, choose an
optimal cutoff time beyond which mini-batch gradient messages from slow workers
are ignored that maximizes overall mini-batch gradient computations per second.
In keeping with earlier findings we observe that, under realistic conditions,
eagerly discarding the mini-batch gradient computations of stragglers not only
increases throughput but actually increases the overall rate of convergence as
a function of wall-clock time by virtue of eliminating idleness. The principal
novel contribution and finding of this work goes beyond this by demonstrating
that using the predicted run-times from a generative model of cluster worker
performance to dynamically adjust the cutoff improves substantially over the
static-cutoff prior art, leading to, among other things, significantly reduced
deep neural net training times on large computer clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1&quot;&gt;Michael Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;</dc:creator>
</item></rdf:RDF>