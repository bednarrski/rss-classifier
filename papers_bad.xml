<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03480"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03916"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.00993"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06509"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03417"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03471"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03493"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03501"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03654"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03855"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04205"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04749"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07729"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03418"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03426"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03451"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03488"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03522"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03532"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03569"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03628"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03676"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03699"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03713"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03725"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03759"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03765"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03900"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03913"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03923"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03938"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04023"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04145"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04198"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04220"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.00031"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.00389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.07116"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.02041"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.03077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07352"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.06808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10762"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.07642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00106"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05359"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06360"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07138"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05407"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06061"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10395"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02154"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.03480">
<title>GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders. (arXiv:1802.03480v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03480</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning on graphs has become a popular research topic with many
applications. However, past work has concentrated on learning graph embedding
tasks, which is in contrast with advances in generative models for images and
text. Is it possible to transfer this progress to the domain of graphs? We
propose to sidestep hurdles associated with linearization of such discrete
structures by having a decoder output a probabilistic fully-connected graph of
a predefined maximum size directly at once. Our method is formulated as a
variational autoencoder. We evaluate on the challenging task of molecule
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonovsky_M/0/1/0/all/0/1&quot;&gt;Martin Simonovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1&quot;&gt;Nikos Komodakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03608">
<title>MOEA/D with Angle-based Constrained Dominance Principle for Constrained Multi-objective Optimization Problems. (arXiv:1802.03608v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.03608</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel constraint-handling mechanism named angle-based
constrained dominance principle (ACDP) embedded in a decomposition-based
multi-objective evolutionary algorithm (MOEA/D) to solve constrained
multi-objective optimization problems (CMOPs). To maintain the diversity of the
working population, ACDP utilizes the information of the angle of solutions to
adjust the dominance relation of solutions during the evolutionary process.
This paper uses 14 benchmark instances to evaluate the performance of the
MOEA/D with ACDP (MOEA/D-ACDP). Additionally, an engineering optimization
problem (which is I-beam optimization problem) is optimized. The proposed
MOEA/D-ACDP, and four other decomposition-based CMOEAs, including C-MOEA/D,
MOEA/D-CDP, MOEA/D-Epsilon and MOEA/D-SR are tested by the above benchmarks and
the engineering application. The experimental results manifest that MOEA/D-ACDP
is significantly better than the other four CMOEAs on these test instances and
the real-world case, which indicates that ACDP is more effective for solving
CMOPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinye Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Caimin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_E/0/1/0/all/0/1&quot;&gt;Erik Goodman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03806">
<title>ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Neural Network Accelerators. (arXiv:1802.03806v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.03806</link>
<description rdf:parseType="Literal">&lt;p&gt;Hardware accelerators are being increasingly deployed to boost the
performance and energy efficiency of deep neural network (DNN) inference. In
this paper we propose Thundervolt, a new framework that enables aggressive
voltage underscaling of high-performance DNN accelerators without compromising
classification accuracy even in the presence of high timing error rates. Using
post-synthesis timing simulations of a DNN accelerator modeled on the Google
TPU, we show that Thundervolt enables between 34$\%$-57$\%$ energy savings on
state-of-the-art speech and image recognition benchmarks with less than 1$\%$
loss in classification accuracy and no performance loss. Further, we show that
Thundervolt is synergistic with and can further increase the energy efficiency
of commonly used run-time DNN pruning techniques like Zero-Skip.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jeff Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangineni_K/0/1/0/all/0/1&quot;&gt;Kartheek Rangineni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddharth Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03891">
<title>Three levels of neural reuse through the dynamical lens: structural network, autonomous dynamics and transient dynamics. (arXiv:1802.03891v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.03891</link>
<description rdf:parseType="Literal">&lt;p&gt;The brain in conjunction with the body is able to adapt to new environments
and perform multiple behaviors through reuse of neural resources and transfer
of existing behavioral traits. Although mechanisms that underlie this ability
are not well known, they are largely attributed to neuromodulation. In this
work, we demonstrate that an agent can be multifunctional using the same
sensory and motor systems across behaviors, in the absence of modulatory
mechanisms. Further, we lay out the different levels at which neural reuse can
occur through a dynamical filtering of the brain-body-environment system&apos;s
operation - structural network, autonomous dynamics, and transient dynamics.
Notably, transient dynamics reuse can only be explained by studying the
brain-body-environment system as a whole and not just the brain. The
multifunctional agent we present here demonstrates neural reuse at all three
levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasu_M/0/1/0/all/0/1&quot;&gt;Madhavun Candadai Vasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izquierdo_E/0/1/0/all/0/1&quot;&gt;Eduardo Izquierdo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03916">
<title>Detecting and Correcting for Label Shift with Black Box Predictors. (arXiv:1802.03916v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03916</link>
<description rdf:parseType="Literal">&lt;p&gt;Faced with distribution shift between training and test set, we wish to
detect and quantify the shift, and to correct our classifiers without test set
labels. Motivated by medical diagnosis, where diseases (targets), cause
symptoms (observations), we focus on label shift, where the label marginal
$p(y)$ changes but the conditional $p(x|y)$ does not. We propose Black Box
Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits
arbitrary black box predictors to reduce dimensionality prior to shift
correction. While better predictors give tighter estimates, BBSE works even
when predictors are biased, inaccurate, or uncalibrated, so long as their
confusion matrices are invertible. We prove BBSE&apos;s consistency, bound its
error, and introduce a statistical test that uses BBSE to detect shift. We also
leverage BBSE to correct classifiers. Experiments demonstrate accurate
estimates and improved prediction, even on high-dimensional datasets of natural
images
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1&quot;&gt;Alex Smola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.00993">
<title>Robust Particle Swarm Optimizer based on Chemomimicry. (arXiv:1702.00993v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1702.00993</link>
<description rdf:parseType="Literal">&lt;p&gt;A particle swarm optimizer (PSO) loosely based on the phenomena of
crystallization and a chaos factor which follows the complimentary error
function is described. The method features three phases: diffusion, directed
motion, and nucleation. During the diffusion phase random walk is the only
contributor to particle motion. As the algorithm progresses the contribution
from chaos decreases and movement toward global best locations is pursued until
convergence has occurred. The algorithm was found to be more robust to local
minima in multimodal test functions than a standard PSO algorithm and is
designed for problems which feature experimental precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kneale_C/0/1/0/all/0/1&quot;&gt;Casey Kneale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Booksh_K/0/1/0/all/0/1&quot;&gt;Karl S. Booksh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06509">
<title>Bidirectional deep-reservoir echo state networks. (arXiv:1711.06509v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06509</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a deep architecture for the classification of multivariate time
series. By means of a recurrent and untrained reservoir we generate a vectorial
representation that embeds temporal relationships in the data. To improve the
memorization capability, we implement a bidirectional reservoir, whose last
state captures also past dependencies in the input. We apply dimensionality
reduction to the final reservoir states to obtain compressed fixed size
representations of the time series. These are subsequently fed into a deep
feedforward network trained to perform the final classification. We test our
architecture on benchmark datasets and on a real-world use-case of blood
samples classification. Results show that our method performs better than a
standard echo state network and, at the same time, achieves results comparable
to a fully-trained recurrent network, but with a faster training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1&quot;&gt;Filippo Maria Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1&quot;&gt;Simone Scardapane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokse_S/0/1/0/all/0/1&quot;&gt;Sigurd L&amp;#xf8;kse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01636">
<title>Generalization of Deep Neural Networks for Chest Pathology Classification in X-Rays Using Generative Adversarial Networks. (arXiv:1712.01636v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01636</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical datasets are often highly imbalanced with over-representation of
common medical problems and a paucity of data from rare conditions. We propose
simulation of pathology in images to overcome the above limitations. Using
chest X-rays as a model medical image, we implement a generative adversarial
network (GAN) to create artificial images based upon a modest sized labeled
dataset. We employ a combination of real and artificial images to train a deep
convolutional neural network (DCNN) to detect pathology across five classes of
chest X-rays. Furthermore, we demonstrate that augmenting the original
imbalanced dataset with GAN generated images improves performance of chest
pathology classification using the proposed DCNN in comparison to the same DCNN
trained with the original dataset alone. This improved performance is largely
attributed to balancing of the dataset using GAN generated images, where image
classes that are lacking in example images are preferentially augmented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehinejad_H/0/1/0/all/0/1&quot;&gt;Hojjat Salehinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valaee_S/0/1/0/all/0/1&quot;&gt;Shahrokh Valaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dowdell_T/0/1/0/all/0/1&quot;&gt;Tim Dowdell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1&quot;&gt;Errol Colak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barfett_J/0/1/0/all/0/1&quot;&gt;Joseph Barfett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06434">
<title>EffNet: An Efficient Structure for Convolutional Neural Networks. (arXiv:1801.06434v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06434</link>
<description rdf:parseType="Literal">&lt;p&gt;With the ever increasing application of Convolutional Neural Networks to
costumer products the need emerges for models which can efficiently run on
embedded, mobile hardware. Slimmer models have therefore become a hot research
topic with multiple different approaches which vary from binary networks to
revised convolution layers. We offer our contribution to the latter and propose
a novel convolution block which significantly reduces the computational burden
while surpassing the current state-of-the-art. Our model, dubbed EffNet, is
optimised for models which are slim to begin with and is created to tackle
issues in existing models such as MobileNet and ShuffleNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1&quot;&gt;Ido Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roese_Koerner_L/0/1/0/all/0/1&quot;&gt;Lutz Roese-Koerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1&quot;&gt;Anton Kummert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03396">
<title>Predicting Customer Churn: Extreme Gradient Boosting with Temporal Data. (arXiv:1802.03396v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03396</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting customer churn using large scale time-series data is a
common problem facing many business domains. The creation of model features
across various time windows for training and testing can be particularly
challenging due to temporal issues common to time-series data. In this paper,
we will explore the application of extreme gradient boosting (XGBoost) on a
customer dataset with a wide-variety of temporal features in order to create a
highly-accurate customer churn model. In particular, we describe an effective
method for handling temporally sensitive feature engineering. The proposed
model was submitted in the WSDM Cup 2018 Churn Challenge and achieved
first-place out of 575 teams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gregory_B/0/1/0/all/0/1&quot;&gt;Bryan Gregory&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03417">
<title>Narrow Artificial Intelligence with Machine Learning for Real-Time Estimation of a Mobile Agents Location Using Hidden Markov Models. (arXiv:1802.03417v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03417</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to use a supervised machine learning technique to track the
location of a mobile agent in real time. Hidden Markov Models are used to build
artificial intelligence that estimates the unknown position of a mobile target
moving in a defined environment. This narrow artificial intelligence performs
two distinct tasks. First, it provides real-time estimation of the mobile
agent&apos;s position using the forward algorithm. Second, it uses the Baum-Welch
algorithm as a statistical learning tool to gain knowledge of the mobile
target. Finally, an experimental environment is proposed, namely a video game
that we use to test our artificial intelligence. We present statistical and
graphical results to illustrate the efficiency of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaulac_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Beaulac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larribe_F/0/1/0/all/0/1&quot;&gt;Fabrice Larribe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03471">
<title>On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning. (arXiv:1802.03471v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03471</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples in machine learning has been a topic of intense research
interest, with attacks and defenses being developed in a tight back-and-forth.
Most past defenses are best-effort, heuristic approaches that have all been
shown to be vulnerable to sophisticated attacks. More recently, rigorous
defenses that provide formal guarantees have emerged, but are hard to scale or
generalize. A rigorous and general foundation for designing defenses is
required to get us off this arms race trajectory. We propose leveraging
differential privacy (DP) as a formal building block for robustness against
adversarial examples. We observe that the semantic of DP is closely aligned
with the formal definition of robustness to adversarial examples. We propose
PixelDP, a strategy for learning robust deep neural networks based on formal DP
guarantees. PixelDP networks give theoretical guarantees for a subset of their
predictions regarding the robustness against adversarial perturbations of
bounded size. Our evaluation with MNIST, CIFAR-10, and CIFAR-100 shows that
PixelDP networks achieve accuracy under attack on par with the best-performing
defense to date, but additionally certify robustness against meaningful-size
1-norm and 2-norm attacks for 40-60% of their predictions. Our experience
points to DP as a rigorous, broadly applicable, and mechanism-rich foundation
for robust machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lecuyer_M/0/1/0/all/0/1&quot;&gt;Mathias Lecuyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Atlidakis_V/0/1/0/all/0/1&quot;&gt;Vaggelis Atlidakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Geambasu_R/0/1/0/all/0/1&quot;&gt;Roxana Geambasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;Daniel Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jana_S/0/1/0/all/0/1&quot;&gt;Suman Jana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03493">
<title>More Robust Doubly Robust Off-policy Evaluation. (arXiv:1802.03493v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03493</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of off-policy evaluation (OPE) in reinforcement learning
(RL), where the goal is to estimate the performance of a policy from the data
generated by another policy(ies). In particular, we focus on the doubly robust
(DR) estimators that consist of an importance sampling (IS) component and a
performance model, and utilize the low (or zero) bias of IS and low variance of
the model at the same time. Although the accuracy of the model has a huge
impact on the overall performance of DR, most of the work on using the DR
estimators in OPE has been focused on improving the IS part, and not much on
how to learn the model. In this paper, we propose alternative DR estimators,
called more robust doubly robust (MRDR), that learn the model parameter by
minimizing the variance of the DR estimator. We first present a formulation for
learning the DR model in RL. We then derive formulas for the variance of the DR
estimator in both contextual bandits and RL, such that their gradients
w.r.t.~the model parameters can be estimated from the samples, and propose
methods to efficiently minimize the variance. We prove that the MRDR estimators
are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR
in bandits and RL benchmark problems, and compare its performance with the
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1&quot;&gt;Yinlam Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03495">
<title>Generative Adversarial Networks and Probabilistic Graph Models for Hyperspectral Image Classification. (arXiv:1802.03495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.03495</link>
<description rdf:parseType="Literal">&lt;p&gt;High spectral dimensionality and the shortage of annotations make
hyperspectral image (HSI) classification a challenging problem. Recent studies
suggest that convolutional neural networks can learn discriminative spatial
features, which play a paramount role in HSI interpretation. However, most of
these methods ignore the distinctive spectral-spatial characteristic of
hyperspectral data. In addition, a large amount of unlabeled data remains an
unexploited gold mine for efficient data use. Therefore, we proposed an
integration of generative adversarial networks (GANs) and probabilistic
graphical models for HSI classification. Specifically, we used a
spectral-spatial generator and a discriminator to identify land cover
categories of hyperspectral cubes. Moreover, to take advantage of a large
amount of unlabeled data, we adopted a conditional random field to refine the
preliminary classification results generated by GANs. Experimental results
obtained using two commonly studied datasets demonstrate that the proposed
framework achieved encouraging classification accuracy using a small number of
data for training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zilong Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jonathan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03501">
<title>Path Consistency Learning in Tsallis Entropy Regularized MDPs. (arXiv:1802.03501v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03501</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the sparse entropy-regularized reinforcement learning (ERL) problem
in which the entropy term is a special form of the Tsallis entropy. The optimal
policy of this formulation is sparse, i.e.,~at each state, it has non-zero
probability for only a small number of actions. This addresses the main
drawback of the standard Shannon entropy-regularized RL (soft ERL) formulation,
in which the optimal policy is softmax, and thus, may assign a non-negligible
probability mass to non-optimal actions. This problem is aggravated as the
number of actions is increased. In this paper, we follow the work of Nachum et
al. (2017) in the soft ERL setting, and propose a class of novel path
consistency learning (PCL) algorithms, called {\em sparse PCL}, for the sparse
ERL problem that can work with both on-policy and off-policy data. We first
derive a {\em sparse consistency} equation that specifies a relationship
between the optimal value function and policy of the sparse ERL along any
system trajectory. Crucially, a weak form of the converse is also true, and we
quantify the sub-optimality of a policy which satisfies sparse consistency, and
show that as we increase the number of actions, this sub-optimality is better
than that of the soft ERL optimal policy. We then use this result to derive the
sparse PCL algorithms. We empirically compare sparse PCL with its soft
counterpart, and show its advantage, especially in problems with a large number
of actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1&quot;&gt;Yinlam Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03638">
<title>Forget Markov Logic: Efficient Mining of Prediction Rules in Large Graphs. (arXiv:1802.03638v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1802.03638</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph representations of large knowledge bases may comprise billions of
edges. Usually built upon human-generated ontologies, several knowledge bases
do not feature declared ontological rules and are far from being complete.
Current rule mining approaches rely on schemata or store the graph in-memory,
which can be unfeasible for large graphs. In this paper, we introduce
HornConcerto, an algorithm to discover Horn clauses in large graphs without the
need of a schema. Using a standard fact-based confidence score, we can mine
close Horn rules having an arbitrary body size. We show that our method can
outperform existing approaches in terms of runtime and memory consumption and
mine high-quality rules for the link prediction task, achieving
state-of-the-art results on a widely-used benchmark. Moreover, we find that
rules alone can perform inference significantly faster than embedding-based
methods and achieve accuracies on link prediction comparable to
resource-demanding approaches such as Markov Logic Networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soru_T/0/1/0/all/0/1&quot;&gt;Tommaso Soru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdestilhas_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Valdestilhas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marx_E/0/1/0/all/0/1&quot;&gt;Edgard Marx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1&quot;&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03642">
<title>Graph Planning with Expected Finite Horizon. (arXiv:1802.03642v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03642</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph planning gives rise to fundamental algorithmic questions such as
shortest path, traveling salesman problem, etc. A classical problem in discrete
planning is to consider a weighted graph and construct a path that maximizes
the sum of weights for a given time horizon $T$. However, in many scenarios,
the time horizon is not fixed, but the stopping time is chosen according to
some distribution such that the expected stopping time is $T$. If the stopping
time distribution is not known, then to ensure robustness, the distribution is
chosen by an adversary, to represent the worst-case scenario.
&lt;/p&gt;
&lt;p&gt;A stationary plan for every vertex always chooses the same outgoing edge. For
fixed horizon or fixed stopping-time distribution, stationary plans are not
sufficient for optimality. Quite surprisingly we show that when an adversary
chooses the stopping-time distribution with expected stopping time $T$, then
stationary plans are sufficient. While computing optimal stationary plans for
fixed horizon is NP-complete, we show that computing optimal stationary plans
under adversarial stopping-time distribution can be achieved in polynomial
time. Consequently, our polynomial-time algorithm for adversarial stopping time
also computes an optimal plan among all possible plans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doyen_L/0/1/0/all/0/1&quot;&gt;Laurent Doyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03654">
<title>Beyond the One Step Greedy Approach in Reinforcement Learning. (arXiv:1802.03654v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03654</link>
<description rdf:parseType="Literal">&lt;p&gt;The famous Policy Iteration algorithm alternates between policy improvement
and policy evaluation. Implementations of this algorithm with several variants
of the latter evaluation stage, e.g, $n$-step and trace-based returns, have
been analyzed in previous works. However, the case of multiple-step lookahead
policy improvement, despite the recent increase in empirical evidence of its
strength, has to our knowledge not been carefully analyzed yet. In this work,
we introduce the first such analysis. Namely, we formulate variants of
multiple-step policy improvement, derive new algorithms using these definitions
and prove their convergence. Moreover, we show that recent prominent
Reinforcement Learning algorithms are, in fact, instances of our framework. We
thus shed light on their empirical success and give a recipe for deriving new
algorithms for future study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1&quot;&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_G/0/1/0/all/0/1&quot;&gt;Gal Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherrer_B/0/1/0/all/0/1&quot;&gt;Bruno Scherrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03707">
<title>The Need for Speed of AI Applications: Performance Comparison of Native vs. Browser-based Algorithm Implementations. (arXiv:1802.03707v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03707</link>
<description rdf:parseType="Literal">&lt;p&gt;AI applications pose increasing demands on performance, so it is not
surprising that the era of client-side distributed software is becoming
important. On top of many AI applications already using mobile hardware, and
even browsers for computationally demanding AI applications, we are already
witnessing the emergence of client-side (federated) machine learning
algorithms, driven by the interests of large corporations and startups alike.
Apart from mathematical and algorithmic concerns, this trend especially demands
new levels of computational efficiency from client environments. Consequently,
this paper deals with the question of state-of-the-art performance by
presenting a comparison study between native code and different browser-based
implementations: JavaScript, ASM.js as well as WebAssembly on a representative
mix of algorithms. Our results show that current efforts in runtime
optimization push the boundaries well towards (and even beyond) native binary
performance. We analyze the results obtained and speculate on the reasons
behind some surprises, rounding the paper off by outlining future possibilities
as well as some of our own research efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malle_B/0/1/0/all/0/1&quot;&gt;Bernd Malle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giuliani_N/0/1/0/all/0/1&quot;&gt;Nicola Giuliani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kieseberg_P/0/1/0/all/0/1&quot;&gt;Peter Kieseberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holzinger_A/0/1/0/all/0/1&quot;&gt;Andreas Holzinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03855">
<title>MedTQ: Dynamic Topic Discovery and Query Generation for Medical Ontologies. (arXiv:1802.03855v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1802.03855</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical ontology refers to a shared conceptualization for a biomedical
domain of interest that has vastly improved data management and data sharing
through the open data movement. The rapid growth and availability of biomedical
data make it impractical and computationally expensive to perform manual
analysis and query processing with the large scale ontologies. The lack of
ability in analyzing ontologies from such a variety of sources, and supporting
knowledge discovery for clinical practice and biomedical research should be
overcome with new technologies. In this study, we developed a Medical Topic
discovery and Query generation framework (MedTQ), which was composed by a
series of approaches and algorithms. A predicate neighborhood pattern-based
approach introduced has the ability to compute the similarity of predicates
(relations) in ontologies. Given a predicate similarity metric, machine
learning algorithms have been developed for automatic topic discovery and query
generation. The topic discovery algorithm, called the hierarchical K-Means
algorithm was designed by extending an existing supervised algorithm (K-means
clustering) for the construction of a topic hierarchy. In the hierarchical
K-Means algorithm, a level-by-level optimization strategy was selected for
consistent with the strongly association between elements within a topic.
Automatic query generation was facilitated for discovered topic that could be
guided users for interactive query design and processing. Evaluation was
conducted to generate topic hierarchy for DrugBank ontology as a case study.
Results demonstrated that the MedTQ framework can enhance knowledge discovery
by capturing underlying structures from domain specific data and ontologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1&quot;&gt;Feichen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yugyung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04007">
<title>ProofWatch: Watchlist Guidance for Large Theories in E. (arXiv:1802.04007v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.04007</link>
<description rdf:parseType="Literal">&lt;p&gt;Watchlist (also hint list) is a mechanism that allows related proofs to guide
a proof search for a new conjecture. This mechanism has been used with the
Otter and Prover9 theorem provers, both for interactive formalizations and for
human-assisted proving of open conjectures in small theories. In this work we
explore the use of watchlists in large theories coming from first-order
translations of large ITP libraries, aiming at improving hammer-style
automation by smarter internal guidance of the ATP systems. In particular, we
(i) design watchlist-based clause evaluation heuristics inside the E ATP
system, and (ii) develop new proof guiding algorithms that load many previous
proofs inside the ATP and focus the proof search using a dynamically updated
notion of proof matching. The methods are evaluated on a large set of problems
coming from the Mizar library, showing significant improvement of E&apos;s standard
portfolio of strategies, and also of the previous best set of strategies
invented for Mizar by evolutionary methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goertzel_Z/0/1/0/all/0/1&quot;&gt;Zarathustra Goertzel&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakub%5Cr%7Bu%7Dv_J/0/1/0/all/0/1&quot;&gt;Jan Jakub&amp;#x16f;v&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_S/0/1/0/all/0/1&quot;&gt;Stephan Schulz&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1&quot;&gt;Josef Urban&lt;/a&gt; (1) ((1) Czech Technical University in Prague, (2) DHBW Stuttgart)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04095">
<title>A New Multi Criteria Decision Making Method: Approach of Logarithmic Concept (APLOCO). (arXiv:1802.04095v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.04095</link>
<description rdf:parseType="Literal">&lt;p&gt;The primary aim of the study is to introduce APLOCO method which is developed
for the solution of multicriteria decision making problems both theoretically
and practically. In this context, application subject of APLACO constitutes
evaluation of investment potential of different cities in metropolitan status
in Turkey. The secondary purpose of the study is to identify the independent
variables affecting the factories in the operating phase and to estimate the
effect levels of independent variables on the dependent variable in the
organized industrial zones (OIZs), whose mission is to reduce regional
development disparities and to mobilize local production dynamics. For this
purpose, the effect levels of independent variables on dependent variables have
been determined using the multilayer perceptron (MLP) method, which has a wide
use in artificial neural networks (ANNs). The effect levels derived from MLP
have been then used as the weight levels of the decision criteria in APLOCO.
The independent variables included in MLP are also used as the decision
criteria in APLOCO. According to the results obtained from APLOCO, Istanbul
city is the best alternative in term of the investment potential and other
alternatives are Manisa, Denizli, Izmir, Kocaeli, Bursa, Ankara, Adana, and
Antalya, respectively. Although APLOCO is used to solve the ranking problem in
order to show application process in the paper, it can be employed easily in
the solution of classification and selection problems. On the other hand, the
study also shows a rare example of the nested usage of APLOCO which is one of
the methods of operation research as well as MLP used in determination of
weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulut_T/0/1/0/all/0/1&quot;&gt;Tevfik Bulut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04205">
<title>Leveraging Task Knowledge for Robot Motion Planning Under Uncertainty. (arXiv:1802.04205v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1802.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;Noisy observations coupled with nonlinear dynamics pose one of the biggest
challenges in robot motion planning. By decomposing the nonlinear dynamics into
a discrete set of local dynamics models, hybrid dynamics provide a natural way
to model nonlinear dynamics, especially in systems with sudden &quot;jumps&quot; in the
dynamics, due to factors such as contacts. We propose a hierarchical POMDP
planner that develops locally optimal motion plans for hybrid dynamics models.
The hierarchical planner first develops a high-level motion plan to sequence
the local dynamics models to be visited. The high-level plan is then converted
into a detailed cost-optimized continuous state plan. This hierarchical
planning approach results in a decomposition of the POMDP planning problem into
smaller sub-parts that can be solved with significantly lower computational
costs. The ability to sequence the visitation of local dynamics models also
provides a powerful way to leverage the hybrid dynamics to reduce state
uncertainty. We evaluate the proposed planner for two navigation and
localization tasks in simulated domains, as well as an assembly task with a
real robotic manipulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Ajinkya Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04240">
<title>Deep Reinforcement Learning for Solving the Vehicle Routing Problem. (arXiv:1802.04240v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.04240</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an end-to-end framework for solving Vehicle Routing Problem (VRP)
using deep reinforcement learning. In this approach, we train a single model
that finds near-optimal solutions for problem instances sampled from a given
distribution, only by observing the reward signals and following feasibility
rules. Our model represents a parameterized stochastic policy, and by applying
a policy gradient algorithm to optimize its parameters, the trained model
produces the solution as a sequence of consecutive actions in real time,
without the need to re-train for every new problem instance. Our method is
faster in both training and inference than a recent method that solves the
Traveling Salesman Problem (TSP), with nearly identical solution quality. On
the more general VRP, our approach outperforms classical heuristics on
medium-sized instances in both solution quality and computation time (after
training). Our proposed framework can be applied to variants of the VRP such as
the stochastic VRP, and has the potential to be applied more generally to
combinatorial optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazari_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oroojlooy_A/0/1/0/all/0/1&quot;&gt;Afshin Oroojlooy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snyder_L/0/1/0/all/0/1&quot;&gt;Lawrence V. Snyder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1&quot;&gt;Martin Tak&amp;#xe1;&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06060">
<title>Consistent feature attribution for tree ensembles. (arXiv:1706.06060v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06060</link>
<description rdf:parseType="Literal">&lt;p&gt;It is critical in many applications to understand what features are important
for a model, and why individual predictions were made. For tree ensemble
methods these questions are usually answered by attributing importance values
to input features, either globally or for a single prediction. Here we show
that current feature attribution methods are inconsistent, which means changing
the model to rely more on a given feature can actually decrease the importance
assigned to that feature. To address this problem we develop fast exact
solutions for SHAP (SHapley Additive exPlanation) values, which were recently
shown to be the unique additive feature attribution method based on conditional
expectations that is both consistent and locally accurate. We integrate these
improvements into the latest version of XGBoost, demonstrate the
inconsistencies of current methods, and show how using SHAP values results in
significantly improved supervised clustering performance. Feature importance
values are a key part of understanding widely used models such as gradient
boosting trees and random forests, so improvements to them have broad practical
implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1&quot;&gt;Scott M. Lundberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Su-In Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04749">
<title>Explaining Aviation Safety Incidents Using Deep Temporal Multiple Instance Learning. (arXiv:1710.04749v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04749</link>
<description rdf:parseType="Literal">&lt;p&gt;Although aviation accidents are rare, safety incidents occur more frequently
and require a careful analysis to detect and mitigate risks in a timely manner.
Analyzing safety incidents using operational data and producing event-based
explanations is invaluable to airline companies as well as to governing
organizations such as the Federal Aviation Administration (FAA) in the United
States. However, this task is challenging because of the complexity involved in
mining multi-dimensional heterogeneous time series data, the lack of
time-step-wise annotation of events in a flight, and the lack of scalable tools
to perform analysis over a large number of events. In this work, we propose a
precursor mining algorithm that identifies events in the multidimensional time
series that are correlated with the safety incident. Precursors are valuable to
systems health and safety monitoring and in explaining and forecasting safety
incidents. Current methods suffer from poor scalability to high dimensional
time series data and are inefficient in capturing temporal behavior. We propose
an approach by combining multiple-instance learning (MIL) and deep recurrent
neural networks (DRNN) to take advantage of MIL&apos;s ability to learn using weakly
supervised data and DRNN&apos;s ability to model temporal behavior. We describe the
algorithm, the data, the intuition behind taking a MIL approach, and a
comparative analysis of the proposed algorithm with baseline models. We also
discuss the application to a real-world aviation safety problem using data from
a commercial airline company and discuss the model&apos;s abilities and
shortcomings, with some final remarks about possible deployment directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janakiraman_V/0/1/0/all/0/1&quot;&gt;Vijay Manikandan Janakiraman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03168">
<title>Greenhouse: A Zero-Positive Machine Learning System for Time-Series Anomaly Detection. (arXiv:1801.03168v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03168</link>
<description rdf:parseType="Literal">&lt;p&gt;This short paper describes our ongoing research on Greenhouse - a
zero-positive machine learning system for time-series anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tae Jun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1&quot;&gt;Justin Gottschlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1&quot;&gt;Nesime Tatbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metcalf_E/0/1/0/all/0/1&quot;&gt;Eric Metcalf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zdonik_S/0/1/0/all/0/1&quot;&gt;Stan Zdonik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07729">
<title>The Shape of Art History in the Eyes of the Machine. (arXiv:1801.07729v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07729</link>
<description rdf:parseType="Literal">&lt;p&gt;How does the machine classify styles in art? And how does it relate to art
historians&apos; methods for analyzing style? Several studies have shown the ability
of the machine to learn and predict style categories, such as Renaissance,
Baroque, Impressionism, etc., from images of paintings. This implies that the
machine can learn an internal representation encoding discriminative features
through its visual analysis. However, such a representation is not necessarily
interpretable. We conducted a comprehensive study of several of the
state-of-the-art convolutional neural networks applied to the task of style
classification on 77K images of paintings, and analyzed the learned
representation through correlation analysis with concepts derived from art
history. Surprisingly, the networks could place the works of art in a smooth
temporal arrangement mainly based on learning style labels, without any a
priori knowledge of time of creation, the historical time and context of
styles, or relations between styles. The learned representations showed that
there are few underlying factors that explain the visual variations of style in
art. Some of these factors were found to correlate with style patterns
suggested by Heinrich W\&quot;olfflin (1846-1945). The learned representations also
consistently highlighted certain artists as the extreme distinctive
representative of their styles, which quantitatively confirms art historian
observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elgammal_A/0/1/0/all/0/1&quot;&gt;Ahmed Elgammal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzone_M/0/1/0/all/0/1&quot;&gt;Marian Mazzone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Diana Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03418">
<title>Predicting University Students&apos; Academic Success and Choice of Major using Random Forests. (arXiv:1802.03418v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03418</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a large data set containing every course taken by every
undergraduate student in a major university in Canada over 10 years is
analyzed. Modern machine learning algorithms can use large data sets to build
useful tools for the data provider, in this case, the university. In this
article, two classifiers are constructed using random forests. To begin, the
first two semesters of courses completed by a student are used to predict if
they will obtain an undergraduate degree. Secondly, for the students that
completed a program, their major choice is predicted using once again the first
few courses they&apos;ve registered to. A classification tree is an intuitive and
powerful classifier and building a random forest of trees lowers the variance
of the classifier and also prevents overfitting. Random forests also allow for
reliable variable importance measurements. These measures explain what
variables are useful to both of the classifiers and can be used to better
understand what is statistically related to the students&apos; choices. The results
are two accurate classifiers and a variable importance analysis that provides
useful information to the university.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beaulac_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Beaulac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosenthal_J/0/1/0/all/0/1&quot;&gt;Jeffrey S. Rosenthal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03426">
<title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. (arXiv:1802.03426v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03426</link>
<description rdf:parseType="Literal">&lt;p&gt;UMAP (Uniform Manifold Approximation and Projection) is a novel manifold
learning technique for dimension reduction. UMAP is constructed from a
theoretical framework based in Riemannian geometry and algebraic topology. The
result is a practical scalable algorithm that applies to real world data. The
UMAP algorithm is competitive with t-SNE for visualization quality, and
arguably preserves more of the global structure with superior run time
performance. Furthermore, UMAP as described has no computational restrictions
on embedding dimension, making it viable as a general purpose dimension
reduction technique for machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McInnes_L/0/1/0/all/0/1&quot;&gt;Leland McInnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Healy_J/0/1/0/all/0/1&quot;&gt;John Healy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03451">
<title>Estimating the Spectral Density of Large Implicit Matrices. (arXiv:1802.03451v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03451</link>
<description rdf:parseType="Literal">&lt;p&gt;Many important problems are characterized by the eigenvalues of a large
matrix. For example, the difficulty of many optimization problems, such as
those arising from the fitting of large models in statistics and machine
learning, can be investigated via the spectrum of the Hessian of the empirical
loss function. Network data can be understood via the eigenstructure of a graph
Laplacian matrix using spectral graph theory. Quantum simulations and other
many-body problems are often characterized via the eigenvalues of the solution
space, as are various dynamic systems. However, naive eigenvalue estimation is
computationally expensive even when the matrix can be represented; in many of
these situations the matrix is so large as to only be available implicitly via
products with vectors. Even worse, one may only have noisy estimates of such
matrix vector products. In this work, we combine several different techniques
for randomized estimation and show that it is possible to construct unbiased
estimators to answer a broad class of questions about the spectra of such
implicit matrices, even in the presence of noise. We validate these methods on
large-scale problems in which graph theory and random matrix theory provide
ground truth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adams_R/0/1/0/all/0/1&quot;&gt;Ryan P. Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pennington_J/0/1/0/all/0/1&quot;&gt;Jeffrey Pennington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Matthew J. Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Jamie Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ovadia_Y/0/1/0/all/0/1&quot;&gt;Yaniv Ovadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Patton_B/0/1/0/all/0/1&quot;&gt;Brian Patton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saunderson_J/0/1/0/all/0/1&quot;&gt;James Saunderson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03475">
<title>Communication-Computation Efficient Gradient Coding. (arXiv:1802.03475v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03475</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops coding techniques to reduce the running time of
distributed learning tasks. It characterizes the fundamental tradeoff to
compute gradients (and more generally vector summations) in terms of three
parameters: computation load, straggler tolerance and communication cost. It
further gives an explicit coding scheme that achieves the optimal tradeoff
based on recursive polynomial constructions, coding both across data subsets
and vector components. As a result, the proposed scheme allows to minimize the
running time for gradient computations. Implementations are made on Amazon EC2
clusters using Python with mpi4py package. Results show that the proposed
scheme maintains the same generalization error while reducing the running time
by $32\%$ compared to uncoded schemes and $23\%$ compared to prior coded
schemes focusing only on stragglers (Tandon et al., ICML 2017).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Min Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbe_E/0/1/0/all/0/1&quot;&gt;Emmanuel Abbe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03488">
<title>Generalization of an Upper Bound on the Number of Nodes Needed to Achieve Linear Separability. (arXiv:1802.03488v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03488</link>
<description rdf:parseType="Literal">&lt;p&gt;An important issue in neural network research is how to choose the number of
nodes and layers such as to solve a classification problem. We provide new
intuitions based on earlier results by An et al. (2015) by deriving an upper
bound on the number of nodes in networks with two hidden layers such that
linear separability can be achieved. Concretely, we show that if the data can
be described in terms of N finite sets and the used activation function f is
non-constant, increasing and has a left asymptote, we can derive how many nodes
are needed to linearly separate these sets. This will be an upper bound that
depends on the structure of the data. This structure can be analyzed using an
algorithm. For the leaky rectified linear activation function, we prove
separately that under some conditions on the slope, the same number of layers
and nodes as for the aforementioned activation functions is sufficient. We
empirically validate our claims.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Troost_M/0/1/0/all/0/1&quot;&gt;Marjolein Troost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seeliger_K/0/1/0/all/0/1&quot;&gt;Katja Seeliger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gerven_M/0/1/0/all/0/1&quot;&gt;Marcel van Gerven&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03497">
<title>Modeling Dynamics with Deep Transition-Learning Networks. (arXiv:1802.03497v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03497</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov processes, both classical and higher order, are often used to model
dynamic processes, such as stock prices, molecular dynamics, and Monte Carlo
methods. Previous works have shown that an autoencoder can be formulated as a
specific type of Markov chain. Here, we propose a generative neural network
known as a transition encoder, or \emph{transcoder}, which learns such
continuous-state dynamic processes. We show that the transcoder is able to
learn both deterministic and stochastic dynamic processes on several systems.
We explore a number of applications of the transcoder including generating
unseen trajectories and examining the propensity for chaos in a dynamic system.
Further, we show that the transcoder can speed up Markov Chain Monte Carlo
(MCMC) sampling to a convergent distribution by training it to make several
steps at a time. Finally, we show that the hidden layers of a transcoder are
useful for visualization and salient feature extraction of the transition
process itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dijk_D/0/1/0/all/0/1&quot;&gt;David van Dijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gigante_S/0/1/0/all/0/1&quot;&gt;Scott Gigante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strzalkowski_A/0/1/0/all/0/1&quot;&gt;Alexander Strzalkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1&quot;&gt;Guy Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1&quot;&gt;Smita Krishnaswamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03522">
<title>Enhanced version of AdaBoostM1 with J48 Tree learning method. (arXiv:1802.03522v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03522</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning focuses on the construction and study of systems that can
learn from data. This is connected with the classification problem, which
usually is what Machine Learning algorithms are designed to solve. When a
machine learning method is used by people with no special expertise in machine
learning, it is important that the method be robust in classification, in the
sense that reasonable performance is obtained with minimal tuning of the
problem at hand. Algorithms are evaluated based on how robust they can classify
the given data. In this paper, we propose a quantifiable measure of robustness,
and describe a particular learning method that is robust according to this
measure in the context of classification problem. We proposed Adaptive Boosting
(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold
(P) and number of iterations (I) for boosting algorithm. To benchmark the
performance, we used the baseline classifier, AdaBoostM1 with Decision Stump as
base learner without tuning parameters. By tuning parameters and using J48 as
base learner, we are able to reduce the overall average error rate ratio
(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2
for evaluation sets of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kang_K/0/1/0/all/0/1&quot;&gt;Kyongche Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michalak_J/0/1/0/all/0/1&quot;&gt;Jack Michalak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03532">
<title>Bayesian Optimization Using Monotonicity Information and Its Application in Machine Learning Hyperparameter. (arXiv:1802.03532v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03532</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an algorithm for a family of optimization problems where the
objective can be decomposed as a sum of functions with monotonicity properties.
The motivating problem is optimization of hyperparameters of machine learning
algorithms, where we argue that the objective, validation error, can be
decomposed as monotonic functions of the hyperparameters. Our proposed
algorithm adapts Bayesian optimization methods to incorporate the monotonicity
constraints. We illustrate the advantages of exploiting monotonicity using
illustrative examples and demonstrate the improvements in optimization
efficiency for some machine learning hyperparameter tuning applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welch_W/0/1/0/all/0/1&quot;&gt;William J. Welch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03569">
<title>Riemannian Manifold Kernel for Persistence Diagrams. (arXiv:1802.03569v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03569</link>
<description rdf:parseType="Literal">&lt;p&gt;Algebraic topology methods have recently played an important role for
statistical analysis with complicated geometric structured data. Among them,
persistent homology is a well-known tool to extract robust topological
features, and outputs as persistence diagrams. Unfortunately, persistence
diagrams are point multi-sets which can not be used in machine learning
algorithms for vector data. To deal with it, an emerged approach is to use
kernel methods. Besides that, geometry for persistence diagrams is also an
important factor. A popular geometry for persistence diagrams is the
Wasserstein metric. However, Wasserstein distance is not negative definite.
Thus, it is limited to build positive definite kernels upon the Wasserstein
distance without approximation. In this work, we explore an alternative
Riemannian manifold geometry, namely the Fisher information metric. By building
upon the geodesic distance on the Riemannian manifold, we propose a positive
definite kernel, namely Riemannian manifold kernel. Then, we analyze
eigensystem of the integral operator induced by the proposed kernel for kernel
machines. Based on that, we conduct generalization error bounds via covering
numbers and Rademacher averages for kernel machines using the Riemannian
manifold kernel. Additionally, we also show some nice properties for the
proposed kernel such as stability, infinite divisibility and comparative time
complexity with other kernels for persistence diagrams in term of computation.
Throughout experiments with many different tasks on various benchmark datasets,
we illustrate that the Riemannian manifold kernel improves performances of
other baseline kernels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tam Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamada_M/0/1/0/all/0/1&quot;&gt;Makoto Yamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03604">
<title>Feature-Distributed SVRG for High-Dimensional Linear Classification. (arXiv:1802.03604v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03604</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear classification has been widely used in many high-dimensional
applications like text classification. To perform linear classification for
large-scale tasks, we often need to design distributed learning methods on a
cluster of multiple machines. In this paper, we propose a new distributed
learning method, called feature-distributed stochastic variance reduced
gradient (FD-SVRG) for high-dimensional linear classification. Unlike most
existing distributed learning methods which are instance-distributed, FD-SVRG
is feature-distributed. FD-SVRG has lower communication cost than other
instance-distributed methods when the data dimensionality is larger than the
number of data instances. Experimental results on real data demonstrate that
FD-SVRG can outperform other state-of-the-art distributed methods for
high-dimensional linear classification in terms of both communication cost and
wall-clock time, when the dimensionality is larger than the number of instances
in training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gong-Duo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shen-Yi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wu-Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03628">
<title>Learning Correlation Space for Time Series. (arXiv:1802.03628v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03628</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approximation algorithm for efficient correlation search in
time series data. In our method, we use Fourier transform and neural network to
embed time series into a low-dimensional Euclidean space. The given space is
learned such that time series correlation can be effectively approximated from
Euclidean distance between corresponding embedded vectors. Therefore, search
for correlated time series can be done using an index in the embedding space
for efficient nearest neighbor search. Our theoretical analysis illustrates
that our method&apos;s accuracy can be guaranteed under certain regularity
conditions. We further conduct experiments on real-world datasets and the
results show that our method indeed outperforms the baseline solution. In
particular, for approximation of correlation, our method reduces the
approximation loss by a half in most test cases compared to the baseline
solution. For top-$k$ highest correlation search, our method improves the
precision from 5\% to 20\% while the query time is similar to the baseline
approach query time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1&quot;&gt;Hoang Thanh Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Francesco Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinn_M/0/1/0/all/0/1&quot;&gt;Mathieu Sinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03644">
<title>Learning to Recommend via Inverse Optimal Matching. (arXiv:1802.03644v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03644</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider recommendation in the context of optimal matching, i.e., we need
to pair or match a user with an item in an optimal way. The framework is
particularly relevant when the supply of an individual item is limited and it
can only satisfy a small number of users even though it may be preferred by
many. We leverage the methodology of optimal transport of discrete
distributions and formulate an inverse optimal transport problem in order to
learn the cost which gives rise to the observed matching. It leads to a
non-convex optimization problem which is solved by alternating optimization. A
key novel aspect of our formulation is the incorporation of marginal relaxation
via regularized Wasserstein distance, significantly improving the robustness of
the method in the face of observed empirical matchings. Our model has wide
applicability including labor market, online dating, college application
recommendation. We back up our claims with experiments on both synthetic data
and real world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruilin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaojing Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haomin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03676">
<title>Differentiable Dynamic Programming for Structured Prediction and Attention. (arXiv:1802.03676v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03676</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic programming (DP) solves a variety of structured combinatorial
problems by iteratively breaking them down into smaller subproblems. In spite
of their versatility, DP algorithms are usually non-differentiable, which
hampers their use as a layer in neural networks trained by backpropagation. To
address this issue, we propose to smooth the max operator in the dynamic
programming recursion, using a strongly convex regularizer. This allows to
relax both the optimal value and solution of the original combinatorial
problem, and turns a broad class of DP algorithms into differentiable
operators. Theoretically, we provide a new probabilistic perspective on
backpropagating through these DP operators, and relate them to inference in
graphical models. We derive two particular instantiations of our framework, a
smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm
for time-series alignment. We showcase these instantiations on two structured
prediction tasks and on structured and sparse attention for neural machine
translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mensch_A/0/1/0/all/0/1&quot;&gt;Arthur Mensch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blondel_M/0/1/0/all/0/1&quot;&gt;Mathieu Blondel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03688">
<title>On the Rates of Convergence from Surrogate Risk Minimizers to the Bayes Optimal Classifier. (arXiv:1802.03688v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03688</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the rates of convergence from empirical surrogate risk minimizers to
the Bayes optimal classifier. Specifically, we introduce the notion of
\emph{consistency intensity} to characterize a surrogate loss function and
exploit this notion to obtain the rate of convergence from an empirical
surrogate risk minimizer to the Bayes optimal classifier, enabling fair
comparisons of the excess risks of different surrogate risk minimizers. The
main result of the paper has practical implications including (1) showing that
hinge loss is superior to logistic and exponential loss in the sense that its
empirical minimizer converges faster to the Bayes optimal classifier and (2)
guiding to modify surrogate loss functions to accelerate the convergence to the
Bayes optimal classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03692">
<title>Nearly Optimal Adaptive Procedure for Piecewise-Stationary Bandit: a Change-Point Detection Approach. (arXiv:1802.03692v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03692</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-armed bandit (MAB) is a class of online learning problems where a
learning agent aims to maximize its expected cumulative reward while repeatedly
selecting to pull arms with unknown reward distributions. In this paper, we
consider a scenario in which the arms&apos; reward distributions may change in a
piecewise-stationary fashion at unknown time steps. By connecting
change-detection techniques with classic UCB algorithms, we motivate and
propose a learning algorithm called M-UCB, which can detect and adapt to
changes, for the considered scenario. We also establish an $O(\sqrt{MKT\log
T})$ regret bound for M-UCB, where $T$ is the number of time steps, $K$ is the
number of arms, and $M$ is the number of stationary segments. % and $\Delta$ is
the gap between the expected rewards of the optimal and best suboptimal arms.
Comparison with the best available lower bound shows that M-UCB is nearly
optimal in $T$ up to a logarithmic factor. We also compare M-UCB with
state-of-the-art algorithms in a numerical experiment based on a public Yahoo!
dataset. In this experiment, M-UCB achieves about $50 \%$ regret reduction with
respect to the best performing state-of-the-art algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yao Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03699">
<title>PCA-Based Missing Information Imputation for Real-Time Crash Likelihood Prediction Under Imbalanced Data. (arXiv:1802.03699v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03699</link>
<description rdf:parseType="Literal">&lt;p&gt;The real-time crash likelihood prediction has been an important research
topic. Various classifiers, such as support vector machine (SVM) and tree-based
boosting algorithms, have been proposed in traffic safety studies. However, few
research focuses on the missing data imputation in real-time crash likelihood
prediction, although missing values are commonly observed due to breakdown of
sensors or external interference. Besides, classifying imbalanced data is also
a difficult problem in real-time crash likelihood prediction, since it is hard
to distinguish crash-prone cases from non-crash cases which compose the
majority of the observed samples. In this paper, principal component analysis
(PCA) based approaches, including LS-PCA, PPCA, and VBPCA, are employed for
imputing missing values, while two kinds of solutions are developed to solve
the problem in imbalanced data. The results show that PPCA and VBPCA not only
outperform LS-PCA and other imputation methods (including mean imputation and
k-means clustering imputation), in terms of the root mean square error (RMSE),
but also help the classifiers achieve better predictive performance. The two
solutions, i.e., cost-sensitive learning and synthetic minority oversampling
technique (SMOTE), help improve the sensitivity by adjusting the classifiers to
pay more attention to the minority class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1&quot;&gt;Jintao Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuaichao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiqun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03713">
<title>Optimizing Neural Networks in the Equivalent Class Space. (arXiv:1802.03713v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03713</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been widely observed that many activation functions and pooling
methods of neural network models have (positive-) rescaling-invariant property,
including ReLU, PReLU, max-pooling, and average pooling, which makes
fully-connected neural networks (FNNs) and convolutional neural networks (CNNs)
invariant to (positive) rescaling operation across layers. This may cause
unneglectable problems with their optimization: (1) different NN models could
be equivalent, but their gradients can be very different from each other; (2)
it can be proven that the loss functions may have many spurious critical points
in the redundant weight space. To tackle these problems, in this paper, we
first characterize the rescaling-invariant properties of NN models using
equivalent classes and prove that the dimension of the equivalent class space
is significantly smaller than the dimension of the original weight space. Then
we represent the loss function in the compact equivalent class space and
develop novel algorithms that conduct optimization of the NN models directly in
the equivalent class space. We call these algorithms Equivalent Class
Optimization (abbreviated as EC-Opt) algorithms. Moreover, we design efficient
tricks to compute the gradients in the equivalent class, which almost have no
extra computational complexity as compared to standard back-propagation (BP).
We conducted experimental study to demonstrate the effectiveness of our
proposed new optimization algorithms. In particular, we show that by using the
idea of EC-Opt, we can significantly improve the accuracy of the learned model
(for both FNN and CNN), as compared to using conventional stochastic gradient
descent algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meng_Q/0/1/0/all/0/1&quot;&gt;Qi Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qiwei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03725">
<title>Evolving Latent Space Model for Dynamic Networks. (arXiv:1802.03725v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1802.03725</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks observed in the real world like social networks, collaboration
networks etc., exhibit temporal dynamics, i.e. nodes and edges appear and/or
disappear over time. In this paper, we propose a generative, latent space
based, statistical model for such networks (called dynamic networks). We
consider the case where the number of nodes is fixed, but the presence of edges
can vary over time. Our model allows the number of communities in the network
to be different at different time steps. We use a neural network based
methodology to perform approximate inference in the proposed model and its
simplified version. Experiments done on synthetic and real-world networks for
the task of community detection and link prediction demonstrate the utility and
effectiveness of our model as compared to other similar existing approaches. To
the best of our knowledge, this is the first work that integrates statistical
modeling of dynamic networks with deep learning for community detection and
link prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shubham Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gaurav Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dukkipati_A/0/1/0/all/0/1&quot;&gt;Ambedkar Dukkipati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03752">
<title>Supervised classification of Dermatological diseases by Deep neural networks. (arXiv:1802.03752v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03752</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a deep learning based classifier for common skin
ailments, to help people without easy access to dermatologists. We have
confirmed that it can classify at approximately 80% accuracy on average, when
primary care doctors are reported to have 53% success as per recent literature.
Dermatological diseases are common in every population and have a wide spectrum
in severity. With a shortage of dermatological experts being observed in many
countries, machine learning solutions can offer timely medical advice regarding
existence of common skin diseases. The paper implements supervised
classification of nine distinct dermatological diseases which have high
occurrence in East Asian countries. Our current attempt establishes that deep
learning based techniques are viable avenues for preliminary information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Sourav Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamasaki_T/0/1/0/all/0/1&quot;&gt;Toshihiko Yamasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imaizumi_H/0/1/0/all/0/1&quot;&gt;Hideaki Imaizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hirano_H/0/1/0/all/0/1&quot;&gt;Hiromi Hirano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03759">
<title>Multi-set Canonical Correlation Analysis simply explained. (arXiv:1802.03759v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03759</link>
<description rdf:parseType="Literal">&lt;p&gt;There are a multitude of methods to perform multi-set correlated component
analysis (MCCA), including some that require iterative solutions. The methods
differ on the criterion they optimize and the constraints placed on the
solutions. This note focuses perhaps on the simplest version, which can be
solved in a single step as the eigenvectors of matrix ${\bf D}^{-1} {\bf R}$.
Here ${\bf R}$ is the covariance matrix of the concatenated data, and ${\bf D}$
is its block-diagonal. This note shows that this solution maximizes inter-set
correlation (ISC) without further constraints. It also relates the solution to
a two step procedure, which first whitens each dataset using PCA, and then
performs an additional PCA on the concatenated and whitened data. Both these
solutions are known, although a clear derivation and simple implementation are
hard to find. This short note aims to remedy this.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parra_L/0/1/0/all/0/1&quot;&gt;Lucas C Parra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03765">
<title>Convex Formulations for Fair Principal Component Analysis. (arXiv:1802.03765v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03765</link>
<description rdf:parseType="Literal">&lt;p&gt;Though there is a growing body of literature on fairness for supervised
learning, the problem of incorporating fairness into unsupervised learning has
been less well-studied. This paper studies fairness in the context of principal
component analysis (PCA). We first present a definition of fairness for
dimensionality reduction, and our definition can be interpreted as saying that
a reduction is fair if information about a protected class (e.g., race or
gender) cannot be inferred from the dimensionality-reduced data points. Next,
we develop convex optimization formulations that can improve the fairness (with
respect to our definition) of PCA and kernel PCA. These formulations are
semidefinite programs (SDP&apos;s), and we demonstrate the effectiveness of our
formulations using several datasets. We conclude by showing how our approach
can be used to perform a fair (with respect to age) clustering of health data
that may be used to set health insurance rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olfat_M/0/1/0/all/0/1&quot;&gt;Matt Olfat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aswani_A/0/1/0/all/0/1&quot;&gt;Anil Aswani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03801">
<title>SGD and Hogwild! Convergence Without the Bounded Gradients Assumption. (arXiv:1802.03801v1 [math.OC])</title>
<link>http://arxiv.org/abs/1802.03801</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient descent (SGD) is the optimization algorithm of choice in
many machine learning applications such as regularized empirical risk
minimization and training deep neural networks. The classical analysis of
convergence of SGD is carried out under the assumption that the norm of the
stochastic gradient is uniformly bounded. While this might hold for some loss
functions, it is always violated for cases where the objective function is
strongly convex. In (Bottou et al.,2016) a new analysis of convergence of SGD
is performed under the assumption that stochastic gradients are bounded with
respect to the true gradient norm. Here we show that for stochastic problems
arising in machine learning such bound always holds. Moreover, we propose an
alternative convergence analysis of SGD with diminishing learning rate regime,
which is results in more relaxed conditions that those in (Bottou et al.,2016).
We then move on the asynchronous parallel setting, and prove convergence of the
Hogwild! algorithm in the same regime, obtaining the first convergence results
for this method in the case of diminished learning rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Lam M. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong Ha Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dijk_M/0/1/0/all/0/1&quot;&gt;Marten van Dijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scheinberg_K/0/1/0/all/0/1&quot;&gt;Katya Scheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Takac_M/0/1/0/all/0/1&quot;&gt;Martin Tak&amp;#xe1;&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03840">
<title>Uncharted Forest a Technique for Exploratory Data Analysis of Provenance Studies. (arXiv:1802.03840v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03840</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploratory data analysis is a crucial task for developing effective
classification models from high dimensional datasets. We explore the utility of
a new unsupervised tree ensemble which we call, uncharted forest, for purposes
of elucidating class associations, sample-sample associations, class
heterogeneity, and uninformative classes for provenance studies. Uncharted
forest partitions data along random variables which offer the most gain from
various gain metrics, namely variance. After each tree is grown, a tally of
every terminal node&apos;s sample membership is constructed such that a
probabilistic measure for each sample being partitioned with one another can be
stored in one matrix. That matrix may be readily viewed as a heat map, and the
probabilities can be quantified via metrics which account for class or cluster
membership. We display the advantages and limitations of this technique by
applying it to 1 exemplary dataset and 3 provenance study datasets. The method
is also validated by comparing the sample association metrics to clustering
algorithms with known variance based clustering mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kneale_C/0/1/0/all/0/1&quot;&gt;Casey Kneale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_S/0/1/0/all/0/1&quot;&gt;Steven D. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03848">
<title>Region Detection in Markov Random Fields: Gaussian Case. (arXiv:1802.03848v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03848</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we consider the problem of model selection in Gaussian Markov
fields in the sample deficient scenario. The benchmark information-theoretic
results in the case of d-regular graphs require the number of samples to be at
least proportional to the logarithm of the number of vertices to allow
consistent graph recovery. When the number of samples is less than this amount,
reliable detection of all edges is impossible. In many applications, it is more
important to learn the distribution of the edge (coupling) parameters over the
network than the specific locations of the edges. Assuming that the entire
graph can be partitioned into a number of spatial regions with similar edge
parameters and reasonably regular boundaries, we develop new
information-theoretic sample complexity bounds and show that even bounded
number of samples can be enough to consistently recover these regions. We also
introduce and analyze an efficient region growing algorithm capable of
recovering the regions with high accuracy. We show that it is consistent and
demonstrate its performance benefits in synthetic simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soloveychik_I/0/1/0/all/0/1&quot;&gt;Ilya Soloveychik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03877">
<title>Gaussian Process Classification with Privileged Information by Soft-to-Hard Labeling Transfer. (arXiv:1802.03877v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03877</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning using privileged information is an attractive problem setting that
helps many learning scenarios in the real world. A state-of-the-art method of
Gaussian process classification (GPC) with privileged information is GPC+,
which incorporates privileged information into a noise term of the likelihood.
A drawback of GPC+ is that it requires numerical quadrature to calculate the
posterior distribution of the latent function, which is extremely
time-consuming. To overcome this limitation, we propose a novel classification
method with privileged information based on Gaussian processes, called
&quot;soft-label-transferred Gaussian process (SLT-GP).&quot; Our basic idea is that we
construct another learning task of predicting soft labels (continuous values)
obtained from privileged information and we perform transfer learning from this
task to the target task of predicting hard labels. We derive a PAC-Bayesian
bound of our proposed method, which justifies optimizing hyperparameters by the
empirical Bayes method. We also experimentally show the usefulness of our
proposed method compared with GPC and GPC+.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kamesawa_R/0/1/0/all/0/1&quot;&gt;Ryosuke Kamesawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03888">
<title>Consistent Individualized Feature Attribution for Tree Ensembles. (arXiv:1802.03888v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03888</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting predictions from tree ensemble methods such as gradient boosting
machines and random forests is important, yet feature attribution for trees is
often heuristic and not individualized for each prediction. Here we show that
popular feature attribution methods are inconsistent, meaning they can lower a
feature&apos;s assigned importance when the true impact of that feature actually
increases. This is a fundamental problem that casts doubt on any comparison
between features. To address it we turn to recent applications of game theory
and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation)
values, which are the unique consistent and locally accurate attribution
values. We then extend SHAP values to interaction effects and define SHAP
interaction values. We propose a rich visualization of individualized feature
attributions that improves over classic attribution summaries and partial
dependence plots, and a unique &quot;supervised&quot; clustering (clustering based on
feature attributions). We demonstrate better agreement with human intuition
through a user study, exponential improvements in run time, improved clustering
performance, and better identification of influential features. An
implementation of our algorithm has also been merged into XGBoost and LightGBM,
see &lt;a href=&quot;http://github.com/slundberg/shap&quot;&gt;this http URL&lt;/a&gt; for details.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1&quot;&gt;Scott M. Lundberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erion_G/0/1/0/all/0/1&quot;&gt;Gabriel G. Erion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Su-In Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03900">
<title>Q-learning with Nearest Neighbors. (arXiv:1802.03900v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03900</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of model-free reinforcement learning for
infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous
state space and unknown transition kernels, when only a single sample path of
the system is available. We focus on the classical approach of Q-learning where
the goal is to learn the optimal Q-function. We propose the Nearest Neighbor
Q-Learning approach that utilizes nearest neighbor regression method to learn
the Q function. We provide finite sample analysis of the convergence rate using
this method. In particular, we establish that the algorithm is guaranteed to
output an $\epsilon$-accurate estimate of the optimal Q-function with high
probability using a number of observations that depends polynomially on
$\epsilon$ and the model parameters. To establish our results, we develop a
robust version of stochastic approximation results; this may be of interest in
its own right.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Devavrat Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qiaomin Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03913">
<title>Assessing the Utility of Weather Data for Photovoltaic Power Prediction. (arXiv:1802.03913v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03913</link>
<description rdf:parseType="Literal">&lt;p&gt;Photovoltaic systems have been widely deployed in recent times to meet the
increased electricity demand as an environmental-friendly energy source. The
major challenge for integrating photovoltaic systems in power systems is the
unpredictability of the solar power generated. In this paper, we analyze the
impact of having access to weather information for solar power generation
prediction and find weather information that can help best predict photovoltaic
power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zafarani_R/0/1/0/all/0/1&quot;&gt;Reza Zafarani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eftekharnejad_S/0/1/0/all/0/1&quot;&gt;Sara Eftekharnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Patel_U/0/1/0/all/0/1&quot;&gt;Urvi Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03923">
<title>Safe Triplet Screening for Distance Metric Learning. (arXiv:1802.03923v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03923</link>
<description rdf:parseType="Literal">&lt;p&gt;We study safe screening for metric learning. Distance metric learning can
optimize a metric over a set of triplets, each one of which is defined by a
pair of same class instances and an instance in a different class. However, the
number of possible triplets is quite huge even for a small dataset. Our safe
triplet screening identifies triplets which can be safely removed from the
optimization problem without losing the optimality. Compared with existing safe
screening studies, triplet screening is particularly significant because of (1)
the huge number of possible triplets, and (2) the semi-definite constraint in
the optimization. We derive several variants of screening rules, and analyze
their relationships. Numerical experiments on benchmark datasets demonstrate
the effectiveness of safe triplet screening.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yoshida_T/0/1/0/all/0/1&quot;&gt;Tomoki Yoshida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karasuyama_M/0/1/0/all/0/1&quot;&gt;Masayuki Karasuyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03938">
<title>Revisiting the Vector Space Model: Sparse Weighted Nearest-Neighbor Method for Extreme Multi-Label Classification. (arXiv:1802.03938v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03938</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning has played an important role in information retrieval (IR)
in recent times. In search engines, for example, query keywords are accepted
and documents are returned in order of relevance to the given query; this can
be cast as a multi-label ranking problem in machine learning. Generally, the
number of candidate documents is extremely large (from several thousand to
several million); thus, the classifier must handle many labels. This problem is
referred to as extreme multi-label classification (XMLC). In this paper, we
propose a novel approach to XMLC termed the Sparse Weighted Nearest-Neighbor
Method. This technique can be derived as a fast implementation of
state-of-the-art (SOTA) one-versus-rest linear classifiers for very sparse
datasets. In addition, we show that the classifier can be written as a sparse
generalization of a representer theorem with a linear kernel. Furthermore, our
method can be viewed as the vector space model used in IR. Finally, we show
that the Sparse Weighted Nearest-Neighbor Method can process data points in
real time on XMLC datasets with equivalent performance to SOTA models, with a
single thread and smaller storage footprint. In particular, our method exhibits
superior performance to the SOTA models on a dataset with 3 million labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aoshima_T/0/1/0/all/0/1&quot;&gt;Tatsuhiro Aoshima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kei Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Minami_M/0/1/0/all/0/1&quot;&gt;Mihoko Minami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03981">
<title>Spectral Filtering for General Linear Dynamical Systems. (arXiv:1802.03981v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03981</link>
<description rdf:parseType="Literal">&lt;p&gt;We give a polynomial-time algorithm for learning latent-state linear
dynamical systems without system identification, and without assumptions on the
spectral radius of the system&apos;s transition matrix. The algorithm extends the
recently introduced technique of spectral filtering, previously applied only to
systems with a symmetric transition matrix, using a novel convex relaxation to
allow for the efficient identification of phases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1&quot;&gt;Elad Hazan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Holden Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cyril Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03987">
<title>Latent variable time-varying network inference. (arXiv:1802.03987v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03987</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications of finance, biology and sociology, complex systems
involve entities interacting with each other. These processes have the
peculiarity of evolving over time and of comprising latent factors, which
influence the system without being explicitly measured. In this work we present
latent variable time-varying graphical lasso (LTGL), a method for multivariate
time-series graphical modelling that considers the influence of hidden or
unmeasurable factors. The estimation of the contribution of the latent factors
is embedded in the model which produces both sparse and low-rank components for
each time point. In particular, the first component represents the connectivity
structure of observable variables of the system, while the second represents
the influence of hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both components,
providing an accurate evolutionary pattern of the system. We derive a tractable
optimisation algorithm based on alternating direction method of multipliers,
and develop a scalable and efficient implementation which exploits proximity
operators in closed form. LTGL is extensively validated on synthetic data,
achieving optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art methods for
graphical inference. We conclude with the application of LTGL to real case
studies, from biology and finance, to illustrate how our method can be
successfully employed to gain insights on multivariate time-series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tomasi_F/0/1/0/all/0/1&quot;&gt;Federico Tomasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tozzo_V/0/1/0/all/0/1&quot;&gt;Veronica Tozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salzo_S/0/1/0/all/0/1&quot;&gt;Saverio Salzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verri_A/0/1/0/all/0/1&quot;&gt;Alessandro Verri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04020">
<title>Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning. (arXiv:1802.04020v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.04020</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SCAL, an algorithm designed to perform efficient
exploration-exploitation in any unknown weakly-communicating Markov Decision
Process (MDP) for which an upper bound c on the span of the optimal bias
function is known. For an MDP with S states, A actions and Gamma &amp;lt;= S possible
next states, we prove a regret bound of O(c\sqrt{Gamma SAT}), which
significantly improves over existing algorithms (e.g., UCRL and PSRL), whose
regret scales linearly with the MDP diameter D. In fact, the optimal bias span
is finite and often much smaller than D (e.g., D=infinity in non-communicating
MDPs). A similar result was originally derived by Bartlett and Tewari (2009)
for REGAL.C, for which no tractable algorithm is available. In this paper, we
relax the optimization problem at the core of REGAL.C, we carefully analyze its
properties, and we provide the first computationally efficient algorithm to
solve it. Finally, we report numerical simulations supporting our theoretical
findings and showing how SCAL significantly outperforms UCRL in MDPs with large
diameter and small span.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fruit_R/0/1/0/all/0/1&quot;&gt;Ronan Fruit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1&quot;&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1&quot;&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortner_R/0/1/0/all/0/1&quot;&gt;Ronald Ortner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04023">
<title>Fair and Diverse DPP-based Data Summarization. (arXiv:1802.04023v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.04023</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling methods that choose a subset of the data proportional to its
diversity in the feature space are popular for data summarization. However,
recent studies have noted the occurrence of bias (under- or over-representation
of a certain gender or race) in such data summarization methods. In this paper
we initiate a study of the problem of outputting a diverse and fair summary of
a given dataset. We work with a well-studied determinantal measure of diversity
and corresponding distributions (DPPs) and present a framework that allows us
to incorporate a general class of fairness constraints into such distributions.
Coming up with efficient algorithms to sample from these constrained
determinantal distributions, however, suffers from a complexity barrier and we
present a fast sampler that is provably good when the input vectors satisfy a
natural property. Our experimental results on a real-world and an image dataset
show that the diversity of the samples produced by adding fairness constraints
is not too far from the unconstrained case, and we also provide a theoretical
explanation of it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1&quot;&gt;L. Elisa Celis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1&quot;&gt;Vijay Keswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Straszak_D/0/1/0/all/0/1&quot;&gt;Damian Straszak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Amit Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kathuria_T/0/1/0/all/0/1&quot;&gt;Tarun Kathuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04034">
<title>Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks. (arXiv:1802.04034v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.04034</link>
<description rdf:parseType="Literal">&lt;p&gt;High sensitivity of neural networks against malicious perturbations on inputs
causes security concerns. We aim to ensure perturbation invariance in their
predictions. However, prior work requires strong assumptions on network
structures and massive computational costs, and thus their applications are
limited. In this paper, based on Lipschitz constants and prediction margins, we
present a widely applicable and computationally efficient method to lower-bound
the size of adversarial perturbations that networks can never be deceived.
Moreover, we propose an efficient training procedure to strengthen perturbation
invariance. In experimental evaluations, our method showed its ability to
provide a strong guarantee for even large networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuzuku_Y/0/1/0/all/0/1&quot;&gt;Yusuke Tsuzuku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04064">
<title>Practical Evaluation and Optimization of Contextual Bandit Algorithms. (arXiv:1802.04064v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.04064</link>
<description rdf:parseType="Literal">&lt;p&gt;We study and empirically optimize contextual bandit learning, exploration,
and problem encodings across 500+ datasets, creating a reference for
practitioners and discovering or reinforcing a number of natural open problems
for researchers. Across these experiments we show that minimizing the amount of
exploration is a key design goal for practical performance. Remarkably, many
problems can be solved purely via the implicit exploration imposed by the
diversity of contexts. For practitioners, we introduce a number of practical
improvements to common exploration algorithms including Bootstrap Thompson
sampling, Online Cover, and $\epsilon$-greedy. We also detail a new form of
reduction to regression for learning from exploration data. Overall, this is a
thorough study and review of contextual bandit methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1&quot;&gt;Alberto Bietti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Langford_J/0/1/0/all/0/1&quot;&gt;John Langford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04065">
<title>Predicting short-term Bitcoin price fluctuations from buy and sell orders. (arXiv:1802.04065v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.04065</link>
<description rdf:parseType="Literal">&lt;p&gt;Bitcoin is the first decentralized digital cryptocurrency, which has showed
significant market capitalization growth in last few years. It is important to
understand what drives the fluctuations of the Bitcoin exchange price and to
what extent they are predictable. In this paper, we study the ability to make
short-term prediction of the exchange price fluctuations (measured with
volatility) towards the United States dollar. We use the data of buy and sell
orders collected from one of the largest Bitcoin digital trading offices in
2016 and 2017. We construct a generative temporal mixture model of the
volatility and trade order book data, which is able to out-perform the current
state-of-the-art machine learning and time-series statistical models. With the
gate weighting function of our generative temporal mixture model, we are able
to detect regimes when the features of buy and sell orders significantly
affects the future high volatility periods. Furthermore, we provide insights
into dynamical importance of specific features from order book such as market
spread, depth, volume and ask/bid slope to explain future short-term price
fluctuations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tian Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Antulov_Fantulin_N/0/1/0/all/0/1&quot;&gt;Nino Antulov-Fantulin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04085">
<title>Efficient Empirical Risk Minimization with Smooth Loss Functions in Non-interactive Local Differential Privacy. (arXiv:1802.04085v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.04085</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the Empirical Risk Minimization problem in the
non-interactive local model of differential privacy. We first show that if the
ERM loss function is $(\infty, T)$-smooth, then we can avoid a dependence of
the sample complexity, to achieve error $\alpha$, on the exponential of the
dimensionality $p$ with base $1/\alpha$ ({\em i.e.,} $\alpha^{-p}$), which
answers a question in \cite{smith2017interaction}. Our approach is based on
Bernstein polynomial approximation. Then, we propose player-efficient
algorithms with $1$-bit communication complexity and $O(1)$ computation cost
for each player. The error bound is asymptotically the same as the original
one. Also with additional assumptions we show a server efficient algorithm with
polynomial running time. At last, we propose (efficient) non-interactive
locally differential private algorithms, based on different types of polynomial
approximations, for learning the set of k-way marginal queries and the set of
smooth queries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1&quot;&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinhui Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04087">
<title>Deep learning based supervised semantic segmentation of Electron Cryo-Subtomograms. (arXiv:1802.04087v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1802.04087</link>
<description rdf:parseType="Literal">&lt;p&gt;Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for
the 3D visualization of cellular structure and organization at submolecular
resolution. It enables analyzing the native structures of macromolecular
complexes and their spatial organization inside single cells. However, due to
the high degree of structural complexity and practical imaging limitations,
systematic macromolecular structural recovery inside CECT images remains
challenging. Particularly, the recovery of a macromolecule is likely to be
biased by its neighbor structures due to the high molecular crowding. To reduce
the bias, here we introduce a novel 3D convolutional neural network inspired by
Fully Convolutional Network and Encoder-Decoder Architecture for the supervised
segmentation of macromolecules of interest in subtomograms. The tests of our
models on realistically simulated CECT data demonstrate that our new approach
has significantly improved segmentation performance compared to our baseline
approach. Also, we demonstrate that the proposed model has generalization
ability to segment new structures that do not exist in training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_R/0/1/0/all/0/1&quot;&gt;Ruogu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Freyberg_Z/0/1/0/all/0/1&quot;&gt;Zachary Freyberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04145">
<title>DCFNet: Deep Neural Network with Decomposed Convolutional Filters. (arXiv:1802.04145v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.04145</link>
<description rdf:parseType="Literal">&lt;p&gt;Filters in a Convolutional Neural Network (CNN) contain model parameters
learned from enormous amounts of data. In this paper, we suggest to decompose
convolutional filters in CNN as a truncated expansion with pre-fixed bases,
namely the Decomposed Convolutional Filters network (DCFNet), where the
expansion coefficients remain learned from data. Such a structure not only
reduces the number of trainable parameters and computation, but also imposes
filter regularity by bases truncation. Through extensive experiments, we
consistently observe that DCFNet maintains accuracy for image classification
tasks with a significant reduction of model parameters, particularly with
Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we
analyze the representation stability of DCFNet with respect to input
variations, and prove representation stability under generic assumptions on the
expansion coefficients. The analysis is consistent with the empirical
observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Calderbank_R/0/1/0/all/0/1&quot;&gt;Robert Calderbank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sapiro_G/0/1/0/all/0/1&quot;&gt;Guillermo Sapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04170">
<title>Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches. (arXiv:1802.04170v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1802.04170</link>
<description rdf:parseType="Literal">&lt;p&gt;Healthcare companies must submit pharmaceutical drugs or medical devices to
regulatory bodies before marketing new technology. Regulatory bodies frequently
require transparent and interpretable computational modelling to justify a new
healthcare technology, but researchers may have several competing models for a
biological system and too little data to discriminate between the models. In
design of experiments for model discrimination, the goal is to design maximally
informative physical experiments in order to discriminate between rival
predictive models. Prior work has focused either on analytical approaches,
which cannot manage all functions, or on data-driven approaches, which may have
computational difficulties or lack interpretable marginal predictive
distributions. We develop a methodology introducing Gaussian process surrogates
in lieu of the original mechanistic models. We thereby extend existing design
and model discrimination methods developed for analytical models to cases of
non-analytical models in a computationally efficient manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Olofsson_S/0/1/0/all/0/1&quot;&gt;Simon Olofsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1&quot;&gt;Marc Peter Deisenroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Misener_R/0/1/0/all/0/1&quot;&gt;Ruth Misener&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04198">
<title>client2vec: Towards Systematic Baselines for Banking Applications. (arXiv:1802.04198v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.04198</link>
<description rdf:parseType="Literal">&lt;p&gt;The workflow of data scientists normally involves potentially inefficient
processes such as data mining, feature engineering and model selection. Recent
research has focused on automating this workflow, partly or in its entirety, to
improve productivity. We choose the former approach and in this paper share our
experience in designing the client2vec: an internal library to rapidly build
baselines for banking applications. Client2vec uses marginalized stacked
denoising autoencoders on current account transactions data to create vector
embeddings which represent the behaviors of our clients. These representations
can then be used in, and optimized against, a variety of tasks such as client
segmentation, profiling and targeting. Here we detail how we selected the
algorithmic machinery of client2vec and the data it works on and present
experimental results on several business cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baldassini_L/0/1/0/all/0/1&quot;&gt;Leonardo Baldassini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Serrano_J/0/1/0/all/0/1&quot;&gt;Jose Antonio Rodr&amp;#xed;guez Serrano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04204">
<title>Fast Interactive Image Retrieval using large-scale unlabeled data. (arXiv:1802.04204v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.04204</link>
<description rdf:parseType="Literal">&lt;p&gt;An interactive image retrieval system learns which images in the database
belong to a user&apos;s query concept, by analyzing the example images and feedback
provided by the user. The challenge is to retrieve the relevant images with
minimal user interaction. In this work, we propose to solve this problem by
posing it as a binary classification task of classifying all images in the
database as being relevant or irrelevant to the user&apos;s query concept. Our
method combines active learning with graph-based semi-supervised learning
(GSSL) to tackle this problem. Active learning reduces the number of user
interactions by querying the labels of the most informative points and GSSL
allows to use abundant unlabeled data along with the limited labeled data
provided by the user. To efficiently find the most informative point, we use an
uncertainty sampling based method that queries the label of the point nearest
to the decision boundary of the classifier. We estimate this decision boundary
using our heuristic of adaptive threshold. To utilize huge volumes of unlabeled
data we use an efficient approximation based method that reduces the complexity
of GSSL from $O(n^3)$ to $O(n)$, making GSSL scalable. We make the classifier
robust to the diversity and noisy labels associated with images in large
databases by incorporating information from multiple modalities such as visual
information extracted from deep learning based models and semantic information
extracted from the WordNet. High F1 scores within few relevance feedback rounds
in our experiments with concepts defined on AnimalWithAttributes and Imagenet
(1.2 million images) datasets indicate the effectiveness and scalability of our
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1&quot;&gt;Akshay Mehra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1&quot;&gt;Jihun Hamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Belkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04220">
<title>Augment and Reduce: Stochastic Inference for Large Categorical Distributions. (arXiv:1802.04220v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.04220</link>
<description rdf:parseType="Literal">&lt;p&gt;Categorical distributions are ubiquitous in machine learning, e.g., in
classification, language models, and recommendation systems. They are also at
the core of discrete choice models. However, when the number of possible
outcomes is very large, using categorical distributions becomes computationally
expensive, as the complexity scales linearly with the number of outcomes. To
address this problem, we propose augment and reduce (A&amp;amp;R), a method to
alleviate the computational complexity. A&amp;amp;R uses two ideas: latent variable
augmentation and stochastic variational inference. It maximizes a lower bound
on the marginal likelihood of the data. Unlike existing methods which are
specific to softmax, A&amp;amp;R is more general and is amenable to other categorical
models, such as multinomial probit. On several large-scale classification
problems, we show that A&amp;amp;R provides a tighter bound on the marginal likelihood
and has better predictive performance than existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruiz_F/0/1/0/all/0/1&quot;&gt;Francisco J. R. Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Titsias_M/0/1/0/all/0/1&quot;&gt;Michalis K. Titsias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dieng_A/0/1/0/all/0/1&quot;&gt;Adji B. Dieng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.00031">
<title>Deep Convolutional Neural Networks on Cartoon Functions. (arXiv:1605.00031v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1605.00031</link>
<description rdf:parseType="Literal">&lt;p&gt;Wiatowski and B\&quot;olcskei, 2015, proved that deformation stability and
vertical translation invariance of deep convolutional neural network-based
feature extractors are guaranteed by the network structure per se rather than
the specific convolution kernels and non-linearities. While the translation
invariance result applies to square-integrable functions, the deformation
stability bound holds for band-limited functions only. Many signals of
practical relevance (such as natural images) exhibit, however, sharp and curved
discontinuities and are, hence, not band-limited. The main contribution of this
paper is a deformation stability result that takes these structural properties
into account. Specifically, we establish deformation stability bounds for the
class of cartoon functions introduced by Donoho, 2001.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grohs_P/0/1/0/all/0/1&quot;&gt;Philipp Grohs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiatowski_T/0/1/0/all/0/1&quot;&gt;Thomas Wiatowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1&quot;&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.00389">
<title>Stream Clipper: Scalable Submodular Maximization on Stream. (arXiv:1606.00389v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1606.00389</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a streaming submodular maximization algorithm &quot;stream clipper&quot;
that performs as well as the offline greedy algorithm on document/video
summarization in practice. It adds elements from a stream either to a solution
set $S$ or to an extra buffer $B$ based on two adaptive thresholds, and
improves $S$ by a final greedy step that starts from $S$ adding elements from
$B$. During this process, swapping elements out of $S$ can occur if doing so
yields improvements. The thresholds adapt based on if current memory
utilization exceeds a budget, e.g., it increases the lower threshold, and
removes from the buffer $B$ elements below the new lower threshold. We show
that, while our approximation factor in the worst case is $1/2$ (like in
previous work, and corresponding to the tight bound), we show that there are
data-dependent conditions where our bound falls within the range $[1/2,
1-1/e]$. In news and video summarization experiments, the algorithm
consistently outperforms other streaming methods, and, while using
significantly less computation and memory, performs similarly to the offline
greedy algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeff Bilmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.07116">
<title>Online Classification with Complex Metrics. (arXiv:1610.07116v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.07116</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework and analysis of consistent binary classification for
complex and non-decomposable performance metrics such as the F-measure and the
Jaccard measure. The proposed framework is general, as it applies to both batch
and online learning, and to both linear and non-linear models. Our work follows
recent results showing that the Bayes optimal classifier for many complex
metrics is given by a thresholding of the conditional probability of the
positive class. This manuscript extends this thresholding characterization --
showing that the utility is strictly locally quasi-concave with respect to the
threshold for a wide range of models and performance metrics. This, in turn,
motivates simple normalized gradient ascent updates for threshold estimation.
We present a finite-sample regret analysis for the resulting procedure. In
particular, the risk for the batch case converges to the Bayes risk at the same
rate as that of the underlying conditional probability estimation, and the risk
of proposed online algorithm converges at a rate that depends on the
conditional probability estimation risk. For instance, in the special case
where the conditional probability model is logistic regression, our procedure
achieves $O(\frac{1}{\sqrt{n}})$ sample complexity, both for batch and online
training. Empirical evaluation shows that the proposed algorithms out-perform
alternatives in practice, with comparable or better prediction performance and
reduced run time for various metrics and datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bowei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koyejo_O/0/1/0/all/0/1&quot;&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhong_K/0/1/0/all/0/1&quot;&gt;Kai Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.02041">
<title>Does Distributionally Robust Supervised Learning Give Robust Classifiers?. (arXiv:1611.02041v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.02041</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributionally Robust Supervised Learning (DRSL) is necessary for building
reliable machine learning systems. When machine learning is deployed in the
real world, its performance can be significantly degraded because test data may
follow a different distribution from training data. Previous DRSL explicitly
considers the worst-case distribution shift by minimizing the adversarially
reweighted training loss. In this paper, we theoretically analyze the previous
DRSL in a classification scenario. We reveal a rather surprising fact that the
previous DRSL ends up giving classifiers optimal for the training distribution
even though it is designed to be robust to change from the training
distribution. Motivated by our analysis, we also propose novel DRSL that
overcomes this limitation. We establish its convergence property and
demonstrate its effectiveness through experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Weihua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.03077">
<title>A More General Robust Loss Function. (arXiv:1701.03077v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1701.03077</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a two-parameter loss function which can be viewed as a
generalization of many popular loss functions used in robust statistics: the
Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, and generalized Charbonnier
loss functions (and by transitivity the L2, L1, L1-L2, and
pseudo-Huber/Charbonnier loss functions). If this penalty is viewed as a
negative log-likelihood, it yields a general probability distribution that
includes normal and Cauchy distributions as special cases. We describe and
visualize this loss and its corresponding distribution, and document several of
their useful properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1&quot;&gt;Jonathan T. Barron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08420">
<title>Embarrassingly parallel inference for Gaussian processes. (arXiv:1702.08420v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08420</link>
<description rdf:parseType="Literal">&lt;p&gt;Training Gaussian process-based models typically involves an $ O(N^3)$
computational bottleneck. Popular methods for overcoming this matrix inversion
problem cannot adequately model all types of latent functions, and are often
not parallelizable. We present an embarrassingly parallel method that takes
advantage of inverting block diagonal approximations, while maintaining much of
the expressivity of a full covariance matrix. By using importance sampling to
average over different realizations of low-rank GP approximations, we ensure
our algorithm is both asymptotically unbiased and embarrassingly parallel. We
show comparable or improved performance over competing methods, on a range of
synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Michael Minyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Williamson_S/0/1/0/all/0/1&quot;&gt;Sinead A. Williamson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07352">
<title>A Riemannian approach for structured low-rank matrix learning. (arXiv:1704.07352v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07352</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a low-rank matrix, constrained to lie in
a linear subspace, and introduce a novel factorization for modeling such
matrices. A salient feature of the proposed factorization scheme is it
decouples the low-rank and the structural constraints onto separate factors. We
formulate the optimization problem on the Riemannian spectrahedron manifold,
where the Riemannian framework allows to develop computationally efficient
conjugate gradient and trust-region algorithms. Experiments on problems such as
Hankel matrix learning, non-negative matrix completion, and robust matrix
completion demonstrate the efficacy of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jawanpuria_P/0/1/0/all/0/1&quot;&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.06808">
<title>Analysis of Thompson Sampling for Gaussian Process Optimization in the Bandit Setting. (arXiv:1705.06808v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.06808</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the global optimization of a function over a continuous domain.
At every evaluation attempt, we can observe the function at a chosen point in
the domain and we reap the reward of the value observed. We assume that drawing
these observations are expensive and noisy. We frame it as a continuum-armed
bandit problem with a Gaussian Process prior on the function. In this regime,
most algorithms have been developed to minimize some form of regret. Contrary
to this popular norm, in this paper, we study the convergence of the sequential
point $\boldsymbol{x}^t$ to the global optimizer $\boldsymbol{x}^*$ for the
Thompson Sampling approach. Under some assumptions and regularity conditions,
we show an exponential rate of convergence to the true optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basu_K/0/1/0/all/0/1&quot;&gt;Kinjal Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Souvik Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07774">
<title>Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients. (arXiv:1705.07774v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07774</link>
<description rdf:parseType="Literal">&lt;p&gt;The ADAM optimizer is exceedingly popular in the deep learning community.
Often it works very well, sometimes it doesn&apos;t. Why? We interpret ADAM as a
combination of two aspects: for each weight, the update direction is determined
by the sign of stochastic gradients, whereas the update magnitude is determined
by an estimate of their relative variance. We disentangle these two aspects and
analyze them in isolation, gaining insight into the mechanisms underlying ADAM.
This analysis also extends recent results on adverse effects of ADAM on
generalization, isolating the sign aspect as the problematic one. Transferring
the variance adaptation to SGD gives rise to a novel method, completing the
practitioner&apos;s toolbox for problems where ADAM fails.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balles_L/0/1/0/all/0/1&quot;&gt;Lukas Balles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1&quot;&gt;Philipp Hennig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10762">
<title>Generative Models of Visually Grounded Imagination. (arXiv:1705.10762v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10762</link>
<description rdf:parseType="Literal">&lt;p&gt;It is easy for people to imagine what a man with pink hair looks like, even
if they have never seen such a person before. We call the ability to create
images of novel semantic concepts visually grounded imagination. In this paper,
we show how we can modify variational auto-encoders to perform this task. Our
method uses a novel training objective, and a novel product-of-experts
inference network, which can handle partially specified (abstract) concepts in
a principled and efficient way. We also propose a set of easy-to-compute
evaluation metrics that capture our intuitive notions of what it means to have
good visual imagination, namely correctness, coverage, and compositionality
(the 3 C&apos;s). Finally, we perform a detailed comparison of our method with two
existing joint image-attribute VAE methods (the JMVAE method of Suzuki et.al.
and the BiVCCA method of Wang et.al.) by applying them to two datasets: the
MNIST-with-attributes dataset (which we introduce here), and the CelebA
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedantam_R/0/1/0/all/0/1&quot;&gt;Ramakrishna Vedantam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1&quot;&gt;Ian Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jonathan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kevin Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.07642">
<title>A Variance Maximization Criterion for Active Learning. (arXiv:1706.07642v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.07642</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning aims to train a classifier as fast as possible with as few
labels as possible. The core element in virtually any active learning strategy
is the criterion that measures the usefulness of the unlabeled data based on
which new points to be labeled are picked. We propose a novel approach which we
refer to as maximizing variance for active learning or MVAL for short. MVAL
measures the value of unlabeled instances by evaluating the rate of change of
output variables caused by changes in the next sample to be queried and its
potential labelling. In a sense, this criterion measures how unstable the
classifier&apos;s output is for the unlabeled data points under perturbations of the
training data. MVAL maintains, what we refer to as, retraining information
matrices to keep track of these output scores and exploits two kinds of
variance to measure the informativeness and representativeness, respectively.
By fusing these variances, MVAL is able to select the instances which are both
informative and representative. We employ our technique both in combination
with logistic regression and support vector machines and demonstrate that MVAL
achieves state-of-the-art performance in experiments on a large number of
standard benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yazhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loog_M/0/1/0/all/0/1&quot;&gt;Marco Loog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00106">
<title>First and Second Order Methods for Online Convolutional Dictionary Learning. (arXiv:1709.00106v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00106</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional sparse representations are a form of sparse representation with
a structured, translation invariant dictionary. Most convolutional dictionary
learning algorithms to date operate in batch mode, requiring simultaneous
access to all training images during the learning process, which results in
very high memory usage and severely limits the training data that can be used.
Very recently, however, a number of authors have considered the design of
online convolutional dictionary learning algorithms that offer far better
scaling of memory and computational cost with training set size than batch
methods. This paper extends our prior work, improving a number of aspects of
our previous algorithm; proposing an entirely new one, with better performance,
and that supports the inclusion of a spatial mask for learning from incomplete
data; and providing a rigorous theoretical analysis of these methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jialin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Cardona_C/0/1/0/all/0/1&quot;&gt;Cristina Garcia-Cardona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1&quot;&gt;Brendt Wohlberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04114">
<title>EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples. (arXiv:1709.04114v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04114</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1&quot;&gt;Yash Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05359">
<title>Information-Theoretic Representation Learning for Positive-Unlabeled Classification. (arXiv:1710.05359v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05359</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in weakly supervised classification allow us to train a
classifier only from positive and unlabeled (PU) data. However, existing PU
classification methods typically require an accurate estimate of the
class-prior probability, which is a critical bottleneck particularly for
high-dimensional data. This problem has been commonly addressed by applying
principal component analysis in advance, but such unsupervised dimension
reduction can collapse underlying class structure. In this paper, we propose a
novel representation learning method from PU data based on the
information-maximization principle. Our method does not require class-prior
estimation and thus can be used as a preprocessing method for PU
classification. Through experiments, we demonstrate that our method combined
with deep neural networks highly improves the accuracy of PU class-prior
estimation, leading to state-of-the-art PU classification performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sakai_T/0/1/0/all/0/1&quot;&gt;Tomoya Sakai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06360">
<title>Good Arm Identification via Bandit Feedback. (arXiv:1710.06360v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06360</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a novel stochastic multi-armed bandit problem called {\em good
arm identification} (GAI), where a good arm is defined as an arm with expected
reward greater than or equal to a given threshold. GAI is a pure-exploration
problem that a single agent repeats a process of outputting an arm as soon as
it is identified as a good one before confirming the other arms are actually
not good. The objective of GAI is to minimize the number of samples for each
process. We find that GAI faces a new kind of dilemma, the {\em
exploration-exploitation dilemma of confidence}, which is different difficulty
from the best arm identification. As a result, an efficient design of
algorithms for GAI is quite different from that for the best arm
identification. We derive a lower bound on the sample complexity of GAI that is
tight up to the logarithmic factor $\mathrm{O}(\log \frac{1}{\delta})$ for
acceptance error rate $\delta$. We also develop an algorithm whose sample
complexity almost matches the lower bound. We also confirm experimentally that
our proposed algorithm outperforms naive algorithms in synthetic settings based
on a conventional bandit problem and clinical trial researches for rheumatoid
arthritis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kano_H/0/1/0/all/0/1&quot;&gt;Hideaki Kano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Honda_J/0/1/0/all/0/1&quot;&gt;Junya Honda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sakamaki_K/0/1/0/all/0/1&quot;&gt;Kentaro Sakamaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matsuura_K/0/1/0/all/0/1&quot;&gt;Kentaro Matsuura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nakamura_A/0/1/0/all/0/1&quot;&gt;Atsuyoshi Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07138">
<title>Binary Classification from Positive-Confidence Data. (arXiv:1710.07138v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07138</link>
<description rdf:parseType="Literal">&lt;p&gt;Reducing labeling costs in supervised learning is a critical issue in many
practical machine learning applications. In this paper, we consider
positive-confidence (Pconf) classification, the problem of training a binary
classifier only from positive data equipped with confidence. Pconf
classification can be regarded as a discriminative extension of one-class
classification (which is aimed at &quot;describing&quot; the positive class by
clustering-related methods), with ability to tune hyper-parameters for
&quot;classifying&quot; positive and negative samples. Pconf classification is also
related to positive-unlabeled (PU) classification (which uses hard-labeled
positive data and unlabeled data), but the difference is that it enables us to
avoid estimating the class priors, which is a critical bottleneck in typical PU
classification methods. For the Pconf classification problem, we provide a
simple empirical risk minimization framework and give a formulation for
linear-in-parameter models that can be implemented easily and computationally
efficiently. We also theoretically establish the consistency and estimation
error bound for Pconf classification, and demonstrate the practical usefulness
of the proposed method for deep neural networks through experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ishida_T/0/1/0/all/0/1&quot;&gt;Takashi Ishida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00382">
<title>A Large Dimensional Study of Regularized Discriminant Analysis Classifiers. (arXiv:1711.00382v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00382</link>
<description rdf:parseType="Literal">&lt;p&gt;This article carries out a large dimensional analysis of standard regularized
discriminant analysis classifiers designed on the assumption that data arise
from a Gaussian mixture model with different means and covariances. The
analysis relies on fundamental results from random matrix theory (RMT) when
both the number of features and the cardinality of the training data within
each class grow large at the same pace. Under mild assumptions, we show that
the asymptotic classification error approaches a deterministic quantity that
depends only on the means and covariances associated with each class as well as
the problem dimensions. Such a result permits a better understanding of the
performance of regularized discriminant analsysis, in practical large but
finite dimensions, and can be used to determine and pre-estimate the optimal
regularization parameter that minimizes the misclassification error
probability. Despite being theoretically valid only for Gaussian data, our
findings are shown to yield a high accuracy in predicting the performances
achieved with real data sets drawn from the popular USPS data base, thereby
making an interesting connection between theory and practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elkhalil_K/0/1/0/all/0/1&quot;&gt;Khalil Elkhalil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kammoun_A/0/1/0/all/0/1&quot;&gt;Abla Kammoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Couillet_R/0/1/0/all/0/1&quot;&gt;Romain Couillet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Al_Naffouri_T/0/1/0/all/0/1&quot;&gt;Tareq Y. Al-Naffouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alouini_M/0/1/0/all/0/1&quot;&gt;Mohamed-Slim Alouini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04460">
<title>Blind Source Separation Using Mixtures of Alpha-Stable Distributions. (arXiv:1711.04460v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04460</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new blind source separation algorithm based on mixtures of
alpha-stable distributions. Complex symmetric alpha-stable distributions have
been recently showed to better model audio signals in the time-frequency domain
than classical Gaussian distributions thanks to their larger dynamic range.
However, inference of these models is notoriously hard to perform because their
probability density functions do not have a closed-form expression in general.
Here, we introduce a novel method for estimating mixture of alpha-stable
distributions based on characteristic function matching. We apply this to the
blind estimation of binary masks in individual frequency bands from
multichannel convolutive audio mixes. We show that the proposed method yields
better separation performance than Gaussian-based binary-masking methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1&quot;&gt;Nicolas Keriven&lt;/a&gt; (DMA), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deleforge_A/0/1/0/all/0/1&quot;&gt;Antoine Deleforge&lt;/a&gt; (PANAMA), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liutkus_A/0/1/0/all/0/1&quot;&gt;Antoine Liutkus&lt;/a&gt; (ZENITH)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04955">
<title>Scalable Peaceman-Rachford Splitting Method with Proximal Terms. (arXiv:1711.04955v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04955</link>
<description rdf:parseType="Literal">&lt;p&gt;Along with developing of Peaceman-Rachford Splittling Method (PRSM), many
batch algorithms based on it have been studied very deeply. But almost no
algorithm focused on the performance of stochastic version of PRSM. In this
paper, we propose a new stochastic algorithm based on PRSM, prove its
convergence rate in ergodic sense, and test its performance on both artificial
and real data. We show that our proposed algorithm, Stochastic Scalable PRSM
(SS-PRSM), enjoys the $O(1/K)$ convergence rate, which is the same as those
newest stochastic algorithms that based on ADMM but faster than general
Stochastic ADMM (which is $O(1/\sqrt{K})$). Our algorithm also owns wide
flexibility, outperforms many state-of-the-art stochastic algorithms coming
from ADMM, and has low memory cost in large-scale splitting optimization
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Na_S/0/1/0/all/0/1&quot;&gt;Sen Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mingyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1&quot;&gt;Mladen Kolar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05407">
<title>MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis. (arXiv:1711.05407v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05407</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability has emerged as a crucial aspect of machine learning, aimed
at providing insights into the working of complex neural networks. However,
existing solutions vary vastly based on the nature of the interpretability
task, with each use case requiring substantial time and effort. This paper
introduces MARGIN, a simple yet general approach to address a large set of
interpretability tasks ranging from identifying prototypes to explaining image
predictions. MARGIN exploits ideas rooted in graph signal analysis to determine
influential nodes in a graph, which are defined as those nodes that maximally
describe a function defined on the graph. By carefully defining task-specific
graphs and functions, we demonstrate that MARGIN outperforms existing
approaches in a number of disparate interpretability challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anirudh_R/0/1/0/all/0/1&quot;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sridhar_R/0/1/0/all/0/1&quot;&gt;Rahul Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bremer_T/0/1/0/all/0/1&quot;&gt;Timo Bremer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06104">
<title>Towards better understanding of gradient-based attribution methods for Deep Neural Networks. (arXiv:1711.06104v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06104</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the flow of information in Deep Neural Networks (DNNs) is a
challenging problem that has gain increasing attention over the last few years.
While several methods have been proposed to explain network predictions, there
have been only a few attempts to compare them from a theoretical perspective.
What is more, no exhaustive empirical comparison has been performed in the
past. In this work, we analyze four gradient-based attribution methods and
formally prove conditions of equivalence and approximation between them. By
reformulating two of these methods, we construct a unified framework which
enables a direct comparison, as well as an easier implementation. Finally, we
propose a novel evaluation metric, called Sensitivity-n and test the
gradient-based attribution methods alongside with a simple perturbation-based
attribution method on several datasets in the domains of image and text
classification, using various network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ancona_M/0/1/0/all/0/1&quot;&gt;Marco Ancona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceolini_E/0/1/0/all/0/1&quot;&gt;Enea Ceolini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1&quot;&gt;Cengiz &amp;#xd6;ztireli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1&quot;&gt;Markus Gross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00287">
<title>Minimally Faithful Inversion of Graphical Models. (arXiv:1712.00287v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00287</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference amortization methods allow the sharing of statistical strength
across related observations when learning to perform posterior inference.
Generally this requires the inversion of the dependency structure in the
generative model, as the modeller must design and learn a distribution to
approximate the posterior. Previous methods invert the dependency structure in
a heuristic way and fail to capture the dependencies in the model, therefore
limiting the performance of the eventual inference algorithm. We introduce an
algorithm for faithfully and minimally inverting the graphical model structure
of any generative model. Such an inversion has two crucial properties: a) it
does not encode any independence assertions absent from the model, and b) for a
given inversion, it encodes as many true independence assertions as possible.
Our algorithm works by simulating variable elimination on the generative model
to reparametrize the distribution. We show with experiments how such minimal
inversions can assist in performing better inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Webb_S/0/1/0/all/0/1&quot;&gt;Stefan Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Golinski_A/0/1/0/all/0/1&quot;&gt;Adam Golinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zinkov_R/0/1/0/all/0/1&quot;&gt;Robert Zinkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siddharth_N/0/1/0/all/0/1&quot;&gt;N. Siddharth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06061">
<title>Nearly Optimal Robust Subspace Tracking and Dynamic Robust PCA. (arXiv:1712.06061v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06061</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the robust subspace tracking (RST) problem and obtain one of the
first provable guarantees for it. The goal of RST is to track data that lies in
a slowly changing low-dimensional subspace, and the subspaces themselves, while
being robust to corruption by (often large magnitude) sparse outliers. It can
be simply interpreted as a dynamic (time-varying) extension of robust PCA, with
the minor difference that RST also requires an online algorithm (short tracking
delay). We propose an algorithm called NORST (nearly optimal RST) and prove
that it solves both RST and dynamic robust PCA under weakened versions of
standard RPCA assumptions, slow subspace change, and two simple extra
assumptions (a lower bound on outlier magnitudes, and independence of the
columns of the low-rank matrix).
&lt;/p&gt;
&lt;p&gt;Our guarantee shows that NORST enjoys a near optimal tracking delay of $O(r
\log n \log(1/\varepsilon))$. Its required delay between subspace change times
is the same, and its memory complexity is $n$ times this value. Here $n$ is the
ambient space dimension and $r$ is the dimension of the changing subspaces in
which the true data lies. Thus both these are also nearly optimal. Finally, our
guarantee also shows that NORST has the best outlier tolerance compared with
all previous RPCA or RST methods, both theoretically and empirically (including
for real videos), without requiring any model on outlier support sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1&quot;&gt;Praneeth Narayanamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1&quot;&gt;Namrata Vaswani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06202">
<title>A graph-embedded deep feedforward network for disease outcome classification and feature selection using gene expression data. (arXiv:1801.06202v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06202</link>
<description rdf:parseType="Literal">&lt;p&gt;Gene expression data represents a unique challenge in predictive model
building, because of the small number of samples $(n)$ compared to the huge
amount of features $(p)$. This &quot;$n&amp;lt;&amp;lt;p$&quot; property has hampered application of
deep learning techniques for disease outcome classification. Sparse learning by
incorporating external gene network information could be a potential solution
to this issue. Still, the problem is very challenging because (1) there are
tens of thousands of features and only hundreds of training samples, (2) the
scale-free structure of the gene network is unfriendly to the setup of
convolutional neural networks. To address these issues and build a robust
classification model, we propose the Graph-Embedded Deep Feedforward Networks
(GEDFN), to integrate external relational information of features into the deep
neural network architecture. The method is able to achieve sparse connection
between network layers to prevent overfitting. To validate the method&apos;s
capability, we conducted both simulation experiments and a real data analysis
using a breast cancer RNA-seq dataset from The Cancer Genome Atlas (TCGA). The
resulting high classification accuracy and easily interpretable feature
selection results suggest the method is a useful addition to the current
classification models and feature selection procedures. The method is available
at https://github.com/yunchuankong/NetworkNeuralNetwork.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Yunchuan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianwei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08881">
<title>Correlated Components Analysis --- Extracting Reliable Dimensions in Multivariate Data. (arXiv:1801.08881v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08881</link>
<description rdf:parseType="Literal">&lt;p&gt;How does one find data dimensions that are reliably expressed across
repetitions? For example, in neuroscience one may want to identify combinations
of brain signals that are reliably activated across multiple trials or
subjects. For a clinical assessment with multiple ratings, one may want to
identify an aggregate score that is reliably reproduced across raters. The
approach proposed here --- &quot;correlated components analysis&quot; --- is to identify
components that maximally correlate between repetitions (e.g. trials, subjects,
raters). This can be expressed as the maximization of the ratio of
between-repetition to within-repetition covariance, resulting in a generalized
eigenvalue problem. We show that covariances can be computed efficiently
without explicitly considering all pairs of repetitions, that the result is
equivalent to multi-class linear discriminant analysis for unbiased signals,
and that the approach also maximize reliability, defined as the mean divided by
the deviation across repetitions. We also extend the method to non-linear
components using kernels, discuss regularization to improve numerical
stability, present parametric and non-parametric tests to establish statistical
significance, and provide code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parra_L/0/1/0/all/0/1&quot;&gt;Lucas C. Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haufe_S/0/1/0/all/0/1&quot;&gt;Stefan Haufe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dmochowski_J/0/1/0/all/0/1&quot;&gt;Jacek P. Dmochowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09819">
<title>Transformation Autoregressive Networks. (arXiv:1801.09819v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09819</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental task of general density estimation $p(x)$ has been of keen
interest to machine learning. In this work, we attempt to systematically
characterize methods for density estimation. Broadly speaking, most of the
existing methods can be categorized into either using: \textit{a})
autoregressive models to estimate the conditional factors of the chain rule,
$p(x_{i}\, |\, x_{i-1}, \ldots)$; or \textit{b}) non-linear transformations of
variables of a simple base distribution. Based on the study of the
characteristics of these categories, we propose multiple novel methods for each
category. For example we proposed RNN based transformations to model
non-Markovian dependencies. Further, through a comprehensive study over both
real world and synthetic data, we show for that jointly leveraging
transformations of variables and autoregressive conditional models, results in
a considerable improvement in performance. We illustrate the use of our models
in outlier detection and image modeling. Finally we introduce a novel data
driven framework for learning a family of distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliva_J/0/1/0/all/0/1&quot;&gt;Junier B. Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Avinava Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaheer_M/0/1/0/all/0/1&quot;&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10395">
<title>Probabilistic Recurrent State-Space Models. (arXiv:1801.10395v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10395</link>
<description rdf:parseType="Literal">&lt;p&gt;State-space models (SSMs) are a highly expressive model class for learning
patterns in time series data and for system identification. Deterministic
versions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex
time series data. Fully probabilistic SSMs, however, are often found hard to
train, even for smaller problems. To overcome this limitation, we propose a
novel model formulation and a scalable training algorithm based on doubly
stochastic variational inference and Gaussian processes. In contrast to
existing work, the proposed variational approximation allows one to fully
capture the latent state temporal correlations. These correlations are the key
to robust training. The effectiveness of the proposed PR-SSM is evaluated on a
set of real-world benchmark datasets in comparison to state-of-the-art
probabilistic model learning methods. Scalability and robustness are
demonstrated on a high dimensional problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doerr_A/0/1/0/all/0/1&quot;&gt;Andreas Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daniel_C/0/1/0/all/0/1&quot;&gt;Christian Daniel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schiegg_M/0/1/0/all/0/1&quot;&gt;Martin Schiegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_Tuong_D/0/1/0/all/0/1&quot;&gt;Duy Nguyen-Tuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaal_S/0/1/0/all/0/1&quot;&gt;Stefan Schaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Toussaint_M/0/1/0/all/0/1&quot;&gt;Marc Toussaint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02154">
<title>Critical Percolation as a Framework to Analyze the Training of Deep Networks. (arXiv:1802.02154v1 [cond-mat.dis-nn] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.02154</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we approach two relevant deep learning topics: i) tackling of
graph structured input data and ii) a better understanding and analysis of deep
networks and related learning algorithms. With this in mind we focus on the
topological classification of reachability in a particular subset of planar
graphs (Mazes). Doing so, we are able to model the topology of data while
staying in Euclidean space, thus allowing its processing with standard CNN
architectures. We suggest a suitable architecture for this problem and show
that it can express a perfect solution to the classification task. The shape of
the cost function around this solution is also derived and, remarkably, does
not depend on the size of the maze in the large maze limit. Responsible for
this behavior are rare events in the dataset which strongly regulate the shape
of the cost function near this global minimum. We further identify an obstacle
to learning in the form of poorly performing local minima in which the network
chooses to ignore some of the inputs. We further support our claims with
training experiments and numerical analysis of the cost function on networks
with up to $128$ layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ringel_Z/0/1/0/all/0/1&quot;&gt;Zohar Ringel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bem_R/0/1/0/all/0/1&quot;&gt;Rodrigo de Bem&lt;/a&gt;</dc:creator>
</item></rdf:RDF>