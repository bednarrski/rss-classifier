<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07921"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07980"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08157"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06462"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07903"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07989"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07991"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07992"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08166"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.08266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02162"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06670"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.08186">
<title>Dual approach for object tracking based on optical flow and swarm intelligence. (arXiv:1808.08186v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08186</link>
<description rdf:parseType="Literal">&lt;p&gt;In Computer Vision,object tracking is a very old and complex problem.Though
there are several existing algorithms for object tracking, still there are
several challenges remain to be solved. For instance, variation of illumination
of light, noise, occlusion, sudden start and stop of moving object, shading
etc,make the object tracking a complex problem not only for dynamic background
but also for static background. In this paper we propose a dual approach for
object tracking based on optical flow and swarm Intelligence.The optical flow
based KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the
target object from first frame to last frame of a video sequence;whereas swarm
Intelligence based PSO (Particle Swarm Optimization) tracker simultaneously
tracks the boundary information of the target object from second frame to last
frame of the same video sequence.This dual function of tracking makes the
trackers very much robust with respect to the above stated problems. The
flexibility of our approach is that it can be successfully applicable in
variable background as well as static background.We compare the performance of
the proposed dual tracking algorithm with several benchmark datasets and obtain
very competitive results in general and in most of the cases we obtained
superior results using dual tracking algorithm. We also compare the performance
of the proposed dual tracker with some existing PSO based algorithms for
tracking and achieved better results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_R/0/1/0/all/0/1&quot;&gt;Rajesh Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_K/0/1/0/all/0/1&quot;&gt;Kumar S. Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07899">
<title>A Century Long Commitment to Assessing Artificial Intelligence and its Impact on Society. (arXiv:1808.07899v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07899</link>
<description rdf:parseType="Literal">&lt;p&gt;In September 2016, Stanford&apos;s &quot;One Hundred Year Study on Artificial
Intelligence&quot; project (AI100) issued the first report of its planned long-term
periodic assessment of artificial intelligence (AI) and its impact on society.
The report, entitled &quot;Artificial Intelligence and Life in 2030,&quot; examines eight
domains of typical urban settings on which AI is likely to have impact over the
coming years: transportation, home and service robots, healthcare, education,
public safety and security, low-resource communities, employment and workplace,
and entertainment. It aims to provide the general public with a scientifically
and technologically accurate portrayal of the current state of AI and its
potential and to help guide decisions in industry and governments, as well as
to inform research and development in the field. This article by the chair of
the 2016 Study Panel and the inaugural chair of the AI100 Standing Committee
describes the origins of this ambitious longitudinal study, discusses the
framing of the inaugural report, and presents the report&apos;s main findings. It
concludes with a brief description of the AI100 project&apos;s ongoing efforts and
planned next steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosz_B/0/1/0/all/0/1&quot;&gt;Barbara J. Grosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07921">
<title>SOTER: Programming Safe Robotics System using Runtime Assurance. (arXiv:1808.07921v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.07921</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous robots increasingly depend on third-party off-the-shelf components
and complex machine-learning techniques. This trend makes it challenging to
provide strong design-time certification of correct operation. To address this
challenge, we present SOTER, a programming framework that integrates the core
principles of runtime assurance to enable the use of uncertified controllers,
while still providing safety guarantees.
&lt;/p&gt;
&lt;p&gt;Runtime Assurance (RTA) is an approach used for safety-critical systems where
design-time analysis is coupled with run-time techniques to switch between
unverified advanced controllers and verified simple controllers. In this paper,
we present a runtime assurance programming framework for modular design of
provably-safe robotics software. \tool provides language primitives to
declaratively construct a \rta module consisting of an advanced controller
(untrusted), a safe controller (trusted), and the desired safety specification
(S). If the RTA module is well formed then the framework provides a formal
guarantee that it satisfies property S. The compiler generates code for
monitoring system state and switching control between the advanced and safe
controller in order to guarantee S. RTA allows complex systems to be
constructed through the composition of RTA modules.
&lt;/p&gt;
&lt;p&gt;To demonstrate the efficacy of our framework, we consider a real-world
case-study of building a safe drone surveillance system. Our experiments both
in simulation and on actual drones show that RTA-enabled RTA ensures safety of
the system, including when untrusted third-party components have bugs or
deviate from the desired behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1&quot;&gt;Ankush Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shromona Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1&quot;&gt;Sanjit A. Seshia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_N/0/1/0/all/0/1&quot;&gt;Natarajan Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1&quot;&gt;Ashish Tiwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07980">
<title>Ontology Reasoning with Deep Neural Networks. (arXiv:1808.07980v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07980</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to conduct logical reasoning is a fundamental aspect of
intelligent behavior, and thus an important problem along the way to
human-level artificial intelligence. Traditionally, symbolic methods from the
field of knowledge representation and reasoning have been used to equip agents
with capabilities that resemble human reasoning qualities. More recently,
however, there has been an increasing interest in applying alternative
approaches based on machine learning rather than logic-based formalisms to
tackle this kind of tasks. Here, we make use of state-of-the-art methods for
training deep neural networks to devise a novel model that is closely coupled
to symbolic reasoning methods, and thus able to learn how to effectively
perform basic ontology reasoning. This term describes an important and at the
same time very natural kind of problem settings where the rules for conducting
reasoning are specified alongside with the actual information. Many problems in
practice may be viewed as such reasoning tasks, which is why the presented
approach is applicable to a plethora of important real-world problems. To
demonstrate the effectiveness of the suggested method, we present the outcomes
of several experiments that have been conducted on both toy datasets as well as
real-world data, which show that our model learned to perform precise reasoning
on a number of diverse inference tasks that require comprehensive deductive
proficiencies. Furthermore, it turned out that the suggested model suffers much
less from different obstacles that prohibit symbolic reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohenecker_P/0/1/0/all/0/1&quot;&gt;Patrick Hohenecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1&quot;&gt;Thomas Lukasiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08157">
<title>Different but Equal: Comparing User Collaboration with Digital Personal Assistants vs. Teams of Expert Agents. (arXiv:1808.08157v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1808.08157</link>
<description rdf:parseType="Literal">&lt;p&gt;This work compares user collaboration with conversational personal assistants
vs. teams of expert chatbots. Two studies were performed to investigate whether
each approach affects accomplishment of tasks and collaboration costs.
Participants interacted with two equivalent financial advice chatbot systems,
one composed of a single conversational adviser and the other based on a team
of four experts chatbots. Results indicated that users had different forms of
experiences but were equally able to achieve their goals. Contrary to the
expected, there were evidences that in the teamwork situation that users were
more able to predict agent behavior better and did not have an overhead to
maintain common ground, indicating similar collaboration costs. The results
point towards the feasibility of either of the two approaches for user
collaboration with conversational agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinhanez_C/0/1/0/all/0/1&quot;&gt;Claudio S. Pinhanez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Candello_H/0/1/0/all/0/1&quot;&gt;Heloisa Candello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pichiliani_M/0/1/0/all/0/1&quot;&gt;Mauro C. Pichiliani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasconcelos_M/0/1/0/all/0/1&quot;&gt;Marisa Vasconcelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerra_M/0/1/0/all/0/1&quot;&gt;Melina Guerra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayser_M/0/1/0/all/0/1&quot;&gt;Ma&amp;#xed;ra G. de Bayser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalin_P/0/1/0/all/0/1&quot;&gt;Paulo Cavalin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08473">
<title>Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training. (arXiv:1804.08473v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08473</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic generation of natural language from images has attracted extensive
attention. In this paper, we take one step further to investigate generation of
poetic language (with multiple lines) to an image for automatic poetry
creation. This task involves multiple challenges, including discovering poetic
clues from the image (e.g., hope from green), and generating poems to satisfy
both relevance to the image and poeticness in language level. To solve the
above challenges, we formulate the task of poem generation into two correlated
sub-tasks by multi-adversarial training via policy gradient, through which the
cross-modal relevance and poetic language style can be ensured. To extract
poetic clues from images, we propose to learn a deep coupled visual-poetic
embedding, in which the poetic representation from objects, sentiments and
scenes in an image can be jointly learned. Two discriminative networks are
further introduced to guide the poem generation, including a multi-modal
discriminator and a poem-style discriminator. To facilitate the research, we
have released two poem datasets by human annotators with two distinct
properties: 1) the first human annotated image-to-poem pair dataset (with 8,292
pairs in total), and 2) to-date the largest public English poem corpus dataset
(with 92,265 different poems in total). Extensive experiments are conducted
with 8K images, among which 1.5K image are randomly picked for evaluation. Both
objective and subjective evaluations show the superior performances against the
state-of-the-art methods for poem generation from images. Turing test carried
out with over 500 human subjects, among which 30 evaluators are poetry experts,
demonstrates the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jianlong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Makoto P. Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1&quot;&gt;Masatoshi Yoshikawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01876">
<title>An Efficient Deep Reinforcement Learning Model for Urban Traffic Control. (arXiv:1808.01876v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01876</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban Traffic Control (UTC) plays an essential role in Intelligent
Transportation System (ITS) but remains difficult. Since model-based UTC
methods may not accurately describe the complex nature of traffic dynamics in
all situations, model-free data-driven UTC methods, especially reinforcement
learning (RL) based UTC methods, received increasing interests in the last
decade. However, existing DL approaches did not propose an efficient algorithm
to solve the complicated multiple intersections control problems whose
state-action spaces are vast. To solve this problem, we propose a Deep
Reinforcement Learning (DRL) algorithm that combines several tricks to master
an appropriate control strategy within an acceptable time. This new algorithm
relaxes the fixed traffic demand pattern assumption and reduces human invention
in parameter tuning. Simulation experiments have shown that our method
outperforms traditional rule-based approaches and has the potential to handle
more complex traffic problems in the real world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yilun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xingyuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei-Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06462">
<title>Cross-Modal Health State Estimation. (arXiv:1808.06462v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06462</link>
<description rdf:parseType="Literal">&lt;p&gt;Individuals create and consume more diverse data about themselves today than
any time in history. Sources of this data include wearable devices, images,
social media, geospatial information and more. A tremendous opportunity rests
within cross-modal data analysis that leverages existing domain knowledge
methods to understand and guide human health. Especially in chronic diseases,
current medical practice uses a combination of sparse hospital based biological
metrics (blood tests, expensive imaging, etc.) to understand the evolving
health status of an individual. Future health systems must integrate data
created at the individual level to better understand health status perpetually,
especially in a cybernetic framework. In this work we fuse multiple user
created and open source data streams along with established biomedical domain
knowledge to give two types of quantitative state estimates of cardiovascular
health. First, we use wearable devices to calculate cardiorespiratory fitness
(CRF), a known quantitative leading predictor of heart disease which is not
routinely collected in clinical settings. Second, we estimate inherent genetic
traits, living environmental risks, circadian rhythm, and biological metrics
from a diverse dataset. Our experimental results on 24 subjects demonstrate how
multi-modal data can provide personalized health insight. Understanding the
dynamic nature of health status will pave the way for better health based
recommendation engines, better clinical decision making and positive lifestyle
changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_N/0/1/0/all/0/1&quot;&gt;Nitish Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1&quot;&gt;Vaibhav Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putzel_P/0/1/0/all/0/1&quot;&gt;Preston J. Putzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhimaraju_H/0/1/0/all/0/1&quot;&gt;Hari Bhimaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1&quot;&gt;Srikanth Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Ramesh C. Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07903">
<title>LIFT: Reinforcement Learning in Computer Systems by Learning From Demonstrations. (arXiv:1808.07903v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07903</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning approaches have long appealed to the data management
community due to their ability to learn to control dynamic behavior from raw
system performance. Recent successes in combining deep neural networks with
reinforcement learning have sparked significant new interest in this domain.
However, practical solutions remain elusive due to large training data
requirements, algorithmic instability, and lack of standard tools. In this
work, we introduce LIFT, an end-to-end software stack for applying deep
reinforcement learning to data management tasks. While prior work has
frequently explored applications in simulations, LIFT centers on utilizing
human expertise to learn from demonstrations, thus lowering online training
times. We further introduce TensorForce, a TensorFlow library for applied deep
reinforcement learning exposing a unified declarative interface to common RL
algorithms, thus providing a backend to LIFT. We demonstrate the utility of
LIFT in two case studies in database compound indexing and resource management
in stream processing. Results show LIFT controllers initialized from
demonstrations can outperform human baselines and heuristics across latency
metrics and space usage by up to 70%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaarschmidt_M/0/1/0/all/0/1&quot;&gt;Michael Schaarschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhnle_A/0/1/0/all/0/1&quot;&gt;Alexander Kuhnle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1&quot;&gt;Ben Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fricke_K/0/1/0/all/0/1&quot;&gt;Kai Fricke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gessert_F/0/1/0/all/0/1&quot;&gt;Felix Gessert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoneki_E/0/1/0/all/0/1&quot;&gt;Eiko Yoneki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07912">
<title>Multivariate Extension of Matrix-based Renyi&apos;s {\alpha}-order Entropy Functional. (arXiv:1808.07912v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1808.07912</link>
<description rdf:parseType="Literal">&lt;p&gt;The matrix-based Renyi&apos;s {\alpha}-order entropy functional was recently
introduced using the normalized eigenspectrum of an Hermitian matrix of the
projected data in the reproducing kernel Hilbert space (RKHS). However, the
current theory in the matrix-based Renyi&apos;s {\alpha}-order entropy functional
only defines the entropy of a single variable or mutual information between two
random variables. In information theory and machine learning communities, one
is also frequently interested in multivariate information quantities, such as
the multivariate joint entropy and different interactive quantities among
multiple variables. In this paper, we first define the matrix-based Renyi&apos;s
{\alpha}-order joint entropy among multiple variables. We then show how this
definition can ease the estimation of various information quantities that
measure the interactions among multiple variables, such as interactive
information and total correlation. We finally present an application to feature
selection to show how our definition provides a simple yet powerful way to
estimate a widely-acknowledged intractable quantity from data. A real example
on hyperspectral image (HSI) band selection is also provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giraldo_L/0/1/0/all/0/1&quot;&gt;Luis Gonzalo Sanchez Giraldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07983">
<title>Analysis of Noise Contrastive Estimation from the Perspective of Asymptotic Variance. (arXiv:1808.07983v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07983</link>
<description rdf:parseType="Literal">&lt;p&gt;There are many models, often called unnormalized models, whose normalizing
constants are not calculated in closed form. Maximum likelihood estimation is
not directly applicable to unnormalized models. Score matching, contrastive
divergence method, pseudo-likelihood, Monte Carlo maximum likelihood, and noise
contrastive estimation (NCE) are popular methods for estimating parameters of
such models. In this paper, we focus on NCE. The estimator derived from NCE is
consistent and asymptotically normal because it is an M-estimator. NCE
characteristically uses an auxiliary distribution to calculate the normalizing
constant in the same spirit of the importance sampling. In addition, there are
several candidates as objective functions of NCE.
&lt;/p&gt;
&lt;p&gt;We focus on how to reduce asymptotic variance. First, we propose a method for
reducing asymptotic variance by estimating the parameters of the auxiliary
distribution. Then, we determine the form of the objective functions, where the
asymptotic variance takes the smallest values in the original estimator class
and the proposed estimator classes. We further analyze the robustness of the
estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Uehara_M/0/1/0/all/0/1&quot;&gt;Masatoshi Uehara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matsuda_T/0/1/0/all/0/1&quot;&gt;Takeru Matsuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Komaki_F/0/1/0/all/0/1&quot;&gt;Fumiyasu Komaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07989">
<title>A Semi-Markov Chain Approach to Modeling Respiratory Patterns Prior to Extubation in Preterm Infants. (arXiv:1808.07989v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1808.07989</link>
<description rdf:parseType="Literal">&lt;p&gt;After birth, extremely preterm infants often require specialized respiratory
management in the form of invasive mechanical ventilation (IMV). Protracted IMV
is associated with detrimental outcomes and morbidities. Premature extubation,
on the other hand, would necessitate reintubation which is risky, technically
challenging and could further lead to lung injury or disease. We present an
approach to modeling respiratory patterns of infants who succeeded extubation
and those who required reintubation which relies on Markov models. We compare
the use of traditional Markov chains to semi-Markov models which emphasize
cross-pattern transitions and timing information, and to multi-chain Markov
models which can concisely represent non-stationarity in respiratory behavior
over time. The models we developed expose specific, unique similarities as well
as vital differences between the two populations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Onu_C/0/1/0/all/0/1&quot;&gt;Charles C. Onu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanbar_L/0/1/0/all/0/1&quot;&gt;Lara J. Kanbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shalish_W/0/1/0/all/0/1&quot;&gt;Wissam Shalish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brown_K/0/1/0/all/0/1&quot;&gt;Karen A. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+SantAnna_G/0/1/0/all/0/1&quot;&gt;Guilherme M. Sant&amp;#x27;Anna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kearney_R/0/1/0/all/0/1&quot;&gt;Robert E. Kearney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07991">
<title>Predicting Extubation Readiness in Extreme Preterm Infants based on Patterns of Breathing. (arXiv:1808.07991v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07991</link>
<description rdf:parseType="Literal">&lt;p&gt;Extremely preterm infants commonly require intubation and invasive mechanical
ventilation after birth. While the duration of mechanical ventilation should be
minimized in order to avoid complications, extubation failure is associated
with increases in morbidities and mortality. As part of a prospective
observational study aimed at developing an accurate predictor of extubation
readiness, Markov and semi-Markov chain models were applied to gain insight
into the respiratory patterns of these infants, with more robust time-series
modeling using semi-Markov models. This model revealed interesting similarities
and differences between newborns who succeeded extubation and those who failed.
The parameters of the model were further applied to predict extubation
readiness via generative (joint likelihood) and discriminative (support vector
machine) approaches. Results showed that up to 84\% of infants who failed
extubation could have been accurately identified prior to extubation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1&quot;&gt;Charles C. Onu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanbar_L/0/1/0/all/0/1&quot;&gt;Lara J. Kanbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalish_W/0/1/0/all/0/1&quot;&gt;Wissam Shalish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1&quot;&gt;Karen A. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+SantAnna_G/0/1/0/all/0/1&quot;&gt;Guilherme M. Sant&amp;#x27;Anna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kearney_R/0/1/0/all/0/1&quot;&gt;Robert E. Kearney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07992">
<title>Undersampling and Bagging of Decision Trees in the Analysis of Cardiorespiratory Behavior for the Prediction of Extubation Readiness in Extremely Preterm Infants. (arXiv:1808.07992v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07992</link>
<description rdf:parseType="Literal">&lt;p&gt;Extremely preterm infants often require endotracheal intubation and
mechanical ventilation during the first days of life. Due to the detrimental
effects of prolonged invasive mechanical ventilation (IMV), clinicians aim to
extubate infants as soon as they deem them ready. Unfortunately, existing
strategies for prediction of extubation readiness vary across clinicians and
institutions, and lead to high reintubation rates. We present an approach using
Random Forest classifiers for the analysis of cardiorespiratory variability to
predict extubation readiness. We address the issue of data imbalance by
employing random undersampling of examples from the majority class before
training each Decision Tree in a bag. By incorporating clinical domain
knowledge, we further demonstrate that our classifier could have identified 71%
of infants who failed extubation, while maintaining a success detection rate of
78%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanbar_L/0/1/0/all/0/1&quot;&gt;Lara J. Kanbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1&quot;&gt;Charles C. Onu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalish_W/0/1/0/all/0/1&quot;&gt;Wissam Shalish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1&quot;&gt;Karen A. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+SantAnna_G/0/1/0/all/0/1&quot;&gt;Guilherme M. Sant&amp;#x27;Anna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kearney_R/0/1/0/all/0/1&quot;&gt;Robert E. Kearney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08068">
<title>Self-Paced Multi-Task Clustering. (arXiv:1808.08068v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08068</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task clustering (MTC) has attracted a lot of research attentions in
machine learning due to its ability in utilizing the relationship among
different tasks. Despite the success of traditional MTC models, they are either
easy to stuck into local optima, or sensitive to outliers and noisy data. To
alleviate these problems, we propose a novel self-paced multi-task clustering
(SPMTC) paradigm. In detail, SPMTC progressively selects data examples to train
a series of MTC models with increasing complexity, thus highly decreases the
risk of trapping into poor local optima. Furthermore, to reduce the negative
influence of outliers and noisy data, we design a soft version of SPMTC to
further improve the clustering performance. The corresponding SPMTC framework
can be easily solved by an alternating optimization method. The proposed model
is guaranteed to converge and experiments on real data sets have demonstrated
its promising results compared with state-of-the-art multi-task clustering
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yazhou Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Que_X/0/1/0/all/0/1&quot;&gt;Xiaofan Que&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1&quot;&gt;Dezhong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08124">
<title>Insect cyborgs: Biological feature generators improve machine learning accuracy on limited data. (arXiv:1808.08124v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1808.08124</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite many successes, machine learning (ML) methods such as neural nets
often struggle to learn given small training sets. In contrast, biological
neural nets (BNNs) excel at fast learning. We can thus look to BNNs for tools
to improve performance of ML methods in this low-data regime.
&lt;/p&gt;
&lt;p&gt;The insect olfactory network, though simple, can learn new odors very
rapidly. Its two key structures are a layer with competitive inhibition (the
Antennal Lobe, AL), followed by a high dimensional sparse plastic layer (the
Mushroom Body, MB). This AL-MB network can rapidly learn not only odors but
also handwritten digits, better in fact than standard ML methods in the
few-shot regime.
&lt;/p&gt;
&lt;p&gt;In this work, we deploy the AL-MB network as an automatic feature generator,
using its Readout Neurons as additional features for standard ML classifiers.
We hypothesize that the AL-MB structure has a strong intrinsic clustering
ability; and that its Readout Neurons, used as input features, will boost the
performance of ML methods.
&lt;/p&gt;
&lt;p&gt;We find that these &quot;insect cyborgs&quot;, ie classifiers that are part-moth and
part-ML method, deliver significantly better performance than baseline ML
methods alone on a generic (non-spatial) 85-feature, 10-class task derived from
the MNIST dataset. Accuracy improves by an average of 6% to 33% for N &amp;lt; 15
training samples per class, and by 6% to 10% for N &amp;gt; 15. Remarkably, these
moth-generated features increase ML accuracy even when the ML method&apos;s baseline
accuracy already exceeds the AL-MB&apos;s own limited capacity.
&lt;/p&gt;
&lt;p&gt;The two structures in the AL-MB, a competitive inhibition layer and a
high-dimensional sparse layer with Hebbian plasticity, are novel in the context
of artificial NNs but endemic in BNNs. We believe they can be deployed either
prepended as feature generators or inserted as layers into deep NNs, to
potentially improve ML performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delahunt_C/0/1/0/all/0/1&quot;&gt;Charles B Delahunt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J Nathan Kutz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08166">
<title>An Empirical Study of Rich Subgroup Fairness for Machine Learning. (arXiv:1808.08166v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08166</link>
<description rdf:parseType="Literal">&lt;p&gt;Kearns et al. [2018] recently proposed a notion of rich subgroup fairness
intended to bridge the gap between statistical and individual notions of
fairness. Rich subgroup fairness picks a statistical fairness constraint (say,
equalizing false positive rates across protected groups), but then asks that
this constraint hold over an exponentially or infinitely large collection of
subgroups defined by a class of functions with bounded VC dimension. They give
an algorithm guaranteed to learn subject to this constraint, under the
condition that it has access to oracles for perfectly learning absent a
fairness constraint. In this paper, we undertake an extensive empirical
evaluation of the algorithm of Kearns et al. On four real datasets for which
fairness is a concern, we investigate the basic convergence of the algorithm
when instantiated with fast heuristics in place of learning oracles, measure
the tradeoffs between fairness and accuracy, and compare this approach with the
recent algorithm of Agarwal et al. [2018], which implements weaker and more
traditional marginal fairness constraints defined by individual protected
attributes. We find that in general, the Kearns et al. algorithm converges
quickly, large gains in fairness can be obtained with mild costs to accuracy,
and that optimizing accuracy subject only to marginal fairness leads to
classifiers with substantial subgroup unfairness. We also provide a number of
analyses and visualizations of the dynamics and behavior of the Kearns et al.
algorithm. Overall we find this algorithm to be effective on real data, and
rich subgroup fairness to be a viable notion in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1&quot;&gt;Michael Kearns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1&quot;&gt;Seth Neel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1&quot;&gt;Aaron Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08195">
<title>GoT-WAVE: Temporal network alignment using graphlet-orbit transitions. (arXiv:1808.08195v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08195</link>
<description rdf:parseType="Literal">&lt;p&gt;Global pairwise network alignment (GPNA) aims to find a one-to-one node
mapping between two networks that identifies conserved network regions. GPNA
algorithms optimize node conservation (NC) and edge conservation (EC). NC
quantifies topological similarity between nodes. Graphlet-based degree vectors
(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were
used as a dynamic NC measure within the first-ever algorithms for GPNA of
temporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger
networks. We recently developed a different graphlet-based measure of temporal
node similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead
of DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new
approach, GoT-WAVE.
&lt;/p&gt;
&lt;p&gt;On synthetic networks, GoT-WAVE improves DynaWAVE&apos;s accuracy by 25% and speed
by 64%. On real networks, when optimizing only dynamic NC, each method is
superior ~50% of the time. While DynaWAVE benefits more from also optimizing
dynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a
promising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.
Future work on better incorporating dynamic EC may yield further improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aparicio_D/0/1/0/all/0/1&quot;&gt;David Apar&amp;#xed;cio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_P/0/1/0/all/0/1&quot;&gt;Pedro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milenkovic_T/0/1/0/all/0/1&quot;&gt;Tijana Milenkovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1&quot;&gt;Fernando Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.08266">
<title>Visualizing and Understanding Sum-Product Networks. (arXiv:1608.08266v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1608.08266</link>
<description rdf:parseType="Literal">&lt;p&gt;Sum-Product Networks (SPNs) are recently introduced deep tractable
probabilistic models by which several kinds of inference queries can be
answered exactly and in a tractable time. Up to now, they have been largely
used as black box density estimators, assessed only by comparing their
likelihood scores only. In this paper we explore and exploit the inner
representations learned by SPNs. We do this with a threefold aim: first we want
to get a better understanding of the inner workings of SPNs; secondly, we seek
additional ways to evaluate one SPN model and compare it against other
probabilistic models, providing diagnostic tools to practitioners; lastly, we
want to empirically evaluate how good and meaningful the extracted
representations are, as in a classic Representation Learning framework. In
order to do so we revise their interpretation as deep neural networks and we
propose to exploit several visualization techniques on their node activations
and network outputs under different types of inference queries. To investigate
these models as feature extractors, we plug some SPNs, learned in a greedy
unsupervised fashion on image datasets, in supervised classification learning
tasks. We extract several embedding types from node activations by filtering
nodes by their type, by their associated feature abstraction level and by their
scope. In a thorough empirical comparison we prove them to be competitive
against those generated from popular feature extractors as Restricted Boltzmann
Machines. Finally, we investigate embeddings generated from random
probabilistic marginal queries as means to compare other tractable
probabilistic models on a common ground, extending our experiments to Mixtures
of Trees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1&quot;&gt;Antonio Vergari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauro_N/0/1/0/all/0/1&quot;&gt;Nicola Di Mauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esposito_F/0/1/0/all/0/1&quot;&gt;Floriana Esposito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02162">
<title>A trans-disciplinary review of deep learning research for water resources scientists. (arXiv:1712.02162v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02162</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL), a new-generation of artificial neural network research,
has transformed industries, daily lives and various scientific disciplines in
recent years. DL represents significant progress in the ability of neural
networks to automatically engineer problem-relevant features and capture highly
complex data distributions. I argue that DL can help address several major new
and old challenges facing research in water sciences such as
inter-disciplinarity, data discoverability, hydrologic scaling, equifinality,
and needs for parameter regionalization. This review paper is intended to
provide water resources scientists and hydrologists in particular with a simple
technical overview, trans-disciplinary progress update, and a source of
inspiration about the relevance of DL to water. The review reveals that various
physical and geoscientific disciplines have utilized DL to address data
challenges, improve efficiency, and gain scientific insights. DL is especially
suited for information extraction from image-like data and sequential data.
Techniques and experiences presented in other disciplines are of high relevance
to water research. Meanwhile, less noticed is that DL may also serve as a
scientific exploratory tool. A new area termed &apos;AI neuroscience,&apos; where
scientists interpret the decision process of deep networks and derive insights,
has been born. This budding sub-discipline has demonstrated methods including
correlation-based analysis, inversion of network-extracted features,
reduced-order approximations by interpretable models, and attribution of
network decisions to inputs. Moreover, DL can also use data to condition
neurons that mimic problem-specific fundamental organizing units, thus
revealing emergent behaviors of these units. Vast opportunities exist for DL to
propel advances in water sciences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chaopeng Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09064">
<title>Time Series Analysis via Matrix Estimation. (arXiv:1802.09064v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09064</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an algorithm to impute and forecast a time series by transforming
the observed time series into a matrix, utilizing matrix estimation to recover
missing values and de-noise observed entries, and performing linear regression
to make predictions. At the core of our analysis is a representation result,
which states that for a large model class, the transformed matrix obtained from
the time series via our algorithm is (approximately) low-rank. This, in effect,
generalizes the widely used Singular Spectrum Analysis (SSA) in literature, and
allows us to establish a rigorous link between time series analysis and matrix
estimation. The key is to construct a matrix with non-overlapping entries
rather than with the Hankel matrix as done in the literature, including in SSA.
We provide finite sample analysis for imputation and prediction leading to the
asymptotic consistency of our method. A salient feature of our algorithm is
that it is model agnostic both with respect to the underlying time dynamics as
well as the noise model in the observations. Being noise agnostic makes our
algorithm applicable to the setting where the state is hidden and we only have
access to its noisy observations a la a Hidden Markov Model, e.g., observing a
Poisson process with a time-varying parameter without knowing that the process
is Poisson, but still recovering the time-varying parameter accurately. As part
of the forecasting algorithm, an important task is to perform regression with
noisy observations of the features a la an error- in-variable regression. In
essence, our approach suggests a matrix estimation based method for such a
setting, which could be of interest in its own right. Through synthetic and
real-world datasets, we demonstrate that our algorithm outperforms standard
software packages (including R libraries) in the presence of missing data as
well as high levels of noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Anish Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amjad_M/0/1/0/all/0/1&quot;&gt;Muhammad Jehangir Amjad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Devavrat Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dennis Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00057">
<title>Understanding Autoencoders with Information Theoretic Concepts. (arXiv:1804.00057v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00057</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their great success in practical applications, there is still a lack
of theoretical and systematic methods to analyze deep neural networks. In this
paper, we illustrate an advanced information theoretic methodology to
understand the dynamics of learning and the design of autoencoders, a special
type of deep learning architectures that resembles a communication channel. By
generalizing the information plane to any cost function, and inspecting the
roles and dynamics of different layers using layer-wise information quantities,
we emphasize the role that mutual information plays in quantifying learning
from data. We further suggest and also experimentally validate, for mean square
error training, three fundamental properties regarding the layer-wise flow of
information and intrinsic dimensionality of the bottleneck layer, using
respectively the data processing inequality and the identification of a
bifurcation point in the information plane that is controlled by the given
data. Our observations have direct impact on the optimal design of
autoencoders, the design of alternative feedforward training methods, and even
in the problem of generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01771">
<title>Cycle-Consistent Adversarial Learning as Approximate Bayesian Inference. (arXiv:1806.01771v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01771</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the problem of learning interdomain correspondences in the
absence of paired data as Bayesian inference in a latent variable model (LVM),
where one seeks the underlying hidden representations of entities from one
domain as entities from the other domain. First, we introduce implicit latent
variable models, where the prior over hidden representations can be specified
flexibly as an implicit distribution. Next, we develop a new variational
inference (VI) algorithm for this model based on minimization of the symmetric
Kullback-Leibler (KL) divergence between a variational joint and the exact
joint distribution. Lastly, we demonstrate that the state-of-the-art
cycle-consistent adversarial learning (CYCLEGAN) models can be derived as a
special case within our proposed VI framework, thus establishing its connection
to approximate Bayesian inference methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tiao_L/0/1/0/all/0/1&quot;&gt;Louis C. Tiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bonilla_E/0/1/0/all/0/1&quot;&gt;Edwin V. Bonilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramos_F/0/1/0/all/0/1&quot;&gt;Fabio Ramos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05833">
<title>On the exact minimization of saturated loss functions for robust regression and subspace estimation. (arXiv:1806.05833v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05833</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with robust regression and subspace estimation and more
precisely with the problem of minimizing a saturated loss function. In
particular, we focus on computational complexity issues and show that an exact
algorithm with polynomial time-complexity with respect to the number of data
can be devised for robust regression and subspace estimation. This result is
obtained by adopting a classification point of view and relating the problems
to the search for a linear model that can approximate the maximal number of
points with a given error. Approximate variants of the algorithms based on
ramdom sampling are also discussed and experiments show that it offers an
accuracy gain over the traditional RANSAC for a similar algorithmic simplicity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauer_F/0/1/0/all/0/1&quot;&gt;Fabien Lauer&lt;/a&gt; (ABC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06670">
<title>Learning deep representations by mutual information estimation and maximization. (arXiv:1808.06670v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06670</link>
<description rdf:parseType="Literal">&lt;p&gt;Many popular representation-learning algorithms use training objectives
defined on the observed data space, which we call pixel-level. This may be
detrimental when only a small fraction of the bits of signal actually matter at
a semantic level. We hypothesize that representations should be learned and
evaluated more directly in terms of their information content and statistical
or structural constraints. To address the first quality, we consider learning
unsupervised representations by maximizing mutual information between part or
all of the input and a high-level feature vector. To address the second, we
control characteristics of the representation by matching to a prior
adversarially. Our method, which we call Deep INFOMAX (DIM), can be used to
learn representations with desired characteristics and which empirically
outperform a number of popular unsupervised learning methods on classification
tasks. DIM opens new avenues for unsupervised learn-ing of representations and
is an important step towards flexible formulations of representation learning
objectives catered towards specific end-goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedorov_A/0/1/0/all/0/1&quot;&gt;Alex Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lavoie_Marchildon_S/0/1/0/all/0/1&quot;&gt;Samuel Lavoie-Marchildon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grewal_K/0/1/0/all/0/1&quot;&gt;Karan Grewal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trischler_A/0/1/0/all/0/1&quot;&gt;Adam Trischler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item></rdf:RDF>