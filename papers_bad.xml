<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05192"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10937"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11002"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11080"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03871"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05437"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11135"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03454"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10815"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10986"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11132"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03194"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04425"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09390"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10264"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09974"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10535"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1708.06238">
<title>Stochastic IMT (insulator-metal-transition) neurons: An interplay of thermal and threshold noise at bifurcation. (arXiv:1708.06238v4 [cs.ET] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06238</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial neural networks can harness stochasticity in multiple ways to
enable a vast class of computationally powerful models. Electronic
implementation of such stochastic networks is currently limited to addition of
algorithmic noise to digital machines which is inherently inefficient; albeit
recent efforts to harness physical noise in devices for stochasticity have
shown promise. To succeed in fabricating electronic neuromorphic networks we
need experimental evidence of devices with measurable and controllable
stochasticity which is complemented with the development of reliable
statistical models of such observed stochasticity. Current research literature
has sparse evidence of the former and a complete lack of the latter. This
motivates the current article where we demonstrate a stochastic neuron using an
insulator-metal-transition (IMT) device, based on electrically induced
phase-transition, in series with a tunable resistance. We show that an IMT
neuron has dynamics similar to a piecewise linear FitzHugh-Nagumo (FHN) neuron
and incorporates all characteristics of a spiking neuron in the device
phenomena. We experimentally demonstrate spontaneous stochastic spiking along
with electrically controllable firing probabilities using Vanadium Dioxide
(VO$_2$) based IMT neurons which show a sigmoid-like transfer function. The
stochastic spiking is explained by two noise sources - thermal noise and
threshold fluctuations, which act as precursors of bifurcation. As such, the
IMT neuron is modeled as an Ornstein-Uhlenbeck (OU) process with a fluctuating
boundary resulting in transfer curves that closely match experiments. As one of
the first comprehensive studies of a stochastic neuron hardware and its
statistical properties, this article would enable efficient implementation of a
large class of neuro-mimetic networks and algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parihar_A/0/1/0/all/0/1&quot;&gt;Abhinav Parihar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jerry_M/0/1/0/all/0/1&quot;&gt;Matthew Jerry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1&quot;&gt;Suman Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raychowdhury_A/0/1/0/all/0/1&quot;&gt;Arijit Raychowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06379">
<title>Improvements to context based self-supervised learning. (arXiv:1711.06379v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06379</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a set of methods to improve on the results of self-supervised
learning using context. We start with a baseline of patch based arrangement
context learning and go from there. Our methods address some overt problems
such as chromatic aberration as well as other potential problems such as
spatial skew and mid-level feature neglect. We prevent problems with testing
generalization on common self-supervised benchmark tests by using different
datasets during our development. The results of our methods combined yield top
scores on all standard self-supervised benchmarks, including classification and
detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and &quot;linear
tests&quot; on the ImageNet and CSAIL Places datasets. We obtain an improvement over
our baseline method of between 4.0 to 7.1 percentage points on transfer
learning classification tests. We also show results on different standard
network architectures to demonstrate generalization as well as portability. All
data, models and programs are available at:
https://gdo-datasci.llnl.gov/selfsupervised/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundhenk_T/0/1/0/all/0/1&quot;&gt;T. Nathan Mundhenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1&quot;&gt;Daniel Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Barry Y. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05192">
<title>Real-time Cardiovascular MR with Spatio-temporal De-aliasing using Deep Learning - Proof of Concept in Congenital Heart Disease. (arXiv:1803.05192v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05192</link>
<description rdf:parseType="Literal">&lt;p&gt;PURPOSE: Real-time assessment of ventricular volumes requires high
acceleration factors. Residual convolutional neural networks (CNN) have shown
potential for removing artifacts caused by data undersampling. In this study we
investigated the effect of different radial sampling patterns on the accuracy
of a CNN. We also acquired actual real-time undersampled radial data in
patients with congenital heart disease (CHD), and compare CNN reconstruction to
Compressed Sensing (CS).
&lt;/p&gt;
&lt;p&gt;METHODS: A 3D (2D plus time) CNN architecture was developed, and trained
using 2276 gold-standard paired 3D data sets, with 14x radial undersampling.
Four sampling schemes were tested, using 169 previously unseen 3D &apos;synthetic&apos;
test data sets. Actual real-time tiny Golden Angle (tGA) radial SSFP data was
acquired in 10 new patients (122 3D data sets), and reconstructed using the 3D
CNN as well as a CS algorithm; GRASP.
&lt;/p&gt;
&lt;p&gt;RESULTS: Sampling pattern was shown to be important for image quality, and
accurate visualisation of cardiac structures. For actual real-time data,
overall reconstruction time with CNN (including creation of aliased images) was
shown to be more than 5x faster than GRASP. Additionally, CNN image quality and
accuracy of biventricular volumes was observed to be superior to GRASP for the
same raw data.
&lt;/p&gt;
&lt;p&gt;CONCLUSION: This paper has demonstrated the potential for the use of a 3D CNN
for deep de-aliasing of real-time radial data, within the clinical setting.
Clinical measures of ventricular volumes using real-time data with CNN
reconstruction are not statistically significantly different from the
gold-standard, cardiac gated, BH techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1&quot;&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1&quot;&gt;Simon Arridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1&quot;&gt;Felix Lucka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muthurangu_V/0/1/0/all/0/1&quot;&gt;Vivek Muthurangu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeden_J/0/1/0/all/0/1&quot;&gt;Jennifer A. Steeden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10459">
<title>Graphite: Iterative Generative Modeling of Graphs. (arXiv:1803.10459v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10459</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphs are a fundamental abstraction for modeling relational data. However,
graphs are discrete and combinatorial in nature, and learning representations
suitable for machine learning tasks poses statistical and computational
challenges. In this work, we propose Graphite an algorithmic framework for
unsupervised learning of representations over nodes in a graph using deep
latent variable generative models. Our model is based on variational
autoencoders (VAE), and differs from existing VAE frameworks for data
modalities such as images, speech, and text in the use of graph neural networks
for parameterizing both the generative model (i.e., decoder) and inference
model (i.e., encoder). The use of graph neural networks directly incorporates
inductive biases due to the spatial, local structure of graphs directly in the
generative model. Moreover, we draw novel connections between graph neural
networks and approximate inference via kernel embeddings of distributions. We
demonstrate empirically that Graphite outperforms state-of-the-art approaches
for the tasks of density estimation, link prediction, and node classification
on synthetic and benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zweig_A/0/1/0/all/0/1&quot;&gt;Aaron Zweig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10937">
<title>Best arm identification in multi-armed bandits with delayed feedback. (arXiv:1803.10937v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10937</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generalization of the best arm identification problem in
stochastic multi-armed bandits (MAB) to the setting where every pull of an arm
is associated with delayed feedback. The delay in feedback increases the
effective sample complexity of standard algorithms, but can be offset if we
have access to partial feedback received before a pull is completed. We propose
a general framework to model the relationship between partial and delayed
feedback, and as a special case we introduce efficient algorithms for settings
where the partial feedback are biased or unbiased estimators of the delayed
feedback. Additionally, we propose a novel extension of the algorithms to the
parallel MAB setting where an agent can control a batch of arms. Our
experiments in real-world settings, involving policy search and hyperparameter
optimization in computational sustainability domains for fast charging of
batteries and wildlife corridor construction, demonstrate that exploiting the
structure of partial feedback can lead to significant improvements over
baselines in both sequential and parallel MAB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markov_T/0/1/0/all/0/1&quot;&gt;Todor Markov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attia_P/0/1/0/all/0/1&quot;&gt;Peter Attia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_N/0/1/0/all/0/1&quot;&gt;Norman Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perkins_N/0/1/0/all/0/1&quot;&gt;Nicholas Perkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_B/0/1/0/all/0/1&quot;&gt;Bryan Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Michael Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harris_S/0/1/0/all/0/1&quot;&gt;Stephen Harris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chueh_W/0/1/0/all/0/1&quot;&gt;William Chueh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10981">
<title>A Review of Literature on Parallel Constraint Solving. (arXiv:1803.10981v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.10981</link>
<description rdf:parseType="Literal">&lt;p&gt;As multicore computing is now standard, it seems irresponsible for
constraints researchers to ignore the implications of it. Researchers need to
address a number of issues to exploit parallelism, such as: investigating which
constraint algorithms are amenable to parallelisation; whether to use shared
memory or distributed computation; whether to use static or dynamic
decomposition; and how to best exploit portfolios and cooperating search. We
review the literature, and see that we can sometimes do quite well, some of the
time, on some instances, but we are far from a general solution. Yet there
seems to be little overall guidance that can be given on how best to exploit
multicore computers to speed up constraint solving. We hope at least that this
survey will provide useful pointers to future researchers wishing to correct
this situation.
&lt;/p&gt;
&lt;p&gt;Under consideration in Theory and Practice of Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gent_I/0/1/0/all/0/1&quot;&gt;Ian P. Gent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCreesh_C/0/1/0/all/0/1&quot;&gt;Ciaran McCreesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miguel_I/0/1/0/all/0/1&quot;&gt;Ian Miguel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_N/0/1/0/all/0/1&quot;&gt;Neil C.A. Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nightingale_P/0/1/0/all/0/1&quot;&gt;Peter Nightingale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prosser_P/0/1/0/all/0/1&quot;&gt;Patrick Prosser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unsworth_C/0/1/0/all/0/1&quot;&gt;Chris Unsworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11002">
<title>Modified SMOTE Using Mutual Information and Different Sorts of Entropies. (arXiv:1803.11002v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.11002</link>
<description rdf:parseType="Literal">&lt;p&gt;SMOTE is one of the oversampling techniques for balancing the datasets and it
is considered as a pre-processing step in learning algorithms. In this paper,
four new enhanced SMOTE are proposed that include an improved version of KNN in
which the attribute weights are defined by mutual information firstly and then
they are replaced by maximum entropy, Renyi entropy and Tsallis entropy. These
four pre-processing methods are combined with 1NN and J48 classifiers and their
performance are compared with the previous methods on 11 imbalanced datasets
from KEEL repository. The results show that these pre-processing methods
improves the accuracy compared with the previous stablished works. In addition,
as a case study, the first pre-processing method is applied on transportation
data of Tehran-Bazargan Highway in Iran with IR equal to 36.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifirad_S/0/1/0/all/0/1&quot;&gt;Sima Sharifirad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazari_A/0/1/0/all/0/1&quot;&gt;Azra Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1&quot;&gt;Mehdi Ghatee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11060">
<title>COBRAS: Fast, Iterative, Active Clustering with Pairwise Constraints. (arXiv:1803.11060v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.11060</link>
<description rdf:parseType="Literal">&lt;p&gt;Constraint-based clustering algorithms exploit background knowledge to
construct clusterings that are aligned with the interests of a particular user.
This background knowledge is often obtained by allowing the clustering system
to pose pairwise queries to the user: should these two elements be in the same
cluster or not? Active clustering methods aim to minimize the number of queries
needed to obtain a good clustering by querying the most informative pairs
first. Ideally, a user should be able to answer a couple of these queries,
inspect the resulting clustering, and repeat these two steps until a
satisfactory result is obtained. We present COBRAS, an approach to active
clustering with pairwise constraints that is suited for such an interactive
clustering process. A core concept in COBRAS is that of a super-instance: a
local region in the data in which all instances are assumed to belong to the
same cluster. COBRAS constructs such super-instances in a top-down manner to
produce high-quality results early on in the clustering process, and keeps
refining these super-instances as more pairwise queries are given to get more
detailed clusterings later on. We experimentally demonstrate that COBRAS
produces good clusterings at fast run times, making it an excellent candidate
for the iterative clustering scenario outlined above.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Craenendonck_T/0/1/0/all/0/1&quot;&gt;Toon Van Craenendonck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Duman&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolputte_E/0/1/0/all/0/1&quot;&gt;Elia Van Wolputte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blockeel_H/0/1/0/all/0/1&quot;&gt;Hendrik Blockeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11080">
<title>3D Consistent Biventricular Myocardial Segmentation Using Deep Learning for Mesh Generation. (arXiv:1803.11080v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.11080</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel automated method to segment the myocardium of both left
and right ventricles in MRI volumes. The segmentation is consistent in 3D
across the slices such that it can be directly used for mesh generation. Two
specific neural networks with multi-scale coarse-to-fine prediction structure
are proposed to cope with the small training dataset and trained using an
original loss function. The former segments a slice in the middle of the
volume. Then the latter iteratively propagates the slice segmentations towards
the base and the apex, in a spatially consistent way. We perform 5-fold
cross-validation on the 15 cases from STACOM to validate the method. For
training, we use real cases and their synthetic variants generated by combining
motion simulation and image synthesis. Accurate and consistent testing results
are obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qiao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delingette_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Delingette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duchateau_N/0/1/0/all/0/1&quot;&gt;Nicolas Duchateau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayache_N/0/1/0/all/0/1&quot;&gt;Nicholas Ayache&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03871">
<title>Knowledge Completion for Generics using Guided Tensor Factorization. (arXiv:1612.03871v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03871</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a knowledge base or KB containing (noisy) facts about common nouns or
generics, such as &quot;all trees produce oxygen&quot; or &quot;some animals live in forests&quot;,
we consider the problem of inferring additional such facts at a precision
similar to that of the starting KB. Such KBs capture general knowledge about
the world, and are crucial for various applications such as question answering.
Different from commonly studied named entity KBs such as Freebase, generics KBs
involve quantification, have more complex underlying regularities, tend to be
more incomplete, and violate the commonly used locally closed world assumption
(LCWA). We show that existing KB completion methods struggle with this new
task, and present the first approach that is successful. Our results
demonstrate that external information, such as relation schemas and entity
taxonomies, if used appropriately, can be a surprisingly powerful tool in this
setting. First, our simple yet effective knowledge guided tensor factorization
approach achieves state-of-the-art results on two generics KBs (80% precise)
for science, doubling their size at 74%-86% precision. Second, our novel
taxonomy guided, submodular, active learning method for collecting annotations
about rare entities (e.g., oriole, a bird) is 6x more effective at inferring
further new facts about them than multiple active learning baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1&quot;&gt;Hanie Sedghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1&quot;&gt;Ashish Sabharwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04049">
<title>Information Design in Crowdfunding under Thresholding Policies. (arXiv:1709.04049v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04049</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowdfunding has emerged as a prominent way for entrepreneurs to secure
funding without sophisticated intermediation. In crowdfunding, an entrepreneur
often has to decide how to disclose the campaign status in order to collect as
many contributions as possible. Such decisions are difficult to make primarily
due to incomplete information. We propose information design as a tool to help
the entrepreneur to improve revenue by influencing backers&apos; beliefs. We
introduce a heuristic algorithm to dynamically compute information-disclosure
policies for the entrepreneur, followed by an empirical evaluation to
demonstrate its competitiveness over the widely-adopted immediate-disclosure
policy. Our results demonstrate that the immediate-disclosure policy is not
optimal when backers follow thresholding policies despite its ease of
implementation. With appropriate heuristics, an entrepreneur can benefit from
dynamic information disclosure. Our work sheds light on information design in a
dynamic setting where agents make decisions using thresholding policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_J/0/1/0/all/0/1&quot;&gt;Jacob W. Crandall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopes_C/0/1/0/all/0/1&quot;&gt;Cristina V. Lopes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05437">
<title>A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects. (arXiv:1709.05437v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05437</link>
<description rdf:parseType="Literal">&lt;p&gt;Tracking humans that are interacting with the other subjects or environment
remains unsolved in visual tracking, because the visibility of the human of
interests in videos is unknown and might vary over time. In particular, it is
still difficult for state-of-the-art human trackers to recover complete human
trajectories in crowded scenes with frequent human interactions. In this work,
we consider the visibility status of a subject as a fluent variable, whose
change is mostly attributed to the subject&apos;s interaction with the surrounding,
e.g., crossing behind another object, entering a building, or getting into a
vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the
causal-effect relations between an object&apos;s visibility fluent and its
activities, and develop a probabilistic graph model to jointly reason the
visibility fluent change (e.g., from visible to invisible) and track humans in
videos. We formulate this joint task as an iterative search of a feasible
causal graph structure that enables fast search algorithm, e.g., dynamic
programming method. We apply the proposed method on challenging video sequences
to evaluate its capabilities of estimating visibility fluent changes of
subjects and tracking subjects of interests over time. Results with comparisons
demonstrate that our method outperforms the alternative trackers and can
recover complete trajectories of humans in complicated scenarios with frequent
human interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanlu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Lei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaobai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jianwen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07558">
<title>Classification Driven Dynamic Image Enhancement. (arXiv:1710.07558v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07558</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks rely on image texture and structure to serve as
discriminative features to classify the image content. Image enhancement
techniques can be used as preprocessing steps to help improve the overall image
quality and in turn improve the overall effectiveness of a CNN. Existing image
enhancement methods, however, are designed to improve the perceptual quality of
an image for a human observer. In this paper, we are interested in learning
CNNs that can emulate image enhancement and restoration, but with the overall
goal to improve image classification and not necessarily human perception. To
this end, we present a unified CNN architecture that uses a range of
enhancement filters that can enhance image-specific details via end-to-end
dynamic filter learning. We demonstrate the effectiveness of this strategy on
four challenging benchmark datasets for fine-grained, object, scene, and
texture classification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD.
Experiments using our proposed enhancement show promising results on all the
datasets. In addition, our approach is capable of improving the performance of
all generic CNN architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vivek Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diba_A/0/1/0/all/0/1&quot;&gt;Ali Diba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neven_D/0/1/0/all/0/1&quot;&gt;Davy Neven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1&quot;&gt;Michael S. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11135">
<title>Video Captioning via Hierarchical Reinforcement Learning. (arXiv:1711.11135v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11135</link>
<description rdf:parseType="Literal">&lt;p&gt;Video captioning is the task of automatically generating a textual
description of the actions in a video. Although previous work (e.g.
sequence-to-sequence model) has shown promising results in abstracting a coarse
description of a short video, it is still very challenging to caption a video
containing multiple fine-grained actions with a detailed description. This
paper aims to address the challenge by proposing a novel hierarchical
reinforcement learning framework for video captioning, where a high-level
Manager module learns to design sub-goals and a low-level Worker module
recognizes the primitive actions to fulfill the sub-goal. With this
compositional framework to reinforce video captioning at different levels, our
approach significantly outperforms all the baseline methods on a newly
introduced large-scale dataset for fine-grained video captioning. Furthermore,
our non-ensemble model has already achieved the state-of-the-art results on the
widely-used MSR-VTT dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiawei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03454">
<title>Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks. (arXiv:1801.03454v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03454</link>
<description rdf:parseType="Literal">&lt;p&gt;In an effort to understand the meaning of the intermediate representations
captured by deep networks, recent papers have tried to associate specific
semantic concepts to individual neural network filter responses, where
interesting correlations are often found, largely by focusing on extremal
filter responses. In this paper, we show that this approach can favor
easy-to-interpret cases that are not necessarily representative of the average
behavior of a representation.
&lt;/p&gt;
&lt;p&gt;A more realistic but harder-to-study hypothesis is that semantic
representations are distributed, and thus filters must be studied in
conjunction. In order to investigate this idea while enabling systematic
visualization and quantification of multiple filter responses, we introduce the
Net2Vec framework, in which semantic concepts are mapped to vectorial
embeddings based on corresponding filter responses. By studying such
embeddings, we are able to show that 1., in most cases, multiple filters are
required to code for a concept, that 2., often filters are not concept specific
and help encode multiple concepts, and that 3., compared to single filter
activations, filter embeddings are able to better characterize the meaning of a
representation and its relationship to other concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1&quot;&gt;Ruth Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10806">
<title>Learning to Become an Expert: Deep Networks Applied To Super-Resolution Microscopy. (arXiv:1803.10806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.10806</link>
<description rdf:parseType="Literal">&lt;p&gt;With super-resolution optical microscopy, it is now possible to observe
molecular interactions in living cells. The obtained images have a very high
spatial precision but their overall quality can vary a lot depending on the
structure of interest and the imaging parameters. Moreover, evaluating this
quality is often difficult for non-expert users. In this work, we tackle the
problem of learning the quality function of super- resolution images from
scores provided by experts. More specifically, we are proposing a system based
on a deep neural network that can provide a quantitative quality measure of a
STED image of neuronal structures given as input. We conduct a user study in
order to evaluate the quality of the predictions of the neural network against
those of a human expert. Results show the potential while highlighting some of
the limits of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robitaille_L/0/1/0/all/0/1&quot;&gt;Louis-&amp;#xc9;mile Robitaille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_A/0/1/0/all/0/1&quot;&gt;Audrey Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1&quot;&gt;Marc-Andr&amp;#xe9; Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koninck_P/0/1/0/all/0/1&quot;&gt;Paul De Koninck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavoie_Cardinal_F/0/1/0/all/0/1&quot;&gt;Flavie Lavoie-Cardinal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10815">
<title>Supervising Feature Influence. (arXiv:1803.10815v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10815</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal influence measures for machine learnt classifiers shed light on the
reasons behind classification, and aid in identifying influential input
features and revealing their biases. However, such analyses involve evaluating
the classifier using datapoints that may be atypical of its training
distribution. Standard methods for training classifiers that minimize empirical
risk do not constrain the behavior of the classifier on such datapoints. As a
result, training to minimize empirical risk does not distinguish among
classifiers that agree on predictions in the training distribution but have
wildly different causal influences. We term this problem covariate shift in
causal testing and formally characterize conditions under which it arises. As a
solution to this problem, we propose a novel active learning algorithm that
constrains the influence measures of the trained model. We prove that any two
predictors whose errors are close on both the original training distribution
and the distribution of atypical points are guaranteed to have causal
influences that are also close. Further, we empirically demonstrate with
synthetic labelers that our algorithm trains models that (i) have similar
causal influences as the labeler&apos;s model, and (ii) generalize better to
out-of-distribution points while (iii) retaining their accuracy on
in-distribution points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1&quot;&gt;Shayak Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mardziel_P/0/1/0/all/0/1&quot;&gt;Piotr Mardziel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1&quot;&gt;Anupam Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1&quot;&gt;Matthew Fredrikson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10840">
<title>Defending against Adversarial Images using Basis Functions Transformations. (arXiv:1803.10840v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10840</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the effectiveness of various approaches that defend against
adversarial attacks on deep networks via manipulations based on basis function
representations of images. Specifically, we experiment with low-pass filtering,
PCA, JPEG compression, low resolution wavelet approximation, and
soft-thresholding. We evaluate these defense techniques using three types of
popular attacks in black, gray and white-box settings. Our results show JPEG
compression tends to outperform the other tested defenses in most of the
settings considered, in addition to soft-thresholding, which performs well in
specific cases, and yields a more mild decrease in accuracy on benign examples.
In addition, we also mathematically derive a novel white-box attack in which
the adversarial perturbation is composed only of terms corresponding a to
pre-determined subset of the basis functions, of which a &quot;low frequency attack&quot;
is a special case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garritano_J/0/1/0/all/0/1&quot;&gt;James Garritano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamada_Y/0/1/0/all/0/1&quot;&gt;Yutaro Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weinberger_E/0/1/0/all/0/1&quot;&gt;Ethan Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cloninger_A/0/1/0/all/0/1&quot;&gt;Alex Cloninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10846">
<title>Non-Convex Matrix Completion Against a Semi-Random Adversary. (arXiv:1803.10846v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10846</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix completion is a well-studied problem with many machine learning
applications. In practice, the problem is often solved by non-convex
optimization algorithms. However, the current theoretical analysis for
non-convex algorithms relies heavily on the assumption that every entry is
observed with exactly the same probability $p$, which is not realistic in
practice.
&lt;/p&gt;
&lt;p&gt;In this paper, we investigate a more realistic semi-random model, where the
probability of observing each entry is at least $p$. Even with this mild
semi-random perturbation, we can construct counter-examples where existing
non-convex algorithms get stuck in bad local optima.
&lt;/p&gt;
&lt;p&gt;In light of the negative results, we propose a pre-processing step that tries
to re-weight the semi-random input, so that it becomes &quot;similar&quot; to a random
input. We give a nearly-linear time algorithm for this problem, and show that
after our pre-processing, all the local minima of the non-convex objective can
be used to approximately recover the underlying ground-truth matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rong Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10884">
<title>Structural Risk Minimization for $C^{1,1}(\mathbb{R}^d)$ Regression. (arXiv:1803.10884v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10884</link>
<description rdf:parseType="Literal">&lt;p&gt;One means of fitting functions to high-dimensional data is by providing
smoothness constraints. Recently, the following smooth function approximation
problem was proposed by \citet*{herbert2014computing}: given a finite set $E
\subset \mathbb{R}^d$ and a function $f: E \rightarrow \mathbb{R}$, interpolate
the given information with a function $\widehat{f} \in \dot{C}^{1,
1}(\mathbb{R}^d)$ (the class of first-order differentiable functions with
Lipschitz gradients) such that $\widehat{f}(a) = f(a)$ for all $a \in E$, and
the value of $\mathrm{Lip}(\nabla \widehat{f})$ is minimal. An algorithm is
provided that constructs such an approximating function $\widehat{f}$ and
estimates the optimal Lipschitz constant $\mathrm{Lip}(\nabla \widehat{f})$ in
the noiseless setting.
&lt;/p&gt;
&lt;p&gt;We address statistical aspects of reconstructing the approximating function
$\widehat{f}$ from a closely-related class $C^{1, 1}(\mathbb{R}^d)$ given
samples from noisy data. We observe independent and identically distributed
samples $y(a) = f(a) + \xi(a)$ for $a \in E$, where $\xi(a)$ is a noise term
and the set $E \subset \mathbb{R}^d$ is fixed and known. We obtain uniform
bounds relating the empirical risk and true risk over the class
$\mathcal{F}_{\widetilde{M}} = \{f \in C^{1, 1}(\mathbb{R}^d) \mid
\mathrm{Lip}(\nabla f) \leq \widetilde{M}\}$, where the quantity
$\widetilde{M}$ grows with the number of samples at a rate governed by the
metric entropy of the class $C^{1, 1}(\mathbb{R}^d)$. Finally, we provide an
implementation using Vaidya&apos;s algorithm, supporting our results via numerical
experiments on simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gustafson_A/0/1/0/all/0/1&quot;&gt;Adam Gustafson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hirn_M/0/1/0/all/0/1&quot;&gt;Matthew Hirn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohammed_K/0/1/0/all/0/1&quot;&gt;Kitty Mohammed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narayanan_H/0/1/0/all/0/1&quot;&gt;Hariharan Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jason Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10888">
<title>An Empirical Analysis of Constrained Support Vector Quantile Regression for Nonparametric Probabilistic Forecasting of Wind Power. (arXiv:1803.10888v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10888</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty analysis in the form of probabilistic forecasting can provide
significant improvements in decision-making processes in the smart power grid
for better integrating renewable energies such as wind. Whereas point
forecasting provides a single expected value, probabilistic forecasts provide
more information in the form of quantiles, prediction intervals, or full
predictive densities. This paper analyzes the effectiveness of an approach for
nonparametric probabilistic forecasting of wind power that combines support
vector machines and nonlinear quantile regression with non-crossing
constraints. A numerical case study is conducted using publicly available wind
data from the Global Energy Forecasting Competition 2014. Multiple quantiles
are estimated to form 20%, 40%, 60% and 80% prediction intervals which are
evaluated using the pinball loss function and reliability measures. Three
benchmark models are used for comparison where results demonstrate the proposed
approach leads to significantly better performance while preventing the problem
of overlapping quantile estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hatalis_K/0/1/0/all/0/1&quot;&gt;Kostas Hatalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kishore_S/0/1/0/all/0/1&quot;&gt;Shalinee Kishore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scheinberg_K/0/1/0/all/0/1&quot;&gt;Katya Scheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lamadrid_A/0/1/0/all/0/1&quot;&gt;Alberto Lamadrid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10986">
<title>Improving accuracy of Winograd convolution for DNNs. (arXiv:1803.10986v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10986</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern deep neural networks (DNNs) spend a large amount of their execution
time computing convolutions. Winograd&apos;s minimal algorithm for small
convolutions can greatly reduce the number of arithmetic operations. However, a
large reduction in floating point (FP) operations in these algorithms can
result in significantly reduced FP accuracy of the result. In this paper we
propose several methods for reducing the FP error of these algorithms. Minimal
convolution algorithms depend on the selection of several numeric
\textit{points} that have a large impact on the accuracy of the result. Some
points are known to be better than others, but there is no systematic method
selecting points for small convolutions. We show that there are a relatively
small number of important cases for DNN convolution, that can be searched
empirically. We compared both standard and modified versions of the Winograd
algorithm. Further, we demonstrate that both the ordering and value of the
points is important, and we propose a canonical evaluation ordering that both
reduces FP error and the size of the search space based on Huffman coding. We
find that good point selections depend on the values of the points themselves
and on symmetries between different points. We show that sets of points with
symmetric groups give better results. In addition, we explore other methods to
reduce FP error, including mixed-precision convolution, and pairwise addition
across DNN channels. Using our methods we can significantly reduce FP error for
a given Winograd convolution block size, which allows larger block sizes and
reduced computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barabasz_B/0/1/0/all/0/1&quot;&gt;Barbara Barabasz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1&quot;&gt;Andrew Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1&quot;&gt;David Gregg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10996">
<title>Dihedral angle prediction using generative adversarial networks. (arXiv:1803.10996v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/1803.10996</link>
<description rdf:parseType="Literal">&lt;p&gt;Several dihedral angles prediction methods were developed for protein
structure prediction and their other applications. However, distribution of
predicted angles would not be similar to that of real angles. To address this
we employed generative adversarial networks (GAN). Generative adversarial
networks are composed of two adversarially trained networks: a discriminator
and a generator. A discriminator distinguishes samples from a dataset and
generated samples while a generator generates realistic samples. Although the
discriminator of GANs is trained to estimate density, GAN model is intractable.
On the other hand, noise-contrastive estimation (NCE) was introduced to
estimate a normalization constant of an unnormalized statistical model and thus
the density function. In this thesis, we introduce noise-contrastive estimation
generative adversarial networks (NCE-GAN) which enables explicit density
estimation of a GAN model. And a new loss for the generator is proposed. We
also propose residue-wise variants of auxiliary classifier GAN (AC-GAN) and
Semi-supervised GAN to handle sequence information in a window. In our
experiment, the conditional generative adversarial network (C-GAN), AC-GAN and
Semi-supervised GAN were compared. And experiments done with improved
conditions were invested. We identified a phenomenon of AC-GAN that
distribution of its predicted angles is composed of unusual clusters. The
distribution of the predicted angles of Semi-supervised GAN was most similar to
the Ramachandran plot. We found that adding the output of the NCE as an
additional input of the discriminator is helpful to stabilize the training of
the GANs and to capture the detailed structures. Adding regression loss and
using predicted angles by regression loss only model could improve the
conditional generation performance of the C-GAN and AC-GAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeongki Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11132">
<title>Notes on computational-to-statistical gaps: predictions using statistical physics. (arXiv:1803.11132v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.11132</link>
<description rdf:parseType="Literal">&lt;p&gt;In these notes we describe heuristics to predict computational-to-statistical
gaps in certain statistical problems. These are regimes in which the underlying
statistical problem is information-theoretically possible although no efficient
algorithm exists, rendering the problem essentially unsolvable for large
instances. The methods we describe here are based on mature, albeit
non-rigorous, tools from statistical physics.
&lt;/p&gt;
&lt;p&gt;These notes are based on a lecture series given by the authors at the Courant
Institute of Mathematical Sciences in New York City, on May 16th, 2017.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bandeira_A/0/1/0/all/0/1&quot;&gt;Afonso S. Bandeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perry_A/0/1/0/all/0/1&quot;&gt;Amelia Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wein_A/0/1/0/all/0/1&quot;&gt;Alexander S. Wein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11136">
<title>Conformal Prediction in Learning Under Privileged Information Paradigm with Applications in Drug Discovery. (arXiv:1803.11136v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.11136</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores conformal prediction in the learning under privileged
information (LUPI) paradigm. We use the SVM+ realization of LUPI in an
inductive conformal predictor, and apply it to the MNIST benchmark dataset and
three datasets in drug discovery. The results show that using privileged
information produces valid models and improves efficiency compared to standard
SVM, however the improvement varies between the tested datasets and is not
substantial in the drug discovery applications. More importantly, using SVM+ in
a conformal prediction framework enables valid prediction intervals at
specified significance levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gauraha_N/0/1/0/all/0/1&quot;&gt;Niharika Gauraha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carlsson_L/0/1/0/all/0/1&quot;&gt;Lars Carlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spjuth_O/0/1/0/all/0/1&quot;&gt;Ola Spjuth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11159">
<title>Stochastic Gradient Hamiltonian Monte Carlo with Variance Reduction for Bayesian Inference. (arXiv:1803.11159v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.11159</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient-based Monte Carlo sampling algorithms, like Langevin dynamics and
Hamiltonian Monte Carlo, are important methods for Bayesian inference. In
large-scale settings, full-gradients are not affordable and thus stochastic
gradients evaluated on mini-batches are used as a replacement. In order to
reduce the high variance of noisy stochastic gradients, [Dubey et al., 2016]
applied the standard variance reduction technique on stochastic gradient
Langevin dynamics and obtained both theoretical and experimental improvements.
In this paper, we apply the variance reduction tricks on Hamiltonian Monte
Carlo and achieve better theoretical convergence results compared with the
variance-reduced Langevin dynamics. Moreover, we apply the symmetric splitting
scheme in our variance-reduced Hamiltonian Monte Carlo algorithms to further
improve the theoretical results. The experimental results are also consistent
with the theoretical results. As our experiment shows, variance-reduced
Hamiltonian Monte Carlo demonstrates better performance than variance-reduced
Langevin dynamics in Bayesian regression and classification tasks on real-world
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhize Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00060">
<title>Achieving non-discrimination in prediction. (arXiv:1703.00060v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00060</link>
<description rdf:parseType="Literal">&lt;p&gt;Discrimination-aware classification is receiving an increasing attention in
data science fields. The pre-process methods for constructing a
discrimination-free classifier first remove discrimination from the training
data, and then learn the classifier from the cleaned data. However, they lack a
theoretical guarantee for the potential discrimination when the classifier is
deployed for prediction. In this paper, we fill this gap by mathematically
bounding the probability of the discrimination in prediction being within a
given interval in terms of the training data and classifier. We adopt the
causal model for modeling the data generation mechanism, and formally defining
discrimination in population, in a dataset, and in prediction. We obtain two
important theoretical results: (1) the discrimination in prediction can still
exist even if the discrimination in the training data is completely removed;
and (2) not all pre-process methods can ensure non-discrimination in prediction
even though they can achieve non-discrimination in the modified training data.
Based on the results, we develop a two-phase framework for constructing a
discrimination-free classifier with a theoretical guarantee. The experiments
demonstrate the theoretical results and show the effectiveness of our two-phase
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongkai Wu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt; (1) ((1) University of Arkansas)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10887">
<title>Efficient, sparse representation of manifold distance matrices for classical scaling. (arXiv:1705.10887v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10887</link>
<description rdf:parseType="Literal">&lt;p&gt;Geodesic distance matrices can reveal shape properties that are largely
invariant to non-rigid deformations, and thus are often used to analyze and
represent 3-D shapes. However, these matrices grow quadratically with the
number of points. Thus for large point sets it is common to use a low-rank
approximation to the distance matrix, which fits in memory and can be
efficiently analyzed using methods such as multidimensional scaling (MDS). In
this paper we present a novel sparse method for efficiently representing
geodesic distance matrices using biharmonic interpolation. This method exploits
knowledge of the data manifold to learn a sparse interpolation operator that
approximates distances using a subset of points. We show that our method is 2x
faster and uses 20x less memory than current leading methods for solving MDS on
large point sets, with similar quality. This enables analyses of large point
sets that were previously infeasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turek_J/0/1/0/all/0/1&quot;&gt;Javier S. Turek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huth_A/0/1/0/all/0/1&quot;&gt;Alexander Huth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03194">
<title>Sensitivity Analysis for Mirror-Stratifiable Convex Functions. (arXiv:1707.03194v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03194</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a set of sensitivity analysis and activity identification
results for a class of convex functions with a strong geometric structure, that
we coined &quot;mirror-stratifiable&quot;. These functions are such that there is a
bijection between a primal and a dual stratification of the space into
partitioning sets, called strata. This pairing is crucial to track the strata
that are identifiable by solutions of parametrized optimization problems or by
iterates of optimization algorithms. This class of functions encompasses all
regularizers routinely used in signal and image processing, machine learning,
and statistics. We show that this &quot;mirror-stratifiable&quot; structure enjoys a nice
sensitivity theory, allowing us to study stability of solutions of optimization
problems to small perturbations, as well as activity identification of
first-order proximal splitting-type algorithms. Existing results in the
literature typically assume that, under a non-degeneracy condition, the active
set associated to a minimizer is stable to small perturbations and is
identified in finite time by optimization schemes. In contrast, our results do
not require any non-degeneracy assumption: in consequence, the optimal active
set is not necessarily stable anymore, but we are able to track precisely the
set of identifiable strata.We show that these results have crucial implications
when solving challenging ill-posed inverse problems via regularization, a
typical scenario where the non-degeneracy condition is not fulfilled. Our
theoretical results, illustrated by numerical simulations, allow to
characterize the instability behaviour of the regularized solutions, by
locating the set of all low-dimensional strata that can be potentially
identified by these solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fadili_J/0/1/0/all/0/1&quot;&gt;Jalal Fadili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malick_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Malick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Peyre_G/0/1/0/all/0/1&quot;&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06034">
<title>Stochastic Variance Reduction for Policy Gradient Estimation. (arXiv:1710.06034v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06034</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in policy gradient methods and deep learning have
demonstrated their applicability for complex reinforcement learning problems.
However, the variance of the performance gradient estimates obtained from the
simulation is often excessive, leading to poor sample efficiency. In this
paper, we apply the stochastic variance reduced gradient descent (SVRG) to
model-free policy gradient to significantly improve the sample-efficiency. The
SVRG estimation is incorporated into a trust-region Newton conjugate gradient
framework for the policy optimization. On several Mujoco tasks, our method
achieves significantly better performance compared to the state-of-the-art
model-free policy gradient methods in robotic continuous control such as trust
region policy optimization (TRPO)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianbing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jian Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04425">
<title>Message Passing Stein Variational Gradient Descent. (arXiv:1711.04425v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04425</link>
<description rdf:parseType="Literal">&lt;p&gt;Stein variational gradient descent (SVGD) is a remarkable recent Bayesian
inference method, which has stronger approximating ability than traditional
variational inference methods, and is more effective than Monte Carlo methods
with the same particle size. However, we observed that SVGD still manifests
particle degeneracy as the dimension increases: particles tend to collapse on
local modes. We take an initial step towards understanding this phenomenon by
analyzing the repulsive force and find that there exists a negative correlation
between the repulsive force and the dimensionality which should be blamed for
this phenomenon. We also propose Message Passing SVGD (MP-SVGD) to solve this
problem. By leveraging the conditional independence structure of probabilistic
graphical models (PGMs), MP-SVGD converts the original high dimensional global
inference problem into a set of local ones over the Markov blanket with lower
dimensions. Experimental results show its advantages of exploring structural
information over SVGD and particle efficiency and approximation flexibility
over other inference methods on graphical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhuo_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06373">
<title>Thoracic Disease Identification and Localization with Limited Supervision. (arXiv:1711.06373v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06373</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate identification and localization of abnormalities from radiology
images play an integral part in clinical diagnosis and treatment planning.
Building a highly accurate prediction model for these tasks usually requires a
large number of images manually annotated with labels and finding sites of
abnormalities. In reality, however, such annotated data are expensive to
acquire, especially the ones with location annotations. We need methods that
can work well with only a small amount of location annotations. To address this
challenge, we present a unified approach that simultaneously performs disease
identification and localization through the same underlying model for all
images. We demonstrate that our approach can effectively leverage both class
information as well as limited location annotation, and significantly
outperforms the comparative reference baseline in both classification and
localization tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01587">
<title>SpectralNet: Spectral Clustering using Deep Neural Networks. (arXiv:1801.01587v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is a leading and popular technique in unsupervised data
analysis. Two of its major limitations are scalability and generalization of
the spectral embedding (i.e., out-of-sample-extension). In this paper we
introduce a deep learning approach to spectral clustering that overcomes the
above shortcomings. Our network, which we call SpectralNet, learns a map that
embeds input data points into the eigenspace of their associated graph
Laplacian matrix and subsequently clusters them. We train SpectralNet using a
procedure that involves constrained stochastic optimization. Stochastic
optimization allows it to scale to large datasets, while the constraints, which
are implemented using a special-purpose output layer, allow us to keep the
network output orthogonal. Moreover, the map learned by SpectralNet naturally
generalizes the spectral embedding to unseen data points. To further improve
the quality of the clustering, we replace the standard pairwise Gaussian
affinities with affinities leaned from unlabeled data using a Siamese network.
Additional improvement can be achieved by applying the network to code
representations produced, e.g., by standard autoencoders. Our end-to-end
learning procedure is fully unsupervised. In addition, we apply VC dimension
theory to derive a lower bound on the size of SpectralNet. State-of-the-art
clustering results are reported on the Reuters dataset. Our implementation is
publicly available at https://github.com/kstant0725/SpectralNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Henry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nadler_B/0/1/0/all/0/1&quot;&gt;Boaz Nadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basri_R/0/1/0/all/0/1&quot;&gt;Ronen Basri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09390">
<title>Nonlinear Dimensionality Reduction on Graphs. (arXiv:1801.09390v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09390</link>
<description rdf:parseType="Literal">&lt;p&gt;In this era of data deluge, many signal processing and machine learning tasks
are faced with high-dimensional datasets, including images, videos, as well as
time series generated from social, commercial and brain network interactions.
Their efficient processing calls for dimensionality reduction techniques
capable of properly compressing the data while preserving task-related
characteristics, going beyond pairwise data correlations. The present paper
puts forth a nonlinear dimensionality reduction framework that accounts for
data lying on known graphs. The novel framework encompasses most of the
existing dimensionality reduction methods, but it is also capable of capturing
and preserving possibly nonlinear correlations that are ignored by linear
methods. Furthermore, it can take into account information from multiple
graphs. The proposed algorithms were tested on synthetic as well as real
datasets to corroborate their effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanning Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traganitis_P/0/1/0/all/0/1&quot;&gt;Panagiotis A. Traganitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03848">
<title>Region Detection in Markov Random Fields: Gaussian Case. (arXiv:1802.03848v8 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03848</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of model selection in Gaussian Markov fields in the
sample deficient scenario. The benchmark information-theoretic results in the
case of d-regular graphs require the number of samples to be at least
proportional to the logarithm of the number of vertices to allow consistent
graph recovery. When the number of samples is less than this amount, reliable
detection of all edges is impossible. In many applications, it is more
important to learn the distribution of the edge (coupling) parameters over the
network than the specific locations of the edges. Assuming that the entire
graph can be partitioned into a number of spatial regions with similar edge
parameters and reasonably regular boundaries, we develop new
information-theoretic sample complexity bounds and show that a bounded number
of samples can be sufficient to consistently recover these regions. Finally, we
introduce and analyze an efficient region growing algorithm capable of
recovering the regions with high accuracy. We show that it is consistent and
demonstrate its performance benefits in synthetic simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soloveychik_I/0/1/0/all/0/1&quot;&gt;Ilya Soloveychik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04085">
<title>Efficient Empirical Risk Minimization with Smooth Loss Functions in Non-interactive Local Differential Privacy. (arXiv:1802.04085v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04085</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the Empirical Risk Minimization problem in the
non-interactive local model of differential privacy. We first show that if the
ERM loss function is $(\infty, T)$-smooth, then we can avoid a dependence of
the sample complexity, to achieve error $\alpha$, on the exponential of the
dimensionality $p$ with base $1/\alpha$ ({\em i.e.,} $\alpha^{-p}$), which
answers a question in \cite{smith2017interaction}. Our approach is based on
Bernstein polynomial approximation. Then, we propose player-efficient
algorithms with $1$-bit communication complexity and $O(1)$ computation cost
for each player. The error bound is asymptotically the same as the original
one. Also with additional assumptions we show a server efficient algorithm with
polynomial running time. At last, we propose (efficient) non-interactive
locally differential private algorithms, based on different types of polynomial
approximations, for learning the set of k-way marginal queries and the set of
smooth queries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1&quot;&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinhui Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08397">
<title>Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation. (arXiv:1802.08397v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08397</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank modeling plays a pivotal role in signal processing and machine
learning, with applications ranging from collaborative filtering, video
surveillance, medical imaging, to dimensionality reduction and adaptive
filtering. Many modern high-dimensional data and interactions thereof can be
modeled as lying approximately in a low-dimensional subspace or manifold,
possibly with additional structures, and its proper exploitations lead to
significant reduction of costs in sensing, computation and storage. In recent
years, there is a plethora of progress in understanding how to exploit low-rank
structures using computationally efficient procedures in a provable manner,
including both convex and nonconvex approaches. On one side, convex relaxations
such as nuclear norm minimization often lead to statistically optimal
procedures for estimating low-rank matrices, where first-order methods are
developed to address the computational challenges; on the other side, there is
emerging evidence that properly designed nonconvex procedures, such as
projected gradient descent, often provide globally optimal solutions with a
much lower computational cost in many problems. This survey article will
provide a unified overview of these recent advances on low-rank matrix
estimation from incomplete measurements. Attention is paid to rigorous
characterization of the performance of these algorithms, and to problems where
the low-rank matrix have additional structural properties that require new
algorithmic designs and theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yuejie Chi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09188">
<title>Analysis of Langevin Monte Carlo via convex optimization. (arXiv:1802.09188v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09188</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide new insights on the Unadjusted Langevin Algorithm.
We show that this method can be formulated as a first order optimization
algorithm of an objective functional defined on the Wasserstein space of order
$2$. Using this interpretation and techniques borrowed from convex
optimization, we give a non-asymptotic analysis of this method to sample from
logconcave smooth target distribution on $\mathbb{R}^d$. Based on this
interpretation, we propose two new methods for sampling from a non-smooth
target distribution, which we analyze as well. Besides, these new algorithms
are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD)
algorithm, which is a popular extension of the Unadjusted Langevin Algorithm.
Similar to SGLD, they only rely on approximations of the gradient of the target
log density and can be used for large-scale Bayesian inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1&quot;&gt;Alain Durmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Majewski_S/0/1/0/all/0/1&quot;&gt;Szymon Majewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miasojedow_B/0/1/0/all/0/1&quot;&gt;B&amp;#x142;a&amp;#x17c;ej Miasojedow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10264">
<title>Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods. (arXiv:1802.10264v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.10264</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore deep reinforcement learning algorithms for
vision-based robotic grasping. Model-free deep reinforcement learning (RL) has
been successfully applied to a range of challenging environments, but the
proliferation of algorithms makes it difficult to discern which particular
approach would be best suited for a rich, diverse task like grasping. To answer
this question, we propose a simulated benchmark for robotic grasping that
emphasizes off-policy learning and generalization to unseen objects. Off-policy
learning enables utilization of grasping data over a wide variety of objects,
and diversity is important to enable the method to generalize to new objects
that were not seen during training. We evaluate the benchmark tasks against a
variety of Q-function estimation methods, a method previously proposed for
robotic grasping with deep neural network models, and a novel approach based on
a combination of Monte Carlo return estimation and an off-policy correction.
Our results indicate that several simple methods provide a surprisingly strong
competitor to popular algorithms such as double Q-learning, and our analysis of
stability sheds light on the relative tradeoffs between the algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quillen_D/0/1/0/all/0/1&quot;&gt;Deirdre Quillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1&quot;&gt;Eric Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1&quot;&gt;Julian Ibarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07200">
<title>Training Recurrent Neural Networks as a Constraint Satisfaction Problem. (arXiv:1803.07200v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07200</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new approach for training artificial neural networks
using techniques for solving the constraint satisfaction problem (CSP). The
quotient gradient system (QGS) is a trajectory based method for solving the
CSP. This study converts the training set of a neural network into a CSP and
uses the QGS to find its solutions. The QGS finds the global minimum of the
optimization problem by tracking trajectories of a nonlinear dynamical system
and does not stop at a local minimum of the optimization problem. Lyapunov
theory is used to prove the asymptotic stability of the solutions with and
without the presence of measurement errors. Numerical examples illustrate the
effectiveness of the proposed methodology and compare it to a genetic algorithm
and error backpropagation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodabandehlou_H/0/1/0/all/0/1&quot;&gt;Hamid Khodabandehlou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadali_M/0/1/0/all/0/1&quot;&gt;Mohammad Sami Fadali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08161">
<title>Entropy-based closure for probabilistic learning on manifolds. (arXiv:1803.08161v2 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08161</link>
<description rdf:parseType="Literal">&lt;p&gt;In a recent paper, the authors proposed a general methodology for
probabilistic learning on manifolds. The method was used to generate numerical
samples that are statistically consistent with an existing dataset construed as
a realization from a non-Gaussian random vector. The manifold structure is
learned using diffusion manifolds and the statistical sample generation is
accomplished using a projected Ito stochastic differential equation. This
probabilistic learning approach has been extended to polynomial chaos
representation of databases on manifolds and to probabilistic nonconvex
constrained optimization with a fixed budget of function evaluations. The
methodology introduces an isotropic-diffusion kernel with hyperparameter
{\epsilon}. Currently, {\epsilon} is more or less arbitrarily chosen. In this
paper, we propose a selection criterion for identifying an optimal value of
{\epsilon}, based on a maximum entropy argument. The result is a comprehensive,
closed, probabilistic model for characterizing data sets with hidden
constraints. This entropy argument ensures that out of all possible models,
this is the one that is the most uncertain beyond any specified constraints,
which is selected. Applications are presented for several databases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Soizea_C/0/1/0/all/0/1&quot;&gt;C. Soizea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ghanem_R/0/1/0/all/0/1&quot;&gt;R. Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Safta_C/0/1/0/all/0/1&quot;&gt;C. Safta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huan_X/0/1/0/all/0/1&quot;&gt;X. Huan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vane_Z/0/1/0/all/0/1&quot;&gt;Z. P. Vane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Oefelein_J/0/1/0/all/0/1&quot;&gt;J. Oefelein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lacaz_G/0/1/0/all/0/1&quot;&gt;G. Lacaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Najm_H/0/1/0/all/0/1&quot;&gt;H. N. Najm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Q. Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;X. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09974">
<title>Inferring network connectivity from event timing patterns. (arXiv:1803.09974v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09974</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing network connectivity from the collective dynamics of a system
typically requires access to its complete continuous-time evolution although
these are often experimentally inaccessible. Here we propose a theory for
revealing physical connectivity of networked systems only from the event time
series their intrinsic collective dynamics generate. Representing the patterns
of event timings in an event space spanned by inter-event and cross-event
intervals, we reveal which other units directly influence the inter-event times
of any given unit. For illustration, we linearize an event space mapping
constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether
the coupling acts in an inhibiting or activating (excitatory) manner. The
proposed model-independent reconstruction theory is scalable to larger networks
and may thus play an important role in the reconstruction of networks from
biology to social science and engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Casadiego_J/0/1/0/all/0/1&quot;&gt;Jose Casadiego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maoutsa_D/0/1/0/all/0/1&quot;&gt;Dimitra Maoutsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Timme_M/0/1/0/all/0/1&quot;&gt;Marc Timme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10161">
<title>Stein Points. (arXiv:1803.10161v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10161</link>
<description rdf:parseType="Literal">&lt;p&gt;An important task in computational statistics and machine learning is to
approximate a posterior distribution $p(x)$ with an empirical measure supported
on a set of representative points $\{x_i\}_{i=1}^n$. This paper focuses on
methods where the selection of points is essentially deterministic, with an
emphasis on achieving accurate approximation when $n$ is small. To this end, we
present `Stein Points&apos;. The idea is to exploit either a greedy or a conditional
gradient method to iteratively minimise a kernel Stein discrepancy between the
empirical measure and $p(x)$. Our empirical results demonstrate that Stein
Points enable accurate approximation of the posterior at modest computational
cost. In addition, theoretical results are provided to establish convergence of
the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wilson Ye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorham_J/0/1/0/all/0/1&quot;&gt;Jackson Gorham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1&quot;&gt;Chris J. Oates&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10535">
<title>Estimating causal effects of time-dependent exposures on a binary endpoint in a high-dimensional setting. (arXiv:1803.10535v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10535</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the intervention calculus when the DAG is absent (IDA) method was
developed to estimate lower bounds of causal effects from observational
high-dimensional data. Originally it was introduced to assess the effect of
baseline biomarkers which do not vary over time. However, in many clinical
settings, measurements of biomarkers are repeated at fixed time points during
treatment exposure and, therefore, this method need to be extended. The purpose
of this paper is then to extend the first step of the IDA, the Peter Clarks
(PC)-algorithm, to a time-dependent exposure in the context of a binary
outcome. We generalised the PC-algorithm for taking into account the
chronological order of repeated measurements of the exposure and propose to
apply the IDA with our new version, the chronologically ordered PC-algorithm
(COPC-algorithm). A simulation study has been performed before applying the
method for estimating causal effects of time-dependent immunological biomarkers
on toxicity, death and progression in patients with metastatic melanoma. The
simulation study showed that the completed partially directed acyclic graphs
(CPDAGs) obtained using COPC-algorithm were structurally closer to the true
CPDAG than CPDAGs obtained using PC-algorithm. Also, causal effects were more
accurate when they were estimated based on CPDAGs obtained using
COPC-algorithm. Moreover, CPDAGs obtained by COPC-algorithm allowed removing
non-chronologic arrows with a variable measured at a time t pointing to a
variable measured at a time t&apos; where t&apos;&amp;lt; t. Bidirected edges were less present
in CPDAGs obtained with the COPC-algorithm, supporting the fact that there was
less variability in causal effects estimated from these CPDAGs. The
COPC-algorithm provided CPDAGs that keep the chronological structure present in
the data, thus allowed to estimate lower bounds of the causal effect of
time-dependent biomarkers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Asvatourian_V/0/1/0/all/0/1&quot;&gt;Vah&amp;#xe9; Asvatourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coutzac_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;lia Coutzac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chaput_N/0/1/0/all/0/1&quot;&gt;Nathalie Chaput&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robert_C/0/1/0/all/0/1&quot;&gt;Caroline Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michiels_S/0/1/0/all/0/1&quot;&gt;Stefan Michiels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lanoy_E/0/1/0/all/0/1&quot;&gt;Emilie Lanoy&lt;/a&gt;</dc:creator>
</item></rdf:RDF>