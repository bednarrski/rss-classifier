<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05850"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05770"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05908"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06414"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05671"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05705"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05726"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05731"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05832"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05854"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05902"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05906"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1407.0208"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1507.03538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.09350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05935"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.05850">
<title>Towards a Theory-Guided Benchmarking Suite for Discrete Black-Box Optimization Heuristics: Profiling $(1+\lambda)$ EA Variants on OneMax and LeadingOnes. (arXiv:1808.05850v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.05850</link>
<description rdf:parseType="Literal">&lt;p&gt;Theoretical and empirical research on evolutionary computation methods
complement each other by providing two fundamentally different approaches
towards a better understanding of black-box optimization heuristics. In
discrete optimization, both streams developed rather independently of each
other, but we observe today an increasing interest in reconciling these two
sub-branches. In continuous optimization, the COCO (COmparing Continuous
Optimisers) benchmarking suite has established itself as an important platform
that theoreticians and practitioners use to exchange research ideas and
questions. No widely accepted equivalent exists in the research domain of
discrete black-box optimization.
&lt;/p&gt;
&lt;p&gt;Marking an important step towards filling this gap, we adjust the COCO
software to pseudo-Boolean optimization problems, and obtain from this a
benchmarking environment that allows a fine-grained empirical analysis of
discrete black-box heuristics. In this documentation we demonstrate how this
test bed can be used to profile the performance of evolutionary algorithms.
More concretely, we study the optimization behavior of several $(1+\lambda)$ EA
variants on the two benchmark problems OneMax and LeadingOnes. This comparison
motivates a refined analysis for the optimization time of the $(1+\lambda)$ EA
on LeadingOnes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1&quot;&gt;Carola Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1&quot;&gt;Furong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijn_S/0/1/0/all/0/1&quot;&gt;Sander van Rijn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1&quot;&gt;Thomas B&amp;#xe4;ck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05770">
<title>Reinforcement Learning for Autonomous Defence in Software-Defined Networking. (arXiv:1808.05770v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1808.05770</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the successful application of machine learning (ML) in a wide range
of domains, adaptability---the very property that makes machine learning
desirable---can be exploited by adversaries to contaminate training and evade
classification. In this paper, we investigate the feasibility of applying a
specific class of machine learning algorithms, namely, reinforcement learning
(RL) algorithms, for autonomous cyber defence in software-defined networking
(SDN). In particular, we focus on how an RL agent reacts towards different
forms of causative attacks that poison its training process, including
indiscriminate and targeted, white-box and black-box attacks. In addition, we
also study the impact of the attack timing, and explore potential
countermeasures such as adversarial training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1&quot;&gt;Benjamin I.P. Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abraham_T/0/1/0/all/0/1&quot;&gt;Tamas Abraham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alpcan_T/0/1/0/all/0/1&quot;&gt;Tansu Alpcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vel_O/0/1/0/all/0/1&quot;&gt;Olivier De Vel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubczenko_D/0/1/0/all/0/1&quot;&gt;David Hubczenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leckie_C/0/1/0/all/0/1&quot;&gt;Christopher Leckie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montague_P/0/1/0/all/0/1&quot;&gt;Paul Montague&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05908">
<title>Improved Language Modeling by Decoding the Past. (arXiv:1808.05908v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.05908</link>
<description rdf:parseType="Literal">&lt;p&gt;Highly regularized LSTMs that model the auto-regressive conditional
factorization of the joint probability distribution of words achieve
state-of-the-art results in language modeling. These models have an implicit
bias towards predicting the next word from a given context. We propose a new
regularization term based on decoding words in the context from the predicted
distribution of the next word. With relatively few additional parameters, our
model achieves absolute improvements of 1.7\% and 2.3\% over the current
state-of-the-art results on the Penn Treebank and WikiText-2 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1&quot;&gt;Siddhartha Brahma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08924">
<title>Learning-Based Mean-Payoff Optimization in an Unknown MDP under Omega-Regular Constraints. (arXiv:1804.08924v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08924</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the problem of maximizing the mean-payoff value with high
probability while satisfying a parity objective in a Markov decision process
(MDP) with unknown probabilistic transition function and unknown reward
function. Assuming the support of the unknown transition function and a lower
bound on the minimal transition probability are known in advance, we show that
in MDPs consisting of a single end component, two combinations of guarantees on
the parity and mean-payoff objectives can be achieved depending on how much
memory one is willing to use. (i) For all $\epsilon$ and $\gamma$ we can
construct an online-learning finite-memory strategy that almost-surely
satisfies the parity objective and which achieves an $\epsilon$-optimal mean
payoff with probability at least $1 - \gamma$. (ii) Alternatively, for all
$\epsilon$ and $\gamma$ there exists an online-learning infinite-memory
strategy that satisfies the parity objective surely and which achieves an
$\epsilon$-optimal mean payoff with probability at least $1 - \gamma$. We
extend the above results to MDPs consisting of more than one end component in a
natural way. Finally, we show that the aforementioned guarantees are tight,
i.e. there are MDPs for which stronger combinations of the guarantees cannot be
ensured.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretinsky_J/0/1/0/all/0/1&quot;&gt;Jan K&amp;#x159;et&amp;#xed;nsk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Raskin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06414">
<title>Combining a Context Aware Neural Network with a Denoising Autoencoder for Measuring String Similarities. (arXiv:1807.06414v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.06414</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring similarities between strings is central for many established and
fast growing research areas including information retrieval, biology, and
natural language processing. The traditional approach for string similarity
measurements is to define a metric over a word space that quantifies and sums
up the differences between characters in two strings. The state-of-the-art in
the area has, surprisingly, not evolved much during the last few decades. The
majority of the metrics are based on a simple comparison between character and
character distributions without consideration for the context of the words.
This paper proposes a string metric that encompasses similarities between
strings based on (1) the character similarities between the words including.
Non-Standard and standard spellings of the same words, and (2) the context of
the words. Our proposal is a neural network composed of a denoising autoencoder
and what we call a context encoder specifically designed to find similarities
between the words based on their context. The experimental results show that
the resulting metrics succeeds in 85.4\% of the cases in finding the correct
version of a non-standard spelling among the closest words, compared to 63.2\%
with the established Normalised-Levenshtein distance. Besides, we show that
words used in similar context are with our approach calculated to be similar
than words with different contexts, which is a desirable property missing in
established string metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazreg_M/0/1/0/all/0/1&quot;&gt;Mehdi Ben Lazreg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05671">
<title>On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization. (arXiv:1808.05671v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05671</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive gradient methods are workhorses in deep learning. However, the
convergence guarantees of adaptive gradient methods for nonconvex optimization
have not been sufficiently studied. In this paper, we provide a sharp analysis
of a recently proposed adaptive gradient method namely partially adaptive
momentum estimation method (Padam) (Chen and Gu, 2018), which admits many
existing adaptive gradient methods such as AdaGrad, RMSProp and AMSGrad as
special cases. Our analysis shows that, for smooth nonconvex functions, Padam
converges to a first-order stationary point at the rate of
$O\big((\sum_{i=1}^d\|\mathbf{g}_{1:T,i}\|_2)^{1/2}/T^{3/4} + d/T\big)$, where
$T$ is the number of iterations, $d$ is the dimension,
$\mathbf{g}_1,\ldots,\mathbf{g}_T$ are the stochastic gradients, and
$\mathbf{g}_{1:T,i} = [g_{1,i},g_{2,i},\ldots,g_{T,i}]^\top$. Our theoretical
result also suggests that in order to achieve faster convergence rate, it is
necessary to use Padam instead of AMSGrad. This is well-aligned with the
empirical results of deep learning reported in Chen and Gu (2018).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Quanquan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05689">
<title>Graph Edit Distance Computation via Graph Neural Networks. (arXiv:1808.05689v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05689</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph similarity search is among the most important graph-based applications,
e.g. finding the chemical compounds that are most similar to a query compound.
Graph similarity/distance computation, such as Graph Edit Distance (GED) and
Maximum Common Subgraph (MCS), is the core operation of graph similarity search
and many other applications, which is usually very costly to compute. Inspired
by the recent success of neural network approaches to several graph
applications, such as node classification and graph classification, we propose
a novel neural network-based approach to address this challenging while
classical graph problem, with the hope to alleviate the computational burden
while preserving a good performance. Our model generalizes to unseen graphs,
and in the worst case runs in linear time with respect to the number of nodes
in two graphs. Taking GED computation as an example, experimental results on
three real graph datasets demonstrate the effectiveness and efficiency of our
approach. Specifically, our model achieves smaller error and great time
reduction compared against several approximate algorithms on GED computation.
To the best of our knowledge, we are among the first to adopt neural networks
to model the similarity between two graphs, and provide a new direction for
future research on graph similarity computation and graph similarity search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Song Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05705">
<title>Mitigation of Adversarial Attacks through Embedded Feature Selection. (arXiv:1808.05705v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1808.05705</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning has become one of the main components for task automation in
many application domains. Despite the advancements and impressive achievements
of machine learning, it has been shown that learning algorithms can be
compromised by attackers both at training and test time. Machine learning
systems are especially vulnerable to adversarial examples where small
perturbations added to the original data points can produce incorrect or
unexpected outputs in the learning algorithms at test time. Mitigation of these
attacks is hard as adversarial examples are difficult to detect. Existing
related work states that the security of machine learning systems against
adversarial examples can be weakened when feature selection is applied to
reduce the systems&apos; complexity. In this paper, we empirically disprove this
idea, showing that the relative distortion that the attacker has to introduce
to succeed in the attack is greater when the target is using a reduced set of
features. We also show that the minimal adversarial examples differ
statistically more strongly from genuine examples with a lower number of
features. However, reducing the feature count can negatively impact the
system&apos;s performance. We illustrate the trade-off between security and accuracy
with specific examples. We propose a design methodology to evaluate the
security of machine learning classifiers with embedded feature selection
against adversarial examples crafted using different attack strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Ziyi Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munoz_Gonzalez_L/0/1/0/all/0/1&quot;&gt;Luis Mu&amp;#xf1;oz-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1&quot;&gt;Emil C. Lupu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05726">
<title>An N Time-Slice Dynamic Chain Event Graph. (arXiv:1808.05726v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05726</link>
<description rdf:parseType="Literal">&lt;p&gt;The Dynamic Chain Event Graph (DCEG) is able to depict many classes of
discrete random processes exhibiting asymmetries in their developments and
context-specific conditional probabilities structures. However, paradoxically,
this very generality has so far frustrated its wide application. So in this
paper we develop an object-oriented method to fully analyse a particularly
useful and feasibly implementable new subclass of these graphical models called
the N Time-Slice DCEG (NT-DCEG). After demonstrating a close relationship
between an NT-DCEG and a specific class of Markov processes, we discuss how
graphical modellers can exploit this connection to gain a deep understanding of
their processes. We also show how to read from the topology of this graph
context-specific independence statements that can then be checked by domain
experts. Our methods are illustrated throughout using examples of dynamic
multivariate processes describing inmate radicalisation in a prison.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Collazo_R/0/1/0/all/0/1&quot;&gt;Rodrigo A. Collazo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Jim Q. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05731">
<title>Efficiently Learning Mixtures of Mallows Models. (arXiv:1808.05731v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1808.05731</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixtures of Mallows models are a popular generative model for ranking data
coming from a heterogeneous population. They have a variety of applications
including social choice, recommendation systems and natural language
processing. Here we give the first polynomial time algorithm for provably
learning the parameters of a mixture of Mallows models with any constant number
of components. Prior to our work, only the two component case had been settled.
Our analysis revolves around a determinantal identity of Zagier which was
proven in the context of mathematical physics, which we use to show polynomial
identifiability and ultimately to construct test functions to peel off one
component at a time.
&lt;/p&gt;
&lt;p&gt;To complement our upper bounds, we show information-theoretic lower bounds on
the sample complexity as well as lower bounds against restricted families of
algorithms that make only local queries. Together, these results demonstrate
various impediments to improving the dependence on the number of components.
They also motivate the study of learning mixtures of Mallows models from the
perspective of beyond worst-case analysis. In this direction, we show that when
the scaling parameters of the Mallows models have separation, there are much
faster learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Allen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1&quot;&gt;Ankur Moitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05819">
<title>Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks. (arXiv:1808.05819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05819</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent algorithmic improvements and hardware breakthroughs resulted in a
number of success stories in the field of AI impacting our daily lives.
However, despite its ubiquity AI is only just starting to make advances in what
may arguably have the largest impact thus far, the nascent field of autonomous
driving. In this work we discuss this important topic and address one of
crucial aspects of the emerging area, the problem of predicting future state of
autonomous vehicle&apos;s surrounding necessary for safe and efficient operations.
We introduce a deep learning-based approach that takes into account current
state of traffic actors and produces rasterized representations of each actor&apos;s
vicinity. The raster images are then used by deep convolutional models to infer
future movement of actors while accounting for inherent uncertainty of the
prediction task. Extensive experiments on real-world data strongly suggest
benefits of the proposed approach. Moreover, following successful tests the
system was deployed to a fleet of autonomous vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djuric_N/0/1/0/all/0/1&quot;&gt;Nemanja Djuric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radosavljevic_V/0/1/0/all/0/1&quot;&gt;Vladan Radosavljevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Henggang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_F/0/1/0/all/0/1&quot;&gt;Fang-Chieh Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tsung-Han Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05832">
<title>Importance mixing: Improving sample reuse in evolutionary policy search methods. (arXiv:1808.05832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05832</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neuroevolution, that is evolutionary policy search methods based on deep
neural networks, have recently emerged as a competitor to deep reinforcement
learning algorithms due to their better parallelization capabilities. However,
these methods still suffer from a far worse sample efficiency. In this paper we
investigate whether a mechanism known as &quot;importance mixing&quot; can significantly
improve their sample efficiency. We provide a didactic presentation of
importance mixing and we explain how it can be extended to reuse more samples.
Then, from an empirical comparison based on a simple benchmark, we show that,
though it actually provides better sample efficiency, it is still far from the
sample efficiency of deep reinforcement learning, though it is more stable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourchot_A/0/1/0/all/0/1&quot;&gt;Alo&amp;#xef;s Pourchot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perrin_N/0/1/0/all/0/1&quot;&gt;Nicolas Perrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1&quot;&gt;Olivier Sigaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05854">
<title>Robust Compressive Phase Retrieval via Deep Generative Priors. (arXiv:1808.05854v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05854</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new framework to regularize the highly ill-posed and
non-linear phase retrieval problem through deep generative priors using simple
gradient descent algorithm. We experimentally show effectiveness of proposed
algorithm for random Gaussian measurements (practically relevant in imaging
through scattering media) and Fourier friendly measurements (relevant in
optical set ups). We demonstrate that proposed approach achieves impressive
results when compared with traditional hand engineered priors including
sparsity and denoising frameworks for number of measurements and robustness
against noise. Finally, we show the effectiveness of the proposed approach on a
real transmission matrix dataset in an actual application of multiple
scattering media imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamshad_F/0/1/0/all/0/1&quot;&gt;Fahad Shamshad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Ali Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05857">
<title>ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant Information. (arXiv:1808.05857v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1808.05857</link>
<description rdf:parseType="Literal">&lt;p&gt;Requirements elicitation requires extensive knowledge and deep understanding
of the problem domain where the final system will be situated. However, in many
software development projects, analysts are required to elicit the requirements
from an unfamiliar domain, which often causes communication barriers between
analysts and stakeholders. In this paper, we propose a requirements ELICitation
Aid tool (ELICA) to help analysts better understand the target application
domain by dynamic extraction and labeling of requirements-relevant knowledge.
To extract the relevant terms, we leverage the flexibility and power of
Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural
language processing tasks. In addition to the information conveyed through
text, ELICA captures and processes non-linguistic information about the
intention of speakers such as their confidence level, analytical tone, and
emotions. The extracted information is made available to the analysts as a set
of labeled snippets with highlighted relevant terms which can also be exported
as an artifact of the Requirements Engineering (RE) process. The application
and usefulness of ELICA are demonstrated through a case study. This study shows
how pre-existing relevant information about the application domain and the
information captured during an elicitation meeting, such as the conversation
and stakeholders&apos; intentions, can be captured and used to support analysts
achieving their tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abad_Z/0/1/0/all/0/1&quot;&gt;Zahra Shakeri Hossein Abad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gervasi_V/0/1/0/all/0/1&quot;&gt;Vincenzo Gervasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zowghi_D/0/1/0/all/0/1&quot;&gt;Didar Zowghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barker_K/0/1/0/all/0/1&quot;&gt;Ken Barker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05889">
<title>Data Consistency Approach to Model Validation. (arXiv:1808.05889v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1808.05889</link>
<description rdf:parseType="Literal">&lt;p&gt;In scientific inference problems, the underlying statistical modeling
assumptions have a crucial impact on the end results. There exist, however,
only a few automatic means for validating these fundamental modelling
assumptions. The contribution in this paper is a general criterion to evaluate
the consistency of a set of statistical models with respect to observed data.
This is achieved by automatically gauging the models&apos; ability to generate data
that is similar to the observed data. Importantly, the criterion follows from
the model class itself and is therefore directly applicable to a broad range of
inference problems with varying data types. The proposed data consistency
criterion is illustrated and evaluated using three synthetic and two real data
sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Svensson_A/0/1/0/all/0/1&quot;&gt;Andreas Svensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zachariah_D/0/1/0/all/0/1&quot;&gt;Dave Zachariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stoica_P/0/1/0/all/0/1&quot;&gt;Petre Stoica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05902">
<title>Learning Supervised Topic Models for Classification and Regression from Crowds. (arXiv:1808.05902v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05902</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing need to analyze large collections of documents has led to great
developments in topic modeling. Since documents are frequently associated with
other related variables, such as labels or ratings, much interest has been
placed on supervised topic models. However, the nature of most annotation
tasks, prone to ambiguity and noise, often with high volumes of documents, deem
learning under a single-annotator assumption unrealistic or unpractical for
most real-world applications. In this article, we propose two supervised topic
models, one for classification and another for regression problems, which
account for the heterogeneity and biases among different annotators that are
encountered in practice when learning from crowds. We develop an efficient
stochastic variational inference algorithm that is able to scale to very large
datasets, and we empirically demonstrate the advantages of the proposed model
over state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodrigues_F/0/1/0/all/0/1&quot;&gt;Filipe Rodrigues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lourenco_M/0/1/0/all/0/1&quot;&gt;Mariana Louren&amp;#xe7;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ribeiro_B/0/1/0/all/0/1&quot;&gt;Bernardete Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pereira_F/0/1/0/all/0/1&quot;&gt;Francisco Pereira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05904">
<title>Correlated Multi-armed Bandits with a Latent Random Source. (arXiv:1808.05904v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05904</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a novel multi-armed bandit framework where the rewards obtained
by pulling the arms are functions of a common latent random variable. The
correlation between arms due to the common random source can be used to design
a generalized upper-confidence-bound (UCB) algorithm that identifies certain
arms as $non-competitive$, and avoids exploring them. As a result, we reduce a
$K$-armed bandit problem to a $C+1$-armed problem, where $C+1$ includes the
best arm and $C$ $competitive$ arms. Our regret analysis shows that the
competitive arms need to be pulled $\mathcal{O}(\log T)$ times, while the
non-competitive arms are pulled only $\mathcal{O}(1)$ times. As a result, there
are regimes where our algorithm achieves a $\mathcal{O}(1)$ regret as opposed
to the typical logarithmic regret scaling of multi-armed bandit algorithms. We
also evaluate lower bounds on the expected regret and prove that our
correlated-UCB algorithm is order-wise optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Samarth Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yagan_O/0/1/0/all/0/1&quot;&gt;Osman Ya&amp;#x11f;an&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05906">
<title>Story Disambiguation: Tracking Evolving News Stories across News and Social Streams. (arXiv:1808.05906v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.05906</link>
<description rdf:parseType="Literal">&lt;p&gt;Following a particular news story online is an important but difficult task,
as the relevant information is often scattered across different domains/sources
(e.g., news articles, blogs, comments, tweets), presented in various formats
and language styles, and may overlap with thousands of other stories. In this
work we join the areas of topic tracking and entity disambiguation, and propose
a framework named Story Disambiguation - a cross-domain story tracking approach
that builds on real-time entity disambiguation and a learning-to-rank framework
to represent and update the rich semantic structure of news stories. Given a
target news story, specified by a seed set of documents, the goal is to
effectively select new story-relevant documents from an incoming document
stream. We represent stories as entity graphs and we model the story tracking
problem as a learning-to-rank task. This enables us to track content with high
accuracy, from multiple domains, in real-time. We study a range of text, entity
and graph based features to understand which type of features are most
effective for representing stories. We further propose new semi-supervised
learning techniques to automatically update the story representation over time.
Our empirical study shows that we outperform the accuracy of state-of-the-art
methods for tracking mixed-domain document streams, while requiring fewer
labeled data to seed the tracked stories. This is particularly the case for
local news stories that are easily over shadowed by other trending stories, and
for complex news stories with ambiguous content in noisy stream environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bichen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thanh-Binh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurley_N/0/1/0/all/0/1&quot;&gt;Neil Hurley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ifrim_G/0/1/0/all/0/1&quot;&gt;Georgiana Ifrim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05917">
<title>A bagging and importance sampling approach to Support Vector Machines. (arXiv:1808.05917v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05917</link>
<description rdf:parseType="Literal">&lt;p&gt;An importance sampling and bagging approach to solving the support vector
machine (SVM) problem in the context of large databases is presented and
evaluated. Our algorithm builds on the nearest neighbors ideas presented in
Camelo at al. (2015). As in that reference, the goal of the present proposal is
to achieve a faster solution of the SVM problem without a significance loss in
the prediction error. The performance of the methodology is evaluated in
benchmark examples and theoretical aspects of subsample methods are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barcenas_R/0/1/0/all/0/1&quot;&gt;R. B&amp;#xe1;rcenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gonzalez__Lima_M/0/1/0/all/0/1&quot;&gt;M. D. G&amp;#xf3;nzalez--Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Quiroz_A/0/1/0/all/0/1&quot;&gt;A. J. Quiroz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05924">
<title>Randomized Least Squares Regression: Combining Model- and Algorithm-Induced Uncertainties. (arXiv:1808.05924v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05924</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the uncertainties in the minimum norm solution of full-rank
regression problems, arising from Gaussian linear models, computed by
randomized (row-wise sampling and, more generally, sketching) algorithms. From
a deterministic perspective, our structural perturbation bounds imply that
least squares problems are less sensitive to multiplicative perturbations than
to additive perturbations. From a probabilistic perspective, our expressions
for the total expectation and variance with regard to both model- and
algorithm-induced uncertainties, are exact, hold for general sketching
matrices, and make no assumptions on the rank of the sketched matrix. The
relative differences between the total bias and variance on the one hand, and
the model bias and variance on the other hand, are governed by two factors: (i)
the expected rank deficiency of the sketched matrix, and (ii) the expected
difference between projectors associated with the original and the sketched
problems. A simple example, based on uniform sampling with replacement,
illustrates the statistical quantities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_J/0/1/0/all/0/1&quot;&gt;Jocelyn T. Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ipsen_I/0/1/0/all/0/1&quot;&gt;Ilse C. F. Ipsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1407.0208">
<title>A Bayes consistent 1-NN classifier. (arXiv:1407.0208v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1407.0208</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that a simple modification of the 1-nearest neighbor classifier
yields a strongly Bayes consistent learner. Prior to this work, the only
strongly Bayes consistent proximity-based method was the k-nearest neighbor
classifier, for k growing appropriately with sample size. We will argue that a
margin-regularized 1-NN enjoys considerable statistical and algorithmic
advantages over the k-NN classifier. These include user-friendly finite-sample
error bounds, as well as time- and memory-efficient learning and test-point
evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging
empirical results are reported.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontorovich_A/0/1/0/all/0/1&quot;&gt;Aryeh Kontorovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1&quot;&gt;Roi Weiss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1507.03538">
<title>Classifying X-ray Binaries: A Probabilistic Approach. (arXiv:1507.03538v3 [astro-ph.HE] UPDATED)</title>
<link>http://arxiv.org/abs/1507.03538</link>
<description rdf:parseType="Literal">&lt;p&gt;In X-ray binary star systems consisting of a compact object that accretes
material from an orbiting secondary star, there is no straightforward means to
decide if the compact object is a black hole or a neutron star. To assist this
classification, we develop a Bayesian statistical model that makes use of the
fact that X-ray binary systems appear to cluster based on their compact object
type when viewed from a 3-dimensional coordinate system derived from X-ray
spectral data. The first coordinate of this data is the ratio of counts in mid
to low energy band (color 1), the second coordinate is the ratio of counts in
high to low energy band (color 2), and the third coordinate is the sum of
counts in all three bands. We use this model to estimate the probabilities that
an X-ray binary system contains a black hole, non-pulsing neutron star, or
pulsing neutron star. In particular, we utilize a latent variable model in
which the latent variables follow a Gaussian process prior distribution, and
hence we are able to induce the spatial correlation we believe exists between
systems of the same type. The utility of this approach is evidenced by the
accurate prediction of system types using Rossi X-ray Timing Explorer All Sky
Monitor data, but it is not flawless. In particular, non-pulsing neutron
systems containing &quot;bursters&quot; that are close to the boundary demarcating
systems containing black holes tend to be classified as black hole systems. As
a byproduct of our analyses, we provide the astronomer with public R code that
can be used to predict the compact object type of X-ray binaries given training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gopalan_G/0/1/0/all/0/1&quot;&gt;Giri Gopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Vrtilek_S/0/1/0/all/0/1&quot;&gt;Saeqa Dil Vrtilek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bornn_L/0/1/0/all/0/1&quot;&gt;Luke Bornn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.09350">
<title>Centrality measures for graphons: Accounting for uncertainty in networks. (arXiv:1707.09350v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.09350</link>
<description rdf:parseType="Literal">&lt;p&gt;As relational datasets modeled as graphs keep increasing in size and their
data-acquisition is permeated by uncertainty, graph-based analysis techniques
can become computationally and conceptually challenging. In particular, node
centrality measures rely on the assumption that the graph is perfectly known --
a premise not necessarily fulfilled for large, uncertain networks. Accordingly,
centrality measures may fail to faithfully extract the importance of nodes in
the presence of uncertainty. To mitigate these problems, we suggest a
statistical approach based on graphon theory: we introduce formal definitions
of centrality measures for graphons and establish their connections to
classical graph centrality measures. A key advantage of this approach is that
centrality measures defined at the modeling level of graphons are inherently
robust to stochastic variations of specific graph realizations. Using the
theory of linear integral operators, we define degree, eigenvector, Katz and
PageRank centrality functions for graphons and establish concentration
inequalities demonstrating that graphon centrality functions arise naturally as
limits of their counterparts defined on sequences of graphs of increasing size.
The same concentration inequalities also provide high-probability bounds
between the graphon centrality functions and the centrality measures on any
sampled graph, thereby establishing a measure of uncertainty of the measured
centrality score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avella_Medina_M/0/1/0/all/0/1&quot;&gt;Marco Avella-Medina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parise_F/0/1/0/all/0/1&quot;&gt;Francesca Parise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1&quot;&gt;Michael T. Schaub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1&quot;&gt;Santiago Segarra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07562">
<title>Efficient inference in stochastic block models with vertex labels. (arXiv:1806.07562v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07562</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the stochastic block model with two communities where vertices
contain side information in the form of a vertex label. These vertex labels may
have arbitrary label distributions, depending on the community memberships. We
analyze a linearized version of the popular belief propagation algorithm. We
show that this algorithm achieves the highest accuracy possible whenever a
certain function of the network parameters has a unique fixed point. Whenever
this function has multiple fixed points, the belief propagation algorithm may
not perform optimally. We show that increasing the information in the vertex
labels may reduce the number of fixed points and hence lead to optimality of
belief propagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stegehuis_C/0/1/0/all/0/1&quot;&gt;Clara Stegehuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1&quot;&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07755">
<title>An empirical study on evaluation metrics of generative adversarial networks. (arXiv:1806.07755v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07755</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating generative adversarial networks (GANs) is inherently challenging.
In this paper, we revisit several representative sample-based evaluation
metrics for GANs, and address the problem of how to evaluate the evaluation
metrics. We start with a few necessary conditions for metrics to produce
meaningful scores, such as distinguishing real from generated samples,
identifying mode dropping and mode collapsing, and detecting overfitting. With
a series of carefully designed experiments, we comprehensively investigate
existing sample-based metrics and identify their strengths and limitations in
practical settings. Based on these results, we observe that kernel Maximum Mean
Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to
satisfy most of the desirable properties, provided that the distances between
samples are computed in a suitable feature space. Our experiments also unveil
interesting properties about the behavior of several popular GAN models, such
as whether they are memorizing training samples, and how far they are from
learning the target distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiantong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Felix Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Weinberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10692">
<title>ActiveRemediation: The Search for Lead Pipes in Flint, Michigan. (arXiv:1806.10692v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10692</link>
<description rdf:parseType="Literal">&lt;p&gt;We detail our ongoing work in Flint, Michigan to detect pipes made of lead
and other hazardous metals. After elevated levels of lead were detected in
residents&apos; drinking water, followed by an increase in blood lead levels in area
children, the state and federal governments directed over $125 million to
replace water service lines, the pipes connecting each home to the water
system. In the absence of accurate records, and with the high cost of
determining buried pipe materials, we put forth a number of predictive and
procedural tools to aid in the search and removal of lead infrastructure.
Alongside these statistical and machine learning approaches, we describe our
interactions with government officials in recommending homes for both
inspection and replacement, with a focus on the statistical model that adapts
to incoming information. Finally, in light of discussions about increased
spending on infrastructure development by the federal government, we explore
how our approach generalizes beyond Flint to other municipalities nationwide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1&quot;&gt;Jacob Abernethy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chojnacki_A/0/1/0/all/0/1&quot;&gt;Alex Chojnacki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farahi_A/0/1/0/all/0/1&quot;&gt;Arya Farahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1&quot;&gt;Eric Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_J/0/1/0/all/0/1&quot;&gt;Jared Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02287">
<title>Outperforming Good-Turing: Preliminary Report. (arXiv:1807.02287v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02287</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating a large alphabet probability distribution from a limited number of
samples is a fundamental problem in machine learning and statistics. A variety
of estimation schemes have been proposed over the years, mostly inspired by the
early work of Laplace and the seminal contribution of Good and Turing. One of
the basic assumptions shared by most commonly-used estimators is the unique
correspondence between the symbol&apos;s sample frequency and its estimated
probability. In this work we tackle this paradigmatic assumption; we claim that
symbols with &quot;similar&quot; frequencies shall be assigned the same estimated
probability value. This way we regulate the number of parameters and improve
generalization. In this preliminary report we show that by applying an ensemble
of such regulated estimators, we introduce a dramatic enhancement in the
estimation accuracy (typically up to 50%), compared to currently known methods.
An implementation of our suggested method is publicly available at the first
author&apos;s web-page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Painsky_A/0/1/0/all/0/1&quot;&gt;Amichai Painsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feder_M/0/1/0/all/0/1&quot;&gt;Meir Feder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05935">
<title>Siamese Survival Analysis with Competing Risks. (arXiv:1807.05935v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05935</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis in the presence of multiple possible adverse events, i.e.,
competing risks, is a pervasive problem in many industries (healthcare,
finance, etc.). Since only one event is typically observed, the incidence of an
event of interest is often obscured by other related competing events. This
nonidentifiability, or inability to estimate true cause-specific survival
curves from empirical data, further complicates competing risk survival
analysis. We introduce Siamese Survival Prognosis Network (SSPN), a novel deep
learning architecture for estimating personalized risk scores in the presence
of competing risks. SSPN circumvents the nonidentifiability problem by avoiding
the estimation of cause-specific survival curves and instead determines
pairwise concordant time-dependent risks, where longer event times are assigned
lower risks. Furthermore, SSPN is able to directly optimize an approximation to
the C-discrimination index, rather than relying on well-known metrics which are
unable to capture the unique requirements of survival analysis with competing
risks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nemchenko_A/0/1/0/all/0/1&quot;&gt;Anton Nemchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyono_T/0/1/0/all/0/1&quot;&gt;Trent Kyono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela Van Der Schaar&lt;/a&gt;</dc:creator>
</item></rdf:RDF>