<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06173"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06103"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06149"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06286"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03858"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06036"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06076"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06214"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06343"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06358"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06362"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.06892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00548"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.06173">
<title>A Discriminative Approach to Bayesian Filtering with Applications to Human Neural Decoding. (arXiv:1807.06173v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06173</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a stationary state-space model that relates a sequence of hidden states
and corresponding measurements or observations, Bayesian filtering provides a
principled statistical framework for inferring the posterior distribution of
the current state given all measurements up to the present time. For example,
the Apollo lunar module implemented a Kalman filter to infer its location from
a sequence of earth-based radar measurements and land safely on the moon.
&lt;/p&gt;
&lt;p&gt;To perform Bayesian filtering, we require a measurement model that describes
the conditional distribution of each observation given state. The Kalman filter
takes this measurement model to be linear, Gaussian. Here we show how a
nonlinear, Gaussian approximation to the distribution of state given
observation can be used in conjunction with Bayes&apos; rule to build a nonlinear,
non-Gaussian measurement model. The resulting approach, called the
Discriminative Kalman Filter (DKF), retains fast closed-form updates for the
posterior. We argue there are many cases where the distribution of state given
measurement is better-approximated as Gaussian, especially when the
dimensionality of measurements far exceeds that of states and the Bernstein-von
Mises theorem applies. Online neural decoding for brain-computer interfaces
provides a motivating example, where filtering incorporates increasingly
detailed measurements of neural activity to provide users control over external
devices. Within the BrainGate2 clinical trial, the DKF successfully enabled
three volunteers with quadriplegia to control an on-screen cursor in real-time
using mental imagery alone. Participant &quot;T9&quot; used the DKF to type out messages
on a tablet PC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burkhart_M/0/1/0/all/0/1&quot;&gt;Michael C. Burkhart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06244">
<title>Context-adaptive neural network based prediction for image compression. (arXiv:1807.06244v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.06244</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a set of neural network architectures, called Prediction
Neural Networks Set (PNNS), based on both fully-connected and convolutional
neural networks, for intra image prediction. The choice of neural network for
predicting a given image block depends on the block size, hence does not need
to be signalled to the decoder. It is shown that, while fully-connected neural
networks give good performance for small block sizes, convolutional neural
networks provide better predictions in large blocks with complex textures.
Thanks to the use of masks of random sizes during training, the neural networks
of PNNS well adapt to the available context that may vary, depending on the
position of the image block to be predicted. When integrating PNNS into a H.265
codec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained.
These gains are on average 0.99% larger than those of prior neural network
based methods. Unlike the H.265 intra prediction modes, which are each
specialized in predicting a specific texture, the proposed PNNS can model a
large set of complex textures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_T/0/1/0/all/0/1&quot;&gt;Thierry Dumas&lt;/a&gt; (Sirocco), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roumy_A/0/1/0/all/0/1&quot;&gt;Aline Roumy&lt;/a&gt; (Sirocco), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillemot_C/0/1/0/all/0/1&quot;&gt;Christine Guillemot&lt;/a&gt; (Sirocco)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06096">
<title>Shielded Decision-Making in MDPs. (arXiv:1807.06096v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06096</link>
<description rdf:parseType="Literal">&lt;p&gt;A prominent problem in artificial intelligence and machine learning is the
safe exploration of an environment. In particular, reinforcement learning is a
well-known technique to determine optimal policies for complicated dynamic
systems, but suffers from the fact that such policies may induce harmful
behavior. We present the concept of a shield that forces decision-making to
provably adhere to safety requirements with high probability. Our method
exploits the inherent uncertainties in scenarios given by Markov decision
processes. We present a method to compute probabilities of decision making
regarding temporal logic constraints. We use that information to realize a
shield that---when applied to a reinforcement learning algorithm---ensures
(near-)optimal behavior both for the safety constraints and for the actual
learning objective. In our experiments, we show on the arcade game PAC-MAN that
the learning efficiency increases as the learning needs orders of magnitude
fewer episodes. We show tradeoffs between sufficient progress in exploration of
the environment and ensuring strict safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jansen_N/0/1/0/all/0/1&quot;&gt;Nils Jansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konighofer_B/0/1/0/all/0/1&quot;&gt;Bettina K&amp;#xf6;nighofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junges_S/0/1/0/all/0/1&quot;&gt;Sebastian Junges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloem_R/0/1/0/all/0/1&quot;&gt;Roderick Bloem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06103">
<title>An agent-based model of an endangered population of the Arctic fox from Mednyi Island. (arXiv:1807.06103v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1807.06103</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence techniques such as agent-based modeling and
probabilistic reasoning have shown promise in modeling complex biological
systems and testing ecological hypotheses through simulation. We develop an
agent-based model of Arctic foxes from Medniy Island while utilizing
Probabilistic Graphical Models to capture the conditional dependencies between
the random variables. Such models provide valuable insights in analyzing
factors behind catastrophic degradation of this population and in revealing
evolutionary mechanisms of its persistence in high-density environment. Using
empirical data from studies in Medniy Island, we create a realistic model of
Arctic foxes as agents, and study their survival and population dynamics under
a variety of conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brilliantova_A/0/1/0/all/0/1&quot;&gt;Angelina Brilliantova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pletenev_A/0/1/0/all/0/1&quot;&gt;Anton Pletenev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doronina_L/0/1/0/all/0/1&quot;&gt;Liliya Doronina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1&quot;&gt;Hadi Hosseini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06149">
<title>Probably approximately correct learning of Horn envelopes from queries. (arXiv:1807.06149v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06149</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an algorithm for learning the Horn envelope of an arbitrary domain
using an expert, or an oracle, capable of answering certain types of queries
about this domain. Attribute exploration from formal concept analysis is a
procedure that solves this problem, but the number of queries it may ask is
exponential in the size of the resulting Horn formula in the worst case. We
recall a well-known polynomial-time algorithm for learning Horn formulas with
membership and equivalence queries and modify it to obtain a polynomial-time
probably approximately correct algorithm for learning the Horn envelope of an
arbitrary domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borchmann_D/0/1/0/all/0/1&quot;&gt;Daniel Borchmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1&quot;&gt;Tom Hanika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obiedkov_S/0/1/0/all/0/1&quot;&gt;Sergei Obiedkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06158">
<title>Generative Adversarial Imitation from Observation. (arXiv:1807.06158v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06158</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation from observation (IfO) is the problem of learning directly from
state-only demonstrations without having access to the demonstrator&apos;s actions.
The lack of action information both distinguishes IfO from most of the
literature in imitation learning, and also sets it apart as a method that may
enable agents to learn from large set of previously inapplicable resources such
as internet videos. In this paper, we propose both a general framework for IfO
approaches and propose a new IfO approach based on generative adversarial
networks called generative adversarial imitation from observation (GAIfO). We
demonstrate that this approach performs comparably to classical imitation
learning approaches (which have access to the demonstrator&apos;s actions) and
significantly outperforms existing imitation from observation methods in
high-dimensional simulation environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1&quot;&gt;Faraz Torabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1&quot;&gt;Garrett Warnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06161">
<title>Explanations for Temporal Recommendations. (arXiv:1807.06161v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06161</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommendation systems are an integral part of Artificial Intelligence (AI)
and have become increasingly important in the growing age of commercialization
in AI. Deep learning (DL) techniques for recommendation systems (RS) provide
powerful latent-feature models for effective recommendation but suffer from the
major drawback of being non-interpretable. In this paper we describe a
framework for explainable temporal recommendations in a DL model. We consider
an LSTM based Recurrent Neural Network (RNN) architecture for recommendation
and a neighbourhood-based scheme for generating explanations in the model. We
demonstrate the effectiveness of our approach through experiments on the
Netflix dataset by jointly optimizing for both prediction accuracy and
explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadhwaj_H/0/1/0/all/0/1&quot;&gt;Homanga Bharadhwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Shruti Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06286">
<title>Preference-Based Monte Carlo Tree Search. (arXiv:1807.06286v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06286</link>
<description rdf:parseType="Literal">&lt;p&gt;Monte Carlo tree search (MCTS) is a popular choice for solving sequential
anytime problems. However, it depends on a numeric feedback signal, which can
be difficult to define. Real-time MCTS is a variant which may only rarely
encounter states with an explicit, extrinsic reward. To deal with such cases,
the experimenter has to supply an additional numeric feedback signal in the
form of a heuristic, which intrinsically guides the agent. Recent work has
shown evidence that in different areas the underlying structure is ordinal and
not numerical. Hence erroneous and biased heuristics are inevitable, especially
in such domains. In this paper, we propose a MCTS variant which only depends on
qualitative feedback, and therefore opens up new applications for MCTS. We also
find indications that translating absolute into ordinal feedback may be
beneficial. Using a puzzle domain, we show that our preference-based MCTS
variant, wich only receives qualitative feedback, is able to reach a
performance level comparable to a regular MCTS baseline, which obtains
quantitative feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joppen_T/0/1/0/all/0/1&quot;&gt;Tobias Joppen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirth_C/0/1/0/all/0/1&quot;&gt;Christian Wirth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1&quot;&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06391">
<title>Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game. (arXiv:1807.06391v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06391</link>
<description rdf:parseType="Literal">&lt;p&gt;Score following is the process of tracking a musical performance (audio) with
respect to a known symbolic representation (a score). We start this paper by
formulating score following as a multimodal Markov Decision Process, the
mathematical foundation for sequential decision making. Given this formal
definition, we address the score following task with state-of-the-art deep
reinforcement learning (RL) algorithms such as synchronous advantage actor
critic (A2C). In particular, we design multimodal RL agents that simultaneously
learn to listen to music, read the scores from images of sheet music, and
follow the audio along in the sheet, in an end-to-end fashion. All this
behavior is learned entirely from scratch, based on a weak and potentially
delayed reward signal that indicates to the agent how close it is to the
correct position in the score. Besides discussing the theoretical advantages of
this learning paradigm, we show in experiments that it is in fact superior
compared to previously proposed methods for score following in raw sheet music
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorfer_M/0/1/0/all/0/1&quot;&gt;Matthias Dorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henkel_F/0/1/0/all/0/1&quot;&gt;Florian Henkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1&quot;&gt;Gerhard Widmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06419">
<title>On Ternary Coding and Three-Valued Logic. (arXiv:1807.06419v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06419</link>
<description rdf:parseType="Literal">&lt;p&gt;Mathematically, ternary coding is more efficient than binary coding. It is
little used in computation because technology for binary processing is already
established and the implementation of ternary coding is more complicated, but
remains relevant in algorithms that use decision trees and in communications.
In this paper we present a new comparison of binary and ternary coding and
their relative efficiencies are computed both for number representation and
decision trees. The implications of our inability to use optimal representation
through mathematics or logic are examined. Apart from considerations of
representation efficiency, ternary coding appears preferable to binary coding
in classification of many real-world problems of artificial intelligence (AI)
and medicine. We examine the problem of identifying appropriate three classes
for domain-specific applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kak_S/0/1/0/all/0/1&quot;&gt;Subhash Kak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00193">
<title>InclusiveFaceNet: Improving Face Attribute Detection with Race and Gender Diversity. (arXiv:1712.00193v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00193</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate an approach to face attribute detection that retains or
improves attribute detection accuracy across gender and race subgroups by
learning demographic information prior to learning the attribute detection
task. The system, which we call InclusiveFaceNet, detects face attributes by
transferring race and gender representations learned from a held-out dataset of
public race and gender identities. Leveraging learned demographic
representations while withholding demographic inference from the downstream
face attribute detection task preserves potential users&apos; demographic privacy
while resulting in some of the best reported numbers to date on attribute
detection in the Faces of the World and CelebA datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hee Jung Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Margaret Mitchell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04697">
<title>Learning to Search with MCTSnets. (arXiv:1802.04697v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04697</link>
<description rdf:parseType="Literal">&lt;p&gt;Planning problems are among the most important and well-studied problems in
artificial intelligence. They are most typically solved by tree search
algorithms that simulate ahead into the future, evaluate future states, and
back-up those evaluations to the root of a search tree. Among these algorithms,
Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely
used. A typical implementation of MCTS uses cleverly designed rules, optimized
to the particular characteristics of the domain. These rules control where the
simulation traverses, what to evaluate in the states that are reached, and how
to back-up those evaluations. In this paper we instead learn where, what and
how to search. Our architecture, which we call an MCTSnet, incorporates
simulation-based search inside a neural network, by expanding, evaluating and
backing-up a vector embedding. The parameters of the network are trained
end-to-end using gradient-based optimisation. When applied to small searches in
the well known planning problem Sokoban, the learned search algorithm
significantly outperformed MCTS baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guez_A/0/1/0/all/0/1&quot;&gt;Arthur Guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;ophane Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonoglou_I/0/1/0/all/0/1&quot;&gt;Ioannis Antonoglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1&quot;&gt;Karen Simonyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierstra_D/0/1/0/all/0/1&quot;&gt;Daan Wierstra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1&quot;&gt;David Silver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08984">
<title>Computational Approaches for Stochastic Shortest Path on Succinct MDPs. (arXiv:1804.08984v3 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the stochastic shortest path (SSP) problem for succinct Markov
decision processes (MDPs), where the MDP consists of a set of variables, and a
set of nondeterministic rules that update the variables. First, we show that
several examples from the AI literature can be modeled as succinct MDPs. Then
we present computational approaches for upper and lower bounds for the SSP
problem: (a)~for computing upper bounds, our method is polynomial-time in the
implicit description of the MDP; (b)~for lower bounds, we present a
polynomial-time (in the size of the implicit description) reduction to
quadratic programming. Our approach is applicable even to infinite-state MDPs.
Finally, we present experimental results to demonstrate the effectiveness of
our approach on several classical examples from the AI literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hongfei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goharshady_A/0/1/0/all/0/1&quot;&gt;Amir Kafshdar Goharshady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okati_N/0/1/0/all/0/1&quot;&gt;Nastaran Okati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03858">
<title>Algorithmic Framework for Model-based Reinforcement Learning with Theoretical Guarantees. (arXiv:1807.03858v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03858</link>
<description rdf:parseType="Literal">&lt;p&gt;While model-based reinforcement learning has empirically been shown to
significantly reduce the sample complexity that hinders model-free RL, the
theoretical understanding of such methods has been rather limited. In this
paper, we introduce a novel algorithmic framework for designing and analyzing
model-based RL algorithms with theoretical guarantees, and a practical
algorithm Optimistic Lower Bounds Optimization (OLBO). In particular, we derive
a theoretical guarantee of monotone improvement for model-based RL with our
framework. We iteratively build a lower bound of the expected reward based on
the estimated dynamical model and sample trajectories, and maximize it jointly
over the policy and the model. Assuming the optimization in each iteration
succeeds, the expected reward is guaranteed to improve. The framework also
incorporates an optimism-driven perspective, and reveals the intrinsic measure
for the model prediction error. Preliminary simulations demonstrate that our
approach outperforms the standard baselines on continuous control benchmark
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huazhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06036">
<title>Pangloss: Fast Entity Linking in Noisy Text Environments. (arXiv:1807.06036v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.06036</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity linking is the task of mapping potentially ambiguous terms in text to
their constituent entities in a knowledge base like Wikipedia. This is useful
for organizing content, extracting structured data from textual documents, and
in machine learning relevance applications like semantic search, knowledge
graph construction, and question answering. Traditionally, this work has
focused on text that has been well-formed, like news articles, but in common
real world datasets such as messaging, resumes, or short-form social media,
non-grammatical, loosely-structured text adds a new dimension to this problem.
&lt;/p&gt;
&lt;p&gt;This paper presents Pangloss, a production system for entity disambiguation
on noisy text. Pangloss combines a probabilistic linear-time key phrase
identification algorithm with a semantic similarity engine based on
context-dependent document embeddings to achieve better than state-of-the-art
results (&amp;gt;5% in F1) compared to other research or commercially available
systems. In addition, Pangloss leverages a local embedded database with a
tiered architecture to house its statistics and metadata, which allows rapid
disambiguation in streaming contexts and on-device disambiguation in low-memory
environments such as mobile phones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conover_M/0/1/0/all/0/1&quot;&gt;Michael Conover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_M/0/1/0/all/0/1&quot;&gt;Matthew Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blackburn_S/0/1/0/all/0/1&quot;&gt;Scott Blackburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skomoroch_P/0/1/0/all/0/1&quot;&gt;Pete Skomoroch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Sam Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06064">
<title>Online Robust Policy Learning in the Presence of Unknown Adversaries. (arXiv:1807.06064v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06064</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing prospect of deep reinforcement learning (DRL) being used in
cyber-physical systems has raised concerns around safety and robustness of
autonomous agents. Recent work on generating adversarial attacks have shown
that it is computationally feasible for a bad actor to fool a DRL policy into
behaving sub optimally. Although certain adversarial attacks with specific
attack models have been addressed, most studies are only interested in off-line
optimization in the data space (e.g., example fitting, distillation). This
paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is
attack model-agnostic and more suited to reinforcement learning, via handling
the attacks in the decision space (as opposed to data space) and directly
mitigating learned bias introduced by the adversary. In MLAH, we learn separate
sub-policies (nominal and adversarial) in an online manner, as guided by a
supervisory master agent that detects the presence of the adversary by
leveraging the advantage function for the sub-policies. We demonstrate that the
proposed algorithm enables policy learning with significantly lower bias as
compared to the state-of-the-art policy learning approaches even in the
presence of heavy state information attacks. We present algorithm analysis and
simulation results using popular OpenAI Gym environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havens_A/0/1/0/all/0/1&quot;&gt;Aaron J. Havens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhanhong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06076">
<title>Dynamic Visual Analytics for Elicitation Meetings with ELICA. (arXiv:1807.06076v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1807.06076</link>
<description rdf:parseType="Literal">&lt;p&gt;Requirements elicitation can be very challenging in projects that require
deep domain knowledge about the system at hand. As analysts have the full
control over the elicitation process, their lack of knowledge about the system
under study inhibits them from asking related questions and reduces the
accuracy of requirements provided by stakeholders. We present ELICA, a generic
interactive visual analytics tool to assist analysts during requirements
elicitation process. ELICA uses a novel information extraction algorithm based
on a combination of Weighted Finite State Transducers (WFSTs) (generative
model) and SVMs (discriminative model). ELICA presents the extracted relevant
information in an interactive GUI (including zooming, panning, and pinching)
that allows analysts to explore which parts of the ongoing conversation (or
specification document) match with the extracted information. In this
demonstration, we show that ELICA is usable and effective in practice, and is
able to extract the related information in real-time. We also demonstrate how
carefully designed features in ELICA facilitate the interactive and dynamic
process of information extraction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abad_Z/0/1/0/all/0/1&quot;&gt;Zahra Shakeri Hossein Abad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Munib Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheema_A/0/1/0/all/0/1&quot;&gt;Abdullah Cheema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gervasi_V/0/1/0/all/0/1&quot;&gt;Vincenzo Gervasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zowghi_D/0/1/0/all/0/1&quot;&gt;Didar Zowghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barker_K/0/1/0/all/0/1&quot;&gt;Ken Barker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06093">
<title>Remaining Useful Life Estimation of Aero-Engines with Self-Joint Prediction of Continuous and Discrete States. (arXiv:1807.06093v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06093</link>
<description rdf:parseType="Literal">&lt;p&gt;The remaining useful life (RUL) estimation generally suffer from this problem
of lacking prior knowledge to predefine the exact failure thresholds for
machinery operating in dynamic environments. In this case, dynamic thresholds
depicted by discrete states are effective to estimate the RUL of dynamic
machinery. Currently, only few work considers the dynamic thresholds, and adopt
different algorithms to predict the continuous and discrete states separately,
which largely increases the complexity of the learning process. In this paper,
we propose a novel prognostics approach for RUL estimation of aero-engines with
self-joint prediction of continuous and discrete states within one learning
framework. With modeling capability of self-adapting structure and parameters
online, the quantized kernel recursive least squares (QKRLS) algorithm is
introduced to predict the degrading states and also determine the discrete
states with the kernel centers. The self-evolving dynamic kernel centers
obtained during building predictors are automatically assigned as the discrete
states for different engines without predefining them. Then, the RUL is
estimated conveniently once the predicted degrading signals fall into the final
fault state based on a distance metric. Finally, the results from turbofan
engine datasets demonstrate the superiority of the proposed approach compared
to other popular approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1&quot;&gt;Rong-Jing Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_H/0/1/0/all/0/1&quot;&gt;Hai-Jun Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhi-Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Badong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06144">
<title>Longitudinal detection of radiological abnormalities with time-modulated LSTM. (arXiv:1807.06144v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06144</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have been successfully employed in
recent years for the detection of radiological abnormalities in medical images
such as plain x-rays. To date, most studies use CNNs on individual examinations
in isolation and discard previously available clinical information. In this
study we set out to explore whether Long-Short-Term-Memory networks (LSTMs) can
be used to improve classification performance when modelling the entire
sequence of radiographs that may be available for a given patient, including
their reports. A limitation of traditional LSTMs, though, is that they
implicitly assume equally-spaced observations, whereas the radiological exams
are event-based, and therefore irregularly sampled. Using both a simulated
dataset and a large-scale chest x-ray dataset, we demonstrate that a simple
modification of the LSTM architecture, which explicitly takes into account the
time lag between consecutive observations, can boost classification
performance. Our empirical results demonstrate improved detection of commonly
reported abnormalities on chest x-rays such as cardiomegaly, consolidation,
pleural effusion and hiatus hernia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Santeramo_R/0/1/0/all/0/1&quot;&gt;Ruggiero Santeramo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Withey_S/0/1/0/all/0/1&quot;&gt;Samuel Withey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montana_G/0/1/0/all/0/1&quot;&gt;Giovanni Montana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06160">
<title>Layer-wise Relevance Propagation for Explainable Recommendations. (arXiv:1807.06160v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06160</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we tackle the problem of explanations in a deep-learning based
model for recommendations by leveraging the technique of layer-wise relevance
propagation. We use a Deep Convolutional Neural Network to extract relevant
features from the input images before identifying similarity between the images
in feature space. Relationships between the images are identified by the model
and layer-wise relevance propagation is used to infer pixel-level details of
the images that may have significantly informed the model&apos;s choice. We evaluate
our method on an Amazon products dataset and demonstrate the efficacy of our
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadhwaj_H/0/1/0/all/0/1&quot;&gt;Homanga Bharadhwaj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06180">
<title>A Data-Driven Approach for Predicting Vegetation-Related Outages in Power Distribution Systems. (arXiv:1807.06180v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06180</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel data-driven approach for predicting the number of
vegetation-related outages that occur in power distribution systems on a
monthly basis. In order to develop an approach that is able to successfully
fulfill this objective, there are two main challenges that ought to be
addressed. The first challenge is to define the extent of the target area. An
unsupervised machine learning approach is proposed to overcome this difficulty.
The second challenge is to correctly identify the main causes of
vegetation-related outages and to thoroughly investigate their nature. In this
paper, these outages are categorized into two main groups: growth-related and
weather-related outages, and two types of models, namely time series and
non-linear machine learning regression models are proposed to conduct the
prediction tasks, respectively. Moreover, various features that can explain the
variability in vegetation-related outages are engineered and employed. Actual
outage data, obtained from a major utility in the U.S., in addition to
different types of weather and geographical data are utilized to build the
proposed approach. Finally, a comprehensive case study is carried out to
demonstrate how the proposed approach can be used to successfully predict the
number of vegetation-related outages and to help decision-makers to detect
vulnerable zones in their systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doostan_M/0/1/0/all/0/1&quot;&gt;Milad Doostan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohrabi_R/0/1/0/all/0/1&quot;&gt;Reza Sohrabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_B/0/1/0/all/0/1&quot;&gt;Badrul Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06203">
<title>Penalized matrix decomposition for denoising, compression, and improved demixing of functional imaging data. (arXiv:1807.06203v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1807.06203</link>
<description rdf:parseType="Literal">&lt;p&gt;Calcium imaging has revolutionized systems neuroscience, providing the
ability to image large neural populations with single-cell resolution. The
resulting datasets are quite large, which has presented a barrier to routine
open sharing of this data, slowing progress in reproducible research. State of
the art methods for analyzing this data are based on non-negative matrix
factorization (NMF); these approaches solve a non-convex optimization problem,
and are effective when good initializations are available, but can break down
in low-SNR settings where common initialization approaches fail. Here we
introduce an approach to compressing and denoising functional imaging data. The
method is based on a spatially-localized penalized matrix decomposition (PMD)
of the data to separate (low-dimensional) signal from (temporally-uncorrelated)
noise. This approach can be applied in parallel on local spatial patches and is
therefore highly scalable, does not impose non-negativity constraints or
require stringent identifiability assumptions (leading to significantly more
robust results compared to NMF), and estimates all parameters directly from the
data, so no hand-tuning is required. We have applied the method to a wide range
of functional imaging data (including one-photon, two-photon, three-photon,
widefield, somatic, axonal, dendritic, calcium, and voltage imaging datasets):
in all cases, we observe ~2-4x increases in SNR and compression rates of
20-300x with minimal visible loss of signal, with no adjustment of
hyperparameters; this in turn facilitates the process of demixing the observed
activity into contributions from individual neurons. We focus on two
challenging applications: dendritic calcium imaging data and voltage imaging
data in the context of optogenetic stimulation. In both cases, we show that our
new approach leads to faster and much more robust extraction of activity from
the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Buchanan_E/0/1/0/all/0/1&quot;&gt;E. Kelly Buchanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kinsella_I/0/1/0/all/0/1&quot;&gt;Ian Kinsella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Ding Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Rong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengcheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gerhard_F/0/1/0/all/0/1&quot;&gt;Felipe Gerhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ferrante_J/0/1/0/all/0/1&quot;&gt;John Ferrante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Ying Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sharon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shaik_M/0/1/0/all/0/1&quot;&gt;Mohammed Shaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yajie Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lu_R/0/1/0/all/0/1&quot;&gt;Rongwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reimer_J/0/1/0/all/0/1&quot;&gt;Jacob Reimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fahey_P/0/1/0/all/0/1&quot;&gt;Paul Fahey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Muhammad_T/0/1/0/all/0/1&quot;&gt;Taliah Muhammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dempsey_G/0/1/0/all/0/1&quot;&gt;Graham Dempsey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hillman_E/0/1/0/all/0/1&quot;&gt;Elizabeth Hillman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ji_N/0/1/0/all/0/1&quot;&gt;Na Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tolias_A/0/1/0/all/0/1&quot;&gt;Andreas Tolias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Paninski_L/0/1/0/all/0/1&quot;&gt;Liam Paninski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06214">
<title>Knockoffs for the mass: new feature importance statistics with false discovery guarantees. (arXiv:1807.06214v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06214</link>
<description rdf:parseType="Literal">&lt;p&gt;An important problem in machine learning and statistics is to identify
features that causally affect the outcome. This is often impossible to do from
purely observational data, and a natural relaxation is to identify features
that are correlated with the outcome even conditioned on all other observed
features. For example, we want to identify that smoking really is correlated
with cancer conditioned on demographics. The knockoff procedure is a recent
breakthrough in statistics that, in theory, can identify truly correlated
features while guaranteeing that the false discovery is limited. The idea is to
create synthetic data -knockoffs- that captures correlations amongst the
features. However there are substantial computational and practical challenges
to generating and using knockoffs. This paper makes several key advances that
enable knockoff application to be more efficient and powerful. We develop an
efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we
systematically evaluate knockoff test statistics and develop new statistics
with improved power. The paper combines new mathematical guarantees with
systematic experiments on real and synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gimenez_J/0/1/0/all/0/1&quot;&gt;Jaime Roquero Gimenez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghorbani_A/0/1/0/all/0/1&quot;&gt;Amirata Ghorbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06343">
<title>Learning with SGD and Random Features. (arXiv:1807.06343v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06343</link>
<description rdf:parseType="Literal">&lt;p&gt;Sketching and stochastic gradient methods are arguably the most common tech-
niques to derive efficient large-scale learning algorithms. In this paper, we
investigate their application in the context of nonparametric statistical
learning. More precisely, we study the estimator defined by stochastic
gradients with mini batches and ran- dom features. The latter can be seen as a
form of nonlinear sketching and used to define approximate kernel methods. The
estimator we consider is not explicitly penalized/constrained and
regularization is implicit. Indeed, our study highlight how different
parameters, such as the number of features, iterations, step-size and mini-
batch size control the learning properties of the solutions. We do this by
deriving optimal finite sample bounds, under standard assumptions. The obtained
results are corroborated and illustrated by numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carratino_L/0/1/0/all/0/1&quot;&gt;Luigi Carratino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06350">
<title>Battery health prediction under generalized conditions using a Gaussian process transition model. (arXiv:1807.06350v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1807.06350</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting the future health of batteries is necessary to ensure
reliable operation, minimise maintenance costs, and calculate the value of
energy storage investments. The complex nature of degradation renders
data-driven approaches a promising alternative to mechanistic modelling. This
study predicts the changes in battery capacity over time using a Bayesian
non-parametric approach based on Gaussian process regression. These changes can
be integrated against an arbitrary input sequence to predict capacity fade in a
variety of usage scenarios, forming a generalised health model. The approach
naturally incorporates varying current, voltage and temperature inputs, crucial
for enabling real world application. A key innovation is the feature selection
step, where arbitrary length current, voltage and temperature measurement
vectors are mapped to fixed size feature vectors, enabling them to be
efficiently used as exogenous variables. The approach is demonstrated on the
open-source NASA Randomised Battery Usage Dataset, with data of 26 cells aged
under randomized operational conditions. Using half of the cells for training,
and half for validation, the method is shown to accurately predict non-linear
capacity fade, with a best case normalised root mean square error of 4.3%,
including accurate estimation of prediction uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Richardson_R/0/1/0/all/0/1&quot;&gt;Robert R. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Howey_D/0/1/0/all/0/1&quot;&gt;David A. Howey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06358">
<title>IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis. (arXiv:1807.06358v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06358</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel introspective variational autoencoder (IntroVAE) model for
synthesizing high-resolution photographic images. IntroVAE is capable of
self-evaluating the quality of its generated samples and improving itself
accordingly. Its inference and generator models are jointly trained in an
introspective way. On one hand, the generator is required to reconstruct the
input images from the noisy outputs of the inference model as normal VAEs. On
the other hand, the inference model is encouraged to classify between the
generated and real samples while the generator tries to fool it as GANs. These
two famous generative frameworks are integrated in a simple yet efficient
single-stream architecture that can be trained in a single stage. IntroVAE
preserves the advantages of VAEs, such as stable training and nice latent
manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires
no extra discriminators, because the inference model itself serves as a
discriminator to distinguish between the generated and real samples.
Experiments demonstrate that our method produces high-resolution
photo-realistic images (e.g., CELEBA images at \(1024^{2}\)), which are
comparable to or better than the state-of-the-art GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhenan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tieniu Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06362">
<title>Confidence Intervals for Testing Disparate Impact in Fair Learning. (arXiv:1807.06362v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06362</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide the asymptotic distribution of the major indexes used in the
statistical literature to quantify disparate treatment in machine learning. We
aim at promoting the use of confidence intervals when testing the so-called
group disparate impact. We illustrate on some examples the importance of using
confidence intervals and not a single value.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Besse_P/0/1/0/all/0/1&quot;&gt;Philippe Besse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barrio_E/0/1/0/all/0/1&quot;&gt;Eustasio del Barrio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gordaliza_P/0/1/0/all/0/1&quot;&gt;Paula Gordaliza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loubes_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Loubes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06446">
<title>Bridging the Gap Between Layout Pattern Sampling and Hotspot Detection via Batch Active Sampling. (arXiv:1807.06446v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06446</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout hotpot detection is one of the main steps in modern VLSI design. A
typical hotspot detection flow is extremely time consuming due to the
computationally expensive mask optimization and lithographic simulation. Recent
researches try to facilitate the procedure with a reduced flow including
feature extraction, training set generation and hotspot detection, where
feature extraction methods and hotspot detection engines are deeply studied.
However, the performance of hotspot detectors relies highly on the quality of
reference layout libraries which are costly to obtain and usually predetermined
or randomly sampled in previous works. In this paper, we propose an active
learning-based layout pattern sampling and hotspot detection flow, which
simultaneously optimizes the machine learning model and the training set that
aims to achieve similar or better hotspot detection performance with much
smaller number of training instances. Experimental results show that our
proposed method can significantly reduce lithography simulation overhead while
attaining satisfactory detection accuracy on designs under both DUV and EUV
lithography technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haoyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabery_C/0/1/0/all/0/1&quot;&gt;Cyrus Tabery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bingqing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06473">
<title>Contextual Memory Trees. (arXiv:1807.06473v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06473</link>
<description rdf:parseType="Literal">&lt;p&gt;We design and study a Contextual Memory Tree (CMT), a learning memory
controller that inserts new memories into an experience store of unbounded
size. It is designed to efficiently query for memories from that store,
supporting logarithmic time insertion and retrieval operations. Hence CMT can
be integrated into existing statistical learning algorithms as an augmented
memory unit without substantially increasing training and inference
computation. We demonstrate the efficacy of CMT by augmenting existing
multi-class and multi-label classification algorithms with CMT and observe
statistical improvement. We also test CMT learning on several image-captioning
tasks to demonstrate that it performs computationally better than a simple
nearest neighbors memory system while benefitting from reward learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beygelzimer_A/0/1/0/all/0/1&quot;&gt;Alina Beygelzimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1&quot;&gt;Hal Daum&amp;#xe9; III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1&quot;&gt;John Langford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1&quot;&gt;Paul Mineiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06489">
<title>Automated Treatment Planning in Radiation Therapy using Generative Adversarial Networks. (arXiv:1807.06489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06489</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge-based planning (KBP) is an automated approach to radiation therapy
treatment planning that involves predicting desirable treatment plans before
they are then corrected to deliverable ones. We propose a generative
adversarial network (GAN) approach for predicting desirable 3D dose
distributions that eschews the previous paradigms of site-specific feature
engineering and predicting low-dimensional representations of the plan.
Experiments on a dataset of oropharyngeal cancer patients show that our
approach significantly outperforms previous methods on several clinical
satisfaction criteria and similarity metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1&quot;&gt;Rafid Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babier_A/0/1/0/all/0/1&quot;&gt;Aaron Babier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McNiven_A/0/1/0/all/0/1&quot;&gt;Andrea McNiven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diamant_A/0/1/0/all/0/1&quot;&gt;Adam Diamant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_T/0/1/0/all/0/1&quot;&gt;Timothy C. Y. Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06497">
<title>Continuous Assortment Optimization with Logit Choice Probabilities under Incomplete Information. (arXiv:1807.06497v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06497</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider assortment optimization in relation to a product for which a
particular attribute can be continuously adjusted. Examples include the
duration of a loan (where each duration corresponds to a specific interest
rate) and the data limit for a cell phone subscription. The question to be
addressed is: how should a retailer determine what to offer to maximize profit?
Representing the assortment as a union of subintervals, the choice of a
customer is modelled as a continuous logit choice model; a capacity constraint
is imposed on the assortment. The problem can be phrased as a multi-armed
bandit, i.e., the objective is to estimate demand over time by sequentially
offering different assortments to incoming costumers. Kernel density estimation
is applied to the observed purchases. We present an explore-then-exploit
policy, which endures at most a regret of order $T^{2/3}$ (neglecting
logarithmic factors). Also, by showing that any policy in the worst case must
endure at least a regret of order $T^{2/3}$, we conclude that our policy can be
regarded as asymptotically optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peeters_Y/0/1/0/all/0/1&quot;&gt;Yannik Peeters&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boer_A/0/1/0/all/0/1&quot;&gt;Arnoud den Boer&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mandjes_M/0/1/0/all/0/1&quot;&gt;Michel Mandjes&lt;/a&gt; (1 and 2) ((1) Amsterdam Business School, University of Amsterdam (2) Korteweg-de Vries Institute for Mathematics, University of Amsterdam)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06530">
<title>An Acceleration Scheme for Memory Limited, Streaming PCA. (arXiv:1807.06530v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06530</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an acceleration scheme for online memory-limited
PCA methods. Our scheme converges to the first $k&amp;gt;1$ eigenvectors in a single
data pass. We provide empirical convergence results of our scheme based on the
spiked covariance model. Our scheme does not require any predefined parameters
such as the eigengap and hence is well facilitated for streaming data
scenarios. Furthermore, we apply our scheme to challenging time-varying systems
where online PCA methods fail to converge. Specifically, we discuss a family of
time-varying systems that are based on Molecular Dynamics simulations where
batch PCA converges to the actual analytic solution of such systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alakkari_S/0/1/0/all/0/1&quot;&gt;Salaheddin Alakkari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dingliana_J/0/1/0/all/0/1&quot;&gt;John Dingliana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06555">
<title>Training Recurrent Neural Networks against Noisy Computations during Inference. (arXiv:1807.06555v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06555</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the robustness of recurrent neural networks when the computations
within the network are noisy. One of the motivations for looking into this
problem is to reduce the high power cost of conventional computing of neural
network operations through the use of analog neuromorphic circuits. Traditional
GPU/CPU-centered deep learning architectures exhibit bottlenecks in
power-restricted applications, such as speech recognition in embedded systems.
The use of specialized neuromorphic circuits, where analog signals passed
through memory-cell arrays are sensed to accomplish matrix-vector
multiplications, promises large power savings and speed gains but brings with
it the problems of limited precision of computations and unavoidable analog
noise.
&lt;/p&gt;
&lt;p&gt;In this paper we propose a method, called {\em Deep Noise Injection
training}, to train RNNs to obtain a set of weights/biases that is much more
robust against noisy computation during inference. We explore several RNN
architectures, such as vanilla RNN and long-short-term memories (LSTM), and
show that after convergence of Deep Noise Injection training the set of trained
weights/biases has more consistent performance over a wide range of noise
powers entering the network during inference. Surprisingly, we find that Deep
Noise Injection training improves overall performance of some networks even for
numerically accurate inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1&quot;&gt;Minghai Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vucinic_D/0/1/0/all/0/1&quot;&gt;Dejan Vucinic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06560">
<title>Temporally Evolving Community Detection and Prediction in Content-Centric Networks. (arXiv:1807.06560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06560</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we consider the problem of combining link, content and temporal
analysis for community detection and prediction in evolving networks. Such
temporal and content-rich networks occur in many real-life settings, such as
bibliographic networks and question answering forums. Most of the work in the
literature (that uses both content and structure) deals with static snapshots
of networks, and they do not reflect the dynamic changes occurring over
multiple snapshots. Incorporating dynamic changes in the communities into the
analysis can also provide useful insights about the changes in the network such
as the migration of authors across communities. In this work, we propose
Chimera, a shared factorization model that can simultaneously account for graph
links, content, and temporal analysis. This approach works by extracting the
latent semantic structure of the network in multidimensional form, but in a way
that takes into account the temporal continuity of these embeddings. Such an
approach simplifies temporal analysis of the underlying network by using the
embedding as a surrogate. A consequence of this simplification is that it is
also possible to use this temporal sequence of embeddings to predict future
communities. We present experimental results illustrating the effectiveness of
the approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Appel_A/0/1/0/all/0/1&quot;&gt;Ana Paula Appel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_R/0/1/0/all/0/1&quot;&gt;Renato L. F. Cunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1&quot;&gt;Charu C. Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terakado_M/0/1/0/all/0/1&quot;&gt;Marcela Megumi Terakado&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06572">
<title>Explicating feature contribution using Random Forest proximity distances. (arXiv:1807.06572v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06572</link>
<description rdf:parseType="Literal">&lt;p&gt;In Random Forests, proximity distances are a metric representation of data
into decision space. By observing how changes in input map to the movement of
instances in this space we are able to determine the independent contribution
of each feature to the decision-making process. For binary feature vectors,
this process is fully specified. As these changes in input move particular
instances nearer to the in-group or out-group, the independent contribution of
each feature can be uncovered. Using this technique, we are able to calculate
the contribution of each feature in determining how black-box decisions were
made. This allows explication of the decision-making process, audit of the
classifier, and post-hoc analysis of errors in classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Whitmore_L/0/1/0/all/0/1&quot;&gt;Leanne S. Whitmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+George_A/0/1/0/all/0/1&quot;&gt;Anthe George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hudson_C/0/1/0/all/0/1&quot;&gt;Corey M. Hudson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06574">
<title>Jensen: An Easily-Extensible C++ Toolkit for Production-Level Machine Learning and Convex Optimization. (arXiv:1807.06574v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06574</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Jensen, an easily extensible and scalable toolkit for
production-level machine learning and convex optimization. Jensen implements a
framework of convex (or loss) functions, convex optimization algorithms
(including Gradient Descent, L-BFGS, Stochastic Gradient Descent, Conjugate
Gradient, etc.), and a family of machine learning classifiers and regressors
(Logistic Regression, SVMs, Least Square Regression, etc.). This framework
makes it possible to deploy and train models with a few lines of code, and also
extend and build upon this by integrating new loss functions and optimization
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1&quot;&gt;Rishabh Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halloran_J/0/1/0/all/0/1&quot;&gt;John T. Halloran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kai Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.06892">
<title>Accelerated Randomized Mirror Descent Algorithms For Composite Non-strongly Convex Optimization. (arXiv:1605.06892v5 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1605.06892</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of minimizing the sum of an average function of a
large number of smooth convex components and a general, possibly
non-differentiable, convex function. Although many methods have been proposed
to solve this problem with the assumption that the sum is strongly convex, few
methods support the non-strongly convex case. Adding a small quadratic
regularization is a common trick used to tackle non-strongly convex problems;
however, it may cause loss of sparsity of solutions or weaken the performance
of the algorithms. Avoiding this trick, we propose an accelerated randomized
mirror descent method for solving this problem without the strongly convex
assumption. Our method extends the deterministic accelerated proximal gradient
methods of Paul Tseng \cite{Tseng} and can be applied even when proximal points
are computed inexactly. We also propose a scheme for solving the problem when
the component functions are non-smooth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hien_L/0/1/0/all/0/1&quot;&gt;Le Thi Khanh Hien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cuong V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Canyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08318">
<title>Proportional Volume Sampling and Approximation Algorithms for A-Optimal Design. (arXiv:1802.08318v5 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08318</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the optimal design problems where the goal is to choose a set of
linear measurements to obtain the most accurate estimate of an unknown vector
in $d$ dimensions. We study the $A$-optimal design variant where the objective
is to minimize the average variance of the error in the maximum likelihood
estimate of the vector being measured. The problem also finds applications in
sensor placement in wireless networks, sparse least squares regression, feature
selection for $k$-means clustering, and matrix approximation. In this paper, we
introduce proportional volume sampling to obtain improved approximation
algorithms for $A$-optimal design. Our main result is to obtain improved
approximation algorithms for the $A$-optimal design problem by introducing the
proportional volume sampling algorithm. Our results nearly optimal bounds in
the asymptotic regime when the number of measurements done, $k$, is
significantly more than the dimension $d$. We also give first approximation
algorithms when $k$ is small including when $k=d$. The proportional
volume-sampling algorithm also gives approximation algorithms for other optimal
design objectives such as $D$-optimal design and generalized ratio objective
matching or improving previous best known results. Interestingly, we show that
a similar guarantee cannot be obtained for the $E$-optimal design problem. We
also show that the $A$-optimal design problem is NP-hard to approximate within
a fixed constant when $k=d$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1&quot;&gt;Aleksandar Nikolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mohit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tantipongpipat_U/0/1/0/all/0/1&quot;&gt;Uthaipon Tao Tantipongpipat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00548">
<title>A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models. (arXiv:1806.00548v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00548</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of including additional knowledge in estimating
sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often
in bioinformatics and neuroimaging applications. Previous joint sGGM estimators
either fail to use existing knowledge or cannot scale-up to many tasks (large
$K$) under a high-dimensional (large $p$) situation. In this paper, we propose
a novel \underline{J}oint \underline{E}lementary \underline{E}stimator
incorporating additional \underline{K}nowledge (JEEK) to infer multiple related
sparse Gaussian Graphical models from large-scale heterogeneous data. Using
domain knowledge as weights, we design a novel hybrid norm as the minimization
objective to enforce the superposition of two weighted sparsity constraints,
one on the shared interactions and the other on the task-specific structural
patterns. This enables JEEK to elegantly consider various forms of existing
knowledge based on the domain at hand and avoid the need to design
knowledge-specific optimization. JEEK is solved through a fast and entry-wise
parallelizable solution that largely improves the computational efficiency of
the state-of-the-art $O(p^5K^4)$ to $O(p^2K^4)$. We conduct a rigorous
statistical analysis showing that JEEK achieves the same convergence rate
$O(\log(Kp)/n_{tot})$ as the state-of-the-art estimators that are much harder
to compute. Empirically, on multiple synthetic datasets and two real-world
data, JEEK outperforms the speed of the state-of-arts significantly while
achieving the same level of prediction accuracy. Available as R tool &quot;jeek&quot;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Beilun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1&quot;&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yanjun Qi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>