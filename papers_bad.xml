<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07870"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06226"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07710"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07738"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07847"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.05971"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00399"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07726"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07753"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07821"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07859"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07879"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07964"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07976"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07994"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08000"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1509.04634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.05988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.06176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07787"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05776"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04893"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.07870">
<title>Reservoir computing approaches for representation and classification of multivariate time series. (arXiv:1803.07870v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.07870</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification of multivariate time series (MTS) has been tackled with a
large variety of methodologies and applied to a wide range of scenarios. Among
the existing approaches, reservoir computing (RC) techniques, which implement a
fixed and high-dimensional recurrent network to process sequential data, are
computationally efficient tools to generate a vectorial, fixed-size
representation of the MTS, which can be further processed by standard
classifiers. Building upon previous works, in this paper we describe and
compare several advanced RC-based approaches to generate unsupervised MTS
representations, with a specific focus on their capability of yielding an
accurate classification. Our main contribution is a new method to encode the
MTS within the parameters of a linear model, trained to predict a
low-dimensional embedding of the reservoir dynamics. We also study the
combination of this representation technique when enhanced with a more complex
bidirectional reservoir and non-linear readouts, such as deep neural networks
with both fixed and flexible activation functions. We compare with
state-of-the-art recurrent networks, standard RC approaches and time series
kernels on multiple classification tasks, showing that the proposed algorithms
can achieve superior classification accuracy, while being vastly more efficient
to train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1&quot;&gt;Filippo Maria Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1&quot;&gt;Simone Scardapane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokse_S/0/1/0/all/0/1&quot;&gt;Sigurd L&amp;#xf8;kse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06226">
<title>Glyph: Symbolic Regression Tools. (arXiv:1803.06226v2 [cs.MS] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06226</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Glyph - a Python package for genetic programming based symbolic
regression. Glyph is designed for usage let by numerical simulations let by
real world experiments. For experimentalists, glyph-remote provides a
separation of tasks: a ZeroMQ interface splits the genetic programming
optimization task from the evaluation of an experimental (or numerical) run.
Glyph can be accessed at &lt;a href=&quot;http://github.com/ambrosys/glyph&quot;&gt;this http URL&lt;/a&gt; . Domain experts are
be able to employ symbolic regression in their experiments with ease, even if
they are not expert programmers. The reuse potential is kept high by a generic
interface design. Glyph is available on PyPI and Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quade_M/0/1/0/all/0/1&quot;&gt;Markus Quade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gout_J/0/1/0/all/0/1&quot;&gt;Julien Gout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abel_M/0/1/0/all/0/1&quot;&gt;Markus Abel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07616">
<title>IntPhys: A Framework and Benchmark for Visual Intuitive Physics Reasoning. (arXiv:1803.07616v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.07616</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to reach human performance on complex visual tasks, artificial
systems need to incorporate a significant amount of understanding of the world
in terms of macroscopic objects, movements, forces, etc. Inspired by work on
intuitive physics in infants, we propose an evaluation framework which
diagnoses how much a given system understands about physics by testing whether
it can tell apart well matched videos of possible versus impossible events. The
test requires systems to compute a physical plausibility score over an entire
video. It is free of bias and can test a range of specific physical reasoning
skills. We then describe the first release of a benchmark dataset aimed at
learning intuitive physics in an unsupervised way, using videos constructed
with a game engine. We describe two Deep Neural Network baseline systems
trained with a future frame prediction objective and tested on the possible
versus impossible discrimination task. The analysis of their results compared
to human data gives novel insights in the potentials and limitations of next
frame prediction architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riochet_R/0/1/0/all/0/1&quot;&gt;Ronan Riochet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_M/0/1/0/all/0/1&quot;&gt;Mario Ynocente Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_M/0/1/0/all/0/1&quot;&gt;Mathieu Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1&quot;&gt;Adam Lerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izard_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe9;ronique Izard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1&quot;&gt;Emmanuel Dupoux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07710">
<title>Inference in Probabilistic Graphical Models by Graph Neural Networks. (arXiv:1803.07710v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07710</link>
<description rdf:parseType="Literal">&lt;p&gt;A useful computation when acting in a complex environment is to infer the
marginal probabilities or most probable states of task-relevant variables.
Probabilistic graphical models can efficiently represent the structure of such
complex data, but performing these inferences is generally difficult.
Message-passing algorithms, such as belief propagation, are a natural way to
disseminate evidence amongst correlated variables while exploiting the graph
structure, but these algorithms can struggle when the conditional dependency
graphs contain loops. Here we use Graph Neural Networks (GNNs) to learn a
message-passing algorithm that solves these inference tasks. We first show that
the architecture of GNNs is well-matched to inference tasks. We then
demonstrate the efficacy of this inference approach by training GNNs on an
ensemble of graphical models and showing that they substantially outperform
belief propagation on loopy graphs. Our message-passing algorithms generalize
out of the training set to larger graphs and graphs with different structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;KiJung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lisa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1&quot;&gt;Xaq Pitkow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07712">
<title>Causal Inference on Discrete Data via Estimating Distance Correlations. (arXiv:1803.07712v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07712</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we deal with the problem of inferring causal directions when
the data is on discrete domain. By considering the distribution of the cause
$P(X)$ and the conditional distribution mapping cause to effect $P(Y|X)$ as
independent random variables, we propose to infer the causal direction via
comparing the distance correlation between $P(X)$ and $P(Y|X)$ with the
distance correlation between $P(Y)$ and $P(X|Y)$. We infer &quot;$X$ causes $Y$&quot; if
the dependence coefficient between $P(X)$ and $P(Y|X)$ is smaller. Experiments
are performed to show the performance of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Furui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chan_L/0/1/0/all/0/1&quot;&gt;Laiwan Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07738">
<title>Speech Emotion Recognition Considering Local Dynamic Features. (arXiv:1803.07738v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1803.07738</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, increasing attention has been directed to the study of the speech
emotion recognition, in which global acoustic features of an utterance are
mostly used to eliminate the content differences. However, the expression of
speech emotion is a dynamic process, which is reflected through dynamic
durations, energies, and some other prosodic information when one speaks. In
this paper, a novel local dynamic pitch probability distribution feature, which
is obtained by drawing the histogram, is proposed to improve the accuracy of
speech emotion recognition. Compared with most of the previous works using
global features, the proposed method takes advantage of the local dynamic
information conveyed by the emotional speech. Several experiments on Berlin
Database of Emotional Speech are conducted to verify the effectiveness of the
proposed method. The experimental results demonstrate that the local dynamic
information obtained with the proposed method is more effective for speech
emotion recognition than the traditional global features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1&quot;&gt;Haotian Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhilei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longbiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1&quot;&gt;Jianwu Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ruiguo Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07847">
<title>On-demand Relational Concept Analysis. (arXiv:1803.07847v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1803.07847</link>
<description rdf:parseType="Literal">&lt;p&gt;Formal Concept Analysis and its associated conceptual structures have been
used to support exploratory search through conceptual navigation. Relational
Concept Analysis (RCA) is an extension of Formal Concept Analysis to process
relational datasets. RCA and its multiple interconnected structures represent
good candidates to support exploratory search in relational datasets, as they
are enabling navigation within a structure as well as between the connected
structures. However, building the entire structures does not present an
efficient solution to explore a small localised area of the dataset, for
instance to retrieve the closest alternatives to a given query. In these cases,
generating only a concept and its neighbour concepts at each navigation step
appears as a less costly alternative. In this paper, we propose an algorithm to
compute a concept and its neighbourhood in extended concept lattices. The
concepts are generated directly from the relational context family, and possess
both formal and relational attributes. The algorithm takes into account two RCA
scaling operators. We illustrate it on an example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazin_A/0/1/0/all/0/1&quot;&gt;Alexandre Bazin&lt;/a&gt; (LIP6), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonnel_J/0/1/0/all/0/1&quot;&gt;Jessie Carbonnel&lt;/a&gt; (MAREL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huchard_M/0/1/0/all/0/1&quot;&gt;Marianne Huchard&lt;/a&gt; (MAREL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_G/0/1/0/all/0/1&quot;&gt;Giacomo Kahn&lt;/a&gt; (LIMOS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08024">
<title>Stacked Cross Attention for Image-Text Matching. (arXiv:1803.08024v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.08024</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of image-text matching. Inferring the
latent semantic alignment between objects or other salient stuffs (e.g. snow,
sky, lawn) and the corresponding words in sentences allows to capture
fine-grained interplay between vision and language, and makes image-text
matching more interpretable. Prior works either simply aggregate the similarity
of all possible pairs of regions and words without attending differentially to
more and less important words or regions, or use a multi-step attentional
process to capture limited number of semantic alignments which is less
interpretable. In this paper, we present Stacked Cross Attention to discover
the full latent alignments using both image regions and words in sentence as
context and infer the image-text similarity. Our approach achieves the
state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,
our approach outperforms the current best methods by 22.1% in text retrieval
from image query, and 18.2% in image retrieval with text query (based on
Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% and
image retrieval by 16.6% (based on Recall@1 using the 5K test set).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-Huei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1&quot;&gt;Gang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Houdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaodong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.05971">
<title>An Integrated Optimization + Learning Approach to Optimal Dynamic Pricing for the Retailer with Multi-type Customers in Smart Grids. (arXiv:1612.05971v3 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1612.05971</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider a realistic and meaningful scenario in the context
of smart grids where an electricity retailer serves three different types of
customers, i.e., customers with an optimal home energy management system
embedded in their smart meters (C-HEMS), customers with only smart meters
(C-SM), and customers without smart meters (C-NONE). The main objective of this
paper is to support the retailer to make optimal day-ahead dynamic pricing
decisions in such a mixed customer pool. To this end, we propose a two-level
decision-making framework where the retailer acting as upper-level agent
firstly announces its electricity prices of next 24 hours and customers acting
as lower-level agents subsequently schedule their energy usages accordingly.
For the lower level problem, we model the price responsiveness of different
customers according to their unique characteristics. For the upper level
problem, we optimize the dynamic prices for the retailer to maximize its profit
subject to realistic market constraints. The above two-level model is tackled
by genetic algorithms (GA) based distributed optimization methods while its
feasibility and effectiveness are confirmed via simulation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanlin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiao-Jun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dent_C/0/1/0/all/0/1&quot;&gt;Chris J. Dent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1&quot;&gt;Dunwei Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00399">
<title>Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. (arXiv:1711.00399v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00399</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been much discussion of the right to explanation in the EU General
Data Protection Regulation, and its existence, merits, and disadvantages.
Implementing a right to explanation that opens the black box of algorithmic
decision-making faces major legal and technical barriers. Explaining the
functionality of complex algorithmic decision-making systems and their
rationale in specific cases is a technically challenging problem. Some
explanations may offer little meaningful information to data subjects, raising
questions around their value. Explanations of automated decisions need not
hinge on the general public understanding how algorithmic systems function.
Even though such interpretability is of great importance and should be pursued,
explanations can, in principle, be offered without opening the black box.
Looking at explanations as a means to help a data subject act rather than
merely understand, one could gauge the scope and content of explanations
according to the specific goal or action they are intended to support. From the
perspective of individuals affected by automated decision-making, we propose
three aims for explanations: (1) to inform and help the individual understand
why a particular decision was reached, (2) to provide grounds to contest the
decision if the outcome is undesired, and (3) to understand what would need to
change in order to receive a desired result in the future, based on the current
decision-making model. We assess how each of these goals finds support in the
GDPR. We suggest data controllers should offer a particular type of
explanation, unconditional counterfactual explanations, to support these three
aims. These counterfactual explanations describe the smallest change to the
world that can be made to obtain a desirable outcome, or to arrive at the
closest possible world, without needing to explain the internal logic of the
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachter_S/0/1/0/all/0/1&quot;&gt;Sandra Wachter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittelstadt_B/0/1/0/all/0/1&quot;&gt;Brent Mittelstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1&quot;&gt;Chris Russell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03160">
<title>A Formalization of Kant&apos;s Second Formulation of the Categorical Imperative. (arXiv:1801.03160v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03160</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a formalization and computational implementation of the second
formulation of Kant&apos;s categorical imperative. This ethical principle requires
an agent to never treat someone merely as a means but always also as an end.
Here we interpret this principle in terms of how persons are causally affected
by actions. We introduce Kantian causal agency models in which moral patients,
actions, goals, and causal influence are represented, and we show how to
formalize several readings of Kant&apos;s categorical imperative that correspond to
Kant&apos;s concept of strict and wide duties towards oneself and others. Stricter
versions handle cases where an action directly causally affects oneself or
others, whereas the wide version maximizes the number of persons being treated
as an end. We discuss limitations of our formalization by pointing to one of
Kant&apos;s cases that the machinery cannot handle in a satisfying way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindner_F/0/1/0/all/0/1&quot;&gt;Felix Lindner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bentzen_M/0/1/0/all/0/1&quot;&gt;Martin Mose Bentzen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07347">
<title>Optimizing Sponsored Search Ranking Strategy by Deep Reinforcement Learning. (arXiv:1803.07347v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07347</link>
<description rdf:parseType="Literal">&lt;p&gt;Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers&apos; side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users&apos; side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Li He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaipeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07612">
<title>Generative Multi-Agent Behavioral Cloning. (arXiv:1803.07612v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07612</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and study the problem of generative multi-agent behavioral
cloning, where the goal is to learn a generative multi-agent policy from
pre-collected demonstration data. Building upon advances in deep generative
models, we present a hierarchical policy framework that can tractably learn
complex mappings from input states to distributions over multi-agent action
spaces. Our framework is flexible and can incorporate high-level domain
knowledge into the structure of the underlying deep graphical model. For
instance, we can effectively learn low-dimensional structures, such as
long-term goals and team coordination, from data. Thus, an additional benefit
of our hierarchical approach is the ability to plan over multiple time scales
for effective long-term planning. We showcase our approach in an application of
modeling team offensive play from basketball tracking data. We show how to
instantiate our framework to effectively model complex interactions between
basketball players and generate realistic multi-agent trajectories of
basketball gameplay over long time periods. We validate our approach using both
quantitative and qualitative evaluations, including a user study comparison
conducted with professional sports analysts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_E/0/1/0/all/0/1&quot;&gt;Eric Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Stephan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucey_P/0/1/0/all/0/1&quot;&gt;Patrick Lucey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07658">
<title>Graph-based regularization for regression problems with highly-correlated designs. (arXiv:1803.07658v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07658</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse models for high-dimensional linear regression and machine learning
have received substantial attention over the past two decades. Model selection,
or determining which features or covariates are the best explanatory variables,
is critical to the interpretability of a learned model. Much of the current
literature assumes that covariates are only mildly correlated. However, in
modern applications ranging from functional MRI to genome-wide association
studies, covariates are highly correlated and do not exhibit key properties
(such as the restricted eigenvalue condition, RIP, or other related
assumptions). This paper considers a high-dimensional regression setting in
which a graph governs both correlations among the covariates and the similarity
among regression coefficients. Using side information about the strength of
correlations among features, we form a graph with edge weights corresponding to
pairwise covariances. This graph is used to define a graph total variation
regularizer that promotes similar weights for highly correlated features. The
graph structure encapsulated by this regularizer helps precondition correlated
features to yield provably accurate estimates. Using graph-based regularizers
to develop theoretical guarantees for highly-correlated covariates has not been
previously examined. This paper shows how our proposed graph-based
regularization yields mean-squared error guarantees for a broad range of
covariance graph structures and correlation strengths which in many cases are
optimal by imposing additional structure on $\beta^{\star}$ which encourages
\emph{alignment} with the covariance graph. Our proposed approach outperforms
other state-of-the-art methods for highly-correlated design in a variety of
experiments on simulated and real fMRI data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raskutti_G/0/1/0/all/0/1&quot;&gt;Garvesh Raskutti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1&quot;&gt;Rebecca Willett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07679">
<title>Product Characterisation towards Personalisation: Learning Attributes from Unstructured Data to Recommend Fashion Products. (arXiv:1803.07679v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07679</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we describe a solution to tackle a common set of challenges in
e-commerce, which arise from the fact that new products are continually being
added to the catalogue. The challenges involve properly personalising the
customer experience, forecasting demand and planning the product range. We
argue that the foundational piece to solve all of these problems is having
consistent and detailed information about each product, information that is
rarely available or consistent given the multitude of suppliers and types of
products. We describe in detail the architecture and methodology implemented at
ASOS, one of the world&apos;s largest fashion e-commerce retailers, to tackle this
problem. We then show how this quantitative understanding of the products can
be leveraged to improve recommendations in a hybrid recommender system
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cardoso_A/0/1/0/all/0/1&quot;&gt;&amp;#xc2;ngelo Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daolio_F/0/1/0/all/0/1&quot;&gt;Fabio Daolio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vargas_S/0/1/0/all/0/1&quot;&gt;Sa&amp;#xfa;l Vargas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07726">
<title>Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval. (arXiv:1803.07726v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07726</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the problem of solving systems of quadratic equations,
namely, recovering an object of interest
$\mathbf{x}^{\natural}\in\mathbb{R}^{n}$ from $m$ quadratic equations/samples
$y_{i}=(\mathbf{a}_{i}^{\top}\mathbf{x}^{\natural})^{2}$, $1\leq i\leq m$. This
problem, also dubbed as phase retrieval, spans multiple domains including
physical sciences and machine learning.
&lt;/p&gt;
&lt;p&gt;We investigate the efficiency of gradient descent (or Wirtinger flow)
designed for the nonconvex least squares problem. We prove that under Gaussian
designs, gradient descent --- when randomly initialized --- yields an
$\epsilon$-accurate solution in $O\big(\log n+\log(1/\epsilon)\big)$ iterations
given nearly minimal samples, thus achieving near-optimal computational and
sample complexities at once. This provides the first global convergence
guarantee concerning vanilla gradient descent for phase retrieval, without the
need of (i) carefully-designed initialization, (ii) sample splitting, or (iii)
sophisticated saddle-point escaping schemes. All of these are achieved by
exploiting the statistical models in analyzing optimization algorithms, via a
leave-one-out approach that enables the decoupling of certain statistical
dependency between the gradient descent iterates and the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yuejie Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianqing Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Cong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07753">
<title>Data-Driven Sparse System Identification. (arXiv:1803.07753v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1803.07753</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the system identification porblem for sparse linear
time-invariant systems. We propose a sparsity promoting Lasso-type estimator to
identify the dynamics of the system with only a limited number of input-state
data samples. Using contemporary results on high-dimensional statistics, we
prove that $\Omega(k_{\max}\log(m+n))$ data samples are enough to reliably
estimate the system dynamics, where $n$ and $m$ are the number of states and
inputs, respectively, and $k_{\max}$ is the maximum number of nonzero elements
in the rows of input and state matrices. The number of samples in the developed
estimator is significantly smaller than the dimension of the problem for sparse
systems, and yet it offers a small estimation error entry-wise. Furthermore, we
show that, unlike the recently celebrated least-squares estimators for system
identification problems, the method developed in this work is capable of
\textit{exact recovery} of the underlying sparsity structure of the system with
the aforementioned number of data samples. Extensive case studies on
synthetically generated systems and physical mass-spring networks are offered
to demonstrate the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fattahi_S/0/1/0/all/0/1&quot;&gt;Salar Fattahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1&quot;&gt;Somayeh Sojoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07819">
<title>Some Theoretical Properties of GANs. (arXiv:1803.07819v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07819</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) are a class of generative algorithms
that have been shown to produce state-of-the art samples, especially in the
domain of image creation. The fundamental principle of GANs is to approximate
the unknown distribution of a given data set by optimizing an objective
function through an adversarial game between a family of generators and a
family of discriminators. In this paper, we offer a better theoretical
understanding of GANs by analyzing some of their mathematical and statistical
properties. We study the deep connection between the adversarial principle
underlying GANs and the Jensen-Shannon divergence, together with some
optimality characteristics of the problem. An analysis of the role of the
discriminator family via approximation arguments is also provided. In addition,
taking a statistical point of view, we study the large sample properties of the
estimated distribution and prove in particular a central limit theorem. Some of
our results are illustrated with simulated examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Biau_G/0/1/0/all/0/1&quot;&gt;G. Biau&lt;/a&gt; (LPSM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cadre_B/0/1/0/all/0/1&quot;&gt;B. Cadre&lt;/a&gt; (ENS Rennes), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sangnier_M/0/1/0/all/0/1&quot;&gt;M. Sangnier&lt;/a&gt; (LPSM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tanielian_U/0/1/0/all/0/1&quot;&gt;U. Tanielian&lt;/a&gt; (LPSM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07821">
<title>Multi-view Metric Learning in Vector-valued Kernel Spaces. (arXiv:1803.07821v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07821</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of metric learning for multi-view data and present a
novel method for learning within-view as well as between-view metrics in
vector-valued kernel spaces, as a way to capture multi-modal structure of the
data. We formulate two convex optimization problems to jointly learn the metric
and the classifier or regressor in kernel feature spaces. An iterative
three-step multi-view metric learning algorithm is derived from the
optimization problems. In order to scale the computation to large training
sets, a block-wise Nystr{\&quot;o}m approximation of the multi-view kernel matrix is
introduced. We justify our approach theoretically and experimentally, and show
its performance on real-world datasets against relevant state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huusari_R/0/1/0/all/0/1&quot;&gt;Riikka Huusari&lt;/a&gt; (LIS, QARMA, AMU), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadri_H/0/1/0/all/0/1&quot;&gt;Hachem Kadri&lt;/a&gt; (QARMA, LIS, AMU), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capponi_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;cile Capponi&lt;/a&gt; (QARMA, LIS, AMU)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07859">
<title>Efficient Structure Learning and Sampling of Bayesian Networks. (arXiv:1803.07859v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07859</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian networks are probabilistic graphical models widely employed to
understand dependencies in high dimensional data, and even to facilitate causal
discovery. Learning the underlying network structure, which is encoded as a
directed acyclic graph (DAG) is highly challenging mainly due to the vast
number of possible networks. Efforts have focussed on two fronts: constraint
based methods that perform conditional independence tests to exclude edges and
score and search approaches which explore the DAG space with greedy or MCMC
schemes. Here we synthesise these two fields in a novel hybrid method which
reduces the complexity of MCMC approaches to that of a constraint based method.
Individual steps in the MCMC scheme only require simple table lookups so that
very long chains can be efficiently obtained. Furthermore, the scheme includes
an iterative procedure to correct for errors from the conditional independence
tests. The algorithm not only offers markedly superior performance to
alternatives, but DAGs can also be sampled from the posterior distribution
enabling full Bayesian modelling averaging for much larger Bayesian networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1&quot;&gt;Jack Kuipers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suter_P/0/1/0/all/0/1&quot;&gt;Polina Suter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1&quot;&gt;Giusi Moffa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07868">
<title>Scalable Generalized Dynamic Topic Models. (arXiv:1803.07868v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07868</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic topic models (DTMs) model the evolution of prevalent themes in
literature, online media, and other forms of text over time. DTMs assume that
word co-occurrence statistics change continuously and therefore impose
continuous stochastic process priors on their model parameters. These dynamical
priors make inference much harder than in regular topic models, and also limit
scalability. In this paper, we present several new results around DTMs. First,
we extend the class of tractable priors from Wiener processes to the generic
class of Gaussian processes (GPs). This allows us to explore topics that
develop smoothly over time, that have a long-term memory or are temporally
concentrated (for event detection). Second, we show how to perform scalable
approximate inference in these models based on ideas around stochastic
variational inference and sparse Gaussian processes. This way we can train a
rich family of DTMs to massive data. Our experiments on several large-scale
datasets show that our generalized model allows us to find interesting patterns
that were not accessible by previous approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jahnichen_P/0/1/0/all/0/1&quot;&gt;Patrick J&amp;#xe4;hnichen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wenzel_F/0/1/0/all/0/1&quot;&gt;Florian Wenzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kloft_M/0/1/0/all/0/1&quot;&gt;Marius Kloft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07879">
<title>An Unsupervised Multivariate Time Series Kernel Approach for Identifying Patients with Surgical Site Infection from Blood Samples. (arXiv:1803.07879v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07879</link>
<description rdf:parseType="Literal">&lt;p&gt;A large fraction of the electronic health records consists of clinical
measurements collected over time, such as blood tests, which provide important
information about the health status of a patient. These sequences of clinical
measurements are naturally represented as time series, characterized by
multiple variables and the presence of missing data, which complicate analysis.
In this work, we propose a surgical site infection detection framework for
patients undergoing colorectal cancer surgery that is completely unsupervised,
hence alleviating the problem of getting access to labelled training data. The
framework is based on powerful kernels for multivariate time series that
account for missing data when computing similarities. Our approach show
superior performance compared to baselines that have to resort to imputation
techniques and performs comparable to a supervised classification baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mikalsen_K/0/1/0/all/0/1&quot;&gt;Karl &amp;#xd8;yvind Mikalsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soguero_Ruiz_C/0/1/0/all/0/1&quot;&gt;Cristina Soguero-Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bianchi_F/0/1/0/all/0/1&quot;&gt;Filippo Maria Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Revhaug_A/0/1/0/all/0/1&quot;&gt;Arthur Revhaug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07952">
<title>An Exercise Fatigue Detection Model Based on Machine Learning Methods. (arXiv:1803.07952v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07952</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposes an exercise fatigue detection model based on real-time
clinical data which includes time domain analysis, frequency domain analysis,
detrended fluctuation analysis, approximate entropy, and sample entropy.
Furthermore, this study proposed a feature extraction method which is combined
with an analytical hierarchy process to analyze and extract critical features.
Finally, machine learning algorithms were adopted to analyze the data of each
feature for the detection of exercise fatigue. The practical experimental
results showed that the proposed exercise fatigue detection model and feature
extraction method could precisely detect the level of exercise fatigue, and the
accuracy of exercise fatigue detection could be improved up to 98.65%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Ming-Yen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chi-Hua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lo_C/0/1/0/all/0/1&quot;&gt;Chi-Chun Lo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07954">
<title>Resilient Monotone Sequential Maximization. (arXiv:1803.07954v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07954</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications in machine learning, optimization, and control require the
sequential selection of a few system elements, such as sensors, data, or
actuators, to optimize the system performance across multiple time steps.
However, in failure-prone and adversarial environments, sensors get attacked,
data get deleted, and actuators fail. Thence, traditional sequential design
paradigms become insufficient and, in contrast, resilient sequential designs
that adapt against system-wide attacks, deletions, or failures become
important. In general, resilient sequential design problems are computationally
hard. Also, even though they often involve objective functions that are
monotone and (possibly) submodular, no scalable approximation algorithms are
known for their solution. In this paper, we provide the first scalable
algorithm, that achieves the following characteristics: system-wide resiliency,
i.e., the algorithm is valid for any number of denial-of-service attacks,
deletions, or failures; adaptiveness, i.e., at each time step, the algorithm
selects system elements based on the history of inflicted attacks, deletions,
or failures; and provable approximation performance, i.e., the algorithm
guarantees for monotone objective functions a solution close to the optimal. We
quantify the algorithm&apos;s approximation performance using a notion of curvature
for monotone (not necessarily submodular) set functions. Finally, we support
our theoretical analyses with simulated experiments, by considering a
control-aware sensor scheduling scenario, namely, sensing-constrained robot
navigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07964">
<title>Stochastic Learning under Random Reshuffling. (arXiv:1803.07964v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07964</link>
<description rdf:parseType="Literal">&lt;p&gt;In empirical risk optimization, it has been observed that stochastic gradient
implementations that rely on random reshuffling of the data achieve better
performance than implementations that rely on sampling the data uniformly.
Recent works have pursued justifications for this behavior by examining the
convergence rate of the learning process under diminishing step-sizes. This
work focuses on the constant step-size case. In this case, convergence is
guaranteed to a small neighborhood of the optimizer albeit at a linear rate.
The analysis establishes analytically that random reshuffling outperforms
uniform sampling by showing explicitly that iterates approach a smaller
neighborhood of size $O(\mu^2)$ around the minimizer rather than $O(\mu)$.
Furthermore, we derive an analytical expression for the steady-state
mean-square-error performance of the algorithm, which helps clarify in greater
detail the differences between sampling with and without replacement. We also
explain the periodic behavior that is observed in random reshuffling
implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_B/0/1/0/all/0/1&quot;&gt;Bicheng Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlaski_S/0/1/0/all/0/1&quot;&gt;Stefan Vlaski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_A/0/1/0/all/0/1&quot;&gt;Ali H. Sayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07976">
<title>A Survey on Application of Machine Learning Techniques in Optical Networks. (arXiv:1803.07976v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1803.07976</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, the amount of data that can be retrieved from communications networks
is extremely high and diverse (e.g., data regarding users behavior, traffic
traces, network alarms, signal quality indicators, etc.). Advanced mathematical
tools are required to extract useful information from this large set of network
data. In particular, Machine Learning (ML) is regarded as a promising
methodological area to perform network-data analysis and enable, e.g.,
automatized network self-configuration and fault management. In this survey we
classify and describe relevant studies dealing with the applications of ML to
optical communications and networking. Optical networks and system are facing
an unprecedented growth in terms of complexity due to the introduction of a
huge number of adjustable parameters (such as routing configurations,
modulation format, symbol rate, coding schemes, etc.), mainly due to the
adoption of, among the others, coherent transmission/reception technology,
advanced digital signal processing and to the presence of nonlinear effects in
optical fiber systems. Although a good number of research papers have appeared
in the last years, the application of ML to optical networks is still in its
early stage. In this survey we provide an introductory reference for
researchers and practitioners interested in this field. To stimulate further
work in this area, we conclude the paper proposing new possible research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musumeci_F/0/1/0/all/0/1&quot;&gt;Francesco Musumeci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rottondi_C/0/1/0/all/0/1&quot;&gt;Cristina Rottondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_A/0/1/0/all/0/1&quot;&gt;Avishek Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_I/0/1/0/all/0/1&quot;&gt;Irene Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zibar_D/0/1/0/all/0/1&quot;&gt;Darko Zibar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1&quot;&gt;Marco Ruffini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tornatore_M/0/1/0/all/0/1&quot;&gt;Massimo Tornatore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07994">
<title>Adversarial Defense based on Structure-to-Signal Autoencoders. (arXiv:1803.07994v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07994</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attack methods have demonstrated the fragility of deep neural
networks. Their imperceptible perturbations are frequently able fool
classifiers into potentially dangerous misclassifications. We propose a novel
way to interpret adversarial perturbations in terms of the effective input
signal that classifiers actually use. Based on this, we apply specially trained
autoencoders, referred to as S2SNets, as defense mechanism. They follow a
two-stage training scheme: first unsupervised, followed by a fine-tuning of the
decoder, using gradients from an existing classifier. S2SNets induce a shift in
the distribution of gradients propagated through them, stripping them from
class-dependent signal. We analyze their robustness against several white-box
and gray-box scenarios on the large ImageNet dataset. Our approach reaches
comparable resilience in white-box attack scenarios as other state-of-the-art
defenses in gray-box scenarios. We further analyze the relationships of
AlexNet, VGG 16, ResNet 50 and Inception v3 in adversarial space, and found
that VGG 16 is the easiest to fool, while perturbations from ResNet 50 are the
most transferable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Folz_J/0/1/0/all/0/1&quot;&gt;Joachim Folz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1&quot;&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1&quot;&gt;Joern Hees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1&quot;&gt;Damian Borth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08000">
<title>Boosting Random Forests to Reduce Bias; One-Step Boosted Forest and its Variance Estimate. (arXiv:1803.08000v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.08000</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose using the principle of boosting to reduce the bias
of a random forest prediction in the regression setting. From the original
random forest fit we extract the residuals and then fit another random forest
to these residuals. We call the sum of these two random forests a
\textit{one-step boosted forest}. We have shown with simulated and real data
that the one-step boosted forest has a reduced bias compared to the original
random forest. The paper also provides a variance estimate of the one-step
boosted forest by an extension of the infinitesimal Jackknife estimator. Using
this variance estimate we can construct prediction intervals for the boosted
forest and we show that they have good coverage probabilities. Combining the
bias reduction and the variance estimate we have shown that the one-step
boosted forest has a significant reduction in predictive mean squared error and
thus an improvement in predictive performance. When applied on datasets from
the UCI database we have empirically proven that the one-step boosted forest
performs better than the random forest and gradient boosting machine
algorithms. Theoretically we can also extend such a boosting process to more
than one step and the same principles outlined in this paper can be used to
find variance estimates for such predictors. Such boosting will reduce bias
even further but it risks over-fitting and also increases the computational
burden.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosal_I/0/1/0/all/0/1&quot;&gt;Indrayudh Ghosal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1&quot;&gt;Giles Hooker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08021">
<title>Error Estimation for Randomized Least-Squares Algorithms via the Bootstrap. (arXiv:1803.08021v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.08021</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the course of the past decade, a variety of randomized algorithms have
been proposed for computing approximate least-squares (LS) solutions in
large-scale settings. A longstanding practical issue is that, for any given
input, the user rarely knows the actual error of an approximate solution
(relative to the exact solution). Likewise, it is difficult for the user to
know precisely how much computation is needed to achieve the desired error
tolerance. Consequently, the user often appeals to worst-case error bounds that
tend to offer only qualitative guidance. As a more practical alternative, we
propose a bootstrap method to compute a posteriori error estimates for
randomized LS algorithms. These estimates permit the user to numerically assess
the error of a given solution, and to predict how much work is needed to
improve a &quot;preliminary&quot; solution. In addition, we provide theoretical
consistency results for the method, which are the first such results in this
context (to the best of our knowledge). From a practical standpoint, the method
also has considerable flexibility, insofar as it can be applied to several
popular sketching algorithms, as well as a variety of error metrics. Moreover,
the extra step of error estimation does not add much cost to an underlying
sketching algorithm. Finally, we demonstrate the effectiveness of the method
with empirical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopes_M/0/1/0/all/0/1&quot;&gt;Miles E. Lopes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shusen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1509.04634">
<title>Modeling and interpolation of the ambient magnetic field by Gaussian processes. (arXiv:1509.04634v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1509.04634</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomalies in the ambient magnetic field can be used as features in indoor
positioning and navigation. By using Maxwell&apos;s equations, we derive and present
a Bayesian non-parametric probabilistic modeling approach for interpolation and
extrapolation of the magnetic field. We model the magnetic field components
jointly by imposing a Gaussian process (GP) prior on the latent scalar
potential of the magnetic field. By rewriting the GP model in terms of a
Hilbert space representation, we circumvent the computational pitfalls
associated with GP modeling and provide a computationally efficient and
physically justified modeling tool for the ambient magnetic field. The model
allows for sequential updating of the estimate and time-dependent changes in
the magnetic field. The model is shown to work well in practice in different
applications: we demonstrate mapping of the magnetic field both with an
inexpensive Raspberry Pi powered robot and on foot using a standard smartphone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kok_M/0/1/0/all/0/1&quot;&gt;Manon Kok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahlstrom_N/0/1/0/all/0/1&quot;&gt;Niklas Wahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkka_S/0/1/0/all/0/1&quot;&gt;Simo S&amp;#xe4;rkk&amp;#xe4;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.05988">
<title>Continuum directions for supervised dimension reduction. (arXiv:1606.05988v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1606.05988</link>
<description rdf:parseType="Literal">&lt;p&gt;Dimension reduction of multivariate data supervised by auxiliary information
is considered. A series of basis for dimension reduction is obtained as
minimizers of a novel criterion. The proposed method is akin to continuum
regression, and the resulting basis is called continuum directions. With a
presence of binary supervision data, these directions continuously bridge the
principal component, mean difference and linear discriminant directions, thus
ranging from unsupervised to fully supervised dimension reduction.
High-dimensional asymptotic studies of continuum directions for binary
supervision reveal several interesting facts. The conditions under which the
sample continuum directions are inconsistent, but their classification
performance is good, are specified. While the proposed method can be directly
used for binary and multi-category classification, its generalizations to
incorporate any form of auxiliary data are also presented. The proposed method
enjoys fast computation, and the performance is better or on par with more
computer-intensive alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Sungkyu Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.06176">
<title>Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks. (arXiv:1704.06176v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1704.06176</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) has been proposed as a complimentary method
to measure bone quality and assess fracture risk. However, manual segmentation
of MR images of bone is time-consuming, limiting the use of MRI measurements in
the clinical practice. The purpose of this paper is to present an automatic
proximal femur segmentation method that is based on deep convolutional neural
networks (CNNs). This study had institutional review board approval and written
informed consent was obtained from all subjects. A dataset of volumetric
structural MR images of the proximal femur from 86 subject were
manually-segmented by an expert. We performed experiments by training two
different CNN architectures with multiple number of initial feature maps and
layers, and tested their segmentation performance against the gold standard of
manual segmentations using four-fold cross-validation. Automatic segmentation
of the proximal femur achieved a high dice similarity score of 0.94$\pm$0.05
with precision = 0.95$\pm$0.02, and recall = 0.94$\pm$0.08 using a CNN
architecture based on 3D convolution exceeding the performance of 2D CNNs. The
high segmentation accuracy provided by CNNs has the potential to help bring the
use of structural MRI measurements of bone quality into clinical practice for
management of osteoporosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deniz_C/0/1/0/all/0/1&quot;&gt;Cem M. Deniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Siyuan Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallyburton_S/0/1/0/all/0/1&quot;&gt;Spencer Hallyburton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welbeck_A/0/1/0/all/0/1&quot;&gt;Arakua Welbeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honig_S/0/1/0/all/0/1&quot;&gt;Stephen Honig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1&quot;&gt;Gregory Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07787">
<title>Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic Programs. (arXiv:1708.07787v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07787</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a dynamic mechanism for the solution of analytically-tractable
substructure in probabilistic programs, using conjugate priors and affine
transformations to reduce variance in Monte Carlo estimators. For inference
with Sequential Monte Carlo, this automatically yields improvements such as
locally-optimal proposals and Rao-Blackwellization. The mechanism maintains a
directed graph alongside the running program that evolves dynamically as
operations are triggered upon it. Nodes of the graph represent random
variables, edges the analytically-tractable relationships between them. Random
variables remain in the graph for as long as possible, to be sampled only when
they are used by the program in a way that cannot be resolved analytically. In
the meantime, they are conditioned on as many observations as possible. We
demonstrate the mechanism with a few pedagogical examples, as well as a
linear-nonlinear state-space model with simulated data, and an epidemiological
model with real data of a dengue outbreak in Micronesia. In all cases one or
more variables are automatically marginalized out to significantly reduce
variance in estimates of the marginal likelihood, in the final case
facilitating a random-weight or pseudo-marginal-type importance sampler for
parameter estimation. We have implemented the approach in Anglican and a new
probabilistic programming language called Birch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murray_L/0/1/0/all/0/1&quot;&gt;Lawrence M. Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lunden_D/0/1/0/all/0/1&quot;&gt;Daniel Lund&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kudlicka_J/0/1/0/all/0/1&quot;&gt;Jan Kudlicka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Broman_D/0/1/0/all/0/1&quot;&gt;David Broman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05776">
<title>Nonsmooth Frank-Wolfe using Uniform Affine Approximations. (arXiv:1710.05776v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05776</link>
<description rdf:parseType="Literal">&lt;p&gt;Frank-Wolfe methods (FW) have gained significant interest in the machine
learning community due to its ability to efficiently solve large problems that
admit a sparse structure (e.g. sparse vectors and low-rank matrices). However
the performance of the existing FW method hinges on the quality of the linear
approximation. This typically restricts FW to smooth functions for which the
approximation quality, indicated by a global curvature measure, is reasonably
good.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a modified FW algorithm amenable to nonsmooth
functions by optimizing for approximation quality over all affine
approximations given a neighborhood of interest. We analyze theoretical
properties of the proposed algorithm and demonstrate that it overcomes many
issues associated with existing methods in the context of nonsmooth low-rank
matrix estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheung_E/0/1/0/all/0/1&quot;&gt;Edward Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuying Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10345">
<title>The Implicit Bias of Gradient Descent on Separable Data. (arXiv:1710.10345v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10345</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that gradient descent on an unregularized logistic regression
problem, for linearly separable datasets, converges to the direction of the
max-margin (hard margin SVM) solution. The result generalizes also to other
monotone decreasing loss functions with an infimum at infinity, to multi-class
problems, and to training a weight layer in a deep network in a certain
restricted setting. Furthermore, we show this convergence is very slow, and
only logarithmic in the convergence of the loss itself. This can help explain
the benefit of continuing to optimize the logistic or cross-entropy loss even
after the training error is zero and the training loss is extremely small, and,
as we show, even if the validation loss increases. Our methodology can also aid
in understanding implicit regularization in more complex models and with other
optimization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoffer_E/0/1/0/all/0/1&quot;&gt;Elad Hoffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nacson_M/0/1/0/all/0/1&quot;&gt;Mor Shpigel Nacson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gunasekar_S/0/1/0/all/0/1&quot;&gt;Suriya Gunasekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srebro_N/0/1/0/all/0/1&quot;&gt;Nathan Srebro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07318">
<title>Predictor Variable Prioritization in Nonlinear Models: A Genetic Association Case Study. (arXiv:1801.07318v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07318</link>
<description rdf:parseType="Literal">&lt;p&gt;The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel,
interpretable, and computationally efficient way to summarize the relative
importance of predictor variables. Methodologically, we develop the &quot;RelATive
cEntrality&quot; (RATE) measure to prioritize candidate genetic variants that are
not just marginally important, but whose associations also stem from
significant covarying relationships with other variants in the data. We
illustrate RATE through Bayesian Gaussian process regression, but the
methodological innovations apply to other nonlinear methods. It is known that
nonlinear models often exhibit greater predictive accuracy than linear models,
particularly for phenotypes generated by complex genetic architectures. With
detailed simulations and an Arabidopsis thaliana QTL mapping study, we show
that applying RATE enables an explanation for this improved performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Crawford_L/0/1/0/all/0/1&quot;&gt;Lorin Crawford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flaxman_S/0/1/0/all/0/1&quot;&gt;Seth R. Flaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Runcie_D/0/1/0/all/0/1&quot;&gt;Daniel E. Runcie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+West_M/0/1/0/all/0/1&quot;&gt;Mike West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04893">
<title>Uncertainty Estimation via Stochastic Batch Normalization. (arXiv:1802.04893v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04893</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate Batch Normalization technique and propose its
probabilistic interpretation. We propose a probabilistic model and show that
Batch Normalization maximazes the lower bound of its marginalized
log-likelihood. Then, according to the new probabilistic model, we design an
algorithm which acts consistently during train and test. However, inference
becomes computationally inefficient. To reduce memory and computational cost,
we propose Stochastic Batch Normalization -- an efficient approximation of
proper inference procedure. This method provides us with a scalable uncertainty
estimation technique. We demonstrate the performance of Stochastic Batch
Normalization on popular architectures (including deep convolutional
architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Atanov_A/0/1/0/all/0/1&quot;&gt;Andrei Atanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1&quot;&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neklyudov_K/0/1/0/all/0/1&quot;&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;</dc:creator>
</item></rdf:RDF>