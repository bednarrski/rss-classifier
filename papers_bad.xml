<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.10371"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02552"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02838"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02850"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05407"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05250"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00442"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11805"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02480"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02602"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02610"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02766"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02814"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.00520"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08873"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01743"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08197"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01357"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1703.10371">
<title>Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks. (arXiv:1703.10371v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1703.10371</link>
<description rdf:parseType="Literal">&lt;p&gt;Biological plastic neural networks are systems of extraordinary computational
capabilities shaped by evolution, development, and lifetime learning. The
interplay of these elements leads to the emergence of adaptive behavior and
intelligence. Inspired by such intricate natural phenomena, Evolved Plastic
Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed
plastic neural networks with a large variety of dynamics, architectures, and
plasticity rules: these artificial systems are composed of inputs, outputs, and
plastic components that change in response to experiences in an environment.
These systems may autonomously discover novel adaptive algorithms, and lead to
hypotheses on the emergence of biological adaptation. EPANNs have seen
considerable progress over the last two decades. Current scientific and
technological advances in artificial neural networks are now setting the
conditions for radically new approaches and results. In particular, the
limitations of hand-designed networks could be overcome by more flexible and
innovative solutions. This paper brings together a variety of inspiring ideas
that define the field of EPANNs. The main methods and results are reviewed.
Finally, new opportunities and developments are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltoggio_A/0/1/0/all/0/1&quot;&gt;Andrea Soltoggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1&quot;&gt;Sebastian Risi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05794">
<title>RAPIDNN: In-Memory Deep Neural Network Acceleration Framework. (arXiv:1806.05794v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05794</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNN) have demonstrated effectiveness for various
applications such as image processing, video segmentation, and speech
recognition. Running state-of-theart DNNs on current systems mostly relies on
either generalpurpose processors, ASIC designs, or FPGA accelerators, all of
which suffer from data movements due to the limited onchip memory and data
transfer bandwidth. In this work, we propose a novel framework, called RAPIDNN,
which processes all DNN operations within the memory to minimize the cost of
data movement. To enable in-memory processing, RAPIDNN reinterprets a DNN model
and maps it into a specialized accelerator, which is designed using
non-volatile memory blocks that model four fundamental DNN operations, i.e.,
multiplication, addition, activation functions, and pooling. The framework
extracts representative operands of a DNN model, e.g., weights and input
values, using clustering methods to optimize the model for in-memory
processing. Then, it maps the extracted operands and their precomputed results
into the accelerator memory blocks. At runtime, the accelerator identifies
computation results based on efficient in-memory search capability which also
provides tunability of approximation to further improve computation efficiency.
Our evaluation shows that RAPIDNN achieves 68.4x, 49.5x energy efficiency
improvement and 48.1x, 10.9x speedup as compared to ISAAC and PipeLayer, the
state-of-the-art DNN accelerators, while ensuring less than 0.3% of quality
loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imani_M/0/1/0/all/0/1&quot;&gt;Mohsen Imani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1&quot;&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeseong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Saransh Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosing_T/0/1/0/all/0/1&quot;&gt;Tajana Rosing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02541">
<title>Efficient Multi-Robot Coverage of a Known Environment. (arXiv:1808.02541v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.02541</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the complete area coverage problem of a known
environment by multiple-robots. Complete area coverage is the problem of moving
an end-effector over all available space while avoiding existing obstacles. In
such tasks, using multiple robots can increase the efficiency of the area
coverage in terms of minimizing the operational time and increase the
robustness in the face of robot attrition. Unfortunately, the problem of
finding an optimal solution for such an area coverage problem with multiple
robots is known to be NP-complete. In this paper we present two approximation
heuristics for solving the multi-robot coverage problem. The first solution
presented is a direct extension of an efficient single robot area coverage
algorithm, based on an exact cellular decomposition. The second algorithm is a
greedy approach that divides the area into equal regions and applies an
efficient single-robot coverage algorithm to each region. We present
experimental results for two algorithms. Results indicate that our approaches
provide good coverage distribution between robots and minimize the workload per
robot, meanwhile ensuring complete coverage of the area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karapetyan_N/0/1/0/all/0/1&quot;&gt;Nare Karapetyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benson_K/0/1/0/all/0/1&quot;&gt;Kelly Benson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKinney_C/0/1/0/all/0/1&quot;&gt;Chris McKinney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taslakian_P/0/1/0/all/0/1&quot;&gt;Perouz Taslakian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekleitis_I/0/1/0/all/0/1&quot;&gt;Ioannis Rekleitis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02552">
<title>Multi-robot Dubins Coverage with Autonomous Surface Vehicles. (arXiv:1808.02552v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.02552</link>
<description rdf:parseType="Literal">&lt;p&gt;In large scale coverage operations, such as marine exploration or aerial
monitoring, single robot approaches are not ideal, as they may take too long to
cover a large area. In such scenarios, multi-robot approaches are preferable.
Furthermore, several real world vehicles are non-holonomic, but can be modeled
using Dubins vehicle kinematics. This paper focuses on environmental monitoring
of aquatic environments using Autonomous Surface Vehicles (ASVs). In
particular, we propose a novel approach for solving the problem of complete
coverage of a known environment by a multi-robot team consisting of Dubins
vehicles. It is worth noting that both multi-robot coverage and Dubins vehicle
coverage are NP-complete problems. As such, we present two heuristics methods
based on a variant of the traveling salesman problem -- k-TSP -- formulation
and clustering algorithms that efficiently solve the problem. The proposed
methods are tested both in simulations to assess their scalability and with a
team of ASVs operating on a lake to ensure their applicability in real world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karapetyan_N/0/1/0/all/0/1&quot;&gt;Nare Karapetyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moulton_J/0/1/0/all/0/1&quot;&gt;Jason Moulton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;Jeremy S. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Alberto Quattrini Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OKane_J/0/1/0/all/0/1&quot;&gt;Jason M. O&amp;#x27;Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekleitis_I/0/1/0/all/0/1&quot;&gt;Ioannis Rekleitis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02608">
<title>End-to-end Speech Recognition with Word-based RNN Language Models. (arXiv:1808.02608v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.02608</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the impact of word-based RNN language models
(RNN-LMs) on the performance of end-to-end automatic speech recognition (ASR).
In our prior work, we have proposed a multi-level LM, in which character-based
and word-based RNN-LMs are combined in hybrid CTC/attention-based ASR. Although
this multi-level approach achieves significant error reduction in the Wall
Street Journal (WSJ) task, two different LMs need to be trained and used for
decoding, which increase the computational cost and memory usage. In this
paper, we further propose a novel word-based RNN-LM, which allows us to decode
with only the word-based LM, where it provides look-ahead word probabilities to
predict next characters instead of the character-based LM, leading competitive
accuracy with less computation compared to the multi-level LM. We demonstrate
the efficacy of the word-based RNN-LMs using a larger corpus, LibriSpeech, in
addition to WSJ we used in the prior work. Furthermore, we show that the
proposed model achieves 5.1 %WER for WSJ Eval&apos;92 test set when the vocabulary
size is increased, which is the best WER reported for end-to-end ASR systems on
this benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1&quot;&gt;Takaaki Hori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaejin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02636">
<title>Cognitive system to achieve human-level accuracy in automated assignment of helpdesk email tickets. (arXiv:1808.02636v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.02636</link>
<description rdf:parseType="Literal">&lt;p&gt;Ticket assignment/dispatch is a crucial part of service delivery business
with lot of scope for automation and optimization. In this paper, we present an
end-to-end automated helpdesk email ticket assignment system, which is also
offered as a service. The objective of the system is to determine the nature of
the problem mentioned in an incoming email ticket and then automatically
dispatch it to an appropriate resolver group (or team) for resolution.
&lt;/p&gt;
&lt;p&gt;The proposed system uses an ensemble classifier augmented with a configurable
rule engine. While design of classifier that is accurate is one of the main
challenges, we also need to address the need of designing a system that is
robust and adaptive to changing business needs. We discuss some of the main
design challenges associated with email ticket assignment automation and how we
solve them. The design decisions for our system are driven by high accuracy,
coverage, business continuity, scalability and optimal usage of computational
resources.
&lt;/p&gt;
&lt;p&gt;Our system has been deployed in production of three major service providers
and currently assigning over 40,000 emails per month, on an average, with an
accuracy close to 90% and covering at least 90% of email tickets. This
translates to achieving human-level accuracy and results in a net saving of
about 23000 man-hours of effort per annum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1&quot;&gt;Atri Mandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malhotra_N/0/1/0/all/0/1&quot;&gt;Nikhil Malhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1&quot;&gt;Shivali Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Anupama Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhara_G/0/1/0/all/0/1&quot;&gt;Giriprasad Sridhara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02838">
<title>On the Effect of Task-to-Worker Assignment in Distributed Computing Systems with Stragglers. (arXiv:1808.02838v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1808.02838</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the expected completion time of some recently proposed algorithms
for distributed computing which redundantly assign computing tasks to multiple
machines in order to tolerate a certain number of machine failures. We
analytically show that not only the amount of redundancy but also the
task-to-machine assignments affect the latency in a distributed system. We
study systems with a fixed number of computing tasks that are split in possibly
overlapping batches, and independent exponentially distributed machine service
times. We show that, for such systems, the uniform replication of non-
overlapping (disjoint) batches of computing tasks achieves the minimum expected
computing time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behrouzi_Far_A/0/1/0/all/0/1&quot;&gt;Amir Behrouzi-Far&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soljanin_E/0/1/0/all/0/1&quot;&gt;Emina Soljanin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02850">
<title>Relaxing and Restraining Queries for OBDA. (arXiv:1808.02850v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1808.02850</link>
<description rdf:parseType="Literal">&lt;p&gt;In ontology-based data access (OBDA), ontologies have been successfully
employed for querying possibly unstructured and incomplete data. In this paper,
we advocate using ontologies not only to formulate queries and compute their
answers, but also for modifying queries by relaxing or restraining them, so
that they can retrieve either more or less answers over a given dataset.
Towards this goal, we first illustrate that some domain knowledge that could be
naturally leveraged in OBDA can be expressed using complex role inclusions
(CRI). Queries over ontologies with CRI are not first-order (FO) rewritable in
general. We propose an extension of DL-Lite with CRI, and show that conjunctive
queries over ontologies in this extension are FO rewritable. Our main
contribution is a set of rules to relax and restrain conjunctive queries (CQs).
Firstly, we define rules that use the ontology to produce CQs that are
relaxations/restrictions over any dataset. Secondly, we introduce a set of
data-driven rules, that leverage patterns in the current dataset, to obtain
more fine-grained relaxations and restrictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andresel_M/0/1/0/all/0/1&quot;&gt;Medina Andre&amp;#x15f;el&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibanez_Garcia_Y/0/1/0/all/0/1&quot;&gt;Yazmin Ib&amp;#xe1;&amp;#xf1;ez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_M/0/1/0/all/0/1&quot;&gt;Magdalena Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simkus_M/0/1/0/all/0/1&quot;&gt;Mantas &amp;#x160;imkus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06892">
<title>Learning to select computations. (arXiv:1711.06892v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06892</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficient use of limited computational resources is an essential
ingredient of intelligence. Selecting computations optimally according to
rational metareasoning would achieve this, but this is computationally
intractable. Inspired by psychology and neuroscience, we propose the first
concrete and domain-general learning algorithm for approximating the optimal
selection of computations: Bayesian metalevel policy search (BMPS). We derive
this general, sample-efficient search algorithm for a computation-selecting
metalevel policy based on the insight that the value of information lies
between the myopic value of information and the value of perfect information.
We evaluate BMPS on three increasingly difficult metareasoning problems: when
to terminate computation, how to allocate computation between competing
options, and planning. Across all three domains, BMPS achieved near-optimal
performance and compared favorably to previously proposed metareasoning
heuristics. Finally, we demonstrate the practical utility of BMPS in an
emergency management scenario, even accounting for the overhead of
metareasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Callaway_F/0/1/0/all/0/1&quot;&gt;Frederick Callaway&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gul_S/0/1/0/all/0/1&quot;&gt;Sayan Gul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_P/0/1/0/all/0/1&quot;&gt;Paul M. Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lieder_F/0/1/0/all/0/1&quot;&gt;Falk Lieder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08365">
<title>Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising. (arXiv:1802.08365v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08365</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiujun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qing Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05407">
<title>Averaging Weights Leads to Wider Optima and Better Generalization. (arXiv:1803.05407v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05407</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are typically trained by optimizing a loss function with
an SGD variant, in conjunction with a decaying learning rate, until
convergence. We show that simple averaging of multiple points along the
trajectory of SGD, with a cyclical or constant learning rate, leads to better
generalization than conventional training. We also show that this Stochastic
Weight Averaging (SWA) procedure finds much broader optima than SGD, and
approximates the recent Fast Geometric Ensembling (FGE) approach with a single
model. Using SWA we achieve notable improvement in test accuracy over
conventional SGD training on a range of state-of-the-art residual networks,
PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and
ImageNet. In short, SWA is extremely easy to implement, improves
generalization, and has almost no computational overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1&quot;&gt;Pavel Izmailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Podoprikhin_D/0/1/0/all/0/1&quot;&gt;Dmitrii Podoprikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garipov_T/0/1/0/all/0/1&quot;&gt;Timur Garipov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05250">
<title>Blockchain to Improve Security and Knowledge in Inter-Agent Communication and Collaboration over Restrict Domains of the Internet Infrastructure. (arXiv:1805.05250v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05250</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the deployment and implementation of a blockchain to
improve the security, knowledge and intelligence during the inter-agent
communication and collaboration processes in restrict domains of the Internet
Infrastructure. It is a work that proposes the application of a blockchain,
platform independent, on a particular model of agents, but that can be used in
similar proposals, once the results on the specific model were satisfactory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribas_J/0/1/0/all/0/1&quot;&gt;Jessica Ribas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00442">
<title>Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization. (arXiv:1807.00442v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00442</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a first order gradient reinforcement learning algorithm,
which can be seen as a variant for Trust Region Policy Optimization(TRPO). This
method, which we call policy optimization with penalized point probability
distance (POP3D), keeps almost all advantageous spheres of proximal policy
optimization (PPO) such as easy implementation, fast learning and high score
capability. In specific, a new surrogate objective without constraint is
proposed, where the point probability distance is applied to prevent update
step from growing too large while contributing to more exploration and
stability than Kullback-Leibler divergence. Conclusions can be drawn based on
Gym Atari and Mujoco experiments that POP3D is an alternative to PPO, because
it achieves state-of-the-art within 40 million frame steps on 49 Atari games
and competitive scores in continuous domain according to two common metrics:
final performance and fast learning ability. Moreover, we release the code on
github https://github.com/cxxgtxy/POP3D.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11805">
<title>Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning. (arXiv:1807.11805v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11805</link>
<description rdf:parseType="Literal">&lt;p&gt;Monitoring of disasters is crucial for mitigating their effects on the
environment and human population, and can be facilitated by the use of unmanned
aerial vehicles (UAV), equipped with camera sensors that produce aerial photos
of the areas of interest. A modern technique for recognition of events based on
aerial photos is deep learning. In this paper, we present the state of the art
work related to the use of deep learning techniques for disaster
identification. We demonstrate the potential of this technique in identifying
disasters with high accuracy, by means of a relatively simple deep learning
model. Based on a dataset of 544 images (containing disaster images such as
fires, earthquakes, collapsed buildings, tsunami and flooding, as well as
non-disaster scenes), our results show an accuracy of 91% achieved, indicating
that deep learning, combined with UAV equipped with camera sensors, have the
potential to predict disasters with high accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamilaris_A/0/1/0/all/0/1&quot;&gt;Andreas Kamilaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenafeta_Boldu_F/0/1/0/all/0/1&quot;&gt;Francesc X. Prenafeta-Bold&amp;#xfa;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02480">
<title>Deep context: end-to-end contextual speech recognition. (arXiv:1808.02480v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1808.02480</link>
<description rdf:parseType="Literal">&lt;p&gt;In automatic speech recognition (ASR) what a user says depends on the
particular context she is in. Typically, this context is represented as a set
of word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E)
ASR sys- tem that utilizes such context. Our approach, which we re- fer to as
Contextual Listen, Attend and Spell (CLAS) jointly- optimizes the ASR
components along with embeddings of the context n-grams. During inference, the
CLAS system can be presented with context phrases which might contain out-of-
vocabulary (OOV) terms not seen during training. We com- pare our proposed
system to a more traditional contextualiza- tion approach, which performs
shallow-fusion between inde- pendently trained LAS and contextual n-gram models
during beam search. Across a number of tasks, we find that the pro- posed CLAS
system outperforms the baseline method by as much as 68% relative WER,
indicating the advantage of joint optimization over individually trained
components. Index Terms: speech recognition, sequence-to-sequence models,
listen attend and spell, LAS, attention, embedded speech recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pundak_G/0/1/0/all/0/1&quot;&gt;Golan Pundak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1&quot;&gt;Tara N. Sainath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prabhavalkar_R/0/1/0/all/0/1&quot;&gt;Rohit Prabhavalkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anjuli Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02513">
<title>Rethinking Numerical Representations for Deep Neural Networks. (arXiv:1808.02513v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.02513</link>
<description rdf:parseType="Literal">&lt;p&gt;With ever-increasing computational demand for deep learning, it is critical
to investigate the implications of the numeric representation and precision of
DNN model weights and activations on computational efficiency. In this work, we
explore unconventional narrow-precision floating-point representations as it
relates to inference accuracy and efficiency to steer the improved design of
future DNN platforms. We show that inference using these custom numeric
representations on production-grade DNNs, including GoogLeNet and VGG, achieves
an average speedup of 7.6x with less than 1% degradation in inference accuracy
relative to a state-of-the-art baseline platform representing the most
sophisticated hardware using single-precision floating point. To facilitate the
use of such customized precision, we also present a novel technique that
drastically reduces the time required to derive the optimal precision
configuration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_P/0/1/0/all/0/1&quot;&gt;Parker Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamirai_B/0/1/0/all/0/1&quot;&gt;Babak Zamirai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shengshuo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1&quot;&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurenzano_M/0/1/0/all/0/1&quot;&gt;Michael Laurenzano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samadi_M/0/1/0/all/0/1&quot;&gt;Mehrzad Samadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papaefthymiou_M/0/1/0/all/0/1&quot;&gt;Marios Papaefthymiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahlke_S/0/1/0/all/0/1&quot;&gt;Scott Mahlke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenisch_T/0/1/0/all/0/1&quot;&gt;Thomas Wenisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jia Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Lingjia Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1&quot;&gt;Jason Mars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02602">
<title>PIVETed-Granite: Computational Phenotypes through Constrained Tensor Factorization. (arXiv:1808.02602v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.02602</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been recently shown that sparse, nonnegative tensor factorization of
multi-modal electronic health record data is a promising approach to
high-throughput computational phenotyping. However, such approaches typically
do not leverage available domain knowledge while extracting the phenotypes;
hence, some of the suggested phenotypes may not map well to clinical concepts
or may be very similar to other suggested phenotypes. To address these issues,
we present a novel, automatic approach called PIVETed-Granite that mines
existing biomedical literature (PubMed) to obtain cannot-link constraints that
are then used as side-information during a tensor-factorization based
computational phenotyping process. The resulting improvements are clearly
observed in experiments using a large dataset from VUMC to identify phenotypes
for hypertensive patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1&quot;&gt;Jette Henderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malin_B/0/1/0/all/0/1&quot;&gt;Bradley A. Malin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1&quot;&gt;Joyce C. Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1&quot;&gt;Joydeep Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02610">
<title>L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data. (arXiv:1808.02610v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.02610</link>
<description rdf:parseType="Literal">&lt;p&gt;We study instancewise feature importance scoring as a method for model
interpretation. Any such method yields, for each predicted instance, a vector
of importance scores associated with the feature vector. Methods based on the
Shapley score have been proposed as a fair way of computing feature
attributions of this kind, but incur an exponential complexity in the number of
features. This combinatorial explosion arises from the definition of the
Shapley value and prevents these methods from being scalable to large data sets
and complex models. We focus on settings in which the data have a graph
structure, and the contribution of features to the target variable is
well-approximated by a graph-structured factorization. In such settings, we
develop two algorithms with linear complexity for instancewise feature
importance scoring. We establish the relationship of our methods to the Shapley
value and another closely related concept known as the Myerson value from
cooperative game theory. We demonstrate on both language and image data that
our algorithms compare favorably with other methods for model interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02766">
<title>Improved survival of cancer patients admitted to the ICU between 2002 and 2011 at a U.S. teaching hospital. (arXiv:1808.02766v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1808.02766</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decades, both critical care and cancer care have improved
substantially. Due to increased cancer-specific survival, we hypothesized that
both the number of cancer patients admitted to the ICU and overall survival
have increased since the millennium change. MIMIC-III, a freely accessible
critical care database of Beth Israel Deaconess Medical Center, Boston, USA was
used to retrospectively study trends and outcomes of cancer patients admitted
to the ICU between 2002 and 2011. Multiple logistic regression analysis was
performed to adjust for confounders of 28-day and 1-year mortality.
&lt;/p&gt;
&lt;p&gt;Out of 41,468 unique ICU admissions, 1,100 hemato-oncologic, 3,953 oncologic
and 49 patients with both a hematological and solid malignancy were analyzed.
Hematological patients had higher critical illness scores than non-cancer
patients, while oncologic patients had similar APACHE-III and SOFA-scores
compared to non-cancer patients. In the univariate analysis, cancer was
strongly associated with mortality (OR= 2.74, 95%CI: 2.56, 2.94). Over the
10-year study period, 28-day mortality of cancer patients decreased by 30%.
This trend persisted after adjustment for covariates, with cancer patients
having significantly higher mortality (OR=2.63, 95%CI: 2.38, 2.88). Between
2002 and 2011, both the adjusted odds of 28-day mortality and the adjusted odds
of 1-year mortality for cancer patients decreased by 6% (95%CI: 4%, 9%). Having
cancer was the strongest single predictor of 1-year mortality in the
multivariate model (OR=4.47, 95%CI: 4.11, 4.84).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sauer_C/0/1/0/all/0/1&quot;&gt;Chris Sauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jinghui Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Celi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02814">
<title>Highly Accelerated Multishot EPI through Synergistic Combination of Machine Learning and Joint Reconstruction. (arXiv:1808.02814v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1808.02814</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: To introduce a combined machine learning (ML) and physics-based
image reconstruction framework that enables navigator-free, highly accelerated
multishot echo planar imaging (msEPI), and demonstrate its application in
high-resolution structural imaging.
&lt;/p&gt;
&lt;p&gt;Methods: Singleshot EPI is an efficient encoding technique, but does not lend
itself well to high-resolution imaging due to severe distortion artifacts and
blurring. While msEPI can mitigate these artifacts, high-quality msEPI has been
elusive because of phase mismatch arising from shot-to-shot physiological
variations which disrupt the combination of the multiple-shot data into a
single image. We employ Deep Learning to obtain an interim magnitude-valued
image with minimal artifacts, which permits estimation of image phase
variations due to shot-to-shot physiological changes. These variations are then
included in a Joint Virtual Coil Sensitivity Encoding (JVC-SENSE)
reconstruction to utilize data from all shots and improve upon the ML solution.
&lt;/p&gt;
&lt;p&gt;Results: Our combined ML + physics approach enabled R=8-fold acceleration
from 2 EPI-shots while providing 1.8-fold error reduction compared to the
MUSSELS, a state-of-the-art reconstruction technique, which is also used as an
input to our ML network. Using 3 shots allowed us to push the acceleration to
R=10-fold, where we obtained a 1.7-fold error reduction over MUSSELS.
&lt;/p&gt;
&lt;p&gt;Conclusion: Combination of ML and JVC-SENSE enabled navigator-free msEPI at
higher accelerations than previously possible while using fewer shots, with
reduced vulnerability to poor generalizability and poor acceptance of
end-to-end ML approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1&quot;&gt;Berkin Bilgic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chatnuntawech_I/0/1/0/all/0/1&quot;&gt;Itthi Chatnuntawech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manhard_M/0/1/0/all/0/1&quot;&gt;Mary Kate Manhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qiyuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Congyu Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cauley_S/0/1/0/all/0/1&quot;&gt;Stephen F. Cauley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Susie Y. Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Polimeni_J/0/1/0/all/0/1&quot;&gt;Jonathan R. Polimeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wald_L/0/1/0/all/0/1&quot;&gt;Lawrence L. Wald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Setsompop_K/0/1/0/all/0/1&quot;&gt;Kawin Setsompop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.00520">
<title>Efficient acquisition rules for model-based approximate Bayesian computation. (arXiv:1704.00520v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.00520</link>
<description rdf:parseType="Literal">&lt;p&gt;Approximate Bayesian computation (ABC) is a method for Bayesian inference
when the likelihood is unavailable but simulating from the model is possible.
However, many ABC algorithms require a large number of simulations, which can
be costly. To reduce the computational cost, Bayesian optimisation (BO) and
surrogate models such as Gaussian processes have been proposed. Bayesian
optimisation enables one to intelligently decide where to evaluate the model
next but common BO strategies are not designed for the goal of estimating the
posterior distribution. Our paper addresses this gap in the literature. We
propose to compute the uncertainty in the ABC posterior density, which is due
to a lack of simulations to estimate this quantity accurately, and define a
loss function that measures this uncertainty. We then propose to select the
next evaluation location to minimise the expected loss. Experiments show that
the proposed method often produces the most accurate approximations as compared
to common BO strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jarvenpaa_M/0/1/0/all/0/1&quot;&gt;Marko J&amp;#xe4;rvenp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gutmann_M/0/1/0/all/0/1&quot;&gt;Michael U. Gutmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pleska_A/0/1/0/all/0/1&quot;&gt;Arijus Pleska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1&quot;&gt;Aki Vehtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marttinen_P/0/1/0/all/0/1&quot;&gt;Pekka Marttinen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08873">
<title>Robust Photometric Stereo via Dictionary Learning. (arXiv:1710.08873v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08873</link>
<description rdf:parseType="Literal">&lt;p&gt;Photometric stereo is a method that seeks to reconstruct the normal vectors
of an object from a set of images of the object illuminated under different
light sources. While effective in some situations, classical photometric stereo
relies on a diffuse surface model that cannot handle objects with complex
reflectance patterns, and it is sensitive to non-idealities in the images. In
this work, we propose a novel approach to photometric stereo that relies on
dictionary learning to produce robust normal vector reconstructions.
Specifically, we develop two formulations for applying dictionary learning to
photometric stereo. We propose a model that applies dictionary learning to
regularize and reconstruct the normal vectors from the images under the classic
Lambertian reflectance model. We then generalize this model to explicitly model
non-Lambertian objects. We investigate both approaches through extensive
experimentation on synthetic and real benchmark datasets and observe
state-of-the-art performance compared to existing robust photometric stereo
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1&quot;&gt;Andrew J. Wagenmaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_B/0/1/0/all/0/1&quot;&gt;Brian E. Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadakuditi_R/0/1/0/all/0/1&quot;&gt;Raj Rao Nadakuditi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09535">
<title>Learning with Biased Complementary Labels. (arXiv:1711.09535v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09535</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the classification problem in which we have access to
easily obtainable surrogate for true labels, namely complementary labels, which
specify classes that observations do \textbf{not} belong to. Let $Y$ and
$\bar{Y}$ be the true and complementary labels, respectively. We first model
the annotation of complementary labels via transition probabilities
$P(\bar{Y}=i|Y=j), i\neq j\in\{1,\cdots,c\}$, where $c$ is the number of
classes. Previous methods implicitly assume that $P(\bar{Y}=i|Y=j), \forall
i\neq j$, are identical, which is not true in practice because humans are
biased toward their own experience. For example, as shown in Figure 1, if an
annotator is more familiar with monkeys than prairie dogs when providing
complementary labels for meerkats, she is more likely to employ &quot;monkey&quot; as a
complementary label. We therefore reason that the transition probabilities will
be different. In this paper, we propose a framework that contributes three main
innovations to learning with \textbf{biased} complementary labels: (1) It
estimates transition probabilities with no bias. (2) It provides a general
method to modify traditional loss functions and extends standard deep neural
network classifiers to learn with biased complementary labels. (3) It
theoretically ensures that the classifier learned with complementary labels
converges to the optimal one learned with true labels. Comprehensive
experiments on several benchmark datasets validate the superiority of our
method to current state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiyu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Mingming Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01743">
<title>A Machine Learning Framework for Stock Selection. (arXiv:1806.01743v2 [q-fin.PM] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01743</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper demonstrates how to apply machine learning algorithms to
distinguish good stocks from the bad stocks. To this end, we construct 244
technical and fundamental features to characterize each stock, and label stocks
according to their ranking with respect to the return-to-volatility ratio.
Algorithms ranging from traditional statistical learning methods to recently
popular deep learning method, e.g. Logistic Regression (LR), Random Forest
(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the
classification task. Genetic Algorithm (GA) is also used to implement feature
selection. The effectiveness of the stock selection strategy is validated in
Chinese stock market in both statistical and practical aspects, showing that:
1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic
Algorithm picks a subset of 114 features and the prediction performances of all
models remain almost unchanged after the selection procedure, which suggests
some features are indeed redundant; 3) LR and DNN are radical models; RF is
risk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios
constructed by our models outperform market average in back tests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;XingYu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;JinHong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;YiFeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;MingWen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dong_T/0/1/0/all/0/1&quot;&gt;Tao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Duan_X/0/1/0/all/0/1&quot;&gt;XiuWen Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00374">
<title>Augmented Cyclic Adversarial Learning for Domain Adaptation. (arXiv:1807.00374v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00374</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a model to perform a task typically requires a large amount of data
from the domains in which the task will be applied. However, it is often the
case that data are abundant in some domains but scarce in others. Domain
adaptation deals with the challenge of adapting a model trained from a
data-rich source domain to perform well in a data-poor target domain. In
general, this requires learning plausible mappings between domains. CycleGAN is
a powerful framework that efficiently learns to map inputs from one domain to
another using adversarial training and a cycle-consistency constraint. However,
the conventional approach of enforcing cycle-consistency via reconstruction may
be overly restrictive in cases where one or more domains have limited training
data. In this paper, we propose an augmented cyclic adversarial learning model
that enforces the cycle-consistency constraint through an external task
specific model, which encourages the preservation of task-relevant content as
opposed to exact reconstruction. We explore digit classification with MNIST and
SVHN in a low-resource setting in supervised, semi and unsupervised situation.
In low-resource supervised setting, the results show that our approach improves
absolute performance by $14\%$ and $4\%$ when adapting SVHN to MNIST and vice
versa, respectively, which outperforms unsupervised domain adaptation methods
that require high-resource unlabeled target domain. Moreover, using only few
unsupervised target data, our approach can still outperforms many high-resource
unsupervised models. In speech domains, we also adopt a speech recognition
model from each domain as the task specific model. Our approach improves
absolute performance of speech recognition by $2\%$ for female speakers in the
TIMIT dataset, where the majority of training samples are from male voices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_Asl_E/0/1/0/all/0/1&quot;&gt;Ehsan Hosseini-Asl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08197">
<title>On Numerical Estimation of Joint Probability Distribution from Lebesgue Integral Quadratures. (arXiv:1807.08197v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08197</link>
<description rdf:parseType="Literal">&lt;p&gt;An important application of Lebesgue integral quadrature[1] is developed.
Given two random processes, $f(x)$ and $g(x)$, two generalized eigenvalue
problems can be formulated and solved. In addition to obtaining two Lebesgue
quadratures (for $f$ and $g$) from two eigenproblems, the projections of $f$--
and $g$-- eigenvectors on each other allow to build a joint distribution
estimator, the most general form of which is a density--matrix correlation. The
examples of the density--matrix correlation can be the value--correlation
$V_{f_i;g_j}$, similar to the regular correlation concept, and a new one, the
probability--correlation $P_{f_i;g_j}$. The theory is implemented numerically;
the software is available under the GPLv3 license.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malyshkin_V/0/1/0/all/0/1&quot;&gt;Vladislav Gennadievich Malyshkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01357">
<title>Recurrent Convolutional Fusion for RGB-D Object Recognition. (arXiv:1808.01357v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01357</link>
<description rdf:parseType="Literal">&lt;p&gt;Technological development aims to produce generations of increasingly
efficient robots able to perform complex tasks. This requires considerable
efforts, from the scientific community, to find new algorithms that solve
computer vision problems, such as object recognition. The diffusion of RGB-D
cameras directed the study towards the research of new architectures able to
exploit the RGB and Depth information. The project that is developed in this
thesis concerns the realization of a new end-to-end architecture for the
recognition of RGB-D objects called RCFusion. Our method generates compact and
highly discriminative multi-modal features by combining complementary RGB and
depth information representing different levels of abstraction. We evaluate our
method on standard object recognition datasets, RGB-D Object Dataset and
JHUIT-50. The experiments performed show that our method outperforms the
existing approaches and establishes new state-of-the-art results for both
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1&quot;&gt;Mirco Planamente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loghmani_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Loghmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1&quot;&gt;Barbara Caputo&lt;/a&gt;</dc:creator>
</item></rdf:RDF>