<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04235"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04353"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04367"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04421"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04452"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04635"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1509.02709"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.00178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04171"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04205"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04241"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04262"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04435"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04438"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04469"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04529"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04614"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.07549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01882"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04097"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.04612">
<title>A Comprehensive Study on the Applications of Machine Learning for the Medical Diagnosis and Prognosis of Asthma. (arXiv:1804.04612v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1804.04612</link>
<description rdf:parseType="Literal">&lt;p&gt;An estimated 300 million people worldwide suffer from asthma, and this number
is expected to increase to 400 million by 2025. Approximately 250,000 people
die prematurely each year from asthma out of which, almost all deaths are
avoidable. Most of these deaths occur because the patients are unaware of their
asthmatic morbidity. If detected early, asthmatic mortality rate can be reduced
by 78%, provided that the patients carry appropriate medication for the same
and/or are in lose vicinity to medical equipment like nebulizers. This study
focuses on the development and valuation of algorithms to diagnose asthma
through symptom intensive questionary, clinical data and medical reports.
Machine Learning Algorithms like Back-propagation model, Context Sensitive
Auto-Associative Memory Neural Network Model, C4.5 Algorithm, Bayesian Network
and Particle Swarm Optimization have been employed for the diagnosis of asthma
and later a comparison is made between their respective prospects. All
algorithms received an accuracy of over 80%. However, the use of Auto
Associative Memory Model (on a layered Artificial Neural Network) displayed
much better results. It reached to an accuracy of over 90% and an inconclusive
diagnosis rate of less than 1% when trained with adequate data. In the end,
na\&quot;ive mobile based applications were developed on Android and iOS that made
use of the self-training auto associative memory model to achieve an accuracy
of nearly 94.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kukreja_S/0/1/0/all/0/1&quot;&gt;Saksham Kukreja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04216">
<title>Market Making via Reinforcement Learning. (arXiv:1804.04216v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.04216</link>
<description rdf:parseType="Literal">&lt;p&gt;Market making is a fundamental trading problem in which an agent provides
liquidity by continually offering to buy and sell a security. The problem is
challenging due to inventory risk, the risk of accumulating an unfavourable
position and ultimately losing money. In this paper, we develop a high-fidelity
simulation of limit order book markets, and use it to design a market making
agent using temporal-difference reinforcement learning. We use a linear
combination of tile codings as a value function approximator, and design a
custom reward function that controls inventory risk. We demonstrate the
effectiveness of our approach by showing that our agent outperforms both simple
benchmark strategies and a recent online learning approach from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1&quot;&gt;Thomas Spooner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fearnley_J/0/1/0/all/0/1&quot;&gt;John Fearnley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savani_R/0/1/0/all/0/1&quot;&gt;Rahul Savani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koukorinis_A/0/1/0/all/0/1&quot;&gt;Andreas Koukorinis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04235">
<title>Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. (arXiv:1804.04235v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04235</link>
<description rdf:parseType="Literal">&lt;p&gt;In several recently proposed stochastic optimization methods (e.g. RMSProp,
Adam, Adadelta), parameter updates are scaled by the inverse square roots of
exponential moving averages of squared past gradients. Maintaining these
per-parameter second-moment estimators requires memory equal to the number of
parameters. For the case of neural network weight matrices, we propose
maintaining only the per-row and per-column sums of these moving averages, and
estimating the per-parameter second moments based on these sums. We demonstrate
empirically that this method produces similar results to the baseline.
Secondly, we show that adaptive methods can produce larger-than-desired updates
when the decay rate of the second moment accumulator is too slow. We propose
update clipping and a gradually increasing decay rate scheme as remedies.
Combining these methods and dropping momentum, we achieve comparable results to
the published Adam regime in training the Transformer model on the WMT 2014
English-German machine translation task, while using very little auxiliary
storage in the optimizer. Finally, we propose scaling the parameter updates
based on the scale of the parameters themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1&quot;&gt;Noam Shazeer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stern_M/0/1/0/all/0/1&quot;&gt;Mitchell Stern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04268">
<title>Incomplete Contracting and AI Alignment. (arXiv:1804.04268v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.04268</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest that the analysis of incomplete contracting developed by law and
economics researchers can provide a useful framework for understanding the AI
alignment problem and help to generate a systematic approach to finding
solutions. We first provide an overview of the incomplete contracting
literature and explore parallels between this work and the problem of AI
alignment. As we emphasize, misalignment between principal and agent is a core
focus of economic analysis. We highlight some technical results from the
economics literature on incomplete contracts that may provide insights for AI
alignment researchers. Our core contribution, however, is to bring to bear an
insight that economists have been urged to absorb from legal scholars and other
behavioral scientists: the fact that human contracting is supported by
substantial amounts of external structure, such as generally available
institutions (culture, law) that can supply implied terms to fill the gaps in
incomplete contracts. We propose a research agenda for AI alignment work that
focuses on the problem of how to build AI that can replicate the human
cognitive processes that connect individual incomplete contracts with this
supporting external structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1&quot;&gt;Gillian Hadfield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04316">
<title>Entangled photons for competitive multi-armed bandit problem: achievement of maximum social reward, equality, and deception prevention. (arXiv:1804.04316v1 [physics.optics])</title>
<link>http://arxiv.org/abs/1804.04316</link>
<description rdf:parseType="Literal">&lt;p&gt;The competitive multi-armed bandit (CMAB) problem is related to social issues
such as maximizing total social benefits while preserving equality among
individuals by overcoming conflicts between individual decisions, which could
seriously decrease social benefits. The study described herein provides
experimental evidence that entangled photons physically resolve the CMAB,
maximizing the social rewards while ensuring equality. Moreover, by exploiting
the requirement that entangled photons share a common polarization basis, we
demonstrated that deception, or delaying the other player receiving a greater
reward, cannot be accomplished in a polarization-entangled-photon-based system,
while deception is achievable in systems based on classical or
polarization-correlated photons. Autonomous alignment schemes for polarization
bases were also experimentally demonstrated based on decision conflict
information. This study provides the foundation for collective decision making
based on polarization-entangled photons and their polarization and value
alignment, which is essential for utilizing quantum light for intelligent
functionalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Naruse_M/0/1/0/all/0/1&quot;&gt;Makoto Naruse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chauvet_N/0/1/0/all/0/1&quot;&gt;Nicolas Chauvet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jegouso_D/0/1/0/all/0/1&quot;&gt;David Jegouso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Boulanger_B/0/1/0/all/0/1&quot;&gt;Benoit Boulanger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Saigo_H/0/1/0/all/0/1&quot;&gt;Hayato Saigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Okamura_K/0/1/0/all/0/1&quot;&gt;Kazuya Okamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hori_H/0/1/0/all/0/1&quot;&gt;Hirokazu Hori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Drezet_A/0/1/0/all/0/1&quot;&gt;Aurelien Drezet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huant_S/0/1/0/all/0/1&quot;&gt;Serge Huant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bachelier_G/0/1/0/all/0/1&quot;&gt;Guillaume Bachelier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04318">
<title>Cross-Modal Retrieval with Implicit Concept Association. (arXiv:1804.04318v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.04318</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional cross-modal retrieval assumes explicit association of concepts
across modalities, where there is no ambiguity in how the concepts are linked
to each other, e.g., when we do the image search with a query &quot;dogs&quot;, we expect
to see dog images. In this paper, we consider a different setting for
cross-modal retrieval where data from different modalities are implicitly
linked via concepts that must be inferred by high-level reasoning; we call this
setting implicit concept association. To foster future research in this
setting, we present a new dataset containing 47K pairs of animated GIFs and
sentences crawled from the web, in which the GIFs depict physical or emotional
reactions to the scenarios described in the text (called &quot;reaction GIFs&quot;). We
report on a user study showing that, despite the presence of implicit concept
association, humans are able to identify video-sentence pairs with matching
concepts, suggesting the feasibility of our task. Furthermore, we propose a
novel visual-semantic embedding network based on multiple instance learning.
Unlike traditional approaches, we compute multiple embeddings from each
modality, each representing different concepts, and measure their similarity by
considering all possible combinations of visual-semantic embeddings in the
framework of multiple instance learning. We evaluate our approach on two
video-sentence datasets with explicit and implicit concept association and
report competitive results compared to existing approaches on cross-modal
retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yale Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1&quot;&gt;Mohammed Soleymani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04353">
<title>Global SNR Estimation of Speech Signals using Entropy and Uncertainty Estimates from Dropout Networks. (arXiv:1804.04353v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.04353</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper demonstrates two novel methods to estimate the global SNR of
speech signals. In both methods, Deep Neural Network-Hidden Markov Model
(DNN-HMM) acoustic model used in speech recognition systems is leveraged for
the additional task of SNR estimation. In the first method, the entropy of the
DNN-HMM output is computed. Recent work on bayesian deep learning has shown
that a DNN-HMM trained with dropout can be used to estimate model uncertainty
by approximating it as a deep Gaussian process. In the second method, this
approximation is used to obtain model uncertainty estimates. Noise specific
regressors are used to predict the SNR from the entropy and model uncertainty.
The DNN-HMM is trained on GRID corpus and tested on different noise profiles
from the DEMAND noise database at SNR levels ranging from -10 dB to 30 dB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aralikatti_R/0/1/0/all/0/1&quot;&gt;Rohith Aralikatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Margam_D/0/1/0/all/0/1&quot;&gt;Dilip Margam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_T/0/1/0/all/0/1&quot;&gt;Tanay Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abhinav_T/0/1/0/all/0/1&quot;&gt;Thanda Abhinav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Venkatesan_S/0/1/0/all/0/1&quot;&gt;Shankar M Venkatesan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04367">
<title>BigSR: an empirical study of real-time expressive RDF stream reasoning on modern Big Data platforms. (arXiv:1804.04367v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.04367</link>
<description rdf:parseType="Literal">&lt;p&gt;The trade-off between language expressiveness and system scalability (E&amp;amp;S) is
a well-known problem in RDF stream reasoning. Higher expressiveness supports
more complex reasoning logic, however, it may also hinder system scalability.
Current research mainly focuses on logical frameworks suitable for stream
reasoning as well as the implementation and the evaluation of prototype
systems. These systems are normally developed in a centralized setting which
suffer from inherent limited scalability, while an in-depth study of applying
distributed solutions to cover E&amp;amp;S is still missing. In this paper, we aim to
explore the feasibility of applying modern distributed computing frameworks to
meet E&amp;amp;S all together. To do so, we first propose BigSR, a technical
demonstrator that supports a positive fragment of the LARS framework. For the
sake of generality and to cover a wide variety of use cases, BigSR relies on
the two main execution models adopted by major distributed execution
frameworks: Bulk Synchronous Processing (BSP) and Record-at-A-Time (RAT).
Accordingly, we implement BigSR on top of Apache Spark Streaming (BSP model)
and Apache Flink (RAT model). In order to conclude on the impacts of BSP and
RAT on E&amp;amp;S, we analyze the ability of the two models to support distributed
stream reasoning and identify several types of use cases characterized by their
levels of support. This classification allows for quantifying the E&amp;amp;S trade-off
by assessing the scalability of each type of use case \wrt its level of
expressiveness. Then, we conduct a series of experiments with 15 queries from 4
different datasets. Our experiments show that BigSR over both BSP and RAT
generally scales up to high throughput beyond million-triples per second (with
or without recursion), and RAT attains sub-millisecond delay for stateless
query operators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiangnan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cure_O/0/1/0/all/0/1&quot;&gt;Olivier Cur&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naacke_H/0/1/0/all/0/1&quot;&gt;Hubert Naacke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1&quot;&gt;Guohui Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04421">
<title>Regularized Greedy Column Subset Selection. (arXiv:1804.04421v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04421</link>
<description rdf:parseType="Literal">&lt;p&gt;The Column Subset Selection Problem provides a natural framework for
unsupervised feature selection. Despite being a hard combinatorial optimization
problem, there exist efficient algorithms that provide good approximations. The
drawback of the problem formulation is that it incorporates no form of
regularization, and is therefore very sensitive to noise when presented with
scarce data. In this paper we propose a regularized formulation of this
problem, and derive a correct greedy algorithm that is similar in efficiency to
existing greedy methods for the unregularized problem. We study its adequacy
for feature selection and propose suitable formulations. Additionally, we
derive a lower bound for the error of the proposed problems. Through various
numerical experiments on real and synthetic data, we demonstrate the
significantly increased robustness and stability of our method, as well as the
improved conditioning of its output, all while remaining efficient for
practical use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordozgoiti_B/0/1/0/all/0/1&quot;&gt;Bruno Ordozgoiti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozo_A/0/1/0/all/0/1&quot;&gt;Alberto Mozo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacalle_J/0/1/0/all/0/1&quot;&gt;Jes&amp;#xfa;s Garc&amp;#xed;a L&amp;#xf3;pez de Lacalle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04452">
<title>Solving Bongard Problems with a Visual Language and Pragmatic Reasoning. (arXiv:1804.04452v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04452</link>
<description rdf:parseType="Literal">&lt;p&gt;More than 50 years ago Bongard introduced 100 visual concept learning
problems as a testbed for intelligent vision systems. These problems are now
known as Bongard problems. Although they are well known in the cognitive
science and AI communities only moderate progress has been made towards
building systems that can solve a substantial subset of them. In the system
presented here, visual features are extracted through image processing and then
translated into a symbolic visual vocabulary. We introduce a formal language
that allows representing complex visual concepts based on this vocabulary.
Using this language and Bayesian inference, complex visual concepts can be
induced from the examples that are provided in each Bongard problem. Contrary
to other concept learning problems the examples from which concepts are induced
are not random in Bongard problems, instead they are carefully chosen to
communicate the concept, hence requiring pragmatic reasoning. Taking pragmatic
reasoning into account we find good agreement between the concepts with high
posterior probability and the solutions formulated by Bongard himself. While
this approach is far from solving all Bongard problems, it solves the biggest
fraction yet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Depeweg_S/0/1/0/all/0/1&quot;&gt;Stefan Depeweg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rothkopf_C/0/1/0/all/0/1&quot;&gt;Constantin A. Rothkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jakel_F/0/1/0/all/0/1&quot;&gt;Frank J&amp;#xe4;kel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04603">
<title>Outline Objects using Deep Reinforcement Learning. (arXiv:1804.04603v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.04603</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation needs both local boundary position information and global
object context information. The performance of the recent state-of-the-art
method, fully convolutional networks, reaches a bottleneck due to the neural
network limit after balancing between the two types of information
simultaneously in an end-to-end training style. To overcome this problem, we
divide the semantic image segmentation into temporal subtasks. First, we find a
possible pixel position of some object boundary; then trace the boundary at
steps within a limited length until the whole object is outlined. We present
the first deep reinforcement learning approach to semantic image segmentation,
called DeepOutline, which outperforms other algorithms in Coco detection
leaderboard in the middle and large size person category in Coco val2017
dataset. Meanwhile, it provides an insight into a divide and conquer way by
reinforcement learning on computer vision problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarcar_S/0/1/0/all/0/1&quot;&gt;Sayan Sarcar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yilin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiangshi Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04604">
<title>Discovery and usage of joint attention in images. (arXiv:1804.04604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.04604</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint visual attention is characterized by two or more individuals looking at
a common target at the same time. The ability to identify joint attention in
scenes, the people involved, and their common target, is fundamental to the
understanding of social interactions, including others&apos; intentions and goals.
In this work we deal with the extraction of joint attention events, and the use
of such events for image descriptions. The work makes two novel contributions.
First, our extraction algorithm is the first which identifies joint visual
attention in single static images. It computes 3D gaze direction, identifies
the gaze target by combining gaze direction with a 3D depth map computed for
the image, and identifies the common gaze target. Second, we use a human study
to demonstrate the sensitivity of humans to joint attention, suggesting that
the detection of such a configuration in an image can be useful for
understanding the image, including the goals of the agents and their joint
activity, and therefore can contribute to image captioning and related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harari_D/0/1/0/all/0/1&quot;&gt;Daniel Harari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullman_S/0/1/0/all/0/1&quot;&gt;Shimon Ullman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04635">
<title>CERES: Distantly Supervised Relation Extraction from the Semi-Structured Web. (arXiv:1804.04635v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.04635</link>
<description rdf:parseType="Literal">&lt;p&gt;The web contains countless semi-structured websites, which can be a rich
source of information for populating knowledge bases. Existing methods for
extracting relations from the DOM trees of semi-structured webpages can achieve
high precision and recall only when manual annotations for each website are
available. Although there have been efforts to learn extractors from
automatically-generated labels, these methods are not sufficiently robust to
succeed in settings with complex schemas and information-rich websites.
&lt;/p&gt;
&lt;p&gt;In this paper we present a new method for automatic extraction from
semi-structured websites based on distant supervision. We automatically
generate training labels by aligning an existing knowledge base with a web page
and leveraging the unique structural characteristics of semi-structured
websites. We then train a classifier based on the potentially noisy and
incomplete labels to predict new relation instances. Our method can compete
with annotation-based techniques in the literature in terms of extraction
quality. A large-scale experiment on over 400,000 pages from dozens of
multi-lingual long-tail websites harvested 1.25 million facts at a precision of
90%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lockard_C/0/1/0/all/0/1&quot;&gt;Colin Lockard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xin Luna Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Einolghozati_A/0/1/0/all/0/1&quot;&gt;Arash Einolghozati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiralkar_P/0/1/0/all/0/1&quot;&gt;Prashant Shiralkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1509.02709">
<title>A Topological Approach to Meta-heuristics: Analytical Results on the BFS vs. DFS Algorithm Selection Problem. (arXiv:1509.02709v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1509.02709</link>
<description rdf:parseType="Literal">&lt;p&gt;Search is a central problem in artificial intelligence, and breadth-first
search (BFS) and depth-first search (DFS) are the two most fundamental ways to
search. In this paper we derive estimates for average BFS and DFS runtime. The
average runtime estimates can be used to allocate resources or judge the
hardness of a problem. They can also be used for selecting the best graph
representation, and for selecting the faster algorithm out of BFS and DFS. They
may also form the basis for an analysis of more advanced search methods. The
paper treats both tree search and graph search. For tree search, we employ a
probabilistic model of goal distribution; for graph search, the analysis
depends on an additional statistic of path redundancy and average branching
factor. As an application, we use the results to predict BFS and DFS runtime on
two concrete grammar problems and on the N-puzzle. Experimental verification
shows that our analytical approximations come close to empirical reality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Everitt_T/0/1/0/all/0/1&quot;&gt;Tom Everitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marcus Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.00178">
<title>Lazily Adapted Constant Kinky Inference for Nonparametric Regression and Model-Reference Adaptive Control. (arXiv:1701.00178v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1701.00178</link>
<description rdf:parseType="Literal">&lt;p&gt;Techniques known as Nonlinear Set Membership prediction, Lipschitz
Interpolation or Kinky Inference are approaches to machine learning that
utilise presupposed Lipschitz properties to compute inferences over unobserved
function values. Provided a bound on the true best Lipschitz constant of the
target function is known a priori they offer convergence guarantees as well as
bounds around the predictions. Considering a more general setting that builds
on Hoelder continuity relative to pseudo-metrics, we propose an online method
for estimating the Hoelder constant online from function value observations
that possibly are corrupted by bounded observational errors. Utilising this to
compute adaptive parameters within a kinky inference rule gives rise to a
nonparametric machine learning method, for which we establish strong universal
approximation guarantees. That is, we show that our prediction rule can learn
any continuous function in the limit of increasingly dense data to within a
worst-case error bound that depends on the level of observational uncertainty.
We apply our method in the context of nonparametric model-reference adaptive
control (MRAC). Across a range of simulated aircraft roll-dynamics and
performance metrics our approach outperforms recently proposed alternatives
that were based on Gaussian processes and RBF-neural networks. For
discrete-time systems, we provide guarantees on the tracking success of our
learning-based controllers both for the batch and the online learning setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Calliess_J/0/1/0/all/0/1&quot;&gt;Jan-Peter Calliess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01102">
<title>Social Media Analysis based on Semanticity of Streaming and Batch Data. (arXiv:1801.01102v2 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1801.01102</link>
<description rdf:parseType="Literal">&lt;p&gt;Languages shared by people differ in different regions based on their
accents, pronunciation and word usages. In this era sharing of language takes
place mainly through social media and blogs. Every second swing of such a micro
posts exist which induces the need of processing those micro posts, in-order to
extract knowledge out of it. Knowledge extraction differs with respect to the
application in which the research on cognitive science fed the necessities for
the same. This work further moves forward such a research by extracting
semantic information of streaming and batch data in applications like Named
Entity Recognition and Author Profiling. In the case of Named Entity
Recognition context of a single micro post has been utilized and context that
lies in the pool of micro posts were utilized to identify the sociolect aspects
of the author of those micro posts. In this work Conditional Random Field has
been utilized to do the entity recognition and a novel approach has been
proposed to find the sociolect aspects of the author (Gender, Age group).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+HB_B/0/1/0/all/0/1&quot;&gt;Barathi Ganesh HB&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04171">
<title>KS(conf ): A Light-Weight Test if a ConvNet Operates Outside of Its Specifications. (arXiv:1804.04171v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04171</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision systems for automatic image categorization have become
accurate and reliable enough that they can run continuously for days or even
years as components of real-world commercial applications. A major open problem
in this context, however, is quality control. Good classification performance
can only be expected if systems run under the specific conditions, in
particular data distributions, that they were trained for. Surprisingly, none
of the currently used deep network architectures has a built-in functionality
that could detect if a network operates on data from a distribution that it was
not trained for and potentially trigger a warning to the human users. In this
work, we describe KS(conf), a procedure for detecting such outside of the
specifications operation. Building on statistical insights, its main step is
the applications of a classical Kolmogorov-Smirnov test to the distribution of
predicted confidence values. We show by extensive experiments using ImageNet,
AwA2 and DAVIS data on a variety of ConvNets architectures that KS(conf)
reliably detects out-of-specs situations. It furthermore has a number of
properties that make it an excellent candidate for practical deployment: it is
easy to implement, adds almost no overhead to the system, works with all
networks, including pretrained ones, and requires no a priori knowledge about
how the data distribution could change.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lampert_C/0/1/0/all/0/1&quot;&gt;Christoph H. Lampert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04176">
<title>Peeking the Impact of Points of Interests on Didi. (arXiv:1804.04176v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1804.04176</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the online car-hailing service, Didi, has emerged as a leader in
the sharing economy. Used by passengers and drivers extensive, it becomes
increasingly important for the car-hailing service providers to minimize the
waiting time of passengers and optimize the vehicle utilization, thus to
improve the overall user experience. Therefore, the supply-demand estimation is
an indispensable ingredient of an efficient online car-hailing service. To
improve the accuracy of the estimation results, we analyze the implicit
relationships between the points of Interest (POI) and the supply-demand gap in
this paper. The different categories of POIs have positive or negative effects
on the estimation, we propose a POI selection scheme and incorporate it into
XGBoost [1] to achieve more accurate estimation results. Our experiment
demonstrates our method provides more accurate estimation results and more
stable estimation results than the existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xuying Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Bing Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04205">
<title>Learning Topics using Semantic Locality. (arXiv:1804.04205v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;The topic modeling discovers the latent topic probability of the given text
documents. To generate the more meaningful topic that better represents the
given document, we proposed a new feature extraction technique which can be
used in the data preprocessing stage. The method consists of three steps.
First, it generates the word/word-pair from every single document. Second, it
applies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.
Third, it uses the K-means algorithm to merge the word pairs that have the
similar semantic meaning.
&lt;/p&gt;
&lt;p&gt;Experiments are carried out on the Open Movie Database (OMDb), Reuters
Dataset and 20NewsGroup Dataset. The mean Average Precision score is used as
the evaluation metric. Comparing our results with other state-of-the-art topic
models, such as Latent Dirichlet allocation and traditional Restricted
Boltzmann Machines. Our proposed data preprocessing can improve the generated
topic accuracy by up to 12.99\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pugdeethosapol_K/0/1/0/all/0/1&quot;&gt;Krittaphat Pugdeethosapol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Sheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qinru Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04206">
<title>Multi-scale Neural Networks for Retinal Blood Vessels Segmentation. (arXiv:1804.04206v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.04206</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing supervised approaches didn&apos;t make use of the low-level features
which are actually effective to this task. And another deficiency is that they
didn&apos;t consider the relation between pixels, which means effective features are
not extracted. In this paper, we proposed a novel convolutional neural network
which make sufficient use of low-level features together with high-level
features and involves atrous convolution to get multi-scale features which
should be considered as effective features. Our model is tested on three
standard benchmarks - DRIVE, STARE, and CHASE databases. The results presents
that our model significantly outperforms existing approaches in terms of
accuracy, sensitivity, specificity, the area under the ROC curve and the
highest prediction speed. Our work provides evidence of the power of wide and
deep neural networks in retinal blood vessels segmentation task which could be
applied on other medical images tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Boheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shenglei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shaohan Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04241">
<title>Capsules for Object Segmentation. (arXiv:1804.04241v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04241</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have shown remarkable results over the
last several years for a wide range of computer vision tasks. A new
architecture recently introduced by Sabour et al., referred to as a capsule
networks with dynamic routing, has shown great initial results for digit
recognition and small image classification. The success of capsule networks
lies in their ability to preserve more information about the input by replacing
max-pooling layers with convolutional strides and dynamic routing, allowing for
preservation of part-whole relationships in the data. This preservation of the
input is demonstrated by reconstructing the input from the output capsule
vectors. Our work expands the use of capsule networks to the task of object
segmentation for the first time in the literature. We extend the idea of
convolutional capsules with locally-connected routing and propose the concept
of deconvolutional capsules. Further, we extend the masked reconstruction to
reconstruct the positive input class. The proposed
convolutional-deconvolutional capsule network, called SegCaps, shows strong
results for the task of object segmentation with substantial decrease in
parameter space. As an example application, we applied the proposed SegCaps to
segment pathological lungs from low dose CT scans and compared its accuracy and
efficiency with other U-Net-based architectures. SegCaps is able to handle
large image sizes (512 x 512) as opposed to baseline capsules (typically less
than 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net
architecture by 95.4% while still providing a better segmentation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+LaLonde_R/0/1/0/all/0/1&quot;&gt;Rodney LaLonde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04262">
<title>The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods. (arXiv:1804.04262v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.04262</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Voice Conversion Challenge 2018, designed as a follow up to
the 2016 edition with the aim of providing a common framework for evaluating
and comparing different state-of-the-art voice conversion (VC) systems. The
objective of the challenge was to perform speaker conversion (i.e. transform
the vocal identity) of a source speaker to a target speaker while maintaining
linguistic information. As an update to the previous challenge, we considered
both parallel and non-parallel data to form the Hub and Spoke tasks,
respectively. A total of 23 teams from around the world submitted their
systems, 11 of them additionally participated in the optional Spoke task. A
large-scale crowdsourced perceptual evaluation was then carried out to rate the
submitted converted speech in terms of naturalness and similarity to the target
speaker identity. In this paper, we present a brief summary of the
state-of-the-art techniques for VC, followed by a detailed explanation of the
challenge tasks and the results that were obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toda_T/0/1/0/all/0/1&quot;&gt;Tomoki Toda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saito_D/0/1/0/all/0/1&quot;&gt;Daisuke Saito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Villavicencio_F/0/1/0/all/0/1&quot;&gt;Fernando Villavicencio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1&quot;&gt;Tomi Kinnunen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04272">
<title>Deep Neural Networks motivated by Partial Differential Equations. (arXiv:1804.04272v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04272</link>
<description rdf:parseType="Literal">&lt;p&gt;Partial differential equations (PDEs) are indispensable for modeling many
physical phenomena and also commonly used for solving image processing tasks.
In the latter area, PDE-based approaches interpret image data as
discretizations of multivariate functions and the output of image processing
algorithms as solutions to certain PDEs. Posing image processing problems in
the infinite dimensional setting provides powerful tools for their analysis and
solution. Over the last three decades, the reinterpretation of classical image
processing tasks through the PDE lens has been creating multiple celebrated
approaches that benefit a vast area of tasks including image segmentation,
denoising, registration, and reconstruction.
&lt;/p&gt;
&lt;p&gt;In this paper, we establish a new PDE-interpretation of deep convolution
neural networks (CNN) that are commonly used for learning tasks involving
speech, image, and video data. Our interpretation includes convolution residual
neural networks (ResNet), which are among the most promising approaches for
tasks such as image classification having improved the state-of-the-art
performance in prestigious benchmark challenges. Despite their recent
successes, deep ResNets still face some critical challenges associated with
their design, immense computational costs and memory requirements, and lack of
understanding of their reasoning.
&lt;/p&gt;
&lt;p&gt;Guided by well-established PDE theory, we derive three new ResNet
architectures that fall two new classes: parabolic and hyperbolic CNNs. We
demonstrate how PDE theory can provide new insights and algorithms for deep
learning and demonstrate the competitiveness of three new CNN architectures
using numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1&quot;&gt;Lars Ruthotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1&quot;&gt;Eldad Haber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04324">
<title>Local reservoir model for choice-based learning. (arXiv:1804.04324v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04324</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision making based on behavioral and neural observations of living systems
has been extensively studied in brain science, psychology, and other
disciplines. Decision-making mechanisms have also been experimentally
implemented in physical processes, such as single photons and chaotic lasers.
The findings of these experiments suggest that there is a certain common basis
in describing decision making, regardless of its physical realizations. In this
study, we propose a local reservoir model to account for choice-based learning
(CBL). CBL describes decision consistency as a phenomenon where making a
certain decision increases the possibility of making that same decision again
later, which has been intensively investigated in neuroscience, psychology,
etc. Our proposed model is inspired by the viewpoint that a decision is
affected by its local environment, which is referred to as a local reservoir.
If the size of the local reservoir is large enough, consecutive decision making
will not be affected by previous decisions, thus showing lower degrees of
decision consistency in CBL. In contrast, if the size of the local reservoir
decreases, a biased distribution occurs within it, which leads to higher
degrees of decision consistency in CBL. In this study, an analytical approach
on local reservoirs is presented, as well as several numerical demonstrations.
Furthermore, a physical architecture for CBL based on single photons is
discussed, and the effects of local reservoirs is numerically demonstrated.
Decision consistency in human decision-making tasks and in recruiting empirical
data are evaluated based on local reservoir. In summary, the proposed local
reservoir model paves a path toward establishing a foundation for computational
mechanisms and the systematic analysis of decision making on different levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Naruse_M/0/1/0/all/0/1&quot;&gt;Makoto Naruse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamamoto_E/0/1/0/all/0/1&quot;&gt;Eiji Yamamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nakao_T/0/1/0/all/0/1&quot;&gt;Takashi Nakao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Akimoto_T/0/1/0/all/0/1&quot;&gt;Takuma Akimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saigo_H/0/1/0/all/0/1&quot;&gt;Hayato Saigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Okamura_K/0/1/0/all/0/1&quot;&gt;Kazuya Okamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ojima_I/0/1/0/all/0/1&quot;&gt;Izumi Ojima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Northoff_G/0/1/0/all/0/1&quot;&gt;Georg Northoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hori_H/0/1/0/all/0/1&quot;&gt;Hirokazu Hori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04378">
<title>Fast Gaussian Process Based Gradient Matching for Parameter Identification in Systems of Nonlinear ODEs. (arXiv:1804.04378v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04378</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter identification and comparison of dynamical systems is a challenging
task in many fields. Bayesian approaches based on Gaussian process regression
over time-series data have been successfully applied to infer the parameters of
a dynamical system without explicitly solving it. While the benefits in
computational cost are well established, a rigorous mathematical framework has
been missing. We offer a novel interpretation which leads to a better
understanding and improvements in state-of-the-art performance in terms of
accuracy for nonlinear dynamical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wenk_P/0/1/0/all/0/1&quot;&gt;Philippe Wenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gotovos_A/0/1/0/all/0/1&quot;&gt;Alkis Gotovos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bauer_S/0/1/0/all/0/1&quot;&gt;Stefan Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorbach_N/0/1/0/all/0/1&quot;&gt;Nico Gorbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buhmann_J/0/1/0/all/0/1&quot;&gt;Joachim M. Buhmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04435">
<title>Variational Composite Autoencoders. (arXiv:1804.04435v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04435</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning in the latent variable model is challenging in the presence of the
complex data structure or the intractable latent variable. Previous variational
autoencoders can be low effective due to the straightforward encoder-decoder
structure. In this paper, we propose a variational composite autoencoder to
sidestep this issue by amortizing on top of the hierarchical latent variable
model. The experimental results confirm the advantages of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04438">
<title>Learned Deformation Stability in Convolutional Neural Networks. (arXiv:1804.04438v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.04438</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional wisdom holds that interleaved pooling layers in convolutional
neural networks lead to stability to small translations and deformations. In
this work, we investigate this claim empirically. We find that while pooling
confers stability to deformation at initialization, the deformation stability
at each layer changes significantly over the course of training and even
decreases in some layers, suggesting that deformation stability is not
unilaterally helpful. Surprisingly, after training, the pattern of deformation
stability across layers is largely independent of whether or not pooling was
present. We then show that a significant factor in determining deformation
stability is filter smoothness. Moreover, filter smoothness and deformation
stability are not simply a consequence of the distribution of input images, but
depend crucially on the joint distribution of images and labels. This work
demonstrates a way in which biases such as deformation stability can in fact be
learned and provides an example of understanding how a simple property of
learned network weights contributes to the overall network computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruderman_A/0/1/0/all/0/1&quot;&gt;Avraham Ruderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabinowitz_N/0/1/0/all/0/1&quot;&gt;Neil Rabinowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1&quot;&gt;Ari S. Morcos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1&quot;&gt;Daniel Zoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04440">
<title>Temporal Interpolation via Motion Field Prediction. (arXiv:1804.04440v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04440</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigated 2D multi-slice dynamic Magnetic Resonance (MR) imaging enables high
contrast 4D MR imaging during free breathing and provides in-vivo observations
for treatment planning and guidance. Navigator slices are vital for
retrospective stacking of 2D data slices in this method. However, they also
prolong the acquisition sessions. Temporal interpolation of navigator slices an
be used to reduce the number of navigator acquisitions without degrading
specificity in stacking. In this work, we propose a convolutional neural
network (CNN) based method for temporal interpolation via motion field
prediction. The proposed formulation incorporates the prior knowledge that a
motion field underlies changes in the image intensities over time. Previous
approaches that interpolate directly in the intensity space are prone to
produce blurry images or even remove structures in the images. Our method
avoids such problems and faithfully preserves the information in the image.
Further, an important advantage of our formulation is that it provides an
unsupervised estimation of bi-directional motion fields. We show that these
motion fields can be used to halve the number of registrations required during
4D reconstruction, thus substantially reducing the reconstruction time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karani_N/0/1/0/all/0/1&quot;&gt;Neerav Karani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tanner_C/0/1/0/all/0/1&quot;&gt;Christine Tanner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Konukoglu_E/0/1/0/all/0/1&quot;&gt;Ender Konukoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04469">
<title>Generative models for local network community detection. (arXiv:1804.04469v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1804.04469</link>
<description rdf:parseType="Literal">&lt;p&gt;Local network community detection aims to find a single community in a large
network, while inspecting only a small part of that network around a given seed
node. This is much cheaper than finding all communities in a network. Most
methods for local community detection are formulated as ad-hoc optimization
problems. In this work, we instead start from a generative model for networks
with community structure. By assuming that the network is uniform, we can
approximate the structure of unobserved parts of the network to obtain a method
for local community detection. We apply this local approximation technique to
two variants of the stochastic block model. To our knowledge, this results in
the first local community detection methods based on probabilistic models.
Interestingly, in the limit, one of the proposed approximations corresponds to
conductance, a popular metric in this field. Experiments on real and synthetic
datasets show comparable or improved results compared to state-of-the-art local
community detection algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laarhoven_T/0/1/0/all/0/1&quot;&gt;Twan van Laarhoven&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04503">
<title>When optimizing nonlinear objectives is no harder than linear objectives. (arXiv:1804.04503v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04503</link>
<description rdf:parseType="Literal">&lt;p&gt;Most systems and learning algorithms optimize average performance or average
loss --- one reason being computational complexity. However, many objectives of
practical interest are more complex than simply average loss. Examples include
balancing performance or loss with fairness across people, as well as balancing
precision and recall. We prove that, from a computational perspective, fairly
general families of complex objectives are not significantly harder to optimize
than standard averages, by providing polynomial-time reductions, i.e.,
algorithms that optimize complex objectives using linear optimizers. The
families of objectives included are arbitrary continuous functions of average
group performances and also convex objectives. We illustrate with applications
to fair machine learning, fair optimization and F1-scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabi_D/0/1/0/all/0/1&quot;&gt;Daniel Alabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Immorlica_N/0/1/0/all/0/1&quot;&gt;Nicole Immorlica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1&quot;&gt;Adam Tauman Kalai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04529">
<title>Online convex optimization and no-regret learning: Algorithms, guarantees and applications. (arXiv:1804.04529v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04529</link>
<description rdf:parseType="Literal">&lt;p&gt;Spurred by the enthusiasm surrounding the &quot;Big Data&quot; paradigm, the
mathematical and algorithmic tools of online optimization have found widespread
use in problems where the trade-off between data exploration and exploitation
plays a predominant role. This trade-off is of particular importance to several
branches and applications of signal processing, such as data mining,
statistical inference, multimedia indexing and wireless communications (to name
but a few). With this in mind, the aim of this tutorial paper is to provide a
gentle introduction to online optimization and learning algorithms that are
asymptotically optimal in hindsight - i.e., they approach the performance of a
virtual algorithm with unlimited computational power and full knowledge of the
future, a property known as no-regret. Particular attention is devoted to
identifying the algorithms&apos; theoretical performance guarantees and to establish
links with classic optimization paradigms (both static and stochastic). To
allow a better understanding of this toolbox, we provide several examples
throughout the tutorial ranging from metric learning to wireless resource
allocation problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belmega_E/0/1/0/all/0/1&quot;&gt;E. Veronica Belmega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mertikopoulos_P/0/1/0/all/0/1&quot;&gt;Panayotis Mertikopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negrel_R/0/1/0/all/0/1&quot;&gt;Romain Negrel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanguinetti_L/0/1/0/all/0/1&quot;&gt;Luca Sanguinetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04563">
<title>Towards integrating spatial localization in convolutional neural networks for brain image segmentation. (arXiv:1804.04563v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.04563</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is an established while rapidly evolving field in
medical imaging. In this paper we focus on the segmentation of brain Magnetic
Resonance Images (MRI) into cerebral structures using convolutional neural
networks (CNN). CNNs achieve good performance by finding effective high
dimensional image features describing the patch content only. In this work, we
propose different ways to introduce spatial constraints into the network to
further reduce prediction inconsistencies.
&lt;/p&gt;
&lt;p&gt;A patch based CNN architecture was trained, making use of multiple scales to
gather contextual information. Spatial constraints were introduced within the
CNN through a distance to landmarks feature or through the integration of a
probability atlas. We demonstrate experimentally that using spatial information
helps to reduce segmentation inconsistencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganaye_P/0/1/0/all/0/1&quot;&gt;Pierre-Antoine Ganaye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sdika_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#xeb;l Sdika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benoit_Cattin_H/0/1/0/all/0/1&quot;&gt;Hugues Benoit-Cattin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04566">
<title>Latent Geometry Inspired Graph Dissimilarities Enhance Affinity Propagation Community Detection in Complex Networks. (arXiv:1804.04566v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04566</link>
<description rdf:parseType="Literal">&lt;p&gt;Affinity propagation is one of the most effective algorithms for data
clustering in high-dimensional feature space. However the numerous attempts to
test its performance for community detection in real complex networks have been
attaining results very far from the state of the art methods such as Infomap
and Louvain. Yet, all these studies agreed that the crucial problem is to
convert the network topology in a &apos;smart-enough&apos; dissimilarity matrix that is
able to properly address the message passing procedure behind affinity
propagation clustering. Here we discuss how to leverage network latent geometry
notions in order to design dissimilarity matrices for affinity propagation
community detection. Our results demonstrate that the dissimilarity measures we
designed bring affinity propagation to outperform current state of the art
methods for community detection, not only on several original real networks,
but also when their structure is corrupted by noise artificially induced by
missing or spurious connectivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cannistraci_C/0/1/0/all/0/1&quot;&gt;Carlo Vittorio Cannistraci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muscoloni_A/0/1/0/all/0/1&quot;&gt;Alessandro Muscoloni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04577">
<title>Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and Some New Implementations. (arXiv:1804.04577v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04577</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we discuss policy iteration methods for approximate solution of
a finite-state discounted Markov decision problem, with a focus on
feature-based aggregation methods and their connection with deep reinforcement
learning schemes. We introduce features of the states of the original problem,
and we formulate a smaller &quot;aggregate&quot; Markov decision problem, whose states
relate to the features. The optimal cost function of the aggregate problem, a
nonlinear function of the features, serves as an architecture for approximation
in value space of the optimal cost function or the cost functions of policies
of the original problem. We discuss properties and possible implementations of
this type of aggregation, including a new approach to approximate policy
iteration. In this approach the policy improvement operation combines
feature-based aggregation with reinforcement learning based on deep neural
networks, which is used to obtain the needed features. We argue that the cost
function of a policy may be approximated much more accurately by the nonlinear
function of the features provided by aggregation, than by the linear function
of the features provided by deep reinforcement learning, thereby potentially
leading to more effective policy improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertsekas_D/0/1/0/all/0/1&quot;&gt;Dimitri P. Bertsekas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04614">
<title>Impulsive Noise Robust Sparse Recovery via Continuous Mixed Norm. (arXiv:1804.04614v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1804.04614</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the problem of sparse signal recovery in the presence
of additive impulsive noise. The heavytailed impulsive noise is well modelled
with stable distributions. Since there is no explicit formulation for the
probability density function of $S\alpha S$ distribution, alternative
approximations like Generalized Gaussian Distribution (GGD) are used which
impose $\ell_p$-norm fidelity on the residual error. In this paper, we exploit
a Continuous Mixed Norm (CMN) for robust sparse recovery instead of
$\ell_p$-norm. We show that in blind conditions, i.e., in case where the
parameters of noise distribution are unknown, incorporating CMN can lead to
near optimal recovery. We apply Alternating Direction Method of Multipliers
(ADMM) for solving the problem induced by utilizing CMN for robust sparse
recovery. In this approach, CMN is replaced with a surrogate function and
Majorization-Minimization technique is incorporated to solve the problem.
Simulation results confirm the efficiency of the proposed method compared to
some recent algorithms in the literature for impulsive noise robust sparse
recovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Javaheri_A/0/1/0/all/0/1&quot;&gt;Amirhossein Javaheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zayyani_H/0/1/0/all/0/1&quot;&gt;Hadi Zayyani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Figueiredo_M/0/1/0/all/0/1&quot;&gt;Mario A. T. Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marvasti_F/0/1/0/all/0/1&quot;&gt;Farrokh Marvasti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04622">
<title>Causal Inference via Kernel Deviance Measures. (arXiv:1804.04622v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04622</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering the causal structure among a set of variables is a fundamental
problem in many areas of science. In this paper, we propose Kernel Conditional
Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery
method based on purely observational data. From a novel interpretation of the
notion of asymmetry between cause and effect, we derive a corresponding
asymmetry measure using the framework of reproducing kernel Hilbert spaces.
Based on this, we propose three decision rules for causal discovery. We
demonstrate the wide applicability of our method across a range of diverse
synthetic datasets. Furthermore, we test our method on real-world time series
data and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we
outperform existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1&quot;&gt;Jovana Mitrovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sejdinovic_D/0/1/0/all/0/1&quot;&gt;Dino Sejdinovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04640">
<title>Fast Counting in Machine Learning Applications. (arXiv:1804.04640v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04640</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose scalable methods to execute counting queries in machine learning
applications. To achieve memory and computational efficiency, we abstract
counting queries and their context such that the counts can be aggregated as a
stream. We demonstrate performance and scalability of the resulting approach on
random queries, and through extensive experimentation using Bayesian networks
learning and association rule mining. Our methods significantly outperform
commonly used ADtrees and hash tables, and are practical alternatives for
processing large-scale data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karan_S/0/1/0/all/0/1&quot;&gt;Subhadeep Karan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eichhorn_M/0/1/0/all/0/1&quot;&gt;Matthew Eichhorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hurlburt_B/0/1/0/all/0/1&quot;&gt;Blake Hurlburt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Iraci_G/0/1/0/all/0/1&quot;&gt;Grant Iraci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zola_J/0/1/0/all/0/1&quot;&gt;Jaroslaw Zola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.07549">
<title>The Stochastic complexity of spin models: Are pairwise models really simple?. (arXiv:1702.07549v3 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/1702.07549</link>
<description rdf:parseType="Literal">&lt;p&gt;Models can be simple for different reasons: because they yield a simple and
computationally efficient interpretation of a generic dataset (e.g. in terms of
pairwise dependences) - as in statistical learning - or because they capture
the essential ingredients of a specific phenomenon - as e.g. in physics -
leading to non-trivial falsifiable predictions. In information theory and
Bayesian inference, the simplicity of a model is precisely quantified in the
stochastic complexity, which measures the number of bits needed to encode its
parameters. In order to understand how simple models look like, we study the
stochastic complexity of spin models with interactions of arbitrary order. We
highlight the existence of invariances with respect to bijections within the
space of operators, which allow us to partition the space of all models into
equivalence classes, in which models share the same complexity. We thus found
that the complexity (or simplicity) of a model is not determined by the order
of the interactions, but rather by their mutual arrangements. Models where
statistical dependencies are localized on non-overlapping groups of few
variables (and that afford predictions on independencies that are easy to
falsify) are simple. On the contrary, fully connected pairwise models, which
are often used in statistical learning, appear to be highly complex, because of
their extended set of interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Beretta_A/0/1/0/all/0/1&quot;&gt;Alberto Beretta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Battistin_C/0/1/0/all/0/1&quot;&gt;Claudia Battistin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Mulatier_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;lia de Mulatier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Mastromatteo_I/0/1/0/all/0/1&quot;&gt;Iacopo Mastromatteo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Marsili_M/0/1/0/all/0/1&quot;&gt;Matteo Marsili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01882">
<title>Hyperbolic Entailment Cones for Learning Hierarchical Embeddings. (arXiv:1804.01882v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01882</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning graph representations via low-dimensional embeddings that preserve
relevant network properties is an important class of problems in machine
learning. We here present a novel method to embed directed acyclic graphs.
Following prior work, we first advocate for using hyperbolic spaces which
provably model tree-like structures better than Euclidean geometry. Second, we
view hierarchical relations as partial orders defined using a family of nested
geodesically convex cones. We prove that these entailment cones admit an
optimal shape with a closed form expression both in the Euclidean and
hyperbolic spaces. Moreover, they canonically define the embedding learning
process. Experiments show significant improvements of our method over strong
recent baselines both in terms of representational capacity and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganea_O/0/1/0/all/0/1&quot;&gt;Octavian-Eugen Ganea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becigneul_G/0/1/0/all/0/1&quot;&gt;Gary B&amp;#xe9;cigneul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04097">
<title>End-to-end Deep Learning of Optical Fiber Communications. (arXiv:1804.04097v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04097</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we implement an optical fiber communication system as an
end-to-end deep neural network, including the complete chain of transmitter,
channel model, and receiver. This approach enables the optimization of the
transceiver in a single end-to-end process. We illustrate the benefits of this
method by applying it to intensity modulation/direct detection (IM/DD) systems
and show that we can achieve bit error rates below the 6.7\% hard-decision
forward error correction (HD-FEC) threshold. We model all componentry of the
transmitter and receiver, as well as the fiber channel, and apply deep learning
to find transmitter and receiver configurations minimizing the symbol error
rate. We propose and verify in simulations a training method that yields robust
and flexible transceivers that allow---without reconfiguration---reliable
transmission over a large range of link dispersions. The results from
end-to-end deep learning are successfully verified for the first time in an
experiment. In particular, we achieve information rates of 42\,Gb/s below the
HD-FEC threshold at distances beyond 40\,km. We find that our results
outperform conventional IM/DD solutions based on 2 and 4 level pulse amplitude
modulation (PAM2/PAM4) with feedforward equalization (FFE) at the receiver. Our
study is the first step towards end-to-end deep learning-based optimization of
optical fiber communication systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanov_B/0/1/0/all/0/1&quot;&gt;Boris Karanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chagnon_M/0/1/0/all/0/1&quot;&gt;Mathieu Chagnon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thouin_F/0/1/0/all/0/1&quot;&gt;F&amp;#xe9;lix Thouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eriksson_T/0/1/0/all/0/1&quot;&gt;Tobias A. Eriksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulow_H/0/1/0/all/0/1&quot;&gt;Henning B&amp;#xfc;low&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavery_D/0/1/0/all/0/1&quot;&gt;Domani&amp;#xe7; Lavery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayvel_P/0/1/0/all/0/1&quot;&gt;Polina Bayvel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalen_L/0/1/0/all/0/1&quot;&gt;Laurent Schmalen&lt;/a&gt;</dc:creator>
</item></rdf:RDF>