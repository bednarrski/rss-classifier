<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04034"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.04034">
<title>Learning Robust Dialog Policies in Noisy Environments. (arXiv:1712.04034v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.04034</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern virtual personal assistants provide a convenient interface for
completing daily tasks via voice commands. An important consideration for these
assistants is the ability to recover from automatic speech recognition (ASR)
and natural language understanding (NLU) errors. In this paper, we focus on
learning robust dialog policies to recover from these errors. To this end, we
develop a user simulator which interacts with the assistant through voice
commands in realistic scenarios with noisy audio, and use it to learn dialog
policies through deep reinforcement learning. We show that dialogs generated by
our simulator are indistinguishable from human generated dialogs, as determined
by human evaluators. Furthermore, preliminary experimental results show that
the learned policies in noisy environments achieve the same execution success
rate with fewer dialog turns compared to fixed rule-based policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazel_Zarandi_M/0/1/0/all/0/1&quot;&gt;Maryam Fazel-Zarandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casale_J/0/1/0/all/0/1&quot;&gt;Jared Casale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1&quot;&gt;Peter Henderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitney_D/0/1/0/all/0/1&quot;&gt;David Whitney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1&quot;&gt;Alborz Geramifard&lt;/a&gt;</dc:creator>
</item></rdf:RDF>