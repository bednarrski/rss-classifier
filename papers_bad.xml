<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03505"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04749"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05250"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11192"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04155"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04591"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04720"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04807"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04810"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04874"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04933"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04958"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04982"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05052"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05071"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05189"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.03159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.06864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.05246"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06838"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06633"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06720"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03284"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03444"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.04513">
<title>Laconic Deep Learning Computing. (arXiv:1805.04513v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.04513</link>
<description rdf:parseType="Literal">&lt;p&gt;We motivate a method for transparently identifying ineffectual computations
in unmodified Deep Learning models and without affecting accuracy.
Specifically, we show that if we decompose multiplications down to the bit
level the amount of work performed during inference for image classification
models can be consistently reduced by two orders of magnitude. In the best case
studied of a sparse variant of AlexNet, this approach can ideally reduce
computation work by more than 500x. We present Laconic a hardware accelerator
that implements this approach to improve execution time, and energy efficiency
for inference with Deep Learning Networks. Laconic judiciously gives up some of
the work reduction potential to yield a low-cost, simple, and energy efficient
design that outperforms other state-of-the-art accelerators. For example, a
Laconic configuration that uses a weight memory interface with just 128 wires
outperforms a conventional accelerator with a 2K-wire weight memory interface
by 2.3x on average while being 2.13x more energy efficient on average. A
Laconic configuration that uses a 1K-wire weight memory interface, outperforms
the 2K-wire conventional accelerator by 15.4x and is 1.95x more energy
efficient. Laconic does not require but rewards advances in model design such
as a reduction in precision, the use of alternate numeric representations that
reduce the number of bits that are &quot;1&quot;, or an increase in weight or activation
sparsity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharify_S/0/1/0/all/0/1&quot;&gt;Sayeh Sharify&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoud_M/0/1/0/all/0/1&quot;&gt;Mostafa Mahmoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lascorz_A/0/1/0/all/0/1&quot;&gt;Alberto Delmas Lascorz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolic_M/0/1/0/all/0/1&quot;&gt;Milos Nikolic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moshovos_A/0/1/0/all/0/1&quot;&gt;Andreas Moshovos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04924">
<title>Emergence and Evolution of Hierarchical Structure in Complex Systems. (arXiv:1805.04924v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.04924</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well known that many complex systems, both in technology and nature,
exhibit hierarchical modularity. What is not well understood however is how
this hierarchical structure (which is fundamentally a network property)
emerges, and how it evolves over time. Further, the norm is that hierarchical
systems are designed incrementally over time to provide support for new outputs
and potentially to new inputs. This is very different than re-designing a new
system &quot;from scratch&quot; after a change in the outputs or inputs.
&lt;/p&gt;
&lt;p&gt;We propose a modeling framework, referred to as Evo-Lexis, that provides
insight to some general and fundamental queries about evolving hierarchical
systems. Evo-Lexis models the system inputs as symbols (&quot;sources&quot;) and the
outputs as sequences of those symbols (&quot;targets&quot;). Evo-Lexis computes the
optimized adjustment of a given hierarchy when the set of targets changes over
time by additions and removals (&quot;incremental design&quot;). Additionally, Evo-Lexis
computes the optimized hierarchy that generates a given set of targets from a
set of sources in a static (non-evolving) setting (&quot;clean-slate design&quot;). The
questions we focus on are:
&lt;/p&gt;
&lt;p&gt;1. How do some key properties of this hierarchy, e.g. depth of the network,
reuse or centrality of each module, complexity (or sequence length) of
intermediate modules, etc., depend on the evolutionary process generating the
new targets? 2. Under what conditions do the emergent hierarchies exhibit the
so called &quot;hourglass effect&quot;? Why are few intermediate modules reused much more
than others? 3. Do intermediate modules persist during the evolution of
hierarchies? Or are there &quot;punctuated equilibria&quot; where the highly reused
modules change significantly? 4. Which are the differences in terms of cost and
structure between the incrementally designed and the corresponding clean-slate
designed hierarchies?
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siyari_P/0/1/0/all/0/1&quot;&gt;Payam Siyari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dovrolis_C/0/1/0/all/0/1&quot;&gt;Constantine Dovrolis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05047">
<title>Triclustering of Gene Expression Microarray data using Evolutionary Approach. (arXiv:1805.05047v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.05047</link>
<description rdf:parseType="Literal">&lt;p&gt;In Tri-clustering, a sub-matrix is being created, which exhibit highly
similar behavior with respect to genes, conditions and time-points. In this
technique, genes with same expression values are discovered across some
fragment of time points, under certain conditions. In this paper, triclustering
using evolutionary algorithm is implemented using a new fitness function
consisting of 3D Mean Square residue (MSR) and Least Square approximation
(LSL). The primary objective is to find triclusters with minimum overlapping,
low MSR, low LSL and covering almost every element of expression matrix, thus
minimizing the overall fitness value. To improve the results of algorithm, new
fitness function is introduced to find good quality triclusters. It is observed
from experiments that, triclustering using EA yielded good quality triclusters.
The experiment was implemented on yeast Saccharomyces dataset. Index
Terms-Tri-clustering, Genetic Algorithm, Mean squared residue, Volume, Weights,
Least square approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Shreya Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vipsita_S/0/1/0/all/0/1&quot;&gt;Swati Vipsita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03505">
<title>Plummer Autoencoders. (arXiv:1802.03505v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03505</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the true density in high-dimensional feature spaces is a
well-known problem in machine learning. We propose a new implicit generative
model based on autoencoders, whose objective function contains a new
regularization term. We show that this regularizer has connections with the
maximum mean discrepancy distance (MMD). Furthemore, we show that the choice of
kernel function is fundamental (i) to ensure the global convergence of training
based on gradient-descent algorithms and (ii) to transform the mini-max game of
generative adversarial networks (GANs) based on MMD into a single minimization.
The resulting algorithm is therefore more stable compared to GANs and ensures
convergence to optimal solutions. The theory is corroborated by extensive
experimental comparisons on synthetic and real-world datasets against several
approaches from the families of GANs and autoencoder-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1&quot;&gt;Emanuele Sansone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_Q/0/1/0/all/0/1&quot;&gt;Quoc-Tin Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1&quot;&gt;Francesco G.B. De Natale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03891">
<title>Three levels of neural reuse in embodied agents. (arXiv:1802.03891v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03891</link>
<description rdf:parseType="Literal">&lt;p&gt;The brain in conjunction with the body is able to adapt to new environments
and perform multiple behaviors through reuse of neural resources and transfer
of existing behavioral traits. Although mechanisms that underlie this ability
are not well known, they are largely attributed to neuromodulation. In this
work, we demonstrate that an agent can be multifunctional using the same
sensory and motor systems across behaviors, in the absence of modulatory
mechanisms. Further, we lay out the different levels at which neural reuse can
occur through a dynamical filtering of the brain-body-environment system&apos;s
operation: structural network, autonomous dynamics, and transient dynamics.
Notably, transient dynamics reuse could only be explained by studying the
brain-body-environment system as a whole and not just the brain. The
multifunctional agent we present here demonstrates neural reuse at all three
levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Candadai_M/0/1/0/all/0/1&quot;&gt;Madhavun Candadai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izquierdo_E/0/1/0/all/0/1&quot;&gt;Eduardo Izquierdo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04582">
<title>TensOrMachine: Probabilistic Boolean Tensor Decomposition. (arXiv:1805.04582v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04582</link>
<description rdf:parseType="Literal">&lt;p&gt;Boolean tensor decomposition approximates data of multi-way binary
relationships as product of interpretable low-rank binary factors, following
the rules of Boolean algebra. Here, we present its first probabilistic
treatment. We facilitate scalable sampling-based posterior inference by
exploitation of the combinatorial structure of the factor conditionals. Maximum
a posteriori decompositions feature higher accuracies than existing techniques
throughout a wide range of simulated conditions. Moreover, the probabilistic
approach facilitates the treatment of missing data and enables model selection
with much greater accuracy. We investigate three real-world data-sets. First,
temporal interaction networks in a hospital ward and behavioural data of
university students demonstrate the inference of instructive latent patterns.
Next, we decompose a tensor with more than 10 billion data points, indicating
relations of gene expression in cancer patients. Not only does this demonstrate
scalability, it also provides an entirely novel perspective on relational
properties of continuous data and, in the present example, on the molecular
heterogeneity of cancer. Our implementation is available on GitHub:
https://github.com/TammoR/LogicalFactorisationMachines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rukat_T/0/1/0/all/0/1&quot;&gt;Tammo Rukat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Chris C. Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yau_C/0/1/0/all/0/1&quot;&gt;Christopher Yau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04748">
<title>Towards Autonomous Reinforcement Learning: Automatic Setting of Hyper-parameters using Bayesian Optimization. (arXiv:1805.04748v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04748</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increase of machine learning usage by industries and scientific
communities in a variety of tasks such as text mining, image recognition and
self-driving cars, automatic setting of hyper-parameter in learning algorithms
is a key factor for achieving satisfactory performance regardless of user
expertise in the inner workings of the techniques and methodologies. In
particular, for a reinforcement learning algorithm, the efficiency of an agent
learning a control policy in an uncertain environment is heavily dependent on
the hyper-parameters used to balance exploration with exploitation. In this
work, an autonomous learning framework that integrates Bayesian optimization
with Gaussian process regression to optimize the hyper-parameters of a
reinforcement learning algorithm, is proposed. Also, a bandits-based approach
to achieve a balance between computational costs and decreasing uncertainty
about the Q-values, is presented. A gridworld example is used to highlight how
hyper-parameter configurations of a learning algorithm (SARSA) are iteratively
improved based on two performance functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barsce_J/0/1/0/all/0/1&quot;&gt;Juan Cruz Barsce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palombarini_J/0/1/0/all/0/1&quot;&gt;Jorge A. Palombarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_E/0/1/0/all/0/1&quot;&gt;Ernesto C. Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04749">
<title>A Cognitive Approach to Real-time Rescheduling using SOAR-RL. (arXiv:1805.04749v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04749</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring flexible and efficient manufacturing of customized products in an
increasing dynamic and turbulent environment without sacrificing cost
effectiveness, product quality and on-time delivery has become a key issue for
most industrial enterprises. A promising approach to cope with this challenge
is the integration of cognitive capabilities in systems and processes with the
aim of expanding the knowledge base used to perform managerial and operational
tasks. In this work, a novel approach to real-time rescheduling is proposed in
order to achieve sustainable improvements in flexibility and adaptability of
production systems through the integration of artificial cognitive
capabilities, involving perception, reasoning/learning and planning skills.
Moreover, an industrial example is discussed where the SOAR cognitive
architecture capabilities are integrated in a software prototype, showing that
the approach enables the rescheduling system to respond to events in an
autonomic way, and to acquire experience through intensive simulation while
performing repair tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barsce_J/0/1/0/all/0/1&quot;&gt;Juan Cruz Barsce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palombarini_J/0/1/0/all/0/1&quot;&gt;Jorge A. Palombarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_E/0/1/0/all/0/1&quot;&gt;Ernesto C. Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04829">
<title>Spatial Uncertainty Sampling for End-to-End Control. (arXiv:1805.04829v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04829</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end trained neural networks (NNs) are a compelling approach to
autonomous vehicle control because of their ability to learn complex tasks
without manual engineering of rule-based decisions. However, challenging road
conditions, ambiguous navigation situations, and safety considerations require
reliable uncertainty estimation for the eventual adoption of full-scale
autonomous vehicles. Bayesian deep learning approaches provide a way to
estimate uncertainty by approximating the posterior distribution of weights
given a set of training data. Dropout training in deep NNs approximates
Bayesian inference in a deep Gaussian process and can thus be used to estimate
model uncertainty. In this paper, we propose a Bayesian NN for end-to-end
control that estimates uncertainty by exploiting feature map correlation during
training. This approach achieves improved model fits, as well as tighter
uncertainty estimates, than traditional element-wise dropout. We evaluate our
algorithms on a challenging dataset collected over many different road types,
times of day, and weather conditions, and demonstrate how uncertainties can be
used in conjunction with a human controller in a parallel autonomous setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1&quot;&gt;Alexander Amini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleimany_A/0/1/0/all/0/1&quot;&gt;Ava Soleimany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1&quot;&gt;Sertac Karaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04955">
<title>Low-pass Recurrent Neural Networks - A memory architecture for longer-term correlation discovery. (arXiv:1805.04955v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04955</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) agents performing complex tasks must be able to
remember observations and actions across sizable time intervals. This is
especially true during the initial learning stages, when exploratory behaviour
can increase the delay between specific actions and their effects. Many new or
popular approaches for learning these distant correlations employ
backpropagation through time (BPTT), but this technique requires storing
observation traces long enough to span the interval between cause and effect.
Besides memory demands, learning dynamics like vanishing gradients and slow
convergence due to infrequent weight updates can reduce BPTT&apos;s practicality;
meanwhile, although online recurrent network learning is a developing topic,
most approaches are not efficient enough to use as replacements. We propose a
simple, effective memory strategy that can extend the window over which BPTT
can learn without requiring longer traces. We explore this approach empirically
on a few tasks and discuss its implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stepleton_T/0/1/0/all/0/1&quot;&gt;Thomas Stepleton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabney_W/0/1/0/all/0/1&quot;&gt;Will Dabney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayakumar_S/0/1/0/all/0/1&quot;&gt;Siddhant M. Jayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soyer_H/0/1/0/all/0/1&quot;&gt;Hubert Soyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1&quot;&gt;Remi Munos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04961">
<title>Multi-Agent Path Finding with Deadlines: Preliminary Results. (arXiv:1805.04961v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04961</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the problem of multi-agent path finding with deadlines
(MAPF-DL). The objective is to maximize the number of agents that can reach
their given goal vertices from their given start vertices within a given
deadline, without colliding with each other. We first show that the MAPF-DL
problem is NP-hard to solve optimally. We then present an optimal MAPF-DL
algorithm based on a reduction of the MAPF-DL problem to a flow problem and a
subsequent compact integer linear programming formulation of the resulting
reduced abstracted multi-commodity flow network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_G/0/1/0/all/0/1&quot;&gt;Glenn Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felner_A/0/1/0/all/0/1&quot;&gt;Ariel Felner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_T/0/1/0/all/0/1&quot;&gt;T. K. Satish Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koenig_S/0/1/0/all/0/1&quot;&gt;Sven Koenig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05081">
<title>Constructing Narrative Event Evolutionary Graph for Script Event Prediction. (arXiv:1805.05081v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.05081</link>
<description rdf:parseType="Literal">&lt;p&gt;Script event prediction requires a model to predict the subsequent event
given an existing event context. Previous models based on event pairs or event
chains cannot make full use of dense event connections, which may limit their
capability of event prediction. To remedy this, we propose constructing an
event graph to better utilize the event network information for script event
prediction. In particular, we first extract narrative event chains from large
quantities of news corpus, and then construct a narrative event evolutionary
graph (NEEG) based on the extracted chains. NEEG can be seen as a knowledge
base that describes event evolutionary principles and patterns. To solve the
inference problem on NEEG, we present a scaled graph neural network (SGNN) to
model event interactions and learn better event representations. Instead of
computing the representations on the whole graph, SGNN processes only the
concerned nodes each time, which makes our model feasible to large-scale
graphs. By comparing the similarity between input context event representations
and candidate event representations, we can choose the most reasonable
subsequent event. Experimental results on widely used New York Times corpus
demonstrate that our model significantly outperforms state-of-the-art baseline
methods, by using standard multiple choice narrative cloze evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05230">
<title>Maximizing Expected Impact in an Agent Reputation Network -- Technical Report. (arXiv:1805.05230v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.05230</link>
<description rdf:parseType="Literal">&lt;p&gt;Many multi-agent systems (MASs) are situated in stochastic environments. Some
such systems that are based on the partially observable Markov decision process
(POMDP) do not take the benevolence of other agents for granted. We propose a
new POMDP-based framework which is general enough for the specification of a
variety of stochastic MAS domains involving the impact of agents on each
other&apos;s reputations. A unique feature of this framework is that actions are
specified as either undirected (regular) or directed (towards a particular
agent), and a new directed transition function is provided for modeling the
effects of reputation in interactions. Assuming that an agent must maintain a
good enough reputation to survive in the network, a planning algorithm is
developed for an agent to select optimal actions in stochastic MASs.
Preliminary evaluation is provided via an example specification and by
determining the algorithm&apos;s complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rens_G/0/1/0/all/0/1&quot;&gt;Gavin Rens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1&quot;&gt;Abhaya Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_T/0/1/0/all/0/1&quot;&gt;Thomas Meyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05250">
<title>Blockchain to Improve Security and Knowledge in Inter-Agent Communication and Collaboration over a Restrict Domains of the Internet Infrastructure. (arXiv:1805.05250v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.05250</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the deployment and implementation of a blockchain to
improve the security, knowledge and intelligence during the inter-agent
communication and collaboration processes in restrict domains of the Internet
Infrastructure. It is a work that proposes the application of a blockchain,
platform independent, on a particular model of agents, but that can be used in
similar proposals, once the results on the specific model were satisfactory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribas_J/0/1/0/all/0/1&quot;&gt;Jessica Ribas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05240">
<title>Weakly-supervised Semantic Parsing with Abstract Examples. (arXiv:1711.05240v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05240</link>
<description rdf:parseType="Literal">&lt;p&gt;Training semantic parsers from weak supervision (denotations) rather than
strong supervision (programs) complicates training in two ways. First, a large
search space of potential programs needs to be explored at training time to
find a correct program. Second, spurious programs that accidentally lead to a
correct denotation add noise to training. In this work we propose that in
closed worlds with clear semantic types, one can substantially alleviate these
problems by utilizing an abstract representation, where tokens in both the
language utterance and program are lifted to an abstract form. We show that
these abstractions can be defined with a handful of lexical rules and that they
result in sharing between different examples that alleviates the difficulties
in training. To test our approach, we develop the first semantic parser for
CNLVR, a challenging visual reasoning dataset, where the search space is large
and overcoming spuriousness is critical, because denotations are either TRUE or
FALSE, and thus random programs are likely to lead to a correct denotation. Our
method substantially improves performance, and reaches 82.5% accuracy, a 14.7%
absolute accuracy improvement compared to the best reported accuracy so far.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1&quot;&gt;Omer Goldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latcinnik_V/0/1/0/all/0/1&quot;&gt;Veronica Latcinnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naveh_U/0/1/0/all/0/1&quot;&gt;Udi Naveh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1&quot;&gt;Amir Globerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01527">
<title>A Quantitative Analysis of Multi-Winner Rules. (arXiv:1801.01527v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01527</link>
<description rdf:parseType="Literal">&lt;p&gt;To choose a suitable multi-winner rule, i.e., a voting rule for selecting a
subset of $k$ alternatives based on a collection of preferences, is a hard and
ambiguous task. Depending on the context, it varies widely what constitutes the
choice of an &quot;optimal&quot; subset. In this paper, we offer a new perspective to
measure the quality of such subsets and---consequently---multi-winner rules. We
provide a quantitative analysis using methods from the theory of approximation
algorithms and estimate how well multi-winner rules approximate two extreme
objectives: diversity as captured by the (Approval) Chamberlin--Courant rule
and individual excellence as captured by Multi-winner Approval Voting. With
both theoretical and experimental methods we classify multi-winner rules in
terms of their quantitative alignment with these two opposing objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lackner_M/0/1/0/all/0/1&quot;&gt;Martin Lackner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skowron_P/0/1/0/all/0/1&quot;&gt;Piotr Skowron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06889">
<title>Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers. (arXiv:1801.06889v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06889</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has recently seen rapid development and received significant
attention due to its state-of-the-art performance on previously-thought hard
problems. However, because of the internal complexity and nonlinear structure
of deep neural networks, the underlying decision making processes for why these
models are achieving such performance are challenging and sometimes mystifying
to interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
which highlights its short yet impactful history and thoroughly summarizes the
state-of-the-art using a human-centered interrogative framework, focusing on
the Five W&apos;s and How (Why, Who, What, How, When, and Where). We conclude by
highlighting research directions and open research problems. This survey helps
researchers and practitioners in both visual analytics and deep learning to
quickly learn key aspects of this young and rapidly growing body of research,
whose impact spans a diverse range of domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohman_F/0/1/0/all/0/1&quot;&gt;Fred Hohman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1&quot;&gt;Minsuk Kahng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pienta_R/0/1/0/all/0/1&quot;&gt;Robert Pienta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10133">
<title>You are your Metadata: Identification and Obfuscation of Social Media Users using Metadata Information. (arXiv:1803.10133v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10133</link>
<description rdf:parseType="Literal">&lt;p&gt;Metadata are associated to most of the information we produce in our daily
interactions and communication in the digital world. Yet, surprisingly,
metadata are often still catergorized as non-sensitive. Indeed, in the past,
researchers and practitioners have mainly focused on the problem of the
identification of a user from the content of a message.
&lt;/p&gt;
&lt;p&gt;In this paper, we use Twitter as a case study to quantify the uniqueness of
the association between metadata and user identity and to understand the
effectiveness of potential obfuscation strategies. More specifically, we
analyze atomic fields in the metadata and systematically combine them in an
effort to classify new tweets as belonging to an account using different
machine learning algorithms of increasing complexity. We demonstrate that
through the application of a supervised learning algorithm, we are able to
identify any user in a group of 10,000 with approximately 96.7% accuracy.
Moreover, if we broaden the scope of our search and consider the 10 most likely
candidates we increase the accuracy of the model to 99.22%. We also found that
data obfuscation is hard and ineffective for this type of data: even after
perturbing 60% of the training data, it is still possible to classify users
with an accuracy higher than 95%. These results have strong implications in
terms of the design of metadata obfuscation strategies, for example for data
set release, not only for Twitter, but, more generally, for most social media
platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_B/0/1/0/all/0/1&quot;&gt;Beatrice Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1&quot;&gt;Mirco Musolesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stringhini_G/0/1/0/all/0/1&quot;&gt;Gianluca Stringhini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00456">
<title>Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning. (arXiv:1804.00456v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00456</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates exploration strategies of Deep Reinforcement Learning
(DRL) methods to learn navigation policies for mobile robots. In particular, we
augment the normal external reward for training DRL algorithms with intrinsic
reward signals measured by curiosity. We test our approach in a mapless
navigation setting, where the autonomous agent is required to navigate without
the occupancy map of the environment, to targets whose relative locations can
be easily acquired through low-cost solutions (e.g., visible light
localization, Wi-Fi signal localization). We validate that the intrinsic
motivation is crucial for improving DRL performance in tasks with challenging
exploration requirements. Our experimental results show that our proposed
method is able to more effectively learn navigation policies, and has better
generalization capabilities in previously unseen environments. A video of our
experimental results can be found at https://goo.gl/pWbpcF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhelo_O/0/1/0/all/0/1&quot;&gt;Oleksii Zhelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_L/0/1/0/all/0/1&quot;&gt;Lei Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11192">
<title>Explainable Recommendation: A Survey and New Perspectives. (arXiv:1804.11192v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1804.11192</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Recommendation refers to the personalized recommendation
algorithms that address the problem of why -- they not only provide the user
with the recommendations, but also make the user aware why such items are
recommended by generating recommendation explanations, which help to improve
the effectiveness, efficiency, persuasiveness, and user satisfaction of
recommender systems. In recent years, a large number of explainable
recommendation approaches -- especially model-based explainable recommendation
algorithms -- have been proposed and adopted in real-world systems.
&lt;/p&gt;
&lt;p&gt;In this survey, we review the work on explainable recommendation that has
been published in or before the year of 2018. We first high-light the position
of explainable recommendation in recommender system research by categorizing
recommendation problems into the 5W, i.e., what, when, who, where, and why. We
then conduct a comprehensive survey of explainable recommendation itself in
terms of three aspects: 1) We provide a chronological research line of
explanations in recommender systems, including the user study approaches in the
early years, as well as the more recent model-based approaches. 2) We provide a
taxonomy for explainable recommendation algorithms, including user-based,
item-based, model-based, and post-model explanations. 3) We summarize the
application of explainable recommendation in different recommendation tasks,
including product recommendation, social recommendation, POI recommendation,
etc. We devote a chapter to discuss the explanation perspectives in the broader
IR and machine learning settings, as well as their relationship with
explainable recommendation research. We end the survey by discussing potential
future research directions to promote the explainable recommendation research
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01954">
<title>Behavioral Cloning from Observation. (arXiv:1805.01954v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01954</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans often learn how to perform tasks via imitation: they observe others
perform a task, and then very quickly infer the appropriate actions to take
based on their observations. While extending this paradigm to autonomous agents
is a well-studied problem in general, there are two particular aspects that
have largely been overlooked: (1) that the learning is done from observation
only (i.e., without explicit action information), and (2) that the learning is
typically done very quickly. In this work, we propose a two-phase, autonomous
imitation learning technique called behavioral cloning from observation (BCO),
that aims to provide improved performance with respect to both of these
aspects. First, we allow the agent to acquire experience in a self-supervised
fashion. This experience is used to develop a model which is then utilized to
learn a particular task by observing an expert perform that task without the
knowledge of the specific actions taken. We experimentally compare BCO to
imitation learning methods, including the state-of-the-art, generative
adversarial imitation learning (GAIL) technique, and we show comparable task
performance in several different simulation domains while exhibiting increased
learning speed after expert trajectories become available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1&quot;&gt;Faraz Torabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1&quot;&gt;Garrett Warnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04032">
<title>From Word to Sense Embeddings: A Survey on Vector Representations of Meaning. (arXiv:1805.04032v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04032</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past years, distributed representations have proven effective and
flexible keepers of prior knowledge to be integrated into downstream
applications. This survey is focused on semantic representation of meaning. We
start from the theoretical background behind word vector space models and
highlight one of their main limitations: the meaning conflation deficiency,
which arises from representing a word with all its possible meanings as a
single vector. Then, we explain how this deficiency can be addressed through a
transition from word level to the more fine-grained level of word senses (in
its broader acceptation) as a method for modelling unambiguous lexical meaning.
We present a comprehensive overview of the wide range of techniques in the two
main branches of sense representation, i.e., unsupervised and knowledge-based.
Finally, this survey covers the main evaluation procedures and provides an
analysis of five important aspects: interpretability, sense granularity,
adaptability to different domains, compositionality and integration into
downstream applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1&quot;&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1&quot;&gt;Mohammad Taher Pilehvar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04155">
<title>Toward `verifying&apos; a Water Treatment System. (arXiv:1712.04155v2 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.04155</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling and verifying real-world cyber-physical systems is challenging,
which is especially so for complex systems where manually modeling is
infeasible. In this work, we report our experience on combining model learning
and abstraction refinement to analyze a challenging system, i.e., a real-world
Secure Water Treatment system (SWaT). Given a set of safety requirements, the
objective is to either show that the system is safe with a high probability (so
that a system shutdown is rarely triggered due to safety violation) or not. As
the system is too complicated to be manually modeled, we apply latest automatic
model learning techniques to construct a set of Markov chains through
abstraction and refinement, based on two long system execution logs (one for
training and the other for testing). For each probabilistic safety property, we
either report it does not hold with a certain level of probabilistic
confidence, or report that it holds by showing the evidence in the form of an
abstract Markov chain. The Markov chains can subsequently be implemented as
runtime monitors in SWaT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yifan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Shengchao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwu Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04577">
<title>Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions. (arXiv:1805.04577v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04577</link>
<description rdf:parseType="Literal">&lt;p&gt;Error bound conditions (EBC) are properties that characterize the growth of
an objective function when a point is moved away from the optimal set. They
have recently received increasing attention in the field of optimization for
developing optimization algorithms with fast convergence. However, the studies
of EBC in statistical learning are hitherto still limited. The main
contributions of this paper are two-fold. First, we develop fast and
intermediate rates of empirical risk minimization (ERM) under EBC for risk
minimization with Lipschitz continuous, and smooth convex random functions.
Second, we establish fast and intermediate rates of an efficient stochastic
approximation (SA) algorithm for risk minimization with Lipschitz continuous
random functions, which requires only one pass of $n$ samples and adapts to
EBC. For both approaches, the convergence rates span a full spectrum between
$\widetilde O(1/\sqrt{n})$ and $\widetilde O(1/n)$ depending on the power
constant in EBC, and could be even faster than $O(1/n)$ in special cases for
ERM. Moreover, these convergence rates are automatically adaptive without using
any knowledge of EBC. Overall, this work not only strengthens the understanding
of ERM for statistical learning but also brings new fast stochastic algorithms
for solving a broad range of statistical learning problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingrui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Rong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04591">
<title>Robust and Scalable Models of Microbiome Dynamics. (arXiv:1805.04591v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04591</link>
<description rdf:parseType="Literal">&lt;p&gt;Microbes are everywhere, including in and on our bodies, and have been shown
to play key roles in a variety of prevalent human diseases. Consequently, there
has been intense interest in the design of bacteriotherapies or &quot;bugs as
drugs,&quot; which are communities of bacteria administered to patients for specific
therapeutic applications. Central to the design of such therapeutics is an
understanding of the causal microbial interaction network and the population
dynamics of the organisms. In this work we present a Bayesian nonparametric
model and associated efficient inference algorithm that addresses the key
conceptual and practical challenges of learning microbial dynamics from time
series microbe abundance data. These challenges include high-dimensional (300+
strains of bacteria in the gut) but temporally sparse and non-uniformly sampled
data; high measurement noise; and, nonlinear and physically non-negative
dynamics. Our contributions include a new type of dynamical systems model for
microbial dynamics based on what we term interaction modules, or learned
clusters of latent variables with redundant interaction structure (reducing the
expected number of interaction coefficients from $O(n^2)$ to $O((\log n)^2)$);
a fully Bayesian formulation of the stochastic dynamical systems model that
propagates measurement and latent state uncertainty throughout the model; and
introduction of a temporally varying auxiliary variable technique to enable
efficient inference by relaxing the hard non-negativity constraint on states.
We apply our method to simulated and real data, and demonstrate the utility of
our technique for system identification from limited data and gaining new
biological insights into bacteriotherapy design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gibson_T/0/1/0/all/0/1&quot;&gt;Travis E. Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gerber_G/0/1/0/all/0/1&quot;&gt;Georg K. Gerber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04634">
<title>Image-derived generative modeling of pseudo-macromolecular structures - towards the statistical assessment of Electron CryoTomography template matching. (arXiv:1805.04634v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1805.04634</link>
<description rdf:parseType="Literal">&lt;p&gt;Cellular Electron CryoTomography (CECT) is a 3D imaging technique that
captures information about the structure and spatial organization of
macromolecular complexes within single cells, in near-native state and at
sub-molecular resolution. Although template matching is often used to locate
macromolecules in a CECT image, it is insufficient as it only measures the
relative structural similarity. Therefore, it is preferable to assess the
statistical credibility of the decision through hypothesis testing, requiring
many templates derived from a diverse population of macromolecular structures.
Due to the very limited number of known structures, we need a generative model
to efficiently and reliably sample pseudo-structures from the complex
distribution of macromolecular structures. To address this challenge, we
propose a novel image-derived approach for performing hypothesis testing for
template matching by constructing generative models using the generative
adversarial network. Finally, we conducted hypothesis testing experiments for
template matching on both simulated and experimental subtomograms, allowing us
to conclude the identity of subtomograms with high statistical credibility and
significantly reducing false positives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zhiguang Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04720">
<title>Do Outliers Ruin Collaboration?. (arXiv:1805.04720v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04720</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a binary classifier from $n$ different
data sources, among which at most an $\eta$ fraction are adversarial. The
overhead is defined as the ratio between the sample complexity of learning in
this setting and that of learning the same hypothesis class on a single data
distribution. We present an algorithm that achieves an $O(\eta n + \ln n)$
overhead, which is proved to be worst-case optimal. We also discuss the
potential challenges to the design of a computationally efficient learning
algorithm with a small overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1&quot;&gt;Mingda Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04740">
<title>Agreement Rate Initialized Maximum Likelihood Estimator for Ensemble Classifier Aggregation and Its Application in Brain-Computer Interface. (arXiv:1805.04740v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04740</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensemble learning is a powerful approach to construct a strong learner from
multiple base learners. The most popular way to aggregate an ensemble of
classifiers is majority voting, which assigns a sample to the class that most
base classifiers vote for. However, improved performance can be obtained by
assigning weights to the base classifiers according to their accuracy. This
paper proposes an agreement rate initialized maximum likelihood estimator
(ARIMLE) to optimally fuse the base classifiers. ARIMLE first uses a simplified
agreement rate method to estimate the classification accuracy of each base
classifier from the unlabeled samples, then employs the accuracies to
initialize a maximum likelihood estimator (MLE), and finally uses the
expectation-maximization algorithm to refine the MLE. Extensive experiments on
visually evoked potential classification in a brain-computer interface
application show that ARIMLE outperforms majority voting, and also achieves
better or comparable performance with several other state-of-the-art classifier
combination approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongrui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1&quot;&gt;Vernon J. Lawhern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_S/0/1/0/all/0/1&quot;&gt;Stephen Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lance_B/0/1/0/all/0/1&quot;&gt;Brent J. Lance&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04755">
<title>A Simple and Effective Model-Based Variable Importance Measure. (arXiv:1805.04755v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04755</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of &quot;big data&quot;, it is becoming more of a challenge to not only
build state-of-the-art predictive models, but also gain an understanding of
what&apos;s really going on in the data. For example, it is often of interest to
know which, if any, of the predictors in a fitted model are relatively
influential on the predicted outcome. Some modern algorithms---like random
forests and gradient boosted decision trees---have a natural way of quantifying
the importance or relative influence of each feature. Other algorithms---like
naive Bayes classifiers and support vector machines---are not capable of doing
so and model-free approaches are generally used to measure each predictor&apos;s
importance. In this paper, we propose a standardized, model-based approach to
measuring predictor importance across the growing spectrum of supervised
learning algorithms. Our proposed method is illustrated through both simulated
and real data examples. The R code to reproduce all of the figures in this
paper is available in the supplementary materials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Greenwell_B/0/1/0/all/0/1&quot;&gt;Brandon M. Greenwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boehmke_B/0/1/0/all/0/1&quot;&gt;Bradley C. Boehmke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCarthy_A/0/1/0/all/0/1&quot;&gt;Andrew J. McCarthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04756">
<title>Predictive Uncertainty in Large Scale Classification using Dropout - Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:1805.04756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04756</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive uncertainty is crucial for many computer vision tasks, from image
classification to autonomous driving systems. Hamiltonian Monte Carlo (HMC) is
an inference method for sampling complex posterior distributions. On the other
hand, Dropout regularization has been proposed as an approximate model
averaging technique that tends to improve generalization in large scale models
such as deep neural networks. Although, HMC provides convergence guarantees for
most standard Bayesian models, it does not handle discrete parameters arising
from Dropout regularization. In this paper, we present a robust methodology for
predictive uncertainty in large scale classification problems, based on Dropout
and Stochastic Gradient Hamiltonian Monte Carlo. Even though Dropout induces a
non-smooth energy function with no such convergence guarantees, the resulting
discretization of the Hamiltonian proves empirical success. The proposed method
allows to effectively estimate predictive accuracy and to provide better
generalization for difficult test examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vergara_D/0/1/0/all/0/1&quot;&gt;Diego Vergara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_S/0/1/0/all/0/1&quot;&gt;Sergio Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdenegro_M/0/1/0/all/0/1&quot;&gt;Mat&amp;#xed;as Valdenegro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jorquera_F/0/1/0/all/0/1&quot;&gt;Felipe Jorquera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04784">
<title>Nonlinear Metric Learning through Geodesic Polylinear Interpolation (ML-GPI). (arXiv:1805.04784v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04784</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a nonlinear distance metric learning scheme based
on the fusion of component linear metrics. Instead of merging displacements at
each data point, our model calculates the velocities induced by the component
transformations, via a geodesic interpolation on a Lie transfor- mation group.
Such velocities are later summed up to produce a global transformation that is
guaranteed to be diffeomorphic. Consequently, pair-wise distances computed this
way conform to a smooth and spatially varying metric, which can greatly benefit
k-NN classification. Experiments on synthetic and real datasets demonstrate the
effectiveness of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bibo Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1&quot;&gt;Charles D. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jundong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04785">
<title>Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models. (arXiv:1805.04785v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04785</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we consider the dynamic assortment selection problem under an
uncapacitated multinomial-logit (MNL) model. By carefully analyzing a revenue
potential function, we show that a trisection based algorithm achieves an
item-independent regret bound of $O(\sqrt{T\log\log T})$, which matches
information theoretical lower bounds up to iterated logarithmic terms. Our
proof technique draws tools from the unimodal/convex bandit literature as well
as adaptive confidence parameters in minimax multi-armed bandit problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04807">
<title>Curriculum Adversarial Training. (arXiv:1805.04807v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04807</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep learning has been applied to many security-sensitive
applications, such as facial authentication. The existence of adversarial
examples hinders such applications. The state-of-the-art result on defense
shows that adversarial training can be applied to train a robust model on MNIST
against adversarial examples; but it fails to achieve a high empirical
worst-case accuracy on a more complex task, such as CIFAR-10 and SVHN. In our
work, we propose curriculum adversarial training (CAT) to resolve this issue.
The basic idea is to develop a curriculum of adversarial examples generated by
attacks with a wide range of strengths. With two techniques to mitigate the
forgetting and the generalization issues, we demonstrate that CAT can improve
the prior art&apos;s empirical worst-case accuracy by a large margin of 25% on
CIFAR-10 and 35% on SVHN. At the same, the model&apos;s performance on
non-adversarial inputs is comparable to the state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1&quot;&gt;Qi-Zhi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Min Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04810">
<title>AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning. (arXiv:1805.04810v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1805.04810</link>
<description rdf:parseType="Literal">&lt;p&gt;Users in various web and mobile applications are vulnerable to attribute
inference attacks, in which an attacker leverages a machine learning classifier
to infer a target user&apos;s private attributes (e.g., location, sexual
orientation, political view) from its public data (e.g., rating scores, page
likes). Existing defenses leverage game theory or heuristics based on
correlations between the public data and attributes. These defenses are not
practical. Specifically, game-theoretic defenses require solving intractable
optimization problems, while correlation-based defenses incur large utility
loss of users&apos; public data.
&lt;/p&gt;
&lt;p&gt;In this paper, we present AttriGuard, a practical defense against attribute
inference attacks. AttriGuard is computationally tractable and has small
utility loss. Our AttriGuard works in two phases. Suppose we aim to protect a
user&apos;s private attribute. In Phase I, for each value of the attribute, we find
a minimum noise such that if we add the noise to the user&apos;s public data, then
the attacker&apos;s classifier is very likely to infer the attribute value for the
user. We find the minimum noise via adapting existing evasion attacks in
adversarial machine learning. In Phase II, we sample one attribute value
according to a certain probability distribution and add the corresponding noise
found in Phase I to the user&apos;s public data. We formulate finding the
probability distribution as solving a constrained convex optimization problem.
We extensively evaluate AttriGuard and compare it with existing methods using a
real-world dataset. Our results show that AttriGuard substantially outperforms
existing methods. Our work is the first one that shows evasion attacks can be
used as defensive techniques for privacy protection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04874">
<title>GAN Q-learning. (arXiv:1805.04874v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04874</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributional reinforcement learning (distributional RL) has seen empirical
success in complex Markov Decision Processes (MDPs) in the setting of nonlinear
function approximation. However, there are many different ways in which one can
leverage the distributional approach to reinforcement learning. In this paper,
we propose GAN Q-learning, a novel distributional RL method based on generative
adversarial networks (GANs) and analyze its performance in simple tabular
environments, as well as OpenAI Gym. We empirically show that our algorithm
leverages the flexibility and blackbox approach of deep learning models while
providing a viable alternative to other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thang Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazoure_B/0/1/0/all/0/1&quot;&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyle_C/0/1/0/all/0/1&quot;&gt;Clare Lyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04927">
<title>Lehmer Transform and its Theoretical Properties. (arXiv:1805.04927v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04927</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new class of transforms that we call {\it Lehmer Transform}
which is motivated by the {\it Lehmer mean function}. The proposed {\it Lehmer
transform} decomposes a function of a sample into their constituting
statistical moments. Theoretical properties of the proposed transform are
presented. This transform could be very useful to provide an alternative method
in analyzing non-stationary signals such as brain wave EEG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ataei_M/0/1/0/all/0/1&quot;&gt;Masoud Ataei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shengyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04933">
<title>Dyna: A Method of Momentum for Stochastic Optimization. (arXiv:1805.04933v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04933</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithm is presented for momentum gradient descent optimization based on
the first-order differential equation of the Newtonian dynamics. The fictitious
mass is introduced to the dynamics of momentum for regularizing the adaptive
stepsize of each individual parameter. The dynamic relaxation is adapted for
stochastic optimization of nonlinear objective functions through an explicit
time integration with varying damping ratio. The adaptive stepsize is optimized
for each individual neural network layer based on the number of inputs. The
adaptive stepsize for every parameter over the entire neural network is
uniformly optimized with one upper bound, independent of sparsity, for better
overall convergence rate. The numerical implementation of the algorithm is
similar to the Adam Optimizer, possessing computational efficiency, similar
memory requirements, etc. There are three hyper-parameters in the algorithm
with clear physical interpretation. Preliminary trials show promise in
performance and convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhidong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04957">
<title>Compressive sensing on diverse STEM scans: real-time feedback, low-dose and dynamic range. (arXiv:1805.04957v1 [physics.ins-det])</title>
<link>http://arxiv.org/abs/1805.04957</link>
<description rdf:parseType="Literal">&lt;p&gt;Scanning Transmission Electron Microscopy (STEM) has become the main stay for
materials characterization on atomic level, with applications ranging from
visualization of localized and extended defects to mapping order parameter
fields. In the last several years, attention was attracted by potential of STEM
to explore beam induced chemical processes and especially manipulating atomic
motion, enabling atom-by-atom fabrication. These applications, as well as
traditional imaging of beam sensitive materials, necessitate increasing dynamic
range of STEM between imaging and manipulation modes, and increasing absolute
scanning/imaging speeds, that can be achieved by combining sparse sensing
methods with non-rectangular scanning trajectories. Here we developed a general
method for real-time reconstruction of sparsely sampled images from high-speed,
non-invasive and diverse scanning pathways. This approach is demonstrated on
both the synthetic data where ground truth is known and the experimental STEM
data. This work lays the foundation for future tasks such as optimal design of
dose efficient scanning strategies and real-time adaptive inference and control
of e-beam induced atomic fabrication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dyck_O/0/1/0/all/0/1&quot;&gt;Ondrej Dyck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kalinin_S/0/1/0/all/0/1&quot;&gt;Sergei V. Kalinin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jesse_S/0/1/0/all/0/1&quot;&gt;Stephen Jesse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04958">
<title>Accelerating Message Passing for MAP with Benders Decomposition. (arXiv:1805.04958v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04958</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel mechanism to tighten the local polytope relaxation for
MAP inference in Markov random fields with low state space variables. We
consider a surjection of the variables to a set of hyper-variables and apply
the local polytope relaxation over these hyper-variables. The state space of
each individual hyper-variable is constructed to be enumerable while the vector
product of pairs is not easily enumerable making message passing inference
intractable.
&lt;/p&gt;
&lt;p&gt;To circumvent the difficulty of enumerating the vector product of state
spaces of hyper-variables we introduce a novel Benders decomposition approach.
This produces an upper envelope describing the message constructed from affine
functions of the individual variables that compose the hyper-variable receiving
the message. The envelope is tight at the minimizers which are shared by the
true message. Benders rows are constructed to be Pareto optimal and are
generated using an efficient procedure targeted for binary problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarkony_J/0/1/0/all/0/1&quot;&gt;Julian Yarkony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaofei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04982">
<title>Index Set Fourier Series Features for Approximating Multi-dimensional Periodic Kernels. (arXiv:1805.04982v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.04982</link>
<description rdf:parseType="Literal">&lt;p&gt;Periodicity is often studied in timeseries modelling with autoregressive
methods but is less popular in the kernel literature, particularly for higher
dimensional problems such as in textures, crystallography, and quantum
mechanics. Large datasets often make modelling periodicity untenable for
otherwise powerful non-parametric methods like Gaussian Processes (GPs) which
typically incur an $\mathcal{O}(N^3)$ computational burden and, consequently,
are unable to scale to larger datasets. To this end we introduce a method
termed \emph{Index Set Fourier Series Features} to tractably exploit
multivariate Fourier series and efficiently decompose periodic kernels on
higher-dimensional data into a series of basis functions. We show that our
approximation produces significantly less predictive error than alternative
approaches such as those based on random Fourier features and achieves better
generalisation on regression problems with periodic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tompkins_A/0/1/0/all/0/1&quot;&gt;Anthony Tompkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramos_F/0/1/0/all/0/1&quot;&gt;Fabio Ramos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05021">
<title>A One-Class Decision Tree Based on Kernel Density Estimation. (arXiv:1805.05021v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.05021</link>
<description rdf:parseType="Literal">&lt;p&gt;One-Class Classification (OCC) is a domain of machine learning which achieves
training by means of a single class sample. The present work aims at developing
a one-class model which addresses concerns of both performance and readability.
To this end, we propose a hybrid OCC method which relies on density estimation
as part of a tree-based learning algorithm. Within a greedy and recursive
approach, our proposal rests on kernel density estimation to split a data
subset on the basis of one or several intervals of interest. Our method shows
favorable performance in comparison with common methods of the literature on a
range of benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Itani_S/0/1/0/all/0/1&quot;&gt;Sarah Itani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lecron_F/0/1/0/all/0/1&quot;&gt;Fabian Lecron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fortemps_P/0/1/0/all/0/1&quot;&gt;Philippe Fortemps&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05052">
<title>A Gentle Introduction to Supervised Machine Learning. (arXiv:1805.05052v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.05052</link>
<description rdf:parseType="Literal">&lt;p&gt;This tutorial is based on the lecture notes for the courses &quot;Machine
Learning: Basic Principles&quot; and &quot;Artificial Intelligence&quot;, which I have taught
during fall 2017 and spring 2018 at Aalto university. The aim is to provide an
accessible introduction to some of the main concepts and methods within
supervised machine learning. Most of the current systems which are con- sidered
as (artificially) intelligent are based on some form of supervised machine
learning. After discussing the main building blocks of a formal machine
learning problem, some of the most popular algorithmic design patterns for
machine learning methods are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05071">
<title>KL-UCB-switch: optimal regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints. (arXiv:1805.05071v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.05071</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of K-armed stochastic bandits with distribution only assumed
to be supported by [0, 1], we introduce a new algorithm, KL-UCB-switch, and
prove that it enjoys simultaneously a distribution-free regret bound of optimal
order \sqrt{KT} and a distribution-dependent regret bound of optimal order as
well, that is, matching the \kappa \ln T lower bound by Lai and Robbins (1985)
and Burnetas and Katehakis (1996).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garivier_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Garivier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hadiji_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;di Hadiji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1&quot;&gt;Pierre Menard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stoltz_G/0/1/0/all/0/1&quot;&gt;Gilles Stoltz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05133">
<title>Model selection with lasso-zero: adding straw to the haystack to better find needles. (arXiv:1805.05133v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1805.05133</link>
<description rdf:parseType="Literal">&lt;p&gt;The high-dimensional linear model $y = X \beta^0 + \epsilon$ is considered
and the focus is put on the problem of recovering the support $S^0$ of the
sparse vector $\beta^0.$ We introduce lasso-zero, a new $\ell_1$-based
estimator whose novelty resides in an &quot;overfit, then threshold&quot; paradigm and
the use of noise dictionaries for overfitting the response. The methodology is
supported by theoretical results obtained in the special case where no noise
dictionary is used. In this case, lasso-zero boils down to thresholding the
basis pursuit solution. We prove that this procedure requires weaker conditions
on $X$ and $S^0$ than the lasso for exact support recovery, and controls the
false discovery rate for orthonormal designs when tuned by the quantile
universal threshold. However it requires a high signal-to-noise ratio, and the
use of noise dictionaries addresses this issue. The threshold selection
procedure is based on a pivotal statistic and does not require knowledge of the
noise level. Numerical simulations show that lasso-zero performs well in terms
of support recovery and provides a good trade-off between high true positive
rate and low false discovery rate compared to competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Descloux_P/0/1/0/all/0/1&quot;&gt;Pascaline Descloux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sardy_S/0/1/0/all/0/1&quot;&gt;Sylvain Sardy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05189">
<title>Randomized Smoothing SVRG for Large-scale Nonsmooth Convex Optimization. (arXiv:1805.05189v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.05189</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of minimizing the average of a large
number of nonsmooth and convex functions. Such problems often arise in typical
machine learning problems as empirical risk minimization, but are
computationally very challenging. We develop and analyze a new algorithm that
achieves robust linear convergence rate, and both its time complexity and
gradient complexity are superior than state-of-art nonsmooth algorithms and
subgradient-based schemes. Besides, our algorithm works without any extra error
bound conditions on the objective function as well as the common
strongly-convex condition. We show that our algorithm has wide applications in
optimization and machine learning problems, and demonstrate experimentally that
it performs well on a large-scale ranking problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenjie Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.03159">
<title>Phase Transitions and a Model Order Selection Criterion for Spectral Graph Clustering. (arXiv:1604.03159v4 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1604.03159</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the longstanding open problems in spectral graph clustering (SGC) is
the so-called model order selection problem: automated selection of the correct
number of clusters. This is equivalent to the problem of finding the number of
connected components or communities in an undirected graph. We propose
automated model order selection (AMOS), a solution to the SGC model selection
problem under a random interconnection model (RIM) using a novel selection
criterion that is based on an asymptotic phase transition analysis. AMOS can
more generally be applied to discovering hidden block diagonal structure in
symmetric non-negative matrices. Numerical experiments on simulated graphs
validate the phase transition analysis, and real-world network data is used to
validate the performance of the proposed model selection procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1&quot;&gt;Alfred O. Hero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.06864">
<title>A probabilistic network for the diagnosis of acute cardiopulmonary diseases. (arXiv:1609.06864v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.06864</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, the development of a probabilistic network for the diagnosis
of acute cardiopulmonary diseases is presented. This paper is a draft version
of the article published after peer review in 2018
(https://doi.org/10.1002/bimj.201600206). A panel of expert physicians
collaborated to specify the qualitative part, that is a directed acyclic graph
defining a factorization of the joint probability distribution of domain
variables. The quantitative part, that is the set of all conditional
probability distributions defined by each factor, was estimated in the Bayesian
paradigm: we applied a special formal representation, characterized by a low
number of parameters and a parameterization intelligible for physicians,
elicited the joint prior distribution of parameters from medical experts, and
updated it by conditioning on a dataset of hospital patient records using
Markov Chain Monte Carlo simulation. Refinement was cyclically performed until
the probabilistic network provided satisfactory Concordance Index values for a
selection of acute diseases and reasonable inference on six fictitious patient
cases. The probabilistic network can be employed to perform medical diagnosis
on a total of 63 diseases (38 acute and 25 chronic) on the basis of up to 167
patient findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Magrini_A/0/1/0/all/0/1&quot;&gt;Alessandro Magrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luciani_D/0/1/0/all/0/1&quot;&gt;Davide Luciani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stefanini_F/0/1/0/all/0/1&quot;&gt;Federico Mattia Stefanini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.05246">
<title>BET on Independence. (arXiv:1610.05246v6 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1610.05246</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of nonparametric dependence detection. Many existing
methods suffer severe power loss due to non-uniform consistency, which we
illustrate with a paradox. To avoid such power loss, we approach the
nonparametric test of independence through the new framework of binary
expansion statistics (BEStat) and binary expansion testing (BET), which examine
dependence through a novel binary expansion filtration approximation of the
copula. Through a Hadamard transform, we find that the symmetry statistics in
the filtration are complete sufficient statistics for dependence. These
statistics are also uncorrelated under the null. By utilizing symmetry
statistics, the BET avoids the problem of non-uniform consistency and improves
upon a wide class of commonly used methods (a) by achieving the minimax rate in
sample size requirement for reliable power and (b) by providing clear
interpretations of global relationships upon rejection of independence. The
binary expansion approach also connects the symmetry statistics with the
current computing system to facilitate efficient bitwise implementation. We
illustrate the BET with a study of the distribution of stars in the night sky
and with an exploratory data analysis of the TCGA breast cancer data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10087">
<title>DICOD: Distributed Convolutional Sparse Coding. (arXiv:1705.10087v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10087</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce DICOD, a convolutional sparse coding algorithm
which builds shift invariant representations for long signals. This algorithm
is designed to run in a distributed setting, with local message passing, making
it communication efficient. It is based on coordinate descent and uses locally
greedy updates which accelerate the resolution compared to greedy coordinate
selection. We prove the convergence of this algorithm and highlight its
computational speed-up which is super-linear in the number of cores used. We
also provide empirical evidence for the acceleration properties of our
algorithm compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1&quot;&gt;Thomas Moreau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudre_L/0/1/0/all/0/1&quot;&gt;Laurent Oudre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1&quot;&gt;Nicolas Vayatis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06838">
<title>A giant with feet of clay: on the validity of the data that feed machine learning in medicine. (arXiv:1706.06838v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06838</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the use of Machine Learning (ML) in medicine by focusing
on the main problem that this computational approach has been aimed at solving
or at least minimizing: uncertainty. To this aim, we point out how uncertainty
is so ingrained in medicine that it biases also the representation of clinical
phenomena, that is the very input of ML models, thus undermining the clinical
significance of their output. Recognizing this can motivate both medical
doctors, in taking more responsibility in the development and use of these
decision aids, and the researchers, in pursuing different ways to assess the
value of these systems. In so doing, both designers and users could take this
intrinsic characteristic of medicine more seriously and consider alternative
approaches that do not &quot;sweep uncertainty under the rug&quot; within an objectivist
fiction, which everyone can come up by believing as true.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabitza_F/0/1/0/all/0/1&quot;&gt;Federico Cabitza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciucci_D/0/1/0/all/0/1&quot;&gt;Davide Ciucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasoini_R/0/1/0/all/0/1&quot;&gt;Raffaele Rasoini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06633">
<title>Nonparametric regression using deep neural networks with ReLU activation function. (arXiv:1708.06633v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06633</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider the multivariate nonparametric regression model. It is shown that
estimators based on sparsely connected deep neural networks with ReLU
activation function and properly chosen network architecture achieve the
minimax rates of convergence (up to log n-factors) under a general composition
assumption on the regression function. The framework includes many well-studied
structural constraints such as (generalized) additive models. While there is a
lot of flexibility in the network architecture, the tuning parameter is the
sparsity of the network. Specifically, we consider large networks with number
of potential network parameters exceeding the sample size. The analysis gives
some insights why multilayer feedforward neural networks perform well in
practice. Interestingly, the depth (number of layers) of the neural network
architectures plays an important role and our theory suggests that for
nonparametric regression scaling the network depth with the logarithm of the
sample size is natural. It is also shown that under the composition assumption
wavelet estimators can only achieve suboptimal rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schmidt_Hieber_J/0/1/0/all/0/1&quot;&gt;Johannes Schmidt-Hieber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06720">
<title>Optimal Rates for Spectral-regularized Algorithms with Least-Squares Regression over Hilbert Spaces. (arXiv:1801.06720v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06720</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study regression problems over a separable Hilbert space
with the square loss, covering non-parametric regression over a reproducing
kernel Hilbert space. We investigate a class of spectral-regularized
algorithms, including ridge regression, principal component analysis, and
gradient methods. We prove optimal, high-probability convergence results in
terms of variants of norms for the studied algorithms, considering a capacity
assumption on the hypothesis space and a general source condition on the target
function. Consequently, we obtain almost sure convergence results with optimal
rates. Our results improve and generalize previous results, filling a
theoretical gap for the non-attainable cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03284">
<title>Mini-Batch Stochastic ADMMs for Nonconvex Nonsmooth Optimization. (arXiv:1802.03284v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03284</link>
<description rdf:parseType="Literal">&lt;p&gt;With the large rising of complex data, the nonconvex models such as nonconvex
loss function and nonconvex regularizer are widely used in machine learning and
pattern recognition. In this paper, we propose a class of mini-batch stochastic
ADMMs (alternating direction method of multipliers) for solving large-scale
nonconvex nonsmooth problems. We prove that, given an appropriate mini-batch
size, the mini-batch stochastic ADMM without variance reduction (VR) technique
is convergent and reaches a convergence rate of $O(1/T)$ to obtain a stationary
point of the nonconvex optimization, where $T$ denotes the number of
iterations. Moreover, we extend the mini-batch stochastic gradient method to
both the nonconvex SVRG-ADMM and SAGA-ADMM proposed in our initial manuscript
\cite{huang2016stochastic}, and prove these mini-batch stochastic ADMMs also
reaches the convergence rate of $O(1/T)$ without condition on the mini-batch
size. In particular, we provide a specific parameter selection for step size
$\eta$ of stochastic gradients and penalty parameter $\rho$ of augmented
Lagrangian function. Finally, extensive experimental results on both simulated
and real-world data demonstrate the effectiveness of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Feihu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Songcan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03335">
<title>Black-box Variational Inference for Stochastic Differential Equations. (arXiv:1802.03335v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03335</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter inference for stochastic differential equations is challenging due
to the presence of a latent diffusion process. Working with an Euler-Maruyama
discretisation for the diffusion, we use variational inference to jointly learn
the parameters and the diffusion paths. We use a standard mean-field
variational approximation of the parameter posterior, and introduce a recurrent
neural network to approximate the posterior for the diffusion paths conditional
on the parameters. This neural network learns how to provide Gaussian state
transitions which bridge between observations in a very similar way to the
conditioned diffusion process. The resulting black-box inference method can be
applied to any SDE system with light tuning requirements. We illustrate the
method on a Lotka-Volterra system and an epidemic model, producing accurate
parameter estimates in a few hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ryder_T/0/1/0/all/0/1&quot;&gt;Thomas Ryder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Golightly_A/0/1/0/all/0/1&quot;&gt;Andrew Golightly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McGough_A/0/1/0/all/0/1&quot;&gt;A. Stephen McGough&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prangle_D/0/1/0/all/0/1&quot;&gt;Dennis Prangle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08012">
<title>Learning Topic Models by Neighborhood Aggregation. (arXiv:1802.08012v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08012</link>
<description rdf:parseType="Literal">&lt;p&gt;Topic models are frequently used in machine learning owing to their high
interpretability and modular structure. However, extending a topic model to
include a supervisory signal, to incorporate pre-trained word embedding vectors
and to include a nonlinear output function is not an easy task because one has
to resort to a highly intricate approximate inference procedure. The present
paper shows that topic modeling can be viewed as implementing a neighborhood
aggregation algorithm where messages are passed through a network defined over
words. From the network view of topic models, nodes correspond to words in a
document and edges correspond to either a relationship describing co-occurring
words in a document or a relationship describing the same word in the corpus.
The network view allows us to extend the model to include supervisory signals,
incorporate pre-trained word embedding vectors and include a nonlinear output
function in a simple manner. Moreover, we describe a simple way to train the
model that is well suited to a semi-supervised setting where we only have
supervisory signals for some portion of the corpus and the goal is to improve
prediction performance in the held-out data. In experiments, we show that our
approach outperforms the state-of-the-art supervised latent Dirichlet
allocation implementation in terms of both held-out document classification
tasks and topic coherence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hisano_R/0/1/0/all/0/1&quot;&gt;Ryohei Hisano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10510">
<title>Automated design of collective variables using supervised machine learning. (arXiv:1802.10510v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.10510</link>
<description rdf:parseType="Literal">&lt;p&gt;Selection of appropriate collective variables for enhancing sampling of
molecular simulations remains an unsolved problem in computational biophysics.
In particular, picking initial collective variables (CVs) is particularly
challenging in higher dimensions. Which atomic coordinates or transforms there
of from a list of thousands should one pick for enhanced sampling runs? How
does a modeler even begin to pick starting coordinates for investigation? This
remains true even in the case of simple two state systems and only increases in
difficulty for multi-state systems. In this work, we solve the initial CV
problem using a data-driven approach inspired by the filed of supervised
machine learning. In particular, we show how the decision functions in
supervised machine learning (SML) algorithms can be used as initial CVs
(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and
Chignolin mini-protein as our test cases, we illustrate how the distance to the
Support Vector Machines&apos; decision hyperplane, the output probability estimates
from Logistic Regression, the outputs from deep neural network classifiers, and
other classifiers may be used to reversibly sample slow structural transitions.
We discuss the utility of other SML algorithms that might be useful for
identifying CVs for accelerating molecular simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sultan_M/0/1/0/all/0/1&quot;&gt;Mohammad M. Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay S. Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07200">
<title>Training Recurrent Neural Networks as a Constraint Satisfaction Problem. (arXiv:1803.07200v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07200</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new approach for training artificial neural networks
using techniques for solving the constraint satisfaction problem (CSP). The
quotient gradient system (QGS) is a trajectory-based method for solving the
CSP. This study converts the training set of a neural network into a CSP and
uses the QGS to find its solutions. The QGS finds the global minimum of the
optimization problem by tracking trajectories of a nonlinear dynamical system
and does not stop at a local minimum of the optimization problem. Lyapunov
theory is used to prove the asymptotic stability of the solutions with and
without the presence of measurement errors. Numerical examples illustrate the
effectiveness of the proposed methodology and compare it to a genetic algorithm
and error backpropagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodabandehlou_H/0/1/0/all/0/1&quot;&gt;Hamid Khodabandehlou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadali_M/0/1/0/all/0/1&quot;&gt;M. Sami Fadali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01825">
<title>Evaluating Hospital Case Cost Prediction Models Using Azure Machine Learning Studio. (arXiv:1804.01825v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Ability for accurate hospital case cost modelling and prediction is critical
for efficient health care financial management and budgetary planning. A
variety of regression machine learning algorithms are known to be effective for
health care cost predictions. The purpose of this experiment was to build an
Azure Machine Learning Studio tool for rapid assessment of multiple types of
regression models. The tool offers environment for comparing 14 types of
regression models in a unified experiment: linear regression, Bayesian linear
regression, decision forest regression, boosted decision tree regression,
neural network regression, Poisson regression, Gaussian processes for
regression, gradient boosted machine, nonlinear least squares regression,
projection pursuit regression, random forest regression, robust regression,
robust regression with mm-type estimators, support vector regression. The tool
presents assessment results arranged by model accuracy in a single table using
five performance metrics. Evaluation of regression machine learning models for
performing hospital case cost prediction demonstrated advantage of robust
regression model, boosted decision tree regression and decision forest
regression. The operational tool has been published to the web and openly
available for experiments and extensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botchkarev_A/0/1/0/all/0/1&quot;&gt;Alexei Botchkarev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10988">
<title>SHARE: Regularization for Deep Learning. (arXiv:1804.10988v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10988</link>
<description rdf:parseType="Literal">&lt;p&gt;Regularization is a big issue for training deep neural networks. In this
paper, we propose a new information-theory-based regularization scheme named
SHADE for SHAnnon DEcay. The originality of the approach is to define a prior
based on conditional entropy, which explicitly decouples the learning of
invariant representations in the regularizer and the learning of correlations
between inputs and labels in the data fitting term. Our second contribution is
to derive a stochastic version of the regularizer compatible with deep
learning, resulting in a tractable training scheme. We empirically validate the
efficiency of our approach to improve classification performances compared to
common regularization schemes on several standard architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blot_M/0/1/0/all/0/1&quot;&gt;Michael Blot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robert_T/0/1/0/all/0/1&quot;&gt;Thomas Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00013">
<title>Constraining Effective Field Theories with Machine Learning. (arXiv:1805.00013v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00013</link>
<description rdf:parseType="Literal">&lt;p&gt;We present powerful new analysis techniques to constrain effective field
theories at the LHC. By leveraging the structure of particle physics processes,
we extract extra information from Monte-Carlo simulations, which can be used to
train neural network models that estimate the likelihood ratio. These methods
scale well to processes with many observables and theory parameters, do not
require any approximations of the parton shower or detector response, and can
be evaluated in microseconds. We show that they allow us to put significantly
stronger bounds on dimension-six operators than existing methods, demonstrating
their potential to improve the precision of the LHC legacy constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Pavez_J/0/1/0/all/0/1&quot;&gt;Juan Pavez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00020">
<title>A Guide to Constraining Effective Field Theories with Machine Learning. (arXiv:1805.00020v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00020</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop, discuss, and compare several inference techniques to constrain
theory parameters in collider experiments. By harnessing the latent-space
structure of particle physics processes, we extract extra information from the
simulator. This augmented data can be used to train neural networks that
precisely estimate the likelihood ratio. The new methods scale well to many
observables and high-dimensional parameter spaces, do not require any
approximations of the parton shower and detector response, and can be evaluated
in microseconds. Using weak-boson-fusion Higgs production as an example
process, we compare the performance of several techniques. The best results are
found for likelihood ratio estimators trained with extra information about the
score, the gradient of the log likelihood function with respect to the theory
parameters. The score also provides sufficient statistics that contain all the
information needed for inference in the neighborhood of the Standard Model.
These methods enable us to put significantly stronger bounds on effective
dimension-six operators than the traditional approach based on histograms. They
also outperform generic machine learning methods that do not make use of the
particle physics structure, demonstrating their potential to substantially
improve the new physics reach of the LHC legacy results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Pavez_J/0/1/0/all/0/1&quot;&gt;Juan Pavez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02232">
<title>Discrete Factorization Machines for Fast Feature-based Recommendation. (arXiv:1805.02232v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02232</link>
<description rdf:parseType="Literal">&lt;p&gt;User and item features of side information are crucial for accurate
recommendation. However, the large number of feature dimensions, e.g., usually
larger than 10^7, results in expensive storage and computational cost. This
prohibits fast recommendation especially on mobile applications where the
computational resource is very limited. In this paper, we develop a generic
feature-based recommendation model, called Discrete Factorization Machine
(DFM), for fast and accurate recommendation. DFM binarizes the real-valued
model parameters (e.g., float32) of every feature embedding into binary codes
(e.g., boolean), and thus supports efficient storage and fast user-item score
computation. To avoid the severe quantization loss of the binarization, we
propose a convergent updating rule that resolves the challenging discrete
optimization of DFM. Through extensive experiments on two real-world datasets,
we show that 1) DFM consistently outperforms state-of-the-art binarized
recommendation models, and 2) DFM shows very competitive performance compared
to its real-valued version (FM), demonstrating the minimized quantization loss.
This work is accepted by IJCAI 2018.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Rui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02627">
<title>Computing the Shattering Coefficient of Supervised Learning Algorithms. (arXiv:1805.02627v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02627</link>
<description rdf:parseType="Literal">&lt;p&gt;The Statistical Learning Theory (SLT) provides the theoretical guarantees for
supervised machine learning based on the Empirical Risk Minimization Principle
(ERMP). Such principle defines an upper bound to ensure the uniform convergence
of the empirical risk Remp(f), i.e., the error measured on a given data sample,
to the expected value of risk R(f) (a.k.a. actual risk), which depends on the
Joint Probability Distribution P(X x Y) mapping input examples x in X to class
labels y in Y. The uniform convergence is only ensured when the Shattering
coefficient N(F,2n) has a polynomial growing behavior. This paper proves the
Shattering coefficient for any Hilbert space H containing the input space X and
discusses its effects in terms of learning guarantees for supervised machine
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mello_R/0/1/0/all/0/1&quot;&gt;Rodrigo Fernandes de Mello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1&quot;&gt;Moacir Antonelli Ponti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_C/0/1/0/all/0/1&quot;&gt;Carlos Henrique Grossi Ferreira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03444">
<title>Controlling the privacy loss with the input feature maps of the layers in convolutional neural networks. (arXiv:1805.03444v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.03444</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the method to sanitize the privacy of the IFM(Input Feature Map)s
that are fed into the layers of CNN(Convolutional Neural Network)s. The method
introduces the degree of the sanitization that makes the application using a
CNN be able to control the privacy loss represented as the ratio of the
probabilistic accuracies for original IFM and sanitized IFM. For the
sanitization of an IFM, the sample-and-hold based approximation scheme is
devised to satisfy an application-specific degree of the sanitization. The
scheme approximates an IFM by replacing all the samples in a window with the
non-zero sample closest to the mean of the sampling window. It also removes the
dependency on CNN configuration by unfolding multi-dimensional IFM tensors into
one-dimensional streams to be approximated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_W/0/1/0/all/0/1&quot;&gt;Woohyung Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sung-Min Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1&quot;&gt;Junho Huh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_I/0/1/0/all/0/1&quot;&gt;Inyup Kang&lt;/a&gt;</dc:creator>
</item></rdf:RDF>