<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00721"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03224"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1507.04564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.04126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00139"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07739"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10348"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11381"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06818"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00047"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.00721">
<title>Improved Runtime Bounds for the Univariate Marginal Distribution Algorithm via Anti-Concentration. (arXiv:1802.00721v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.00721</link>
<description rdf:parseType="Literal">&lt;p&gt;Unlike traditional evolutionary algorithms which produce offspring via
genetic operators, Estimation of Distribution Algorithms (EDAs) sample
solutions from probabilistic models which are learned from selected
individuals. It is hoped that EDAs may improve optimisation performance on
epistatic fitness landscapes by learning variable interactions. However, hardly
any rigorous results are available to support claims about the performance of
EDAs, even for fitness functions without epistasis. The expected runtime of the
Univariate Marginal Distribution Algorithm (UMDA) on OneMax was recently shown
to be in $\mathcal{O}\left(n\lambda\log \lambda\right)$ by Dang and Lehre
(GECCO 2015). Later, Krejca and Witt (FOGA 2017) proved the lower bound
$\Omega\left(\lambda\sqrt{n}+n\log n\right)$ via an involved drift analysis.
&lt;/p&gt;
&lt;p&gt;We prove a $\mathcal{O}\left(n\lambda\right)$ bound, given some restrictions
on the population size. This implies the tight bound $\Theta\left(n\log
n\right)$ when $\lambda=\mathcal{O}\left(\log n\right)$, matching the runtime
of classical EAs. Our analysis uses the level-based theorem and
anti-concentration properties of the Poisson-Binomial distribution. We expect
that these generic methods will facilitate further analysis of EDAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehre_P/0/1/0/all/0/1&quot;&gt;Per Kristian Lehre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phan Trung Hai Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00268">
<title>Algorithmically probable mutations reproduce aspects of evolution such as convergence rate, genetic memory, modularity, diversity explosions, and mass extinction. (arXiv:1709.00268v6 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00268</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural selection explains how life has evolved over millions of years from
more primitive forms. The speed at which this happens, however, has sometimes
defied explanations based on random (uniformly distributed) mutations. Here we
investigate the application of algorithmic mutations (no recombination) to
binary matrices drawn from numerical approximations to algorithmic probability
in order to compare evolutionary convergence rates against the null hypothesis
(uniformly distributed mutations). Results both on synthetic and a small
biological examples lead to an accelerated rate of convergence when using the
algorithmic probability. We also show that algorithmically evolved modularity
provides an advantage that produces a genetic memory. We demonstrate that
regular structures are preserved and carried on when they first occur and can
lead to an accelerated production of diversity and extinction, possibly
explaining naturally occurring phenomena such as diversity explosions (e.g. the
Cambrian) and massive extinctions (e.g. the End Triassic) whose causes have
eluded researchers and are a cause for debate. The approach introduced here
appears to be a better approximation to biological evolution than models based
exclusively upon random uniform mutations, and it also approaches better a
formal version of open-ended evolution based on previous results. The results
validate the motivations and results of Chaitin&apos;s Metabiology programme and
previous suggestions that computation may be an equally important driver of
evolution together, and even before, the action and result of natural
selection. We also show that inducing the method on problems of optimization,
such as genetic algorithms, has the potential to accelerate convergence of
artificial evolutionary algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orozco_S/0/1/0/all/0/1&quot;&gt;Santiago Hern&amp;#xe1;ndez-Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06274">
<title>Mobile Machine Learning Hardware at ARM: A Systems-on-Chip (SoC) Perspective. (arXiv:1801.06274v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06274</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is playing an increasingly significant role in emerging
mobile application domains such as AR/VR, ADAS, etc. Accordingly, hardware
architects have designed customized hardware for machine learning algorithms,
especially neural networks, to improve compute efficiency. However, machine
learning is typically just one processing stage in complex end-to-end
applications, involving multiple components in a mobile Systems-on-a-chip
(SoC). Focusing only on ML accelerators loses bigger optimization opportunity
at the system (SoC) level. This paper argues that hardware architects should
expand the optimization scope to the entire SoC. We demonstrate one particular
case-study in the domain of continuous computer vision where camera sensor,
image signal processor (ISP), memory, and NN accelerator are synergistically
co-designed to achieve optimal system-level efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1&quot;&gt;Matthew Mattina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1&quot;&gt;Paul Whatmough&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00530">
<title>Scalable L\&apos;evy Process Priors for Spectral Kernel Learning. (arXiv:1802.00530v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00530</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian processes are rich distributions over functions, with generalization
properties determined by a kernel function. When used for long-range
extrapolation, predictions are particularly sensitive to the choice of kernel
parameters. It is therefore critical to account for kernel uncertainty in our
predictive distributions. We propose a distribution over kernels formed by
modelling a spectral mixture density with a L\&apos;evy process. The resulting
distribution has support for all stationary covariances--including the popular
RBF, periodic, and Mat\&apos;ern kernels--combined with inductive biases which
enable automatic and data efficient learning, long-range extrapolation, and
state of the art predictive performance. The proposed model also presents an
approach to spectral regularization, as the L\&apos;evy process introduces a
sparsity-inducing prior over mixture components, allowing automatic selection
over model order and pruning of extraneous components. We exploit the algebraic
structure of the proposed process for $\mathcal{O}(n)$ training and
$\mathcal{O}(1)$ predictions. We perform extrapolations having reasonable
uncertainty estimates on several benchmarks, show that the proposed model can
recover flexible ground truth covariances and that it is robust to errors in
initialization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jang_P/0/1/0/all/0/1&quot;&gt;Phillip A. Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loeb_A/0/1/0/all/0/1&quot;&gt;Andrew E. Loeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Davidow_M/0/1/0/all/0/1&quot;&gt;Matthew B. Davidow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00748">
<title>Short-term Memory of Deep RNN. (arXiv:1802.00748v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00748</link>
<description rdf:parseType="Literal">&lt;p&gt;The extension of deep learning towards temporal data processing is gaining an
increasing research interest. In this paper we investigate the properties of
state dynamics developed in successive levels of deep recurrent neural networks
(RNNs) in terms of short-term memory abilities. Our results reveal interesting
insights that shed light on the nature of layering as a factor of RNN design.
Noticeably, higher layers in a hierarchically organized RNN architecture
results to be inherently biased towards longer memory spans even prior to
training of the recurrent connections. Moreover, in the context of Reservoir
Computing framework, our analysis also points out the benefit of a layered
recurrent organization as an efficient approach to improve the memory skills of
reservoir models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1&quot;&gt;Claudio Gallicchio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03224">
<title>Finding Better Active Learners for Faster Literature Reviews. (arXiv:1612.03224v5 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03224</link>
<description rdf:parseType="Literal">&lt;p&gt;Literature reviews can be time-consuming and tedious to complete. By
cataloging and refactoring three state-of-the-art active learning techniques
from evidence-based medicine and legal electronic discovery, this paper finds
and implements FASTREAD, a faster technique for studying a large corpus of
documents. This paper assesses FASTREAD using datasets generated from existing
SE literature reviews (Hall, Wahono, Radjenovi\&apos;c, Kitchenham et al.). Compared
to manual methods, FASTREAD lets researchers find 95% relevant studies after
reviewing an order of magnitude fewer papers. Compared to other
state-of-the-art automatic methods, FASTREAD reviews 20-50% fewer studies while
finding same number of relevant primary studies in a systematic literature
review.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraft_N/0/1/0/all/0/1&quot;&gt;Nicholas A. Kraft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1&quot;&gt;Tim Menzies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07604">
<title>A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications. (arXiv:1709.07604v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07604</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph is an important data representation which appears in a wide diversity
of real-world scenarios. Effective graph analytics provides users a deeper
understanding of what is behind the data, and thus can benefit a lot of useful
applications such as node classification, node recommendation, link prediction,
etc. However, most graph analytics methods suffer the high computation and
space cost. Graph embedding is an effective yet efficient way to solve the
graph analytics problem. It converts the graph data into a low dimensional
space in which the graph structural information and graph properties are
maximally preserved. In this survey, we conduct a comprehensive review of the
literature in graph embedding. We first introduce the formal definition of
graph embedding as well as the related concepts. After that, we propose two
taxonomies of graph embedding which correspond to what challenges exist in
different graph embedding problem settings and how the existing work address
these challenges in their solutions. Finally, we summarize the applications
that graph embedding enables and suggest four promising future research
directions in terms of computation efficiency, problem settings, techniques and
application scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hongyun Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_V/0/1/0/all/0/1&quot;&gt;Vincent W. Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kevin Chen-Chuan Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01711">
<title>Coding-theorem Like Behaviour and Emergence of the Universal Distribution from Resource-bounded Algorithmic Probability. (arXiv:1711.01711v8 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;Previously referred to as `miraculous&apos; in the scientific literature because
of its powerful properties and its wide application as optimal solution to the
problem of induction/inference, (approximations to) Algorithmic Probability
(AP) and the associated Universal Distribution are (or should be) of the
greatest importance in science. Here we investigate the emergence, the rates of
emergence and convergence, and the Coding-theorem like behaviour of AP in
Turing-subuniversal models of computation. We investigate empirical
distributions of computing models in the Chomsky hierarchy. We introduce
measures of algorithmic probability and algorithmic complexity based upon
resource-bounded computation, in contrast to previously thoroughly investigated
distributions produced from the output distribution of Turing machines. This
approach allows for numerical approximations to algorithmic
(Kolmogorov-Chaitin) complexity-based estimations at each of the levels of a
computational hierarchy. We demonstrate that all these estimations are
correlated in rank and that they converge both in rank and values as a function
of computational power, despite fundamental differences between computational
models. In the context of natural processes that operate below the Turing
universal level because of finite resources and physical degradation, the
investigation of natural biases stemming from algorithmic rules may shed light
on the distribution of outcomes. We show that up to 60\% of the
simplicity/complexity bias in distributions produced even by the weakest of the
computational models can be accounted for by Algorithmic Probability in its
approximation to the Universal Distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badillo_L/0/1/0/all/0/1&quot;&gt;Liliana Badillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orozco_S/0/1/0/all/0/1&quot;&gt;Santiago Hern&amp;#xe1;ndez-Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Quiroz_F/0/1/0/all/0/1&quot;&gt;Francisco Hern&amp;#xe1;ndez-Quiroz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02225">
<title>Pose-Normalized Image Generation for Person Re-identification. (arXiv:1712.02225v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02225</link>
<description rdf:parseType="Literal">&lt;p&gt;Person Re-identification (re-id) faces two major challenges: the lack of
cross-view paired training data and learning discriminative identity-sensitive
and view-invariant features in the presence of large pose variations. In this
work, we address both problems by proposing a novel deep person image
generation model for synthesizing realistic person images conditional on pose.
The model is based on a generative adversarial network (GAN) and used
specifically for pose normalization in re-id, thus termed pose-normalization
GAN (PN-GAN). With the synthesized images, we can learn a new type of deep
re-id feature free of the influence of pose variations. We show that this
feature is strong on its own and highly complementary to features learned with
the original images. Importantly, we now have a model that generalizes to any
new re-id dataset without the need for collecting any training data for model
fine-tuning, thus making a deep re-id model truly scalable. Extensive
experiments on five benchmarks show that our model outperforms the
state-of-the-art models, often significantly. In particular, the features
learned on Market-1501 can achieve a Rank-1 accuracy of 68.67% on VIPeR without
any model fine-tuning, beating almost all existing models fine-tuned on the
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xuelin Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00459">
<title>Sensitivity Sampling Over Dynamic Geometric Data Streams with Applications to $k$-Clustering. (arXiv:1802.00459v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1802.00459</link>
<description rdf:parseType="Literal">&lt;p&gt;Sensitivity based sampling is crucial for constructing nearly-optimal coreset
for $k$-means / median clustering. In this paper, we provide a novel data
structure that enables sensitivity sampling over a dynamic data stream, where
points from a high dimensional discrete Euclidean space can be either inserted
or deleted. Based on this data structure, we provide a one-pass coreset
construction for $k$-means %and M-estimator clustering using space
$\widetilde{O}(k\mathrm{poly}(d))$ over $d$-dimensional geometric dynamic data
streams. While previous best known result is only for $k$-median [Braverman,
Frahling, Lang, Sohler, Yang&apos; 17], which cannot be directly generalized to
$k$-means to obtain algorithms with space nearly linear in $k$. To the best of
our knowledge, our algorithm is the first dynamic geometric data stream
algorithm for $k$-means using space polynomial in dimension and nearly optimal
in $k$.
&lt;/p&gt;
&lt;p&gt;We further show that our data structure for maintaining coreset can be
extended as a unified approach for a more general classes of $k$-clustering,
including $k$-median, $M$-estimator clustering, and clusterings with a more
general set of cost functions over distances. For all these tasks, the
space/time of our algorithm is similar to $k$-means with only
$\mathrm{poly}(d)$ factor difference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin F. Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1&quot;&gt;Peilin Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00474">
<title>Bayesian Modeling via Goodness-of-fit. (arXiv:1802.00474v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1802.00474</link>
<description rdf:parseType="Literal">&lt;p&gt;The two key issues of modern Bayesian statistics are: (i) establishing
principled approach for distilling statistical prior that is consistent with
the given data from an initial believable scientific prior; and (ii)
development of a Bayes-frequentist consolidated data analysis workflow that is
more effective than either of the two separately. In this paper, we propose the
idea of &quot;Bayes via goodness of fit&quot; as a framework for exploring these
fundamental questions, in a way that is general enough to embrace almost all of
the familiar probability models. Several illustrative examples show the benefit
of this new point of view as a practical data analysis tool. Relationship with
other Bayesian cultures is also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Subhadeep/0/1/0/all/0/1&quot;&gt;Subhadeep&lt;/a&gt; (Deep) &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mukhopadhyay/0/1/0/all/0/1&quot;&gt;Mukhopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fletcher_D/0/1/0/all/0/1&quot;&gt;Douglas Fletcher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00543">
<title>Modeling polypharmacy side effects with graph convolutional networks. (arXiv:1802.00543v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00543</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of multiple drugs, termed polypharmacy, is common to treat patients
with complex diseases or co-existing medical conditions. However, a major
consequence of polypharmacy is a much higher risk of side effects for the
patient. Polypharmacy side effects emerge because of drug interactions, in
which activity of one drug may change, favorably or unfavorably, if taken with
another drug. The knowledge of drug interactions is limited because these
complex relationships are usually not observed in small clinical testing.
Discovering polypharmacy side effects thus remains a challenge with significant
implications for patient mortality and morbidity. Here we introduce Decagon, an
approach for modeling polypharmacy side effects. The approach constructs a
multimodal graph of protein-protein interactions, drug-protein interactions,
and the polypharmacy side effects, which are represented as drug-drug
interactions, where each side effect is an edge of a different type. Decagon is
developed specifically to handle such multimodal graphs with a large number of
edge types. Our approach develops a new graph convolutional neural network for
multirelational link prediction in multimodal networks. Unlike approaches
limited to predicting simple drug-drug interaction values, Decagon can predict
the exact side effect, if any, through which a given drug combination manifests
clinically. Decagon accurately predicts polypharmacy side effects,
outperforming baselines by up to 69%. Furthermore, Decagon models particularly
well side effects that have a strong molecular basis, while on predominantly
non-molecular side effects, it achieves good performance because of effective
sharing of model parameters across edge types. Decagon creates an opportunity
to use large molecular and patient population data to flag and prioritize
polypharmacy side effects for follow-up analysis via formal pharmacological
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1&quot;&gt;Monica Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00568">
<title>An Instability in Variational Inference for Topic Models. (arXiv:1802.00568v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00568</link>
<description rdf:parseType="Literal">&lt;p&gt;Topic models are Bayesian models that are frequently used to capture the
latent structure of certain corpora of documents or images. Each data element
in such a corpus (for instance each item in a collection of scientific
articles) is regarded as a convex combination of a small number of vectors
corresponding to `topics&apos; or `components&apos;. The weights are assumed to have a
Dirichlet prior distribution. The standard approach towards approximating the
posterior is to use variational inference algorithms, and in particular a mean
field approximation.
&lt;/p&gt;
&lt;p&gt;We show that this approach suffers from an instability that can produce
misleading conclusions. Namely, for certain regimes of the model parameters,
variational inference outputs a non-trivial decomposition into topics. However
--for the same parameter values-- the data contain no actual information about
the true decomposition, and hence the output of the algorithm is uncorrelated
with the true topic decomposition. Among other consequences, the estimated
posterior mean is significantly wrong, and estimated Bayesian credible regions
do not achieve the nominal coverage. We discuss how this instability is
remedied by more accurate mean field approximations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghorbani_B/0/1/0/all/0/1&quot;&gt;Behrooz Ghorbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Javadi_H/0/1/0/all/0/1&quot;&gt;Hamid Javadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Andrea Montanari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1507.04564">
<title>Selecting the best system, large deviations, and multi-armed bandits. (arXiv:1507.04564v2 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/1507.04564</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider the problem of finding a population amongst many with the largest
mean when these means are unknown but population samples can be generated via
simulation. Typically, by selecting a population with the largest sample mean,
it can be shown that the false selection probability decays at an exponential
rate. Lately researchers have sought algorithms that guarantee that this
probability is restricted to a small $\delta$ in order $\log(1/\delta)$
computational time by estimating the associated large deviations rate function
via simulation. We show that such guarantees are misleading. Enroute, we
identify the large deviations principle followed by the empirically estimated
large deviations rate function that may also be of independent interest.
Further, we show a negative result that when populations have unbounded
support, under mild restrictions, any policy that asymptotically identifies the
correct population with probability at least $1-\delta$ for each problem
instance requires more than $O(\log(1/\delta))$ samples in making such a
determination in any problem instance. This suggests that some restrictions are
essential on populations to devise $O(\log(1/\delta))$ algorithms with $1 -
\delta$ correctness guarantees. We note that under restriction on population
moments, such methods are easily designed. We also observe that sequential
methods from stochastic multi-armed bandit literature can be adapted to devise
such algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Glynn_P/0/1/0/all/0/1&quot;&gt;Peter Glynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Juneja_S/0/1/0/all/0/1&quot;&gt;Sandeep Juneja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03957">
<title>Monte Carlo Structured SVI for Two-Level Non-Conjugate Models. (arXiv:1612.03957v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03957</link>
<description rdf:parseType="Literal">&lt;p&gt;The stochastic variational inference (SVI) paradigm, which combines
variational inference, natural gradients, and stochastic updates, was recently
proposed for large-scale data analysis in conjugate Bayesian models and
demonstrated to be effective in several problems. This paper studies a family
of Bayesian latent variable models with two levels of hidden variables but
without any conjugacy requirements, making several contributions in this
context. The first is observing that SVI, with an improved structured
variational approximation, is applicable under more general conditions than
previously thought with the only requirement being that the approximating
variational distribution be in the same family as the prior. The resulting
approach, Monte Carlo Structured SVI (MC-SSVI), significantly extends the scope
of SVI, enabling large-scale learning in non-conjugate models. For models with
latent Gaussian variables we propose a hybrid algorithm, using both standard
and natural gradients, which is shown to improve stability and convergence.
Applications in mixed effects models, sparse Gaussian processes, probabilistic
matrix factorization and correlated topic models demonstrate the generality of
the approach and the advantages of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheth_R/0/1/0/all/0/1&quot;&gt;Rishit Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khardon_R/0/1/0/all/0/1&quot;&gt;Roni Khardon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.04126">
<title>Gaussian-Dirichlet Posterior Dominance in Sequential Learning. (arXiv:1702.04126v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.04126</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of sequential learning from categorical observations
bounded in [0,1]. We establish an ordering between the Dirichlet posterior over
categorical outcomes and a Gaussian posterior under observations with N(0,1)
noise. We establish that, conditioned upon identical data with at least two
observations, the posterior mean of the categorical distribution will always
second-order stochastically dominate the posterior mean of the Gaussian
distribution. These results provide a useful tool for the analysis of
sequential learning under categorical outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osband_I/0/1/0/all/0/1&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00139">
<title>Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel. (arXiv:1709.00139v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00139</link>
<description rdf:parseType="Literal">&lt;p&gt;Support vector data description (SVDD) is a machine learning technique that
is used for single-class classification and outlier detection. The idea of SVDD
is to find a set of support vectors that defines a boundary around data. When
dealing with online or large data, existing batch SVDD methods have to be rerun
in each iteration. We propose an incremental learning algorithm for SVDD that
uses the Gaussian kernel. This algorithm builds on the observation that all
support vectors on the boundary have the same distance to the center of sphere
in a higher-dimensional feature space as mapped by the Gaussian kernel
function. Each iteration involves only the existing support vectors and the new
data point. Moreover, the algorithm is based solely on matrix manipulations;
the support vectors and their corresponding Lagrange multiplier $\alpha_i$&apos;s
are automatically selected and determined in each iteration. It can be seen
that the complexity of our algorithm in each iteration is only $O(k^2)$, where
$k$ is the number of support vectors. Experimental results on some real data
sets indicate that FISVDD demonstrates significant gains in efficiency with
almost no loss in either outlier detection accuracy or objective function
value.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hansi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenhao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kakde_D/0/1/0/all/0/1&quot;&gt;Deovrat Kakde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chaudhuri_A/0/1/0/all/0/1&quot;&gt;Arin Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07739">
<title>Learning Discrete Weights Using the Local Reparameterization Trick. (arXiv:1710.07739v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07739</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in computer vision make use of large deep neural
networks, utilizing the substantial speedup offered by GPUs. For applications
running on limited hardware, however, high precision real-time processing can
still be a challenge. One approach to solving this problem is training networks
with binary or ternary weights, thus removing the need to calculate
multiplications and significantly reducing memory size. In this work, we
introduce LR-nets (Local reparameterization networks), a new method for
training neural networks with discrete weights using stochastic parameters. We
show how a simple modification to the local reparameterization trick,
previously used to train Gaussian distributed weights, enables the training of
discrete weights. Using the proposed training we test both binary and ternary
models on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art
results on most experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shayer_O/0/1/0/all/0/1&quot;&gt;Oran Shayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levi_D/0/1/0/all/0/1&quot;&gt;Dan Levi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10348">
<title>Multi-level Residual Networks from Dynamical Systems View. (arXiv:1710.10348v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10348</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep residual networks (ResNets) and their variants are widely used in many
computer vision applications and natural language processing tasks. However,
the theoretical principles for designing and training ResNets are still not
fully understood. Recently, several points of view have emerged to try to
interpret ResNet theoretically, such as unraveled view, unrolled iterative
estimation and dynamical systems view. In this paper, we adopt the dynamical
systems point of view, and analyze the lesioning properties of ResNet both
theoretically and experimentally. Based on these analyses, we additionally
propose a novel method for accelerating ResNet training. We apply the proposed
method to train ResNets and Wide ResNets for three image classification
benchmarks, reducing training time by more than 40% with superior or on-par
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_B/0/1/0/all/0/1&quot;&gt;Bo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lili Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haber_E/0/1/0/all/0/1&quot;&gt;Eldad Haber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tung_F/0/1/0/all/0/1&quot;&gt;Frederick Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Begert_D/0/1/0/all/0/1&quot;&gt;David Begert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11381">
<title>Semantic Interpolation in Implicit Models. (arXiv:1710.11381v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11381</link>
<description rdf:parseType="Literal">&lt;p&gt;In implicit models, one often interpolates between sampled points in latent
space. As we show in this paper, care needs to be taken to match-up the
distributional assumptions on code vectors with the geometry of the
interpolating paths. Otherwise, typical assumptions about the quality and
semantics of in-between points may not be justified. Based on our analysis we
propose to modify the prior code distribution to put significantly more
probability mass closer to the origin. As a result, linear interpolation paths
are not only shortest paths, but they are also guaranteed to pass through
high-density regions, irrespective of the dimensionality of the latent space.
Experiments on standard benchmark image datasets demonstrate clear visual
improvements in the quality of the generated samples and exhibit more
meaningful interpolation paths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1&quot;&gt;Yannic Kilcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02519">
<title>Convergence Rates of Variational Posterior Distributions. (arXiv:1712.02519v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02519</link>
<description rdf:parseType="Literal">&lt;p&gt;We study convergence rates of variational posterior distributions for
nonparametric and high-dimensional inference. We formulate general conditions
on prior, likelihood, and variational class that characterize the convergence
rates. Under similar &quot;prior mass and testing&quot; conditions considered in the
literature, the rate is found to be the sum of two terms. The first term stands
for the convergence rate of the true posterior distribution, and the second
term is contributed by the variational approximation error. For a class of
priors that admit the structure of a mixture of product measures, we propose a
novel prior mass condition, under which the variational approximation error of
the generalized mean-field class is dominated by convergence rate of the true
posterior. We demonstrate the applicability of our general results for various
models, prior distributions and variational classes by deriving convergence
rates of the corresponding variational posteriors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fengshuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chao Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06818">
<title>Recovering a Hidden Community in a Preferential Attachment Graph. (arXiv:1801.06818v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06818</link>
<description rdf:parseType="Literal">&lt;p&gt;A message passing algorithm is derived for recovering a dense subgraph within
a graph generated by a variation of the Barab\&apos;{a}si-Albert preferential
attachment model. The estimator is assumed to know the arrival times, or order
of attachment, of the vertices. The derivation of the algorithm is based on
belief propagation under an independence assumption. Two precursors to the
message passing algorithm are analyzed: the first is a degree thresholding (DT)
algorithm and the second is an algorithm based on the arrival times of the
children (C) of a given vertex, where the children of a given vertex are the
vertices that attached to it. Algorithm C significantly outperforms DT, showing
it is beneficial to know the arrival times of the children, beyond simply
knowing the number of them. For fixed fraction of vertices in the community,
fixed number of new edges per arriving vertex, and fixed affinity between
vertices in the community, the probability of error for recovering the label of
a vertex is found as a function of the time of attachment, for either algorithm
DT or C, in the large graph limit, for either algorithm DT or C. By averaging
over the time of attachment, the limit in probability of the fraction of label
errors made over all vertices is identified, for either of the algorithms DT or
C.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sankagiri_S/0/1/0/all/0/1&quot;&gt;Suryanarayana Sankagiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00047">
<title>Matrix completion with deterministic pattern - a geometric perspective. (arXiv:1802.00047v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00047</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the matrix completion problem with a deterministic pattern of
observed entries and aim to find conditions such that there will be (at least
locally) unique solution to the non-convex Minimum Rank Matrix Completion
(MRMC) formulation. We answer the question from a somewhat different point of
view and to give a geometric perspective. We give a sufficient and &quot;almost
necessary&quot; condition (which we call the well-posedness condition) for the local
uniqueness of MRMC solutions and illustrate with some special cases where such
condition can be verified. We also consider the convex relaxation and nuclear
norm minimization formulations. Then we argue that the low-rank approximation
approaches are more stable than MRMC and further propose a sequential
statistical testing procedure to determine the rank of the matrix from observed
entries. Finally, numerical examples verified the validity of our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_A/0/1/0/all/0/1&quot;&gt;Alexander Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
</item></rdf:RDF>