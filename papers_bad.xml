<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.04855"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.08012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00746"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03929"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04099"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04134"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04053"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04140"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.01000"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.08500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01141"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1511.04855">
<title>Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover source-mismatch. (arXiv:1511.04855v2 [cs.MM] UPDATED)</title>
<link>http://arxiv.org/abs/1511.04855</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the BOSS competition, in 2010, most steganalysis approaches use a
learning methodology involving two steps: feature extraction, such as the Rich
Models (RM), for the image representation, and use of the Ensemble Classifier
(EC) for the learning step. In 2015, Qian et al. have shown that the use of a
deep learning approach that jointly learns and computes the features, is very
promising for the steganalysis. In this paper, we follow-up the study of Qian
et al., and show that, due to intrinsic joint minimization, the results
obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural
Network (FNN), if well parameterized, surpass the conventional use of a RM with
an EC. First, numerous experiments were conducted in order to find the best &quot;
shape &quot; of the CNN. Second, experiments were carried out in the clairvoyant
scenario in order to compare the CNN and FNN to an RM with an EC. The results
show more than 16% reduction in the classification error with our CNN or FNN.
Third, experiments were also performed in a cover-source mismatch setting. The
results show that the CNN and FNN are naturally robust to the mismatch problem.
In Addition to the experiments, we provide discussions on the internal
mechanisms of a CNN, and weave links with some previously stated ideas, in
order to understand the impressive results we obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pibre_L/0/1/0/all/0/1&quot;&gt;Lionel Pibre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jerome_P/0/1/0/all/0/1&quot;&gt;Pasquet J&amp;#xe9;r&amp;#xf4;me&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ienco_D/0/1/0/all/0/1&quot;&gt;Dino Ienco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaumont_M/0/1/0/all/0/1&quot;&gt;Marc Chaumont&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.08012">
<title>Deep learning with convolutional neural networks for decoding and visualization of EEG pathology. (arXiv:1708.08012v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.08012</link>
<description rdf:parseType="Literal">&lt;p&gt;We apply convolutional neural networks (ConvNets) to the task of
distinguishing pathological from normal EEG recordings in the Temple University
Hospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet
architectures recently shown to decode task-related information from EEG at
least as well as established algorithms designed for this purpose. In decoding
EEG pathology, both ConvNets reached substantially better accuracies (about 6%
better, ~85% vs. ~79%) than the only published result for this dataset, and
were still better when using only 1 minute of each recording for training and
only six seconds of each recording for testing. We used automated methods to
optimize architectural hyperparameters and found intriguingly different ConvNet
architectures, e.g., with max pooling as the only nonlinearity. Visualizations
of the ConvNet decoding behavior showed that they used spectral power changes
in the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside
other features, consistent with expectations derived from spectral analysis of
the EEG data and from the textual medical reports. Analysis of the textual
medical reports also highlighted the potential for accuracy increases by
integrating contextual information, such as the age of subjects. In summary,
the ConvNets and visualization techniques used in this study constitute a next
step towards clinically useful automated EEG diagnosis and establish a new
baseline for future work on this topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gemein_L/0/1/0/all/0/1&quot;&gt;Lukas Gemein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eggensperger_K/0/1/0/all/0/1&quot;&gt;Katharina Eggensperger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00746">
<title>Bridging the Gap Between Neural Networks and Neuromorphic Hardware with A Neural Network Compiler. (arXiv:1801.00746v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00746</link>
<description rdf:parseType="Literal">&lt;p&gt;Different from developing neural networks (NNs) for general-purpose
processors, the development for NN chips usually faces with some
hardware-specific restrictions, such as limited precision of network signals
and parameters, constrained computation scale, and limited types of non-linear
functions.
&lt;/p&gt;
&lt;p&gt;This paper proposes a general methodology to address the challenges. We
decouple the NN applications from the target hardware by introducing a compiler
that can transform an existing trained, unrestricted NN into an equivalent
network that meets the given hardware&apos;s constraints. We propose multiple
techniques to make the transformation adaptable to different kinds of NN chips,
and reliable for restrict hardware constraints.
&lt;/p&gt;
&lt;p&gt;We have built such a software tool that supports both spiking neural networks
(SNNs) and traditional artificial neural networks (ANNs). We have demonstrated
its effectiveness with a fabricated neuromorphic chip and a
processing-in-memory (PIM) design. Tests show that the inference error caused
by this solution is insignificant and the transformation time is much shorter
than the retraining time. Also, we have studied the parameter-sensitivity
evaluations to explore the tradeoffs between network error and resource
utilization for different transformation strategies, which could provide
insights for co-design optimization of neuromorphic hardware and software.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;YouHui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;WenGuang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuan Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03929">
<title>Formalized Conceptual Spaces with a Geometric Representation of Correlations. (arXiv:1801.03929v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03929</link>
<description rdf:parseType="Literal">&lt;p&gt;The highly influential framework of conceptual spaces provides a geometric
way of representing knowledge. Instances are represented by points in a
similarity space and concepts are represented by convex regions in this space.
After pointing out a problem with the convexity requirement, we propose a
formalization of conceptual spaces based on fuzzy star-shaped sets. Our
formalization uses a parametric definition of concepts and extends the original
framework by adding means to represent correlations between different domains
in a geometric way. Moreover, we define various operations for our
formalization, both for creating new concepts from old ones and for measuring
relations between concepts. We present an illustrative toy-example and sketch a
research project on concept formation that is based on both our formalization
and its implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechberger_L/0/1/0/all/0/1&quot;&gt;Lucas Bechberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhnberger_K/0/1/0/all/0/1&quot;&gt;Kai-Uwe K&amp;#xfc;hnberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03954">
<title>Model-Based Action Exploration. (arXiv:1801.03954v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03954</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has great stride in solving challenging motion
control tasks.
&lt;/p&gt;
&lt;p&gt;Recently there has been a significant amount of work on methods to exploit
the data gathered during training, but less work is done on good methods for
generating data to learn from.
&lt;/p&gt;
&lt;p&gt;For continuous actions domains, the typical method for generating exploratory
actions is by sampling from a Gaussian distribution centred around the mean of
a policy.
&lt;/p&gt;
&lt;p&gt;Although these methods can find an optimal policy, in practise, they do not
scale well, and solving environments with many actions dimensions becomes
impractical.
&lt;/p&gt;
&lt;p&gt;We consider learning a forward dynamics model to predict the result,
($s_{t+1}$), of taking a particular action, ($a$), given a specific observation
of the state, ($s_{t}$).
&lt;/p&gt;
&lt;p&gt;With a model such as this we, can perform what comes more naturally to
biological systems that have already collect experience, we perform internal
predictions of outcomes and endeavour to try actions we believe have a
reasonable chance of success.
&lt;/p&gt;
&lt;p&gt;This method greatly reduces the space of exploratory actions, increasing
learning speed and enables higher quality solutions to difficult problems, such
as robotic locomotion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1&quot;&gt;Glen Berseth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1&quot;&gt;Michiel van de Panne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03968">
<title>Interactive Learning of Acyclic Conditional Preference Networks. (arXiv:1801.03968v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03968</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning of user preferences, as represented by, for example, Conditional
Preference Networks (CP-nets), has become a core issue in AI research. Recent
studies investigate learning of CP-nets from randomly chosen examples or from
membership and equivalence queries. To assess the optimality of learning
algorithms as well as to better understand the combinatorial structure of
classes of CP-nets, it is helpful to calculate certain learning-theoretic
information complexity parameters. This paper determines bounds on or exact
values of some of the most central information complexity parameters, namely
the VC dimension, the (recursive) teaching dimension, the self-directed
learning complexity, and the optimal mistake bound, for classes of acyclic
CP-nets. We further provide an algorithm that learns tree-structured CP-nets
from membership queries. Using our results on complexity parameters, we assess
the optimality of our algorithm as well as that of another query learning
algorithm for acyclic CP-nets presented in the literature. Our algorithm is
near-optimal, and can, under certain assumptions be adapted to the case when
the membership oracle is faulty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alanazi_E/0/1/0/all/0/1&quot;&gt;Eisa Alanazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouhoub_M/0/1/0/all/0/1&quot;&gt;Malek Mouhoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilles_S/0/1/0/all/0/1&quot;&gt;Sandra Zilles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03984">
<title>A Brain-Inspired Trust Management Model to Assure Security in a Cloud based IoT Framework for Neuroscience Applications. (arXiv:1801.03984v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1801.03984</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapid popularity of Internet of Things (IoT) and cloud computing permits
neuroscientists to collect multilevel and multichannel brain data to better
understand brain functions, diagnose diseases, and devise treatments. To ensure
secure and reliable data communication between end-to-end (E2E) devices
supported by current IoT and cloud infrastructure, trust management is needed
at the IoT and user ends. This paper introduces a Neuro-Fuzzy based
Brain-inspired trust management model (TMM) to secure IoT devices and relay
nodes, and to ensure data reliability. The proposed TMM utilizes node
behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference
System and weighted-additive methods respectively to assess the nodes
trustworthiness. In contrast to the existing fuzzy based TMMs, the NS2
simulation results confirm the robustness and accuracy of the proposed TMM in
identifying malicious nodes in the communication network. With the growing
usage of cloud based IoT frameworks in Neuroscience research, integrating the
proposed TMM into the existing infrastructure will assure secure and reliable
data communication among the E2E devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmud_M/0/1/0/all/0/1&quot;&gt;Mufti Mahmud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1&quot;&gt;M. Shamim Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;M. Mostafizur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;M. Arifur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabut_A/0/1/0/all/0/1&quot;&gt;Antesar Shabut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Mamun_S/0/1/0/all/0/1&quot;&gt;Shamim Al-Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1&quot;&gt;Amir Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04099">
<title>Planning with Trust for Human-Robot Collaboration. (arXiv:1801.04099v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1801.04099</link>
<description rdf:parseType="Literal">&lt;p&gt;Trust is essential for human-robot collaboration and user adoption of
autonomous systems, such as robot assistants. This paper introduces a
computational model which integrates trust into robot decision-making.
Specifically, we learn from data a partially observable Markov decision process
(POMDP) with human trust as a latent variable. The trust-POMDP model provides a
principled approach for the robot to (i) infer the trust of a human teammate
through interaction, (ii) reason about the effect of its own actions on human
behaviors, and (iii) choose actions that maximize team performance over the
long term. We validated the model through human subject experiments on a
table-clearing task in simulation (201 participants) and with a real robot (20
participants). The results show that the trust-POMDP improves human-robot team
performance in this task. They further suggest that maximizing trust in itself
may not improve team performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaidis_S/0/1/0/all/0/1&quot;&gt;Stefanos Nikolaidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1&quot;&gt;Harold Soh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1&quot;&gt;Siddhartha Srinivasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04134">
<title>Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic Experiences for Robot Action Execution. (arXiv:1801.04134v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04134</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel deep neural network architecture for representing robot
experiences in an episodic-like memory which facilitates encoding, recalling,
and predicting action experiences. Our proposed unsupervised deep episodic
memory model 1) encodes observed actions in a latent vector space and, based on
this latent encoding, 2) infers action categories, 3) reconstructs original
frames, and 4) predicts future frames. We evaluate the proposed model on two
different large-scale action datasets. Results show that conceptually similar
actions are mapped into the same region of the latent vector space. Results
show that conceptual similarity of videos is reflected by the proximity of
their vector representations in the latent space.Based on this contribution, we
introduce an action matching and retrieval mechanism and evaluate its
performance and generalization capability on a real humanoid robot in an action
execution scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rothfuss_J/0/1/0/all/0/1&quot;&gt;Jonas Rothfuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1&quot;&gt;Eren Erdal Aksoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asfour_T/0/1/0/all/0/1&quot;&gt;Tamim Asfour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04170">
<title>Multilayered Model of Speech. (arXiv:1801.04170v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04170</link>
<description rdf:parseType="Literal">&lt;p&gt;Human speech is the most important part of General Artificial Intelligence
and subject of much research. The hypothesis proposed in this article provides
explanation of difficulties that modern science tackles in the field of human
brain simulation. The hypothesis is based on the author&apos;s conviction that the
brain of any given person has different ability to process and store
information. Therefore, the approaches that are currently used to create
General Artificial Intelligence have to be altered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chistyakov_A/0/1/0/all/0/1&quot;&gt;Andrey Chistyakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03433">
<title>A Model of Multi-Agent Consensus for Vague and Uncertain Beliefs. (arXiv:1612.03433v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03433</link>
<description rdf:parseType="Literal">&lt;p&gt;Consensus formation is investigated for multi-agent systems in which agents&apos;
beliefs are both vague and uncertain. Vagueness is represented by a third truth
state meaning \emph{borderline}. This is combined with a probabilistic model of
uncertainty. A belief combination operator is then proposed which exploits
borderline truth values to enable agents with conflicting beliefs to reach a
compromise. A number of simulation experiments are carried out in which agents
apply this operator in pairwise interactions, under the bounded confidence
restriction that the two agents&apos; beliefs must be sufficiently consistent with
each other before agreement can be reached. As well as studying the consensus
operator in isolation we also investigate scenarios in which agents are
influenced either directly or indirectly by the state of the world. For the
former we conduct simulations which combine consensus formation with belief
updating based on evidence. For the latter we investigate the effect of
assuming that the closer an agent&apos;s beliefs are to the truth the more visible
they are in the consensus building process. In all cases applying the consensus
operators results in the population converging to a single shared belief which
is both crisp and certain. Furthermore, simulations which combine consensus
formation with evidential updating converge faster to a shared opinion which is
closer to the actual state of the world than those in which beliefs are only
changed as a result of directly receiving new evidence. Finally, if agent
interactions are guided by belief quality measured as similarity to the true
state of the world, then applying the consensus operator alone results in the
population converging to a high quality shared belief.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crosscombe_M/0/1/0/all/0/1&quot;&gt;Michael Crosscombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawry_J/0/1/0/all/0/1&quot;&gt;Jonathan Lawry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04502">
<title>Clusters of Driving Behavior from Observational Smartphone Data. (arXiv:1710.04502v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04502</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding driving behaviors is essential for improving safety and
mobility of our transportation systems. Data is usually collected via
simulator-based studies or naturalistic driving studies. Those techniques allow
for understanding relations between demographics, road conditions and safety.
On the other hand, they are very costly and time consuming. Thanks to the
ubiquity of smartphones, we have an opportunity to substantially complement
more traditional data collection techniques with data extracted from phone
sensors, such as GPS, accelerometer gyroscope and camera. We developed
statistical models that provided insight into driver behavior in the San
Francisco metro area based on tens of thousands of driver logs. We used novel
data sources to support our work. We used cell phone sensor data drawn from
five hundred drivers in San Francisco to understand the speed of traffic across
the city as well as the maneuvers of drivers in different areas. Specifically,
we clustered drivers based on their driving behavior. We looked at driver norms
by street and flagged driving behaviors that deviated from the norm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warren_J/0/1/0/all/0/1&quot;&gt;Josh Warren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipkowitz_J/0/1/0/all/0/1&quot;&gt;Jeff Lipkowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolov_V/0/1/0/all/0/1&quot;&gt;Vadim Sokolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09268">
<title>Generalizing Hamiltonian Monte Carlo with Neural Networks. (arXiv:1711.09268v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09268</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a general-purpose method to train Markov chain Monte Carlo
kernels, parameterized by deep neural networks, that converge and mix quickly
to their target distribution. Our method generalizes Hamiltonian Monte Carlo
and is trained to maximize expected squared jumped distance, a proxy for mixing
speed. We demonstrate large empirical gains on a collection of simple but
challenging distributions, for instance achieving a 106x improvement in
effective sample size in one case, and mixing when standard HMC makes no
measurable progress in a second. Finally, we show quantitative and qualitative
gains on a real-world task: latent-variable generative modeling. We release an
open source TensorFlow implementation of the algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Levy_D/0/1/0/all/0/1&quot;&gt;Daniel Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoffman_M/0/1/0/all/0/1&quot;&gt;Matthew D. Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04053">
<title>Noisy Expectation-Maximization: Applications and Generalizations. (arXiv:1801.04053v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.04053</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a noise-injected version of the Expectation-Maximization (EM)
algorithm: the Noisy Expectation Maximization (NEM) algorithm. The NEM
algorithm uses noise to speed up the convergence of the EM algorithm. The NEM
theorem shows that injected noise speeds up the average convergence of the EM
algorithm to a local maximum of the likelihood surface if a positivity
condition holds. The generalized form of the noisy expectation-maximization
(NEM) algorithm allow for arbitrary modes of noise injection including adding
and multiplying noise to the data.
&lt;/p&gt;
&lt;p&gt;We demonstrate these noise benefits on EM algorithms for the Gaussian mixture
model (GMM) with both additive and multiplicative NEM noise injection. A
separate theorem (not presented here) shows that the noise benefit for
independent identically distributed additive noise decreases with sample size
in mixture models. This theorem implies that the noise benefit is most
pronounced if the data is sparse. Injecting blind noise only slowed
convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osoba_O/0/1/0/all/0/1&quot;&gt;Osonde Osoba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kosko_B/0/1/0/all/0/1&quot;&gt;Bart Kosko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04140">
<title>Cosmic String Detection with Tree-Based Machine Learning. (arXiv:1801.04140v1 [astro-ph.CO])</title>
<link>http://arxiv.org/abs/1801.04140</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the use of random forest and gradient boosting, two powerful
tree-based machine learning algorithms, for the detection of cosmic strings in
maps of the cosmic microwave background (CMB), through their unique
Gott-Kaiser-Stebbins effect on the temperature anisotropies.The information in
the maps is compressed into feature vectors before being passed to the learning
units. The feature vectors contain various statistical measures of processed
CMB maps that boost the cosmic string detectability. Our proposed classifiers,
after training, give results improved over or similar to the claimed
detectability levels of the existing methods for string tension, $G\mu$. They
can make $3\sigma$ detection of strings with $G\mu \gtrsim 2.1\times 10^{-10}$
for noise-free, $0.9&apos;$-resolution CMB observations. The minimum detectable
tension increases to $G\mu \gtrsim 3.0\times 10^{-8}$ for a more realistic, CMB
S4-like (II) strategy, still a significant improvement over the previous
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Sadr_A/0/1/0/all/0/1&quot;&gt;A. Vafaei Sadr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Farhang_M/0/1/0/all/0/1&quot;&gt;M. Farhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Movahed_S/0/1/0/all/0/1&quot;&gt;S. M. S. Movahed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bassett_B/0/1/0/all/0/1&quot;&gt;B. Bassett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kunz_M/0/1/0/all/0/1&quot;&gt;M. Kunz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04153">
<title>Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1801.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithfully manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then demonstrate its efficiency in the context of multi-fidelity models for
complex engineering systems, as well as a problem of global illumination in
computer graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04212">
<title>Multinomial logistic model for coinfection diagnosis between arbovirus and malaria in Kedougou. (arXiv:1801.04212v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1801.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;In tropical regions, populations continue to suffer morbidity and mortality
from malaria and arboviral diseases. In Kedougou (Senegal), these illnesses are
all endemic due to the climate and its geographical position. The
co-circulation of malaria parasites and arboviruses can explain the observation
of coinfected cases. Indeed there is strong resemblance in symptoms between
these diseases making problematic targeted medical care of coinfected cases.
This is due to the fact that the origin of illness is not obviously known. Some
cases could be immunized against one or the other of the pathogens, immunity
typically acquired with factors like age and exposure as usual for endemic
area. Then, coinfection needs to be better diagnosed. Using data collected from
patients in Kedougou region, from 2009 to 2013, we adjusted a multinomial
logistic model and selected relevant variables in explaining coinfection
status. We observed specific sets of variables explaining each of the diseases
exclusively and the coinfection. We tested the independence between arboviral
and malaria infections and derived coinfection probabilities from the model
fitting. In case of a coinfection probability greater than a threshold value to
be calibrated on the data, duration of illness above 3 days and age above 10
years-old are mostly indicative of arboviral disease while body temperature
higher than 40{\textdegree}C and presence of nausea or vomiting symptoms during
the rainy season are mostly indicative of malaria disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loum_M/0/1/0/all/0/1&quot;&gt;Mor Absa Loum&lt;/a&gt; (LM-Orsay), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poursat_M/0/1/0/all/0/1&quot;&gt;Marie-Anne Poursat&lt;/a&gt; (LM-Orsay), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sow_A/0/1/0/all/0/1&quot;&gt;Abdourahmane Sow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sall_A/0/1/0/all/0/1&quot;&gt;Amadou Sall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loucoubar_C/0/1/0/all/0/1&quot;&gt;Cheikh Loucoubar&lt;/a&gt; (G4-IPD), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gassiat_E/0/1/0/all/0/1&quot;&gt;Elisabeth Gassiat&lt;/a&gt; (LM-Orsay)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.01000">
<title>Statistical learning for wind power : a modeling and stability study towards forecasting. (arXiv:1610.01000v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1610.01000</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on wind power modeling using machine learning techniques. We show on
real data provided by the wind energy company Ma{\&quot;i}a Eolis, that parametric
models, even following closely the physical equation relating wind production
to wind speed are outperformed by intelligent learning algorithms. In
particular, the CART-Bagging algorithm gives very stable and promising results.
Besides, as a step towards forecast, we quantify the impact of using
deteriorated wind measures on the performances. We show also on this
application that the default methodology to select a subset of predictors
provided in the standard random forest package can be refined, especially when
there exists among the predictors one variable which has a major impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lie Fischer&lt;/a&gt; (UPD7), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montuelle_L/0/1/0/all/0/1&quot;&gt;Lucie Montuelle&lt;/a&gt; (UPD7), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mougeot_M/0/1/0/all/0/1&quot;&gt;Mathilde Mougeot&lt;/a&gt; (UPD7), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Picard_D/0/1/0/all/0/1&quot;&gt;Dominique Picard&lt;/a&gt; (UPD7)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.08500">
<title>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. (arXiv:1706.08500v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.08500</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) excel at creating realistic images
with complex models for which maximum likelihood is infeasible. However, the
convergence of GAN training has still not been proved. We propose a two
time-scale update rule (TTUR) for training GANs with stochastic gradient
descent on arbitrary GAN loss functions. TTUR has an individual learning rate
for both the discriminator and the generator. Using the theory of stochastic
approximation, we prove that the TTUR converges under mild assumptions to a
stationary local Nash equilibrium. The convergence carries over to the popular
Adam optimization, for which we prove that it follows the dynamics of a heavy
ball with friction and thus prefers flat minima in the objective landscape. For
the evaluation of the performance of GANs at image generation, we introduce the
&quot;Fr\&apos;echet Inception Distance&quot; (FID) which captures the similarity of generated
images to real ones better than the Inception Score. In experiments, TTUR
improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
Bedrooms, and the One Billion Word Benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heusel_M/0/1/0/all/0/1&quot;&gt;Martin Heusel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramsauer_H/0/1/0/all/0/1&quot;&gt;Hubert Ramsauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1&quot;&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nessler_B/0/1/0/all/0/1&quot;&gt;Bernhard Nessler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04979">
<title>Quantum transport senses community structure in networks. (arXiv:1711.04979v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04979</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum time evolution exhibits rich physics, attributable to the interplay
between the density and phase of a wave function. However, unlike classical
heat diffusion, the wave nature of quantum mechanics has not yet been
extensively explored in modern data analysis. We propose that the Laplace
transform of quantum transport (QT) can be used to construct an ensemble of
maps from a given complex network to a circle $S^1$, such that closely-related
nodes on the network are grouped into sharply concentrated clusters on $S^1$.
The resulting QT clustering (QTC) algorithm is as powerful as the
state-of-the-art spectral clustering in discerning complex geometric patterns
and more robust when clusters show strong density variations or heterogeneity
in size. The observed phenomenon of QTC can be interpreted as a collective
behavior of the microscopic nodes that evolve as macroscopic cluster orbitals
in an effective tight-binding model recapitulating the network. Python source
code implementing the algorithm and examples are available at
https://github.com/jssong-lab/QTC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenchao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jun S. Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01141">
<title>Stochastic Maximum Likelihood Optimization via Hypernetworks. (arXiv:1712.01141v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01141</link>
<description rdf:parseType="Literal">&lt;p&gt;This work explores maximum likelihood optimization of neural networks through
hypernetworks. A hypernetwork initializes the weights of another network, which
in turn can be employed for typical functional tasks such as regression and
classification. We optimize hypernetworks to directly maximize the conditional
likelihood of target variables given input. Using this approach we obtain
competitive empirical results on regression and classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheikh_A/0/1/0/all/0/1&quot;&gt;Abdul-Saboor Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rasul_K/0/1/0/all/0/1&quot;&gt;Kashif Rasul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Merentitis_A/0/1/0/all/0/1&quot;&gt;Andreas Merentitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bergmann_U/0/1/0/all/0/1&quot;&gt;Urs Bergmann&lt;/a&gt;</dc:creator>
</item></rdf:RDF>