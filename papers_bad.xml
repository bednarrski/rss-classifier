<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04837"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04861"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04905"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04855"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04863"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04932"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04936"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1102.5019"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.06757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08591"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03873"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.05096">
<title>Global optimization test problems based on random field composition. (arXiv:1807.05096v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.05096</link>
<description rdf:parseType="Literal">&lt;p&gt;The development and identification of effective optimization algorithms for
non-convex real-world problems is a challenge in global optimization. Because
theoretical performance analysis is difficult, and problems based on models of
real-world systems are often computationally expensive, several artificial
performance test problems and test function generators have been proposed for
empirical comparative assessment and analysis of metaheuristic optimization
algorithms. These test problems however often lack the complex function
structures and forthcoming difficulties that can appear in real-world problems.
This communication presents a method to systematically build test problems with
various types and degrees of difficulty. By weighted composition of
parameterized random fields, challenging test functions with tunable function
features such as, variance contribution distribution, interaction order, and
nonlinearity can be constructed. The method is described, and its applicability
to optimization performance analysis is described by means of a few basic
examples. The method aims to set a step forward in the systematic generation of
global optimization test problems, which could lead to a better understanding
of the performance of optimization algorithms on problem types with particular
characteristics. On request an introductive MATLAB implementation of a test
function generator based on the presented method is available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sala_R/0/1/0/all/0/1&quot;&gt;Ramses Sala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Baldanzini_N/0/1/0/all/0/1&quot;&gt;Niccol&amp;#xf2; Baldanzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pierini_M/0/1/0/all/0/1&quot;&gt;Marco Pierini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02397">
<title>Quality Diversity Through Surprise. (arXiv:1807.02397v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02397</link>
<description rdf:parseType="Literal">&lt;p&gt;Quality diversity is a recent evolutionary computation paradigm which
maintains an appropriate balance between divergence and convergence and has
achieved promising results in complex problems. There is, however, limited
exploration on how different paradigms of divergent search may impact the
solutions found by quality diversity algorithms. Inspired by the notion of
surprise as an effective driver of divergent search and its orthogonal nature
to novelty this paper investigates the impact of the former to quality
diversity performance. For that purpose we introduce three new quality
diversity algorithms which use surprise as a diversity measure, either on its
own or combined with novelty, and compare their performance against novelty
search with local competition, the state of the art quality diversity
algorithm. The algorithms are tested in a robot maze navigation task, in a
challenging set of 60 deceptive mazes. Our findings suggest that allowing
surprise and novelty to operate synergistically for divergence and in
combination with local competition leads to quality diversity algorithms of
significantly higher efficiency, speed and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gravina_D/0/1/0/all/0/1&quot;&gt;Daniele Gravina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1&quot;&gt;Antonios Liapis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1&quot;&gt;Georgios N. Yannakakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04837">
<title>A game-theoretic approach to timeline-based planning with uncertainty. (arXiv:1807.04837v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;In timeline-based planning, domains are described as sets of independent, but
interacting, components, whose behaviour over time (the set of timelines) is
governed by a set of temporal constraints. A distinguishing feature of
timeline-based planning systems is the ability to integrate planning with
execution by synthesising control strategies for flexible plans. However,
flexible plans can only represent temporal uncertainty, while more complex
forms of nondeterminism are needed to deal with a wider range of realistic
problems. In this paper, we propose a novel game-theoretic approach to
timeline-based planning problems, generalising the state of the art while
uniformly handling temporal uncertainty and nondeterminism. We define a general
concept of timeline-based game and we show that the notion of winning strategy
for these games is strictly more general than that of control strategy for
dynamically controllable flexible plans. Moreover, we show that the problem of
establishing the existence of such winning strategies is decidable using a
doubly exponential amount of space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gigante_N/0/1/0/all/0/1&quot;&gt;Nicola Gigante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Angelo Montanari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayer_M/0/1/0/all/0/1&quot;&gt;Marta Cialdea Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orlandini_A/0/1/0/all/0/1&quot;&gt;Andrea Orlandini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1&quot;&gt;Mark Reynolds&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04861">
<title>Hybrid Temporal Situation Calculus. (arXiv:1807.04861v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.04861</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to model continuous change in Reiter&apos;s temporal situation
calculus action theories has attracted a lot of interest. In this paper, we
propose a new development of his approach, which is directly inspired by hybrid
systems in control theory. Specifically, while keeping the foundations of
Reiter&apos;s axiomatization, we propose an elegant extension of his approach by
adding a time argument to all fluents that represent continuous change.
Thereby, we insure that change can happen not only because of actions, but also
due to the passage of time. We present a systematic methodology to derive, from
simple premises, a new group of axioms which specify how continuous fluents
change over time within a situation. We study regression for our new temporal
basic action theories and demonstrate what reasoning problems can be solved.
Finally, we formally show that our temporal basic action theories indeed
capture hybrid automata.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batusov_V/0/1/0/all/0/1&quot;&gt;Vitaliy Batusov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1&quot;&gt;Giuseppe De Giacomo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soutchanski_M/0/1/0/all/0/1&quot;&gt;Mikhail Soutchanski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04905">
<title>Ultra-Fine Entity Typing. (arXiv:1807.04905v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.04905</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new entity typing task: given a sentence with an entity
mention, the goal is to predict a set of free-form phrases (e.g. skyscraper,
songwriter, or criminal) that describe appropriate types for the target entity.
This formulation allows us to use a new type of distant supervision at large
scale: head words, which indicate the type of the noun phrases they appear in.
We show that these ultra-fine types can be crowd-sourced, and introduce new
evaluation sets that are much more diverse and fine-grained than existing
benchmarks. We present a model that can predict open types, and is trained
using a multitask objective that pools our new head-word supervision with prior
supervision from entity linking. Experimental results demonstrate that our
model is effective in predicting entity types at varying granularity; it
achieves state of the art performance on an existing fine-grained entity typing
benchmark, and sets baselines for our newly-introduced datasets. Our data and
model can be downloaded from: &lt;a href=&quot;http://nlp.cs.washington.edu/entity_type&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Eunsol Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1&quot;&gt;Omer Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04920">
<title>On the Complexity of Iterative Tropical Computation with Applications to Markov Decision Processes. (arXiv:1807.04920v1 [cs.FL])</title>
<link>http://arxiv.org/abs/1807.04920</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the complexity of evaluating powered functions implemented by
straight-line programs (SLPs) over the tropical semiring (i.e., with max and +
operations). In this problem, a given (max,+)-SLP with the same number of input
and output wires is composed with H copies of itself, where H is given in
binary. The problem of evaluating powered SLPs is intimately connected with
iterative arithmetic computations that arise in algorithmic decision making and
operations research. Specifically, it is essentially equivalent to finding
optimal strategies in finite-horizon Markov Decision Processes (MDPs). We show
that evaluating powered SLPs and finding optimal strategies in finite-horizon
MDPs are both EXPTIME-complete problems. This resolves an open problem that
goes back to the seminal 1987 paper on the complexity of MDPs by Papadimitriou
and Tsitsiklis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaji_N/0/1/0/all/0/1&quot;&gt;Nikhil Balaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiefer_S/0/1/0/all/0/1&quot;&gt;Stefan Kiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novotny_P/0/1/0/all/0/1&quot;&gt;Petr Novotn&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirmohammadi_M/0/1/0/all/0/1&quot;&gt;Mahsa Shirmohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05196">
<title>Artificial Intelligence for Long-Term Robot Autonomy: A Survey. (arXiv:1807.05196v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.05196</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous systems will play an essential role in many applications across
diverse domains including space, marine, air, field, road, and service
robotics. They will assist us in our daily routines and perform dangerous,
dirty and dull tasks. However, enabling robotic systems to perform autonomously
in complex, real-world scenarios over extended time periods (i.e. weeks,
months, or years) poses many challenges. Some of these have been investigated
by sub-disciplines of Artificial Intelligence (AI) including navigation &amp;amp;
mapping, perception, knowledge representation &amp;amp; reasoning, planning,
interaction, and learning. The different sub-disciplines have developed
techniques that, when re-integrated within an autonomous system, can enable
robots to operate effectively in complex, long-term scenarios. In this paper,
we survey and discuss AI techniques as &apos;enablers&apos; for long-term robot autonomy,
current progress in integrating these techniques within long-running robotic
systems, and the future challenges and opportunities for AI in long-term
autonomy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunze_L/0/1/0/all/0/1&quot;&gt;Lars Kunze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawes_N/0/1/0/all/0/1&quot;&gt;Nick Hawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duckett_T/0/1/0/all/0/1&quot;&gt;Tom Duckett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanheide_M/0/1/0/all/0/1&quot;&gt;Marc Hanheide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krajnik_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Krajn&amp;#xed;k&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08834">
<title>Measuring and Computing Database Inconsistency via Repairs. (arXiv:1804.08834v3 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08834</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generic numerical measure of inconsistency of a database with
respect to a set of integrity constraints. It is based on an abstract repair
semantics. A particular inconsistency measure associated to cardinality-repairs
is investigated; and we show that it can be computed via answer-set programs.
&lt;/p&gt;
&lt;p&gt;Keywords: Integrity constraints in databases, inconsistent databases,
database repairs, inconsistency measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04800">
<title>Feature Selection for Gender Classification in TUIK Life Satisfaction Survey. (arXiv:1807.04800v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04800</link>
<description rdf:parseType="Literal">&lt;p&gt;As known, attribute selection is a method that is used before the
classification of data mining. In this study, a new data set has been created
by using attributes expressing overall satisfaction in Turkey Statistical
Institute (TSI) Life Satisfaction Survey dataset. Attributes are sorted by
Ranking search method using attribute selection algorithms in a data mining
application. These selected attributes were subjected to a classification test
with Naive Bayes and Random Forest from machine learning algorithms. The
feature selection algorithms are compared according to the number of attributes
selected and the classification accuracy rates achievable with them. In this
study, which is aimed at reducing the dataset volume, the best classification
result comes up with 3 attributes selected by the Chi2 algorithm. The best
classification rate was 73% with the Random Forest classification algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coban_A/0/1/0/all/0/1&quot;&gt;Adil &amp;#xc7;oban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarimer_I/0/1/0/all/0/1&quot;&gt;Ilhan Tar&amp;#x131;mer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04833">
<title>DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures. (arXiv:1807.04833v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04833</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a non-parametric Bayesian latent variable model capable of
learning dependency structures across dimensions in a multivariate setting. Our
approach is based on flexible Gaussian process priors for the generative
mappings and interchangeable Dirichlet process priors to learn the structure.
The introduction of the Dirichlet process as a specific structural prior allows
our model to circumvent issues associated with previous Gaussian process latent
variable models. Inference is performed by deriving an efficient variational
bound on the marginal log-likelihood on the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lawrence_A/0/1/0/all/0/1&quot;&gt;Andrew R. Lawrence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ek_C/0/1/0/all/0/1&quot;&gt;Carl Henrik Ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1&quot;&gt;Neill D. F. Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04855">
<title>A feature agnostic approach for glaucoma detection in OCT volumes. (arXiv:1807.04855v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.04855</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical coherence tomography (OCT) based measurements of retinal layer
thickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell
with inner plexiform layer (GCIPL) are commonly used for the diagnosis and
monitoring of glaucoma. Previously, machine learning techniques have utilized
segmentation-based imaging features such as the peripapillary RNFL thickness
and the cup-to-disc ratio. Here, we propose a deep learning technique that
classifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT
volumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network
(CNN). We compared the accuracy of this technique with various feature-based
machine learning algorithms and demonstrated the superiority of the proposed
deep learning based method.
&lt;/p&gt;
&lt;p&gt;Logistic regression was found to be the best performing classical machine
learning technique with an AUC of 0.89. In direct comparison, the deep learning
approach achieved a substantially higher AUC of 0.94 with the additional
advantage of providing insight into which regions of an OCT volume are
important for glaucoma detection.
&lt;/p&gt;
&lt;p&gt;Computing Class Activation Maps (CAM), we found that the CNN identified
neuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and
its surrounding areas as the regions significantly associated with the glaucoma
classification. These regions anatomically correspond to the well established
and commonly used clinical markers for glaucoma diagnosis such as increased cup
volume, cup diameter, and neuroretinal rim thinning at the superior and
inferior segments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maetschke_S/0/1/0/all/0/1&quot;&gt;Stefan Maetschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antony_B/0/1/0/all/0/1&quot;&gt;Bhavna Antony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_H/0/1/0/all/0/1&quot;&gt;Hiroshi Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garvani_R/0/1/0/all/0/1&quot;&gt;Rahil Garvani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04863">
<title>Avoiding Latent Variable Collapse With Generative Skip Models. (arXiv:1807.04863v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04863</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational autoencoders (VAEs) learn distributions of high-dimensional data.
They model data by introducing a deep latent-variable model and then maximizing
a lower bound of the log marginal likelihood. While VAEs can capture complex
distributions, they also suffer from an issue known as &quot;latent variable
collapse.&quot; Specifically, the lower bound involves an approximate posterior of
the latent variables; this posterior &quot;collapses&quot; when it is set equal to the
prior, i.e., when the posterior is independent of the data. While VAEs learn
good generative models, latent variable collapse prevents them from learning
useful representations. In this paper, we propose a new way to avoid latent
variable collapse. We expand the model class to one that includes skip
connections; these connections enforce strong links between the latent
variables and the likelihood function. We study these generative skip models
both theoretically and empirically. Theoretically, we prove that skip models
increase the mutual information between the observations and the inferred
latent variables. Empirically, on both images (MNIST and Omniglot) and text
(Yahoo), we show that generative skip models lead to less collapse than
existing VAE architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dieng_A/0/1/0/all/0/1&quot;&gt;Adji B. Dieng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04932">
<title>Sequential sampling of Gaussian latent variable models. (arXiv:1807.04932v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04932</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of inferring a latent function in a probabilistic
model of data. When dependencies of the latent function are specified by a
Gaussian process and the data likelihood is complex, efficient computation
often involve Markov chain Monte Carlo sampling with limited applicability to
large data sets. We extend some of these techniques to scale efficiently when
the problem exhibits a sequential structure. We propose an approximation that
enables sequential sampling of both latent variables and associated parameters.
We demonstrate strong performance in growing-data settings that would otherwise
be unfeasible with naive, non-sequential sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tegner_M/0/1/0/all/0/1&quot;&gt;Martin Tegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bloem_Reddy_B/0/1/0/all/0/1&quot;&gt;Benjamin Bloem-Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04936">
<title>Non-Gaussian Component Analysis using Entropy Methods. (arXiv:1807.04936v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04936</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-Gaussian component analysis (NGCA) is a problem in multidimensional data
analysis. Since its formulation in 2006, NGCA has attracted considerable
attention in statistics and machine learning. In this problem, we have a random
variable $X$ in $n$-dimensional Euclidean space. There is an unknown subspace
$U$ of the $n$-dimensional Euclidean space such that the orthogonal projection
of $X$ onto $U$ is standard multidimensional Gaussian and the orthogonal
projection of $X$ onto $V$, the orthogonal complement of $U$, is non-Gaussian,
in the sense that all its one-dimensional marginals are different from the
Gaussian in a certain metric defined in terms of moments. The NGCA problem is
to approximate the non-Gaussian subspace $V$ given samples of $X$.
&lt;/p&gt;
&lt;p&gt;Vectors in $V$ corresponds to &quot;interesting&quot; directions, whereas vectors in
$U$ correspond to the directions where data is very noisy. The most interesting
applications of the NGCA model is for the case when the magnitude of the noise
is comparable to that of the true signal, a setting in which traditional noise
reduction techniques such as PCA don&apos;t apply directly. NGCA is also related to
dimensionality reduction and to other data analysis problems such as ICA.
NGCA-like problems have been studied in statistics for a long time using
techniques such as projection pursuit.
&lt;/p&gt;
&lt;p&gt;We give an algorithm that takes polynomial time in the dimension $n$ and has
an inverse polynomial dependence on the error parameter measuring the angle
distance between the non-Gaussian subspace and the subspace output by the
algorithm. Our algorithm is based on relative entropy as the contrast function
and fits under the projection pursuit framework. The techniques we develop for
analyzing our algorithm may be of use for other related problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1&quot;&gt;Navin Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1&quot;&gt;Abhishek Shetty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05077">
<title>Maximizing Invariant Data Perturbation with Stochastic Optimization. (arXiv:1807.05077v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.05077</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature attribution methods, or saliency maps, are one of the most popular
approaches for explaining the decisions of complex machine learning models such
as deep neural networks. In this study, we propose a stochastic optimization
approach for the perturbation-based feature attribution method. While the
original optimization problem of the perturbation-based feature attribution is
difficult to solve because of the complex constraints, we propose to
reformulate the problem as the maximization of a differentiable function, which
can be solved using gradient-based algorithms. In particular, stochastic
optimization is well-suited for the proposed reformulation, and we can solve
the problem using popular algorithms such as SGD, RMSProp, and Adam. The
experiment on the image classification with VGG16 shows that the proposed
method could identify relevant parts of the images effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ikeno_K/0/1/0/all/0/1&quot;&gt;Kouichi Ikeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hara_S/0/1/0/all/0/1&quot;&gt;Satoshi Hara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1102.5019">
<title>Unsupervised nonparametric detection of unknown objects in noisy images based on percolation theory. (arXiv:1102.5019v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1102.5019</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop an unsupervised, nonparametric, and scalable statistical learning
method for detection of unknown objects in noisy images. The method uses
results from percolation theory and random graph theory. We present an
algorithm that allows to detect objects of unknown shapes and sizes in the
presence of nonparametric noise of unknown level. The noise density is assumed
to be unknown and can be very irregular. The algorithm has linear complexity
and exponential accuracy and is appropriate for real-time systems. We prove
strong consistency and scalability of our method in this setup with minimal
assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Langovoy_M/0/1/0/all/0/1&quot;&gt;Mikhail A. Langovoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wittich_O/0/1/0/all/0/1&quot;&gt;Olaf Wittich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Davies_P/0/1/0/all/0/1&quot;&gt;Patrick Laurie Davies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.06757">
<title>A Nonlinear Dimensionality Reduction Framework Using Smooth Geodesics. (arXiv:1707.06757v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.06757</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing dimensionality reduction methods are adept at revealing hidden
underlying manifolds arising from high-dimensional data and thereby producing a
low-dimensional representation. However, the smoothness of the manifolds
produced by classic techniques over sparse and noisy data is not guaranteed. In
fact, the embedding generated using such data may distort the geometry of the
manifold and thereby produce an unfaithful embedding. Herein, we propose a
framework for nonlinear dimensionality reduction that generates a manifold in
terms of smooth geodesics that is designed to treat problems in which manifold
measurements are either sparse or corrupted by noise. Our method generates a
network structure for given high-dimensional data using a nearest neighbors
search and then produces piecewise linear shortest paths that are defined as
geodesics. Then, we fit points in each geodesic by a smoothing spline to
emphasize the smoothness. The robustness of this approach for sparse and noisy
datasets is demonstrated by the implementation of the method on synthetic and
real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gajamannage_K/0/1/0/all/0/1&quot;&gt;Kelum Gajamannage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paffenroth_R/0/1/0/all/0/1&quot;&gt;Randy Paffenroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bollt_E/0/1/0/all/0/1&quot;&gt;Erik M. Bollt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07077">
<title>Estimation Considerations in Contextual Bandits. (arXiv:1711.07077v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07077</link>
<description rdf:parseType="Literal">&lt;p&gt;Although many contextual bandit algorithms have similar theoretical
guarantees, the characteristics of real-world applications oftentimes result in
large performance dissimilarities across algorithms. We study a consideration
for the exploration vs. exploitation framework that does not arise in
non-contextual bandits: the way exploration is conducted in the present may
affect the bias and variance in the potential outcome model estimation in
subsequent stages of learning. We show that contextual bandit algorithms are
sensitive to the estimation method of the outcome model as well as the
exploration method used, particularly in the presence of rich heterogeneity or
complex outcome models, which can lead to difficult estimation problems along
the path of learning. We propose new contextual bandit designs, combining
parametric and non-parametric statistical estimation methods with causal
inference methods in order to reduce the estimation bias that results from
adaptive treatment assignment. We provide empirical evidence that guides the
choice among the alternatives in different scenarios, such as prejudice
(non-representative user contexts) in the initial training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dimakopoulou_M/0/1/0/all/0/1&quot;&gt;Maria Dimakopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imbens_G/0/1/0/all/0/1&quot;&gt;Guido Imbens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09834">
<title>A Flexible Procedure for Mixture Proportion Estimation in Positive--Unlabeled Learning. (arXiv:1801.09834v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09834</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive--unlabeled (PU) learning considers two samples, a positive set P
with observations from only one class and an unlabeled set U with observations
from two classes. The goal is to classify observations in U. Class mixture
proportion estimation (MPE) in U is a key step in PU learning. Blanchard et al.
[2010] showed that MPE in PU learning is a generalization of the problem of
estimating the proportion of true null hypotheses in multiple testing problems.
Motivated by this idea, we propose reducing the problem to one dimension via
construction of a probabilistic classifier trained on the P and U data sets
followed by application of a one--dimensional mixture proportion method from
the multiple testing literature to the observation class probabilities. The
flexibility of this framework lies in the freedom to choose the classifier and
the one--dimensional MPE method. We prove consistency of two mixture proportion
estimators using bounds from empirical process theory, develop tuning parameter
free implementations, and demonstrate that they have competitive performance on
simulated waveform data and a protein signaling problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;James P. Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08591">
<title>End-to-End Learning for the Deep Multivariate Probit Model. (arXiv:1803.08591v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08591</link>
<description rdf:parseType="Literal">&lt;p&gt;The multivariate probit model (MVP) is a popular classic model for studying
binary responses of multiple entities. Nevertheless, the computational
challenge of learning the MVP model, given that its likelihood involves
integrating over a multidimensional constrained space of latent variables,
significantly limits its application in practice. We propose a flexible deep
generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP),
which is an end-to-end learning scheme that uses an efficient parallel sampling
process of the multivariate probit model to exploit GPU-boosted deep neural
networks. We present both theoretical and empirical analysis of the convergence
behavior of DMVP&apos;s sampling process with respect to the resolution of the
correlation structure. We provide convergence guarantees for DMVP and our
empirical analysis demonstrates the advantages of DMVP&apos;s sampling compared with
standard MCMC-based methods. We also show that when applied to multi-entity
modelling problems, which are natural DMVP applications, DMVP trains faster
than classical MVP, by at least an order of magnitude, captures rich
correlations among entities, and further improves the joint likelihood of
entities compared with several competitive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Di Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yexiang Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1&quot;&gt;Carla P. Gomes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03432">
<title>Hierarchical Clustering with Prior Knowledge. (arXiv:1806.03432v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03432</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical clustering is a class of algorithms that seeks to build a
hierarchy of clusters. It has been the dominant approach to constructing
embedded classification schemes since it outputs dendrograms, which capture the
hierarchical relationship among members at all levels of granularity,
simultaneously. Being greedy in the algorithmic sense, a hierarchical
clustering partitions data at every step solely based on a similarity /
dissimilarity measure. The clustering results oftentimes depend on not only the
distribution of the underlying data, but also the choice of dissimilarity
measure and the clustering algorithm. In this paper, we propose a method to
incorporate prior domain knowledge about entity relationship into the
hierarchical clustering. Specifically, we use a distance function in
ultrametric space to encode the external ontological information. We show that
popular linkage-based algorithms can faithfully recover the encoded structure.
Similar to some regularized machine learning techniques, we add this distance
as a penalty term to the original pairwise distance to regulate the final
structure of the dendrogram. As a case study, we applied this method on real
data in the building of a customer behavior based product taxonomy for an
Amazon service, leveraging the information from a larger Amazon-wide browse
structure. The method is useful when one wants to leverage the relational
information from external sources, or the data used to generate the distance
matrix is noisy and sparse. Our work falls in the category of semi-supervised
or constrained clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaofei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhavala_S/0/1/0/all/0/1&quot;&gt;Satya Dhavala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00431">
<title>Confounding variables can degrade generalization performance of radiological deep learning models. (arXiv:1807.00431v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00431</link>
<description rdf:parseType="Literal">&lt;p&gt;Early results in using convolutional neural networks (CNNs) on x-rays to
diagnose disease have been promising, but it has not yet been shown that models
trained on x-rays from one hospital or one group of hospitals will work equally
well at different hospitals. Before these tools are used for computer-aided
diagnosis in real-world clinical settings, we must verify their ability to
generalize across a variety of hospital systems. A cross-sectional design was
used to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays
from NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904
patients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural
comparisons, performance on chest x-rays from outside hospitals was
significantly lower than on held-out x-rays from the original hospital systems.
CNNs were able to detect where an x-ray was acquired (hospital system, hospital
department) with extremely high accuracy and calibrate predictions accordingly.
The performance of CNNs in diagnosing diseases on x-rays may reflect not only
their ability to identify disease-specific imaging findings on x-rays, but also
their ability to exploit confounding information. Estimates of CNN performance
based on test data from hospital systems used for model training may overstate
their likely real-world performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zech_J/0/1/0/all/0/1&quot;&gt;John R. Zech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badgeley_M/0/1/0/all/0/1&quot;&gt;Marcus A. Badgeley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Manway Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1&quot;&gt;Anthony B. Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titano_J/0/1/0/all/0/1&quot;&gt;Joseph J. Titano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1&quot;&gt;Eric K. Oermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00891">
<title>Optimality and Sub-optimality of PCA I: Spiked Random Matrix Models. (arXiv:1807.00891v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00891</link>
<description rdf:parseType="Literal">&lt;p&gt;A central problem of random matrix theory is to understand the eigenvalues of
spiked random matrix models, introduced by Johnstone, in which a prominent
eigenvector (or &quot;spike&quot;) is planted into a random matrix. These distributions
form natural statistical models for principal component analysis (PCA) problems
throughout the sciences. Baik, Ben Arous and Peche showed that the spiked
Wishart ensemble exhibits a sharp phase transition asymptotically: when the
spike strength is above a critical threshold, it is possible to detect the
presence of a spike based on the top eigenvalue, and below the threshold the
top eigenvalue provides no information. Such results form the basis of our
understanding of when PCA can detect a low-rank signal in the presence of
noise. However, under structural assumptions on the spike, not all information
is necessarily contained in the spectrum. We study the statistical limits of
tests for the presence of a spike, including non-spectral tests. Our results
leverage Le Cam&apos;s notion of contiguity, and include:
&lt;/p&gt;
&lt;p&gt;i) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal
detection threshold for certain natural priors for the spike.
&lt;/p&gt;
&lt;p&gt;ii) For any non-Gaussian Wigner ensemble, PCA is sub-optimal for detection.
However, an efficient variant of PCA achieves the optimal threshold (for
natural priors) by pre-transforming the matrix entries.
&lt;/p&gt;
&lt;p&gt;iii) For the Gaussian Wishart ensemble, the PCA threshold is optimal for
positive spikes (for natural priors) but this is not always the case for
negative spikes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Perry_A/0/1/0/all/0/1&quot;&gt;Amelia Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wein_A/0/1/0/all/0/1&quot;&gt;Alexander S. Wein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bandeira_A/0/1/0/all/0/1&quot;&gt;Afonso S. Bandeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moitra_A/0/1/0/all/0/1&quot;&gt;Ankur Moitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02537">
<title>Fully Scalable Gaussian Processes using Subspace Inducing Inputs. (arXiv:1807.02537v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02537</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce fully scalable Gaussian processes, an implementation scheme that
tackles the problem of treating a high number of training instances together
with high dimensional input data. Our key idea is a representation trick over
the inducing variables called subspace inducing inputs. This is combined with
certain matrix-preconditioning based parametrizations of the variational
distributions that lead to simplified and numerically stable variational lower
bounds. Our illustrative applications are based on challenging extreme
multi-label classification problems with the extra burden of the very large
number of class labels. We demonstrate the usefulness of our approach by
presenting predictive performances together with low computational times in
datasets with extremely large number of instances and input dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panos_A/0/1/0/all/0/1&quot;&gt;Aristeidis Panos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dellaportas_P/0/1/0/all/0/1&quot;&gt;Petros Dellaportas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Titsias_M/0/1/0/all/0/1&quot;&gt;Michalis K. Titsias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03873">
<title>Automatic Gradient Boosting. (arXiv:1807.03873v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03873</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic machine learning performs predictive modeling with high performing
machine learning tools without human interference. This is achieved by making
machine learning applications parameter-free, i.e. only a dataset is provided
while the complete model selection and model building process is handled
internally through (often meta) optimization. Projects like Auto-WEKA and
auto-sklearn aim to solve the Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem resulting in huge configuration spaces. However,
for most real-world applications, the optimization over only a few different
key learning algorithms can not only be sufficient, but also potentially
beneficial. The latter becomes apparent when one considers that models have to
be validated, explained, deployed and maintained. Here, less complex model are
often preferred, for validation or efficiency reasons, or even a strict
requirement. Automatic gradient boosting simplifies this idea one step further,
using only gradient boosting as a single learning algorithm in combination with
model-based hyperparameter tuning, threshold optimization and encoding of
categorical features. We introduce this general framework as well as a concrete
implementation called autoxgboost. It is compared to current AutoML projects on
16 datasets and despite its simplicity is able to achieve comparable results on
about half of the datasets as well as performing best on two.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thomas_J/0/1/0/all/0/1&quot;&gt;Janek Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coors_S/0/1/0/all/0/1&quot;&gt;Stefan Coors&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;</dc:creator>
</item></rdf:RDF>