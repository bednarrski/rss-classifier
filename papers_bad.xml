<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06731"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01782"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01883"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06613"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06666"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06685"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06777"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06874"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06919"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07224"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06650"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06656"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06693"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06713"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06722"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06766"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06962"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.05011"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.00199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.08213"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09602"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05411"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05666"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.06731">
<title>The MOEADr Package - A Component-Based Framework for Multiobjective Evolutionary Algorithms Based on Decomposition. (arXiv:1807.06731v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.06731</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiobjective Evolutionary Algorithms based on Decomposition (MOEA/D)
represent a widely used class of population-based metaheuristics for the
solution of multicriteria optimization problems. We introduce the MOEADr
package, which offers many of these variants as instantiations of a
component-oriented framework. This approach contributes for easier
reproducibility of existing MOEA/D variants from the literature, as well as for
faster development and testing of new composite algorithms. The package offers
an standardized, modular implementation of MOEA/D based on this framework,
which was designed aiming at providing researchers and practitioners with a
standard way to discuss and express MOEA/D variants. In this paper we introduce
the design principles behind the MOEADr package, as well as its current
components. Three case studies are provided to illustrate the main aspects of
the package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campelo_F/0/1/0/all/0/1&quot;&gt;Felipe Campelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batista_L/0/1/0/all/0/1&quot;&gt;Lucas S. Batista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aranha_C/0/1/0/all/0/1&quot;&gt;Claus Aranha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01782">
<title>Adaptive System Identification Using LMS Algorithm Integrated with Evolutionary Computation. (arXiv:1806.01782v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01782</link>
<description rdf:parseType="Literal">&lt;p&gt;System identification is an exceptionally expansive topic and of remarkable
significance in the discipline of signal processing and communication. Our goal
in this paper is to show how simple adaptive FIR and IIR filters can be used in
system modeling and demonstrating the application of adaptive system
identification. The main objective of our research is to study the LMS
algorithm and its improvement by the genetic search approach, namely, LMS-GA,
to search the multi-modal error surface of the IIR filter to avoid local minima
and finding the optimal weight vector when only measured or estimated data are
available. Convergence analysis of the LMS algorithm in the case of coloured
input signal, i.e., correlated input signal is demonstrated on adaptive FIR
filter via power spectral density of the input signals and Fourier transform of
the autocorrelation matrix of the input signal. Simulations have been carried
out on adaptive filtering of FIR and IIR filters and tested on white and
coloured input signals to validate the powerfulness of the genetic-based LMS
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ibraheem_I/0/1/0/all/0/1&quot;&gt;Ibraheem Kasim Ibraheem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01883">
<title>Routes to Open-Endedness in Evolutionary Systems. (arXiv:1806.01883v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01883</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a high-level conceptual framework to help orient the
discussion and implementation of open-endedness in evolutionary systems.
Drawing upon work by Banzhaf et al., three different kinds of open-endedness
are identified: exploratory, expansive, and transformational. These are
characterised in terms of their relationship to the search space of phenotypic
behaviours. A formalism is introduced to describe three key processes required
for an evolutionary process: the generation of a phenotype from a genetic
description, the evaluation of that phenotype, and the reproduction with
variation of individuals according to their evaluation. The formalism makes
explicit various influences in each of these processes that can easily be
overlooked. The distinction is made between intrinsic and extrinsic
implementations of these processes. A discussion then investigates how various
interactions between these processes, and their modes of implementation, can
lead to open-endedness. However, it is demonstrated that these considerations
relate to exploratory open-endedness only. Conditions for the implementation of
the more interesting kinds of open-endedness - expansive and transformational -
are also discussed, emphasizing factors such as multiple domains of behaviour,
transdomain bridges, and non-additive compositional systems. In contrast to a
traditional population genetics analysis, these factors relate not to the
generic evolutionary properties of individuals and populations, but rather to
the nature of the building blocks out of which individual organisms are
constructed, and the laws and properties of the environment in which they
exist. The paper ends with suggestions of how the framework can be used to
categorise and compare the open-ended evolutionary potential of different
systems, and how it might guide the design of systems with greater capacity for
open-ended evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_T/0/1/0/all/0/1&quot;&gt;Tim Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06613">
<title>Deep Reinforcement Learning for Swarm Systems. (arXiv:1807.06613v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1807.06613</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep reinforcement learning (RL) methods have been applied
successfully to multi-agent scenarios. Typically, these methods rely on a
concatenation of agent states to represent the information content required for
decentralized decision making. However, concatenation scales poorly to swarm
systems with a large number of homogeneous agents as it does not exploit the
fundamental properties inherent to these systems: (i) the agents in the swarm
are interchangeable and (ii) the exact number of agents in the swarm is
irrelevant. Therefore, we propose a new state representation for deep
multi-agent RL based on mean embeddings of distributions. We treat the agents
as samples of a distribution and use the empirical mean embedding as input for
a decentralized policy. We define different feature spaces of the mean
embedding using histograms, radial basis functions and a neural network learned
end-to-end. We evaluate the representation on two well known problems from the
swarm literature (rendezvous and pursuit evasion), in a globally and locally
observable setup. For the local setup we furthermore introduce simple
communication protocols. Of all approaches, the mean embedding representation
using neural network features enables the richest information exchange between
neighboring agents facilitating the development of more complex collective
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huttenrauch_M/0/1/0/all/0/1&quot;&gt;Maximilian H&amp;#xfc;ttenrauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sosic_A/0/1/0/all/0/1&quot;&gt;Adrian &amp;#x160;o&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06666">
<title>Payoff Control in the Iterated Prisoner&apos;s Dilemma. (arXiv:1807.06666v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1807.06666</link>
<description rdf:parseType="Literal">&lt;p&gt;Repeated game has long been the touchstone model for agents&apos; long-run
relationships. Previous results suggest that it is particularly difficult for a
repeated game player to exert an autocratic control on the payoffs since they
are jointly determined by all participants. This work discovers that the scale
of a player&apos;s capability to unilaterally influence the payoffs may have been
much underestimated. Under the conventional iterated prisoner&apos;s dilemma, we
develop a general framework for controlling the feasible region where the
players&apos; payoff pairs lie. A control strategy player is able to confine the
payoff pairs in her objective region, as long as this region has feasible
linear boundaries. With this framework, many well-known existing strategies can
be categorized and various new strategies with nice properties can be further
identified. We show that the control strategies perform well either in a
tournament or against a human-like opponent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_D/0/1/0/all/0/1&quot;&gt;Dong Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tao Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06685">
<title>Modular Semantics and Characteristics for Bipolar Weighted Argumentation Graphs. (arXiv:1807.06685v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06685</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the semantics of weighted argumentation graphs that are
bipolar, i.e. contain both attacks and supports for arguments. We build on
previous work by Amgoud, Ben-Naim et. al. We study the various characteristics
of acceptability semantics that have been introduced in these works. We provide
a simplified and mathematically elegant formulation of these characteristics.
The formulation is modular because it cleanly separates aggregation of
attacking and supporting arguments (for a given argument a) from the
computation of their influence on a&apos;s initial weight. We discuss various
semantics for bipolar argumentation graphs in the light of these
characteristics. Based on the modular framework, we prove general convergence
and divergence theorems. We show that all semantics converge for all acyclic
graphs and that no sum-based semantics can converge for all graphs. In
particular, we show divergence of Euler-based semantics for certain cyclic
graphs. We also provide the first semantics for bipolar weighted graphs that
converges for all graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mossakowski_T/0/1/0/all/0/1&quot;&gt;Till Mossakowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuhaus_F/0/1/0/all/0/1&quot;&gt;Fabian Neuhaus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06756">
<title>SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities. (arXiv:1807.06756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06756</link>
<description rdf:parseType="Literal">&lt;p&gt;The detection of software vulnerabilities (or vulnerabilities for short) is
an important problem that has yet to be tackled, as manifested by many
vulnerabilities reported on a daily basis. This calls for machine learning
methods to automate vulnerability detection. Deep learning is attractive for
this purpose because it does not require human experts to manually define
features. Despite the tremendous success of deep learning in other domains, its
applicability to vulnerability detection is not systematically understood. In
order to fill this void, we propose the first systematic framework for using
deep learning to detect vulnerabilities. The framework, dubbed Syntax-based,
Semantics-based, and Vector Representations (SySeVR), focuses on obtaining
program representations that can accommodate syntax and semantic information
pertinent to vulnerabilities. Our experiments with 4 software products
demonstrate the usefulness of the framework: we detect 15 vulnerabilities that
are not reported in the National Vulnerability Database. Among these 15
vulnerabilities, 7 are unknown and have been reported to the vendors, and the
other 8 have been &quot;silently&quot; patched by the vendors when releasing newer
versions of the products.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1&quot;&gt;Deqing Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shouhuai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yawei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoxuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sujuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06757">
<title>On Evaluation of Embodied Navigation Agents. (arXiv:1807.06757v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06757</link>
<description rdf:parseType="Literal">&lt;p&gt;Skillful mobile operation in three-dimensional environments is a primary
topic of study in Artificial Intelligence. The past two years have seen a surge
of creative work on navigation. This creative output has produced a plethora of
sometimes incompatible task definitions and evaluation protocols. To coordinate
ongoing and future research in this area, we have convened a working group to
study empirical methodology in navigation research. The present document
summarizes the consensus recommendations of this working group. We discuss
different problem statements and the role of generalization, present evaluation
measures, and provide standard scenarios that can be used for benchmarking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1&quot;&gt;Peter Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Angel Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Chaplot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1&quot;&gt;Alexey Dosovitskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Saurabh Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1&quot;&gt;Vladlen Koltun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1&quot;&gt;Jana Kosecka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1&quot;&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1&quot;&gt;Manolis Savva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1&quot;&gt;Amir R. Zamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06777">
<title>Planning and Synthesis Under Assumptions. (arXiv:1807.06777v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1807.06777</link>
<description rdf:parseType="Literal">&lt;p&gt;In Reasoning about Action and Planning, one synthesizes the agent plan by
taking advantage of the assumption on how the environment works (that is, one
exploits the environment&apos;s effects, its fairness, its trajectory constraints).
In this paper we study this form of synthesis in detail. We consider
assumptions as constraints on the possible strategies that the environment can
have in order to respond to the agent&apos;s actions. Such constraints may be given
in the form of a planning domain (or action theory), as linear-time formulas
over infinite or finite runs, or as a combination of the two (e.g., FOND under
fairness). We argue though that not all assumption specifications are
meaningful: they need to be consistent, which means that there must exist an
environment strategy fulfilling the assumption in spite of the agent actions.
For such assumptions, we study how to do synthesis/planning for agent goals,
ranging from a classical reachability to goal on traces specified in LTL and
LTLf/LDLf, characterizing the problem both mathematically and algorithmically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aminof_B/0/1/0/all/0/1&quot;&gt;Benjamin Aminof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1&quot;&gt;Giuseppe De Giacomo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murano_A/0/1/0/all/0/1&quot;&gt;Aniello Murano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubin_S/0/1/0/all/0/1&quot;&gt;Sasha Rubin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06813">
<title>Traditional Wisdom and Monte Carlo Tree Search Face-to-Face in the Card Game Scopone. (arXiv:1807.06813v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06813</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the design of a competitive artificial intelligence for Scopone, a
popular Italian card game. We compare rule-based players using the most
established strategies (one for beginners and two for advanced players) against
players using Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo
Tree Search (ISMCTS) with different reward functions and simulation strategies.
MCTS requires complete information about the game state and thus implements a
cheating player while ISMCTS can deal with incomplete information and thus
implements a fair player. Our results show that, as expected, the cheating MCTS
outperforms all the other strategies; ISMCTS is stronger than all the
rule-based players implementing well-known and most advanced strategies and it
also turns out to be a challenging opponent for human players.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palma_S/0/1/0/all/0/1&quot;&gt;Stefano Di Palma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzi_P/0/1/0/all/0/1&quot;&gt;Pier Luca Lanzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06822">
<title>Customer Sharing in Economic Networks with Costs. (arXiv:1807.06822v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1807.06822</link>
<description rdf:parseType="Literal">&lt;p&gt;In an economic market, sellers, infomediaries and customers constitute an
economic network. Each seller has her own customer group and the seller&apos;s
private customers are unobservable to other sellers. Therefore, a seller can
only sell commodities among her own customers unless other sellers or
infomediaries share her sale information to their customer groups. However, a
seller is not incentivized to share others&apos; sale information by default, which
leads to inefficient resource allocation and limited revenue for the sale. To
tackle this problem, we develop a novel mechanism called customer sharing
mechanism (CSM) which incentivizes all sellers to share each other&apos;s sale
information to their private customer groups. Furthermore, CSM also
incentivizes all customers to truthfully participate in the sale. In the end,
CSM not only allocates the commodities efficiently but also optimizes the
seller&apos;s revenue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_D/0/1/0/all/0/1&quot;&gt;Dong Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dengji Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tao Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06874">
<title>An Information-theoretic Framework for the Lossy Compression of Link Streams. (arXiv:1807.06874v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1807.06874</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph compression is a data analysis technique that consists in the
replacement of parts of a graph by more general structural patterns in order to
reduce its description length. It notably provides interesting exploration
tools for the study of real, large-scale, and complex graphs which cannot be
grasped at first glance. This article proposes a framework for the compression
of temporal graphs, that is for the compression of graphs that evolve with
time. This framework first builds on a simple and limited scheme, exploiting
structural equivalence for the lossless compression of static graphs, then
generalises it to the lossy compression of link streams, a recent formalism for
the study of temporal graphs. Such generalisation relies on the natural
extension of (bidimensional) relational data by the addition of a third
temporal dimension. Moreover, we introduce an information-theoretic measure to
quantify and to control the information that is lost during compression, as
well as an algebraic characterisation of the space of possible compression
patterns to enhance the expressiveness of the initial compression scheme. These
contributions lead to the definition of a combinatorial optimisation problem,
that is the Lossy Multistream Compression Problem, for which we provide an
exact algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamarche_Perrin_R/0/1/0/all/0/1&quot;&gt;Robin Lamarche-Perrin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06918">
<title>RARD II: The 2nd Related-Article Recommendation Dataset. (arXiv:1807.06918v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.06918</link>
<description rdf:parseType="Literal">&lt;p&gt;The main contribution of this paper is to introduce and describe a new
recommender-systems dataset (RARD II). It is based on data from a
recommender-system in the digital library and reference management software
domain. As such, it complements datasets from other domains such as books,
movies, and music. The RARD II dataset encompasses 89m recommendations,
covering an item-space of 24m unique items. RARD II provides a range of rich
recommendation data, beyond conventional ratings. For example, in addition to
the usual ratings matrices, RARD II includes the original recommendation logs,
which provide a unique insight into many aspects of the algorithms that
generated the recommendations. In this paper, we summarise the key features of
this dataset release, describing how it was generated and discussing some of
its unique features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1&quot;&gt;Joeran Beel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyth_B/0/1/0/all/0/1&quot;&gt;Barry Smyth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_A/0/1/0/all/0/1&quot;&gt;Andrew Collins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06919">
<title>Backplay: &quot;Man muss immer umkehren&quot;. (arXiv:1807.06919v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06919</link>
<description rdf:parseType="Literal">&lt;p&gt;A long-standing problem in model free reinforcement learning (RL) is that it
requires a large number of trials to learn a good policy, especially in
environments with sparse rewards. We explore a method to increase the sample
efficiency of RL when we have access to demonstrations. Our approach, which we
call Backplay, uses a single demonstration to construct a curriculum for a
given task. Rather than starting each training episode in the environment&apos;s
fixed initial state, we start the agent near the end of the demonstration and
move the starting point backwards during the course of training until we reach
the initial state. We perform experiments in a competitive four player game
(Pommerman) and a path-finding maze game. We find that this weak form of
guidance provides significant gains in sample complexity with a stark advantage
in sparse reward environments. In some cases, standard RL did not yield any
improvement while Backplay reached success rates greater than 50% and
generalized to unseen initial conditions in the same amount of training time.
Additionally, we see that agents trained via Backplay can learn policies
superior to those of the original demonstration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resnick_C/0/1/0/all/0/1&quot;&gt;Cinjon Resnick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1&quot;&gt;Roberta Raileanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1&quot;&gt;Sanyam Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peysakhovich_A/0/1/0/all/0/1&quot;&gt;Alex Peysakhovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06957">
<title>Discrete linear-complexity reinforcement learning in continuous action spaces for Q-learning algorithms. (arXiv:1807.06957v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06957</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we sketch an algorithm that extends the Q-learning
algorithms to the continuous action space domain. Our method is based on the
discretization of the action space. Despite the commonly used discretization
methods, our method does not increase the discretized problem dimensionality
exponentially. We will show that our proposed method is linear in complexity
when the discretization is employed. The variant of the Q-learning algorithm
presented in this work, labeled as Finite Step Q-Learning (FSQ), can be
deployed to both shallow and deep neural network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavallali_P/0/1/0/all/0/1&quot;&gt;Peyman Tavallali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doran_G/0/1/0/all/0/1&quot;&gt;Gary B. Doran Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandrake_L/0/1/0/all/0/1&quot;&gt;Lukas Mandrake&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06996">
<title>Evolving Large-Scale Data Stream Analytics based on Scalable PANFIS. (arXiv:1807.06996v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06996</link>
<description rdf:parseType="Literal">&lt;p&gt;Many distributed machine learning frameworks have recently been built to
speed up the large-scale data learning process. However, most distributed
machine learning used in these frameworks still uses an offline algorithm model
which cannot cope with the data stream problems. In fact, large-scale data are
mostly generated by the non-stationary data stream where its pattern evolves
over time. To address this problem, we propose a novel Evolving Large-scale
Data Stream Analytics framework based on a Scalable Parsimonious Network based
on Fuzzy Inference System (Scalable PANFIS), where the PANFIS evolving
algorithm is distributed over the worker nodes in the cloud to learn
large-scale data stream. Scalable PANFIS framework incorporates the active
learning (AL) strategy and two model fusion methods. The AL accelerates the
distributed learning process to generate an initial evolving large-scale data
stream model (initial model), whereas the two model fusion methods aggregate an
initial model to generate the final model. The final model represents the
update of current large-scale data knowledge which can be used to infer future
data. Extensive experiments on this framework are validated by measuring the
accuracy and running time of four combinations of Scalable PANFIS and other
Spark-based built in algorithms. The results indicate that Scalable PANFIS with
AL improves the training time to be almost two times faster than Scalable
PANFIS without AL. The results also show both rule merging and the voting
mechanisms yield similar accuracy in general among Scalable PANFIS algorithms
and they are generally better than Spark-based algorithms. In terms of running
time, the Scalable PANFIS training time outperforms all Spark-based algorithms
when classifying numerous benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zain_C/0/1/0/all/0/1&quot;&gt;Choiru Za&amp;#x27;in&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pardede_E/0/1/0/all/0/1&quot;&gt;Eric Pardede&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07049">
<title>Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias. (arXiv:1807.07049v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.07049</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven approaches to solving robotic tasks have gained a lot of traction
in recent years. However, most existing policies are trained on large-scale
datasets collected in curated lab settings. If we aim to deploy these models in
unstructured visual environments like people&apos;s homes, they will be unable to
cope with the mismatch in data distribution. In such light, we present the
first systematic effort in collecting a large dataset for robotic grasping in
homes. First, to scale and parallelize data collection, we built a low cost
mobile manipulator assembled for under 3K USD. Second, data collected using low
cost robots suffer from noisy labels due to imperfect execution and calibration
errors. To handle this, we develop a framework which factors out the noise as a
latent variable. Our model is trained on 28K grasps collected in several houses
under an array of different environmental conditions. We evaluate our models by
physically executing grasps on a collection of novel objects in multiple unseen
homes. The models trained with our home dataset showed a marked improvement of
43.7% over a baseline model trained with data collected in lab. Our
architecture which explicitly models the latent noise in the dataset also
performed 10% better than one that did not factor out the noise. We hope this
effort inspires the robotics community to look outside the lab and embrace
learning based approaches to handle inaccurate cheap robots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Adithyavairavan Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_D/0/1/0/all/0/1&quot;&gt;Dhiraj Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1&quot;&gt;Lerrel Pinto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05077">
<title>Transforming Cooling Optimization for Green Data Center via Deep Reinforcement Learning. (arXiv:1709.05077v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05077</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooling system plays a critical role in a modern data center (DC). Developing
an optimal control policy for DC cooling system is a challenging task. The
prevailing approaches often rely on approximating system models that are built
upon the knowledge of mechanical cooling, electrical and thermal management,
which is difficult to design and may lead to sub-optimal or unstable
performances. In this paper, we propose utilizing the large amount of
monitoring data in DC to optimize the control policy. To do so, we cast the
cooling control policy design into an energy cost minimization problem with
temperature constraints, and tap it into the emerging deep reinforcement
learning (DRL) framework. Specifically, we propose an end-to-end cooling
control algorithm (CCA) that is based on the actor-critic framework and an
off-policy offline version of the deep deterministic policy gradient (DDPG)
algorithm. In the proposed CCA, an evaluation network is trained to predict an
energy cost counter penalized by the cooling status of the DC room, and a
policy network is trained to predict optimized control settings when gave the
current load and weather information. The proposed algorithm is evaluated on
the EnergyPlus simulation platform and on a real data trace collected from the
National Super Computing Centre (NSCC) of Singapore. Our results show that the
proposed CCA can achieve about 11% cooling cost saving on the simulation
platform compared with a manually configured baseline control algorithm. In the
trace-based study, we propose a de-underestimation validation mechanism as we
cannot directly test the algorithm on a real DC. Even though with DUE the
results are conservative, we can still achieve about 15% cooling energy saving
on the NSCC data trace if we set the inlet temperature threshold at 26.6 degree
Celsius.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yonggang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_K/0/1/0/all/0/1&quot;&gt;Kyle Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07224">
<title>Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning. (arXiv:1709.07224v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07224</link>
<description rdf:parseType="Literal">&lt;p&gt;Swarm systems constitute a challenging problem for reinforcement learning
(RL) as the algorithm needs to learn decentralized control policies that can
cope with limited local sensing and communication abilities of the agents.
While it is often difficult to directly define the behavior of the agents,
simple communication protocols can be defined more easily using prior knowledge
about the given task. In this paper, we propose a number of simple
communication protocols that can be exploited by deep reinforcement learning to
find decentralized control policies in a multi-robot swarm environment. The
protocols are based on histograms that encode the local neighborhood relations
of the agents and can also transmit task-specific information, such as the
shortest distance and direction to a desired target. In our framework, we use
an adaptation of Trust Region Policy Optimization to learn complex
collaborative tasks, such as formation building and building a communication
link. We evaluate our findings in a simulated 2D-physics environment, and
compare the implications of different communication protocols.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huttenrauch_M/0/1/0/all/0/1&quot;&gt;Maximilian H&amp;#xfc;ttenrauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sosic_A/0/1/0/all/0/1&quot;&gt;Adrian &amp;#x160;o&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06650">
<title>Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions. (arXiv:1807.06650v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06650</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural network architecture based upon the Autoencoder (AE) and
Generative Adversarial Network (GAN) that promotes a convex latent distribution
by training adversarially on latent space interpolations. By using an AE as
both the generator and discriminator of a GAN, we pass a pixel-wise error
function across the discriminator, yielding an AE which produces non-blurry
samples that match both high- and low-level features of the original images.
Interpolations between images in this space remain within the latent-space
distribution of real images as trained by the discriminator, and therfore
preserve realistic resemblances to the network inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sainburg_T/0/1/0/all/0/1&quot;&gt;Tim Sainburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thielk_M/0/1/0/all/0/1&quot;&gt;Marvin Thielk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theilman_B/0/1/0/all/0/1&quot;&gt;Brad Theilman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Migliori_B/0/1/0/all/0/1&quot;&gt;Benjamin Migliori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gentner_T/0/1/0/all/0/1&quot;&gt;Timothy Gentner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06651">
<title>Item Recommendation with Variational Autoencoders and Heterogenous Priors. (arXiv:1807.06651v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06651</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Variational Autoencoders (VAEs) have been shown to be highly
effective in both standard collaborative filtering applications and extensions
such as incorporation of implicit feedback. We extend VAEs to collaborative
filtering with side information, for instance when ratings are combined with
explicit text feedback from the user. Instead of using a user-agnostic standard
Gaussian prior, we incorporate user-dependent priors in the latent VAE space to
encode users&apos; preferences as functions of the review text. Taking into account
both the rating and the text information to represent users in this multimodal
latent space is promising to improve recommendation quality. Our proposed model
is shown to outperform the existing VAE models for collaborative filtering (up
to 29.41% relative improvement in ranking metric) along with other baselines
that incorporate both user ratings and text for item recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karamanolakis_G/0/1/0/all/0/1&quot;&gt;Giannis Karamanolakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cherian_K/0/1/0/all/0/1&quot;&gt;Kevin Raji Cherian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narayan_A/0/1/0/all/0/1&quot;&gt;Ananth Ravi Narayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Da Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jebara_T/0/1/0/all/0/1&quot;&gt;Tony Jebara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06656">
<title>Mixed-Stationary Gaussian Process for Flexible Non-Stationary Modeling of Spatial Outcomes. (arXiv:1807.06656v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06656</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian processes (GPs) are commonplace in spatial statistics. Although many
non-stationary models have been developed, there is arguably a lack of
flexibility compared to equipping each location with its own parameters.
However, the latter suffers from intractable computation and can lead to
overfitting. Taking the instantaneous stationarity idea, we construct a
non-stationary GP with the stationarity parameter individually set at each
location. Then we utilize the non-parametric mixture model to reduce the
effective number of unique parameters. Different from a simple mixture of
independent GPs, the mixture in stationarity allows the components to be
spatial correlated, leading to improved prediction efficiency. Theoretical
properties are examined and a linearly scalable algorithm is provided. The
application is shown through several simulated scenarios as well as the massive
spatiotemporally correlated temperature data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duan_L/0/1/0/all/0/1&quot;&gt;Leo L. Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Szczesniak_R/0/1/0/all/0/1&quot;&gt;Rhonda D. Szczesniak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06693">
<title>Tensor Methods for Additive Index Models under Discordance and Heterogeneity. (arXiv:1807.06693v1 [math.ST])</title>
<link>http://arxiv.org/abs/1807.06693</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the sampling problems and heterogeneity issues common in high-
dimensional big datasets, we consider a class of discordant additive index
models. We propose method of moments based procedures for estimating the
indices of such discordant additive index models in both low and
high-dimensional settings. Our estimators are based on factorizing certain
moment tensors and are also applicable in the overcomplete setting, where the
number of indices is more than the dimensionality of the datasets. Furthermore,
we provide rates of convergence of our estimator in both high and
low-dimensional setting. Establishing such results requires deriving tensor
operator norm concentration inequalities that might be of independent interest.
Finally, we provide simulation results supporting our theory. Our contributions
extend the applicability of tensor methods for novel models in addition to
making progress on understanding theoretical properties of such tensor methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Balasubramanian_K/0/1/0/all/0/1&quot;&gt;Krishnakumar Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianqing Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06711">
<title>Receiver Operating Characteristic Curves and Confidence Bands for Support Vector Machines. (arXiv:1807.06711v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06711</link>
<description rdf:parseType="Literal">&lt;p&gt;Many problems that appear in biomedical decision making, such as diagnosing
disease and predicting response to treatment, can be expressed as binary
classification problems. The costs of false positives and false negatives vary
across application domains and receiver operating characteristic (ROC) curves
provide a visual representation of this trade-off. Nonparametric estimators for
the ROC curve, such as a weighted support vector machine (SVM), are desirable
because they are robust to model misspecification. While weighted SVMs have
great potential for estimating ROC curves, their theoretical properties were
heretofore underdeveloped. We propose a method for constructing confidence
bands for the SVM ROC curve and provide the theoretical justification for the
SVM ROC curve by showing that the risk function of the estimated decision rule
is uniformly consistent across the weight parameter. We demonstrate the
proposed confidence band method and the superior sensitivity and specificity of
the weighted SVM compared to commonly used methods in diagnostic medicine using
simulation studies. We present two illustrative examples: diagnosis of
hepatitis C and a predictive model for treatment response in breast cancer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luckett_D/0/1/0/all/0/1&quot;&gt;Daniel J. Luckett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laber_E/0/1/0/all/0/1&quot;&gt;Eric B. Laber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+El_Kamary_S/0/1/0/all/0/1&quot;&gt;Samer S. El-Kamary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Cheng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jhaveri_R/0/1/0/all/0/1&quot;&gt;Ravi Jhaveri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perou_C/0/1/0/all/0/1&quot;&gt;Charles M. Perou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shebl_F/0/1/0/all/0/1&quot;&gt;Fatma M. Shebl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kosorok_M/0/1/0/all/0/1&quot;&gt;Michael R. Kosorok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06712">
<title>Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation. (arXiv:1807.06712v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06712</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning the level set for which a noisy black-box
function exceeds a given threshold.
&lt;/p&gt;
&lt;p&gt;To efficiently reconstruct the level set, we investigate Gaussian process
(GP) metamodels. Our focus is on strongly stochastic samplers, in particular
with heavy-tailed simulation noise and low signal-to-noise ratio.
&lt;/p&gt;
&lt;p&gt;To guard against noise misspecification, we assess the performance of three
variants: (i) GPs with Student-$t$ observations; (ii) Student-$t$ processes
(TPs); and (iii) classification GPs modeling the sign of the response. As a
fourth extension, we study GP surrogates with monotonicity constraints that are
relevant when the level set is known to be connected. In conjunction with these
metamodels, we analyze several acquisition functions for guiding the sequential
experimental designs, extending existing stepwise uncertainty reduction
criteria to the stochastic contour-finding context. This also motivates our
development of (approximate) updating formulas to efficiently compute such
acquisition functions. Our schemes are benchmarked by using a variety of
synthetic experiments in 1--6 dimensions. We also consider an application of
level set estimation for determining the optimal exercise policy and valuation
of Bermudan options in finance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xiong Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Binois_M/0/1/0/all/0/1&quot;&gt;Mickael Binois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ludkovski_M/0/1/0/all/0/1&quot;&gt;Michael Ludkovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06713">
<title>Dependency Leakage: Analysis and Scalable Estimators. (arXiv:1807.06713v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06713</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we prove the first theoretical results on dependency leakage
-- a phenomenon in which learning on noisy clusters biases cross-validation and
model selection results. This is a major concern for domains involving human
record databases (e.g. medical, census, advertising), which are almost always
noisy due to the effects of record linkage and which require special attention
to machine learning bias. The proposed theoretical properties justify
regularization choices in several existing statistical estimators and allow us
to construct the first hypothesis test for cross-validation bias due to
dependency leakage. Furthermore, we propose a novel matrix sketching technique
which, along with standard function approximation techniques, enables
dramatically improving the sample and computational scalability of existing
estimators. Empirical results on several benchmark datasets validate our
theoretical results and proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barnes_M/0/1/0/all/0/1&quot;&gt;Matt Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubrawski_A/0/1/0/all/0/1&quot;&gt;Artur Dubrawski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06722">
<title>Machine Learning Interpretability: A Science rather than a tool. (arXiv:1807.06722v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06722</link>
<description rdf:parseType="Literal">&lt;p&gt;The term &quot;interpretability&quot; is oftenly used by machine learning researchers
each with their own intuitive understanding of it. There is no universal well
agreed upon definition of interpretability in machine learning. As any type of
science discipline is mainly driven by the set of formulated questions rather
than by different tools in that discipline, e.g. astrophysics is the discipline
that learns the composition of stars, not as the discipline that use the
spectroscopes. Similarly, we propose that machine learning interpretability
should be a discipline that answers specific questions related to
interpretability. These questions can be of statistical, causal and
counterfactual nature. Therefore, there is a need to look into the
interpretability problem of machine learning in the context of questions that
need to be addressed rather than different tools. We discuss about a
hypothetical interpretability framework driven by a question based scientific
approach rather than some specific machine learning model. Using a question
based notion of interpretability, we can step towards understanding the science
of machine learning rather than its engineering. This notion will also help us
understanding any specific problem more in depth rather than relying solely on
machine learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_A/0/1/0/all/0/1&quot;&gt;Abdul Karim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06732">
<title>Motivating the Rules of the Game for Adversarial Example Research. (arXiv:1807.06732v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06732</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in machine learning have led to broad deployment of systems with
impressive performance on important problems. Nonetheless, these systems can be
induced to make errors on data that are surprisingly similar to examples the
learned system handles correctly. The existence of these errors raises a
variety of questions about out-of-sample generalization and whether bad actors
might use such examples to abuse deployed systems. As a result of these
security concerns, there has been a flurry of recent papers proposing
algorithms to defend against such malicious perturbations of correctly handled
examples. It is unclear how such misclassifications represent a different kind
of security problem than other errors, or even other attacker-produced examples
that have no specific relationship to an uncorrupted input. In this paper, we
argue that adversarial example defense papers have, to date, mostly considered
abstract, toy games that do not relate to any specific security concern.
Furthermore, defense papers have not yet precisely described all the abilities
and limitations of attackers that would be relevant in practical security.
Towards this end, we establish a taxonomy of motivations, constraints, and
abilities for more plausible adversaries. Finally, we provide a series of
recommendations outlining a path forward for future work to more clearly
articulate the threat model and perform more meaningful evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1&quot;&gt;Justin Gilmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1&quot;&gt;Ryan P. Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_D/0/1/0/all/0/1&quot;&gt;David Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_G/0/1/0/all/0/1&quot;&gt;George E. Dahl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06766">
<title>Convergence guarantees for RMSProp and ADAM in non-convex optimization and their comparison to Nesterov acceleration on autoencoders. (arXiv:1807.06766v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06766</link>
<description rdf:parseType="Literal">&lt;p&gt;RMSProp and ADAM continue to be extremely popular algorithms for training
neural nets but their theoretical foundations have remained unclear. In this
work we make progress towards that by giving proofs that these adaptive
gradient algorithms are guaranteed to reach criticality for smooth non-convex
objectives and we give bounds on the running time.
&lt;/p&gt;
&lt;p&gt;We then design experiments to compare the performances of RMSProp and ADAM
against Nesterov Accelerated Gradient method on a variety of autoencoder
setups. Through these experiments we demonstrate the interesting sensitivity
that ADAM has to its momentum parameter $\beta_1$. We show that in terms of
getting lower training and test losses, at very high values of the momentum
parameter ($\beta_1 = 0.99$) (and large enough nets if using mini-batches) ADAM
outperforms NAG at any momentum value tried for the latter. On the other hand,
NAG can sometimes do better when ADAM&apos;s $\beta_1$ is set to the most commonly
used value: $\beta_1 = 0.9$. We also report experiments on different
autoencoders to demonstrate that NAG has better abilities in terms of reducing
the gradient norms and finding weights which increase the minimum eigenvalue of
the Hessian of the loss function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Amitabh Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1&quot;&gt;Soham De&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Anirbit Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_E/0/1/0/all/0/1&quot;&gt;Enayat Ullah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06824">
<title>News-based trading strategies. (arXiv:1807.06824v1 [q-fin.TR])</title>
<link>http://arxiv.org/abs/1807.06824</link>
<description rdf:parseType="Literal">&lt;p&gt;The marvel of markets lies in the fact that dispersed information is
instantaneously processed and used to adjust the price of goods, services and
assets. Financial markets are particularly efficient when it comes to
processing information; such information is typically embedded in textual news
that is then interpreted by investors. Quite recently, researchers have started
to automatically determine news sentiment in order to explain stock price
movements. Interestingly, this so-called news sentiment works fairly well in
explaining stock returns. In this paper, we design trading strategies that
utilize textual news in order to obtain profits on the basis of novel
information entering the market. We thus propose approaches for automated
decision-making based on supervised and reinforcement learning. Altogether, we
demonstrate how news-based data can be incorporated into an investment system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Feuerriegel_S/0/1/0/all/0/1&quot;&gt;Stefan Feuerriegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Prendinger_H/0/1/0/all/0/1&quot;&gt;Helmut Prendinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06962">
<title>Active Learning for Segmentation by Optimizing Content Information for Maximal Entropy. (arXiv:1807.06962v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.06962</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation is essential for medical image analysis tasks such as
intervention planning, therapy guidance, diagnosis, treatment decisions. Deep
learning is becoming increasingly prominent for segmentation, where the lack of
annotations, however, often becomes the main limitation. Due to privacy
concerns and ethical considerations, most medical datasets are created,
curated, and allow access only locally. Furthermore, current deep learning
methods are often suboptimal in translating anatomical knowledge between
different medical imaging modalities. Active learning can be used to select an
informed set of image samples to request for manual annotation, in order to
best utilize the limited annotation time of clinical experts for optimal
outcomes, which we focus on in this work. Our contributions herein are two
fold: (1) we enforce domain-representativeness of selected samples using a
proposed penalization scheme to maximize information at the network abstraction
layer, and (2) we propose a Borda-count based sample querying scheme for
selecting samples for segmentation. Comparative experiments with baseline
approaches show that the samples queried with our proposed method, where both
above contributions are combined, result in significantly improved segmentation
performance for this active learning task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdemir_F/0/1/0/all/0/1&quot;&gt;Firat Ozdemir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zixuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1&quot;&gt;Christine Tanner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuernstahl_P/0/1/0/all/0/1&quot;&gt;Philipp Fuernstahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1&quot;&gt;Orcun Goksel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06981">
<title>A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization. (arXiv:1807.06981v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.06981</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of many machine learning techniques depends on the choice of
an appropriate similarity or distance measure on the input space. Similarity
learning (or metric learning) aims at building such a measure from training
data so that observations with the same (resp. different) label are as close
(resp. far) as possible. In this paper, similarity learning is investigated
from the perspective of pairwise bipartite ranking, where the goal is to rank
the elements of a database by decreasing order of the probability that they
share the same label with some query data point, based on the similarity
scores. A natural performance criterion in this setting is pointwise ROC
optimization: maximize the true positive rate under a fixed false positive
rate. We study this novel perspective on similarity learning through a rigorous
probabilistic framework. The empirical version of the problem gives rise to a
constrained optimization formulation involving U-statistics, for which we
derive universal learning rates as well as faster rates under a noise
assumption on the data distribution. We also address the large-scale setting by
analyzing the effect of sampling-based approximations. Our theoretical results
are supported by illustrative numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogel_R/0/1/0/all/0/1&quot;&gt;Robin Vogel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bellet_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Clemencon_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phan Cl&amp;#xe9;men&amp;#xe7;on&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.05011">
<title>Variable projection without smoothness. (arXiv:1601.05011v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1601.05011</link>
<description rdf:parseType="Literal">&lt;p&gt;Variable projection solves structured optimization problems by completely
minimizing over a subset of the variables while iterating over the remaining
variables. Over the last 30 years, the technique has been widely used, with
empirical and theoretical results demonstrating both greater efficacy and
greater stability compared to competing approaches. Classic examples have
exploited closed form projections and smoothness of the objective function. We
extend the approach to broader settings, where the projection subproblems can
be nonsmooth, and can only be solved inexactly by iterative methods. We present
a few case studies on problems occurring frequently in machine-learning and
high-dimensional inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Aravkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1&quot;&gt;Dmitriy Drusvyatskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Leeuwen_T/0/1/0/all/0/1&quot;&gt;Tristan van Leeuwen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.00199">
<title>Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation From Undersampled Data. (arXiv:1610.00199v2 [cs.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1610.00199</link>
<description rdf:parseType="Literal">&lt;p&gt;Subspace learning and matrix factorization problems have great many
applications in science and engineering, and efficient algorithms are critical
as dataset sizes continue to grow. Many relevant problem formulations are
non-convex, and in a variety of contexts it has been observed that solving the
non-convex problem directly is not only efficient but reliably accurate. We
discuss convergence theory for a particular method: first order incremental
gradient descent constrained to the Grassmannian. The output of the algorithm
is an orthonormal basis for a $d$-dimensional subspace spanned by an input
streaming data matrix. We study two sampling cases: where each data vector of
the streaming matrix is fully sampled, or where it is undersampled by a
sampling matrix $A_t\in \mathbb{R}^{m\times n}$ with $m\ll n$. Our results
cover two cases, where $A_t$ is Gaussian or a subset of rows of the identity
matrix. We propose an adaptive stepsize scheme that depends only on the sampled
data and algorithm outputs. We prove that with fully sampled data, the stepsize
scheme maximizes the improvement of our convergence metric at each iteration,
and this method converges from any random initialization to the true subspace,
despite the non-convex formulation and orthogonality constraints. For the case
of undersampled data, we establish monotonic expected improvement on the
defined convergence metric for each iteration with high probability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dejiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balzano_L/0/1/0/all/0/1&quot;&gt;Laura Balzano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.08213">
<title>The 2D Tree Sliding Window Discrete Fourier Transform. (arXiv:1707.08213v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1707.08213</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new algorithm for the 2D Sliding Window Discrete Fourier
Transform (SWDFT). Our algorithm avoids repeating calculations in overlapping
windows by storing them in a tree data-structure based on the ideas of the
Cooley- Tukey Fast Fourier Transform (FFT). For an $N_0 \times N_1$ array and
$n_0 \times n_1$ windows, our algorithm takes $O(N_0 N_1 n_0 n_1)$ operations.
We provide a C implementation of our algorithm for the Radix-2 case, compare
ours with existing algorithms, and show how our algorithm easily extends to
higher dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_L/0/1/0/all/0/1&quot;&gt;Lee F. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eddy_W/0/1/0/all/0/1&quot;&gt;William F. Eddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08795">
<title>On Stein&apos;s Identity and Near-Optimal Estimation in High-dimensional Index Models. (arXiv:1709.08795v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08795</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider estimating the parametric components of semi-parametric multiple
index models in a high-dimensional and non-Gaussian setting. Such models form a
rich class of non-linear models with applications to signal processing, machine
learning and statistics. Our estimators leverage the score function based first
and second-order Stein&apos;s identities and do not require the covariates to
satisfy Gaussian or elliptical symmetry assumptions common in the literature.
Moreover, to handle score functions and responses that are heavy-tailed, our
estimators are constructed via carefully thresholding their empirical
counterparts. We show that our estimator achieves near-optimal statistical rate
of convergence in several settings. We supplement our theoretical results via
simulation experiments that confirm the theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Balasubramanian_K/0/1/0/all/0/1&quot;&gt;Krishnakumar Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08639">
<title>Fast binary embeddings, and quantized compressed sensing with structured matrices. (arXiv:1801.08639v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08639</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with two related problems, namely distance-preserving binary
embeddings and quantization for compressed sensing . First, we propose fast
methods to replace points from a subset $\mathcal{X} \subset \mathbb{R}^n$,
associated with the Euclidean metric, with points in the cube $\{\pm 1\}^m$ and
we associate the cube with a pseudo-metric that approximates Euclidean distance
among points in $\mathcal{X}$. Our methods rely on quantizing fast
Johnson-Lindenstrauss embeddings based on bounded orthonormal systems and
partial circulant ensembles, both of which admit fast transforms. Our
quantization methods utilize noise-shaping, and include Sigma-Delta schemes and
distributed noise-shaping schemes. The resulting approximation errors decay
polynomially and exponentially fast in $m$, depending on the embedding method.
This dramatically outperforms the current decay rates associated with binary
embeddings and Hamming distances. Additionally, it is the first such binary
embedding result that applies to fast Johnson-Lindenstrauss maps while
preserving $\ell_2$ norms.
&lt;/p&gt;
&lt;p&gt;Second, we again consider noise-shaping schemes, albeit this time to quantize
compressed sensing measurements arising from bounded orthonormal ensembles and
partial circulant matrices. We show that these methods yield a reconstruction
error that again decays with the number of measurements (and bits), when using
convex optimization for reconstruction. Specifically, for Sigma-Delta schemes,
the error decays polynomially in the number of measurements, and it decays
exponentially for distributed noise-shaping schemes based on beta encoding.
These results are near optimal and the first of their kind dealing with bounded
orthonormal systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1&quot;&gt;Thang Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1&quot;&gt;Rayan Saab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07927">
<title>The Hidden Vulnerability of Distributed Learning in Byzantium. (arXiv:1802.07927v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07927</link>
<description rdf:parseType="Literal">&lt;p&gt;While machine learning is going through an era of celebrated success,
concerns have been raised about the vulnerability of its backbone: stochastic
gradient descent (SGD). Recent approaches have been proposed to ensure the
robustness of distributed SGD against adversarial (Byzantine) workers sending
poisoned gradients during the training phase. Some of these approaches have
been proven Byzantine-resilient: they ensure the convergence of SGD despite the
presence of a minority of adversarial workers.
&lt;/p&gt;
&lt;p&gt;We show in this paper that convergence is not enough. In high dimension $d
\gg 1$, an adver\-sary can build on the loss function&apos;s non-convexity to make
SGD converge to ineffective models. More precisely, we bring to light that
existing Byzantine-resilient schemes leave a margin of poisoning of
$\Omega\left(f(d)\right)$, where $f(d)$ increases at least like $\sqrt{d~}$.
Based on this leeway, we build a simple attack, and experimentally show its
strong to utmost effectivity on CIFAR-10 and MNIST.
&lt;/p&gt;
&lt;p&gt;We introduce Bulyan, and prove it significantly reduces the attackers leeway
to a narrow $O( \frac{1}{\sqrt{d~}})$ bound. We empirically show that Bulyan
does not suffer the fragility of existing aggregation rules and, at a
reasonable cost in terms of required batch size, achieves convergence as if
only non-Byzantine gradients had been used to update the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mhamdi_E/0/1/0/all/0/1&quot;&gt;El Mahdi El Mhamdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guerraoui_R/0/1/0/all/0/1&quot;&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rouault_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07445">
<title>DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors. (arXiv:1805.07445v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07445</link>
<description rdf:parseType="Literal">&lt;p&gt;Boltzmann machines are powerful distributions that have been shown to be an
effective prior over binary latent variables in variational autoencoders
(VAEs). However, previous methods for training discrete VAEs have used the
evidence lower bound and not the tighter importance-weighted bound. We propose
two approaches for relaxing Boltzmann machines to continuous distributions that
permit training with importance-weighted bounds. These relaxations are based on
generalized overlapping transformations and the Gaussian integral trick.
Experiments on the MNIST and OMNIGLOT datasets show that these relaxations
outperform previous discrete VAEs with Boltzmann priors. The implementation is
available at https://github.com/QuadrantAI/dvae .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vahdat_A/0/1/0/all/0/1&quot;&gt;Arash Vahdat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Andriyash_E/0/1/0/all/0/1&quot;&gt;Evgeny Andriyash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Macready_W/0/1/0/all/0/1&quot;&gt;William G. Macready&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09602">
<title>A Machine-learning framework for automatic reference-free quality assessment in MRI. (arXiv:1806.09602v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09602</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance (MR) imaging offers a wide variety of imaging techniques.
A large amount of data is created per examination which needs to be checked for
sufficient quality in order to derive a meaningful diagnosis. This is a manual
process and therefore time- and cost-intensive. Any imaging artifacts
originating from scanner hardware, signal processing or induced by the patient
may reduce the image quality and complicate the diagnosis or any image
post-processing. Therefore, the assessment or the ensurance of sufficient image
quality in an automated manner is of high interest. Usually no reference image
is available or difficult to define. Therefore, classical reference-based
approaches are not applicable. Model observers mimicking the human observers
(HO) can assist in this task. Thus, we propose a new machine-learning-based
reference-free MR image quality assessment framework which is trained on
HO-derived labels to assess MR image quality immediately after each
acquisition. We include the concept of active learning and present an efficient
blinded reading platform to reduce the effort in the HO labeling procedure.
Derived image features and the applied classifiers (support-vector-machine,
deep neural network) are investigated for a cohort of 250 patients. The MR
image quality assessment framework can achieve a high test accuracy of 93.7$\%$
for estimating quality classes on a 5-point Likert-scale. The proposed MR image
quality assessment framework is able to provide an accurate and efficient
quality estimation which can be used as a prospective quality assurance
including automatic acquisition adaptation or guided MR scanner operation,
and/or as a retrospective quality assessment including support of diagnostic
decisions or quality control in cohort studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kustner_T/0/1/0/all/0/1&quot;&gt;Thomas K&amp;#xfc;stner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1&quot;&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebgott_A/0/1/0/all/0/1&quot;&gt;Annika Liebgott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_M/0/1/0/all/0/1&quot;&gt;Martin Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauch_L/0/1/0/all/0/1&quot;&gt;Lukas Mauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martirosian_P/0/1/0/all/0/1&quot;&gt;Petros Martirosian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_H/0/1/0/all/0/1&quot;&gt;Holger Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwenzer_N/0/1/0/all/0/1&quot;&gt;Nina F. Schwenzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaou_K/0/1/0/all/0/1&quot;&gt;Konstantin Nikolaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bamberg_F/0/1/0/all/0/1&quot;&gt;Fabian Bamberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schick_F/0/1/0/all/0/1&quot;&gt;Fritz Schick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05411">
<title>Sparse Relaxed Regularized Regression: SR3. (arXiv:1807.05411v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05411</link>
<description rdf:parseType="Literal">&lt;p&gt;Regularized regression problems are ubiquitous in statistical modeling,
signal processing, and machine learning. Sparse regression in particular has
been instrumental in scientific model discovery, including compressed sensing
applications, variable selection, and high-dimensional analysis. We propose a
new and highly effective approach for regularized regression, called SR3.
&lt;/p&gt;
&lt;p&gt;The key idea is to solve a relaxation of the regularized problem, which has
three advantages over the state-of-the-art: (1) solutions of the relaxed
problem are superior with respect to errors, false positives, and conditioning,
(2) relaxation allows extremely fast algorithms for both convex and nonconvex
formulations, and (3) the methods apply to composite regularizers such as total
variation (TV) and its nonconvex variants. We demonstrate the improved
performance of SR3 across a range of regularized regression problems with
synthetic and real data, including compressed sensing, LASSO, matrix completion
and TV regularization. To promote reproducible research, we include a companion
Matlab package that implements these popular applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_P/0/1/0/all/0/1&quot;&gt;Peng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Askham_T/0/1/0/all/0/1&quot;&gt;Travis Askham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brunton_S/0/1/0/all/0/1&quot;&gt;Steven L. Brunton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J. Nathan Kutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05666">
<title>Scene Learning: Deep Convolutional Networks For Wind Power Prediction by Embedding Turbines into Grid Space. (arXiv:1807.05666v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05666</link>
<description rdf:parseType="Literal">&lt;p&gt;Wind power prediction is of vital importance in wind power utilization. There
have been a lot of researches based on the time series of the wind power or
speed, but In fact, these time series cannot express the temporal and spatial
changes of wind, which fundamentally hinders the advance of wind power
prediction. In this paper, a new kind of feature that can describe the process
of temporal and spatial variation is proposed, namely, Spatio-Temporal
Features. We first map the data collected at each moment from the wind turbine
to the plane to form the state map, namely, the scene, according to the
relative positions. The scene time series over a period of time is a
multi-channel image, i.e. the Spatio-Temporal Features. Based on the
Spatio-Temporal Features, the deep convolutional network is applied to predict
the wind power, achieving a far better accuracy than the existing methods.
Compared with the starge-of-the-art method, the mean-square error (MSE) in our
method is reduced by 49.83%, and the average time cost for training models can
be shortened by a factor of more than 150.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ruiguo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuewei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenhuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianrong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;</dc:creator>
</item></rdf:RDF>