<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1308.1603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00029"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00332"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00411"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.10651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08618"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00086"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00130"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00255"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00430"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.05684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.06972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.03636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.01604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09834"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1308.1603">
<title>A Note on Topology Preservation in Classification, and the Construction of a Universal Neuron Grid. (arXiv:1308.1603v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1308.1603</link>
<description rdf:parseType="Literal">&lt;p&gt;It will be shown that according to theorems of K. Menger, every neuron grid
if identified with a curve is able to preserve the adopted qualitative
structure of a data space. Furthermore, if this identification is made, the
neuron grid structure can always be mapped to a subset of a universal neuron
grid which is constructable in three space dimensions. Conclusions will be
drawn for established neuron grid types as well as neural fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volz_D/0/1/0/all/0/1&quot;&gt;Dietmar Volz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00029">
<title>Cluster-based Approach to Improve Affect Recognition from Passively Sensed Data. (arXiv:1802.00029v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1802.00029</link>
<description rdf:parseType="Literal">&lt;p&gt;Negative affect is a proxy for mental health in adults. By being able to
predict participants&apos; negative affect states unobtrusively, researchers and
clinicians will be better positioned to deliver targeted, just-in-time mental
health interventions via mobile applications. This work attempts to personalize
the passive recognition of negative affect states via group-based modeling of
user behavior patterns captured from mobility, communication, and activity
patterns. Results show that group models outperform generalized models in a
dataset based on two weeks of users&apos; daily lives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ameko_M/0/1/0/all/0/1&quot;&gt;Mawulolo K. Ameko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Lihua Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boukhechba_M/0/1/0/all/0/1&quot;&gt;Mehdi Boukhechba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daros_A/0/1/0/all/0/1&quot;&gt;Alexander Daros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_P/0/1/0/all/0/1&quot;&gt;Philip I. Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teachman_B/0/1/0/all/0/1&quot;&gt;Bethany A. Teachman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerber_M/0/1/0/all/0/1&quot;&gt;Matthew S. Gerber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1&quot;&gt;Laura E. Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00033">
<title>Technical Report: Adjudication of Coreference Annotations via Answer Set Optimization. (arXiv:1802.00033v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1802.00033</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe the first automatic approach for merging coreference annotations
obtained from multiple annotators into a single gold standard. This merging is
subject to certain linguistic hard constraints and optimization criteria that
prefer solutions with minimal divergence from annotators. The representation
involves an equivalence relation over a large number of elements. We use Answer
Set Programming to describe two representations of the problem and four
objective functions suitable for different datasets. We provide two
structurally different real-world benchmark datasets based on the METU-Sabanci
Turkish Treebank and we report our experiences in using the Gringo, Clasp, and
Wasp tools for computing optimal adjudication results on these datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_P/0/1/0/all/0/1&quot;&gt;Peter Sch&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00295">
<title>pv:SSCategory. (arXiv:1802.00295v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00295</link>
<description rdf:parseType="Literal">&lt;p&gt;The study and publication of historical scientific manuscripts are com- plex
tasks that involve, among others, the explicit representation of the text mean-
ings and reasoning on temporal entities. In this paper we present the first
results of an interdisciplinary project dedicated to the study of Saussure&apos;s
manuscripts. These results aim to fulfill requirements elaborated with
Saussurean humanists. They comprise a model for the representation of
time-varying statements and time-varying domain knowledge (in particular
terminologies) as well as imple- mentation techniques for the semantic indexing
of manuscripts and for temporal reasoning on knowledge extracted from the
manuscripts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljalbout_S/0/1/0/all/0/1&quot;&gt;Sahar Aljalbout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falquet_G/0/1/0/all/0/1&quot;&gt;Gilles Falquet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00332">
<title>Elements of Effective Deep Reinforcement Learning towards Tactical Driving Decision Making. (arXiv:1802.00332v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00332</link>
<description rdf:parseType="Literal">&lt;p&gt;Tactical driving decision making is crucial for autonomous driving systems
and has attracted considerable interest in recent years. In this paper, we
propose several practical components that can speed up deep reinforcement
learning algorithms towards tactical decision making tasks: 1) non-uniform
action skipping as a more stable alternative to action-repetition frame
skipping, 2) a counter-based penalty for lanes on which ego vehicle has less
right-of-road, and 3) heuristic inference-time action masking for apparently
undesirable actions. We evaluate the proposed components in a realistic driving
simulator and compare them with several baselines. Results show that the
proposed scheme provides superior performance in terms of safety, efficiency,
and comfort.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingchu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_P/0/1/0/all/0/1&quot;&gt;Pengfei Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_L/0/1/0/all/0/1&quot;&gt;Lisen Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yinan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chang Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00386">
<title>Crowd Flow Prediction by Deep Spatio-Temporal Transfer Learning. (arXiv:1802.00386v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00386</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowd flow prediction is a fundamental urban computing problem. Recently,
deep learning has been successfully applied to solve this problem, but it
relies on rich historical data. In reality, many cities may suffer from data
scarcity issue when their targeted service or infrastructure is new. To
overcome this issue, this paper proposes a novel deep spatio-temporal transfer
learning framework, called RegionTrans, which can predict future crowd flow in
a data-scarce (target) city by transferring knowledge from a data-rich (source)
city. Leveraging social network check-ins, RegionTrans first links a region in
the target city to certain regions in the source city, expecting that these
inter-city region pairs will share similar crowd flow dynamics. Then, we
propose a deep spatio-temporal neural network structure, in which a hidden
layer is dedicated to keeping the region representation. A source city model is
then trained on its rich historical data with this network structure. Finally,
we propose a region-based cross-city transfer learning algorithm to learn the
target city model from the source city model by minimizing the hidden
representation discrepancy between the inter-city region pairs previously
linked by check-ins. With experiments on real crowd flow, RegionTrans can
outperform state-of-the-arts by reducing up to 10.7% prediction error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00411">
<title>3D Object Dense Reconstruction from a Single Depth View. (arXiv:1802.00411v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.00411</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs
the complete 3D structure of a given object from a single arbitrary depth view
using generative adversarial networks. Unlike existing work which typically
requires multiple views of the same object or class labels to recover the full
3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation
of a depth view of the object as input, and is able to generate the complete 3D
occupancy grid with a high resolution of 256^3 by recovering the
occluded/missing regions. The key idea is to combine the generative
capabilities of autoencoders and the conditional Generative Adversarial
Networks (GAN) framework, to infer accurate and fine-grained 3D structures of
objects in high-dimensional voxel space. Extensive experiments on large
synthetic datasets and real-world Kinect datasets show that the proposed
3D-RecGAN++ significantly outperforms the state of the art in single view 3D
object reconstruction, and is able to reconstruct unseen types of objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_S/0/1/0/all/0/1&quot;&gt;Stefano Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1&quot;&gt;Andrew Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1&quot;&gt;Niki Trigoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongkai Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.10651">
<title>Reliable Decision Support using Counterfactual Models. (arXiv:1703.10651v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.10651</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-makers are faced with the challenge of estimating what is likely to
happen when they take an action. For instance, if I choose not to treat this
patient, are they likely to die? Practitioners commonly use supervised learning
algorithms to fit predictive models that help decision-makers reason about
likely future outcomes, but we show that this approach is unreliable, and
sometimes even dangerous. The key issue is that supervised learning algorithms
are highly sensitive to the policy used to choose actions in the training data,
which causes the model to capture relationships that do not generalize. We
propose using a different learning objective that predicts counterfactuals
instead of predicting outcomes under an existing action policy as in supervised
learning. To support decision-making in temporal settings, we introduce the
Counterfactual Gaussian Process (CGP) to predict the counterfactual future
progression of continuous-time trajectories under sequences of future actions.
We demonstrate the benefits of the CGP on two important decision-support tasks:
risk prediction and &quot;what if?&quot; reasoning for individualized treatment planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schulam_P/0/1/0/all/0/1&quot;&gt;Peter Schulam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saria_S/0/1/0/all/0/1&quot;&gt;Suchi Saria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08618">
<title>JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services. (arXiv:1801.08618v1 [cs.DC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1801.08618</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are among the most influential architectures of deep
learning algorithms, being deployed in many mobile intelligent applications.
End-side services, such as intelligent personal assistants (IPAs), autonomous
cars, and smart home services often employ either simple local models or
complex remote models on the cloud. Mobile-only and cloud-only computations are
currently the status quo approaches. In this paper, we propose an efficient,
adaptive, and practical engine, JointDNN, for collaborative computation between
a mobile device and cloud for DNNs in both inference and training phase.
JointDNN not only provides an energy and performance efficient method of
querying DNNs for the mobile side, but also benefits the cloud server by
reducing the amount of its workload and communications compared to the
cloud-only approach. Given the DNN architecture, we investigate the efficiency
of processing some layers on the mobile device and some layers on the cloud
server. We provide optimization formulations at layer granularity for forward
and backward propagation in DNNs, which can adapt to mobile battery limitations
and cloud server load constraints and quality of service. JointDNN achieves up
to 18X and 32X reductions on the latency and mobile energy consumption of
querying DNNs, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eshratifar_A/0/1/0/all/0/1&quot;&gt;Amir Erfan Eshratifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrishami_M/0/1/0/all/0/1&quot;&gt;Mohammad Saeed Abrishami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1&quot;&gt;Massoud Pedram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00008">
<title>On the Topic of Jets. (arXiv:1802.00008v1 [hep-ph])</title>
<link>http://arxiv.org/abs/1802.00008</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce jet topics: a framework to identify underlying classes of jets
from collider data. Because of a close mathematical relationship between
distributions of observables in jets and emergent themes in sets of documents,
we can apply recent techniques in &quot;topic modeling&quot; to extract jet topics from
data with no input from simulation or theory. As a proof-of-concept with parton
shower samples, we apply jet topics to determine separate quark and gluon
distributions for constituent multiplicity. We also determine separate quark
and gluon rapidity spectra from a mixed Z-plus-jet sample. Because jet topics
are defined directly from hadron-level multi-differential cross sections, one
can predict jet topics from first-principles theoretical calculations, with
potential implications for how to define quark and gluon jets beyond
leading-logarithmic accuracy. These investigations suggest that jet topics will
be useful for extracting underlying jet distributions and fractions in a wide
range of contexts at the Large Hadron Collider.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Metodiev_E/0/1/0/all/0/1&quot;&gt;Eric M. Metodiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Thaler_J/0/1/0/all/0/1&quot;&gt;Jesse Thaler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00043">
<title>Incremental kernel PCA and the Nystr\&quot;om method. (arXiv:1802.00043v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00043</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental versions of batch algorithms are often desired, for increased
time efficiency in the streaming data setting, or increased memory efficiency
in general. In this paper we present a novel algorithm for incremental kernel
PCA, based on rank one updates to the eigendecomposition of the kernel matrix,
which is more computationally efficient than comparable existing algorithms. We
extend our algorithm to incremental calculation of the Nystr\&quot;om approximation
to the kernel matrix, the first such algorithm proposed. Incremental
calculation of the Nystr\&quot;om approximation leads to further gains in memory
efficiency, and allows for empirical evaluation of when a subset of sufficient
size has been obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hallgren_F/0/1/0/all/0/1&quot;&gt;Fredrik Hallgren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Northrop_P/0/1/0/all/0/1&quot;&gt;Paul Northrop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00045">
<title>Composite Gaussian Processes: Scalable Computation and Performance Analysis. (arXiv:1802.00045v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00045</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian process (GP) models provide a powerful tool for prediction but are
computationally prohibitive using large data sets. In such scenarios, one has
to resort to approximate methods. We derive an approximation based on a
composite likelihood approach using a general belief updating framework, which
leads to a recursive computation of the predictor as well as of learning the
hyper-parameters. We then provide an analysis of the derived composite GP model
in predictive and information-theoretic terms. Finally, we evaluate the
approximation with both synthetic data and a real-world application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiuming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zachariah_D/0/1/0/all/0/1&quot;&gt;Dave Zachariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ngai_E/0/1/0/all/0/1&quot;&gt;Edith C. H. Ngai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00047">
<title>Matrix completion with deterministic pattern - a geometric perspective. (arXiv:1802.00047v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00047</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the matrix completion problem with a deterministic pattern of
observed entries and aim to find conditions such that there will be (at least
locally) unique solution to the non-convex Minimum Rank Matrix Completion
(MRMC) formulation. We answer the question from a somewhat different point of
view and to give a geometric perspective. We give a sufficient and &quot;almost
necessary&quot; condition (which we call the well-posedness condition) for the local
uniqueness of MRMC solutions and illustrate with some special cases where such
condition can be verified. We also consider the convex relaxation and nuclear
norm minimization formulations. Then we argue that the low-rank approximation
approaches are more stable than MRMC and further propose a sequential
statistical testing procedure to determine the rank of the matrix from observed
entries. Finally, numerical examples verified the validity of our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_A/0/1/0/all/0/1&quot;&gt;Alexander Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00086">
<title>Optimizing Non-decomposable Measures with Deep Networks. (arXiv:1802.00086v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00086</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a class of algorithms capable of directly training deep neural
networks with respect to large families of task-specific performance measures
such as the F-measure and the Kullback-Leibler divergence that are structured
and non-decomposable. This presents a departure from standard deep learning
techniques that typically use squared or cross-entropy loss functions (that are
decomposable) to train neural networks. We demonstrate that directly training
with task-specific loss functions yields much faster and more stable
convergence across problems and datasets. Our proposed algorithms and
implementations have several novel features including (i) convergence to first
order stationary points despite optimizing complex objective functions; (ii)
use of fewer training samples to achieve a desired level of convergence, (iii)
a substantial reduction in training time, and (iv) a seamless integration of
our implementation into existing symbolic gradient frameworks. We implement our
techniques on a variety of deep architectures including multi-layer perceptrons
and recurrent neural networks and show that on a variety of benchmark and real
data sets, our algorithms outperform traditional approaches to training deep
networks, as well as some recent approaches to task-specific training of neural
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sanyal_A/0/1/0/all/0/1&quot;&gt;Amartya Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Pawan Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kar_P/0/1/0/all/0/1&quot;&gt;Purushottam Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chawla_S/0/1/0/all/0/1&quot;&gt;Sanjay Chawla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sebastiani_F/0/1/0/all/0/1&quot;&gt;Fabrizio Sebastiani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00123">
<title>A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of Multinomials. (arXiv:1802.00123v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00123</link>
<description rdf:parseType="Literal">&lt;p&gt;Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural
networks can provide more powerful mapping capability than the traditional
feedforward neural networks (Sigma-Sigma neural networks). In the existing
literature, in order to reduce the number of the Pi nodes in the Pi layer, a
special multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with
respect to each particular variable sigma_i when the other variables are taken
as constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with
n&amp;gt;1 are not included. This choice may be somehow intuitive, but is not
necessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural
network (MSPSNN) with an adaptive approach to find a better multinomial for a
given problem. To elaborate, we start from a complete multinomial with a given
order. Then we employ a regularization technique in the learning process for
the given problem to reduce the number of monomials used in the multinomial,
and end up with a new SPSNN involving the same number of monomials (= the
number of nodes in the Pi-layer) as in P_s. Numerical experiments on some
benchmark problems show that our MSPSNN behaves better than the traditional
SPSNN with P_s.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_K/0/1/0/all/0/1&quot;&gt;Khidir Shaib Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00130">
<title>Distributed Newton Methods for Deep Neural Networks. (arXiv:1802.00130v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00130</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning involves a difficult non-convex optimization problem with a
large number of weights between any two adjacent layers of a deep structure. To
handle large data sets or complicated networks, distributed training is needed,
but the calculation of function, gradient, and Hessian is expensive. In
particular, the communication and the synchronization cost may become a
bottleneck. In this paper, we focus on situations where the model is
distributedly stored, and propose a novel distributed Newton method for
training deep neural networks. By variable and feature-wise data partitions,
and some careful designs, we are able to explicitly use the Jacobian matrix for
matrix-vector products in the Newton method. Some techniques are incorporated
to reduce the running time as well as the memory consumption. First, to reduce
the communication cost, we propose a diagonalization method such that an
approximate Newton direction can be obtained without communication between
machines. Second, we consider subsampled Gauss-Newton matrices for reducing the
running time as well as the communication cost. Third, to reduce the
synchronization cost, we terminate the process of finding an approximate Newton
direction even though some nodes have not finished their tasks. Details of some
implementation issues in distributed environments are thoroughly investigated.
Experiments demonstrate that the proposed method is effective for the
distributed training of deep neural networks. In compared with stochastic
gradient methods, it is more robust and may give better test accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chien-Chih Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kent Loong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chun-Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keerthi_S/0/1/0/all/0/1&quot;&gt;S. Sathiya Keerthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahajan_D/0/1/0/all/0/1&quot;&gt;Dhruv Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sundararajan_S/0/1/0/all/0/1&quot;&gt;S. Sundararajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chih-Jen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00168">
<title>Deep Learning with Data Dependent Implicit Activation Function. (arXiv:1802.00168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00168</link>
<description rdf:parseType="Literal">&lt;p&gt;Though deep neural networks (DNNs) achieve remarkable performances in many
artificial intelligence tasks, the lack of training instances remains a
notorious challenge. As the network goes deeper, the generalization accuracy
decays rapidly in the situation of lacking massive amounts of training data. In
this paper, we propose novel deep neural network structures that can be
inherited from all existing DNNs with almost the same level of complexity, and
develop simple training algorithms. We show our paradigm successfully resolves
the lack of data issue. Tests on the CIFAR10 and CIFAR100 image recognition
datasets show that the new paradigm leads to 20$\%$ to $30\%$ relative error
rate reduction compared to their base DNNs. The intuition of our algorithms for
deep residual network stems from theories of the partial differential equation
(PDE) control problems. Code will be made available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zuoqiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1&quot;&gt;Stanley J. Osher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00243">
<title>Greedy Active Learning Algorithm for Logistic Regression Models. (arXiv:1802.00243v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00243</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a logistic model-based active learning procedure for binary
classification problems, in which we adopt a batch subject selection strategy
with a modified sequential experimental design method. Moreover, accompanying
the proposed subject selection scheme, we simultaneously conduct a greedy
variable selection procedure such that we can update the classification model
with all labeled training subjects. The proposed algorithm repeatedly performs
both subject and variable selection steps until a prefixed stopping criterion
is reached. Our numerical results show that the proposed procedure has
competitive performance, with smaller training size and a more compact model,
comparing with that of the classifier trained with all variables and a full
data set. We also apply the proposed procedure to a well-known wave data set
(Breiman et al., 1984) to confirm the performance of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsu_H/0/1/0/all/0/1&quot;&gt;Hsiang-Ling Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Chin Ivan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ray-Bing Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00255">
<title>A Nonparametric Delayed Feedback Model for Conversion Rate Prediction. (arXiv:1802.00255v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00255</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting conversion rates (CVRs) in display advertising (e.g., predicting
the proportion of users who purchase an item (i.e., a conversion) after its
corresponding ad is clicked) is important when measuring the effects of ads
shown to users and to understanding the interests of the users. There is
generally a time delay (i.e., so-called {\it delayed feedback}) between the ad
click and conversion. Owing to the delayed feedback, samples that are converted
after an observation period may be treated as negative. To overcome this
drawback, CVR prediction assuming that the time delay follows an exponential
distribution has been proposed. In practice, however, there is no guarantee
that the delay is generated from the exponential distribution, and the best
distribution with which to represent the delay depends on the data. In this
paper, we propose a nonparametric delayed feedback model for CVR prediction
that represents the distribution of the time delay without assuming a
parametric distribution, such as an exponential or Weibull distribution.
Because the distribution of the time delay is modeled depending on the content
of an ad and the features of a user, various shapes of the distribution can be
represented potentially. In experiments, we show that the proposed model can
capture the distribution for the time delay on a synthetic dataset, even when
the distribution is complicated. Moreover, on a real dataset, we show that the
proposed model outperforms the existing method that assumes an exponential
distribution for the time delay in terms of conversion rate prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_Y/0/1/0/all/0/1&quot;&gt;Yuya Yoshikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imai_Y/0/1/0/all/0/1&quot;&gt;Yusaku Imai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00280">
<title>Online optimal exact identification of a quantum change point. (arXiv:1802.00280v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1802.00280</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider online detection strategies for identifying a change point in a
stream of quantum particles allegedly prepared in identical states. We show
that the identification of the change point can be done without error via
sequential local measurements while attaining the optimal performance bound set
by quantum mechanics. In this way, we establish the task of exactly identifying
a quantum change point as an instance where local protocols are as powerful as
global ones. The optimal online detection strategy requires only one bit of
memory between subsequent measurements, and it is amenable to experimental
realization with current technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sentis_G/0/1/0/all/0/1&quot;&gt;Gael Sent&amp;#xed;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Martinez_Vargas_E/0/1/0/all/0/1&quot;&gt;Esteban Mart&amp;#xed;nez-Vargas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Munoz_Tapia_R/0/1/0/all/0/1&quot;&gt;Ramon Mu&amp;#xf1;oz-Tapia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00430">
<title>Linearized Binary Regression. (arXiv:1802.00430v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00430</link>
<description rdf:parseType="Literal">&lt;p&gt;Probit regression was first proposed by Bliss in 1934 to study mortality
rates of insects. Since then, an extensive body of work has analyzed and used
probit or related binary regression methods (such as logistic regression) in
numerous applications and fields. This paper provides a fresh angle to such
well-established binary regression methods. Concretely, we demonstrate that
linearizing the probit model in combination with linear estimators performs on
par with state-of-the-art nonlinear regression methods, such as posterior mean
or maximum aposteriori estimation, for a broad range of real-world regression
problems. We derive exact, closed-form, and nonasymptotic expressions for the
mean-squared error of our linearized estimators, which clearly separates them
from nonlinear regression methods that are typically difficult to analyze. We
showcase the efficacy of our methods and results for a number of synthetic and
real-world datasets, which demonstrates that linearized binary regression finds
potential use in a variety of inference, estimation, signal processing, and
machine learning applications that deal with binary-valued observations or
measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lan_A/0/1/0/all/0/1&quot;&gt;Andrew S. Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chiang_M/0/1/0/all/0/1&quot;&gt;Mung Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Studer_C/0/1/0/all/0/1&quot;&gt;Christoph Studer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.05684">
<title>NYTRO: When Subsampling Meets Early Stopping. (arXiv:1510.05684v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1510.05684</link>
<description rdf:parseType="Literal">&lt;p&gt;Early stopping is a well known approach to reduce the time complexity for
performing training and model selection of large scale learning machines. On
the other hand, memory/space (rather than time) complexity is the main
constraint in many applications, and randomized subsampling techniques have
been proposed to tackle this issue. In this paper we ask whether early stopping
and subsampling ideas can be combined in a fruitful way. We consider the
question in a least squares regression setting and propose a form of randomized
iterative regularization based on early stopping and subsampling. In this
context, we analyze the statistical and computational properties of the
proposed method. Theoretical results are complemented and validated by a
thorough experimental analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Angles_T/0/1/0/all/0/1&quot;&gt;Tomas Angles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Camoriano_R/0/1/0/all/0/1&quot;&gt;Raffaello Camoriano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.06972">
<title>Measuring Sample Quality with Diffusions. (arXiv:1611.06972v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.06972</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard Markov chain Monte Carlo diagnostics, like effective sample size,
are ineffective for biased sampling procedures that sacrifice asymptotic
correctness for computational speed. Recent work addresses this issue for a
class of strongly log-concave target distributions by constructing a computable
discrepancy measure based on Stein&apos;s method that provably determines
convergence to the target. We generalize this approach to cover any target with
a fast-coupling Ito diffusion by bounding the derivatives of Stein equation
solutions in terms of Markov process coupling times. As example applications,
we develop computable and convergence-determining diffusion Stein discrepancies
for log-concave, heavy-tailed, and multimodal targets and use these quality
measures to select the hyperparameters of biased samplers, compare random and
deterministic quadrature rules, and quantify bias-variance tradeoffs in
approximate Markov chain Monte Carlo. Our explicit multivariate Stein factor
bounds may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorham_J/0/1/0/all/0/1&quot;&gt;Jackson Gorham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duncan_A/0/1/0/all/0/1&quot;&gt;Andrew B. Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vollmer_S/0/1/0/all/0/1&quot;&gt;Sebastian J. Vollmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08431">
<title>Boundary-Seeking Generative Adversarial Networks. (arXiv:1702.08431v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08431</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) are a learning framework that rely on
training a discriminator to estimate a measure of difference between a target
and generated distributions. GANs, as normally formulated, rely on the
generated samples being completely differentiable w.r.t. the generative
parameters, and thus do not work for discrete data. We introduce a method for
training GANs with discrete data that uses the estimated difference measure
from the discriminator to compute importance weights for generated samples,
thus providing a policy gradient for training the generator. The importance
weights have a strong connection to the decision boundary of the discriminator,
and we call our method boundary-seeking GANs (BGANs). We demonstrate the
effectiveness of the proposed algorithm with discrete image and character-based
natural language generation. In addition, the boundary-seeking objective
extends to continuous data, which can be used to improve stability of training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jacob_A/0/1/0/all/0/1&quot;&gt;Athul Paul Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Che_T/0/1/0/all/0/1&quot;&gt;Tong Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trischler_A/0/1/0/all/0/1&quot;&gt;Adam Trischler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.03636">
<title>Energy Propagation in Deep Convolutional Neural Networks. (arXiv:1704.03636v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1704.03636</link>
<description rdf:parseType="Literal">&lt;p&gt;Many practical machine learning tasks employ very deep convolutional neural
networks. Such large depths pose formidable computational challenges in
training and operating the network. It is therefore important to understand how
fast the energy contained in the propagated signals (a.k.a. feature maps)
decays across layers. In addition, it is desirable that the feature extractor
generated by the network be informative in the sense of the only signal mapping
to the all-zeros feature vector being the zero input signal. This &quot;trivial
null-set&quot; property can be accomplished by asking for &quot;energy conservation&quot; in
the sense of the energy in the feature vector being proportional to that of the
corresponding input signal. This paper establishes conditions for energy
conservation (and thus for a trivial null-set) for a wide class of deep
convolutional neural network-based feature extractors and characterizes
corresponding feature map energy decay rates. Specifically, we consider general
scattering networks employing the modulus non-linearity and we find that under
mild analyticity and high-pass conditions on the filters (which encompass,
inter alia, various constructions of Weyl-Heisenberg filters, wavelets,
ridgelets, ($\alpha$)-curvelets, and shearlets) the feature map energy decays
at least polynomially fast. For broad families of wavelets and Weyl-Heisenberg
filters, the guaranteed decay rate is shown to be exponential. Moreover, we
provide handy estimates of the number of layers needed to have at least
$((1-\varepsilon)\cdot 100)\%$ of the input signal energy be contained in the
feature vector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiatowski_T/0/1/0/all/0/1&quot;&gt;Thomas Wiatowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grohs_P/0/1/0/all/0/1&quot;&gt;Philipp Grohs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1&quot;&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05978">
<title>Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely Regularized Optimization. (arXiv:1708.05978v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05978</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a wide range of regularized stochastic minimization problems with
two regularization terms, one of which is composed with a linear function. This
optimization model abstracts a number of important applications in artificial
intelligence and machine learning, such as fused Lasso, fused logistic
regression, and a class of graph-guided regularized minimization. The
computational challenges of this model are in two folds. On one hand, the
closed-form solution of the proximal mapping associated with the composed
regularization term or the expected objective function is not available. On the
other hand, the calculation of the full gradient of the expectation in the
objective is very expensive when the number of input data samples is
considerably large. To address these issues, we propose a stochastic variant of
extra-gradient type methods, namely \textsf{Stochastic Primal-Dual Proximal
ExtraGradient descent (SPDPEG)}, and analyze its convergence property for both
convex and strongly convex objectives. For general convex objectives, the
uniformly average iterates generated by \textsf{SPDPEG} converge in expectation
with $O(1/\sqrt{t})$ rate. While for strongly convex objectives, the uniformly
and non-uniformly average iterates generated by \textsf{SPDPEG} converge with
$O(\log(t)/t)$ and $O(1/t)$ rates, respectively. The order of the rate of the
proposed algorithm is known to match the best convergence rate for first-order
stochastic algorithms. Experiments on fused logistic regression and
graph-guided regularized logistic regression problems show that the proposed
algorithm performs very efficiently and consistently outperforms other
competing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianyi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Linbo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Teng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bofeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.01604">
<title>Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. (arXiv:1709.01604v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.01604</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms, when applied to sensitive data, pose a distinct
threat to privacy. A growing body of prior work demonstrates that models
produced by these algorithms may leak specific private information in the
training data to an attacker, either through the models&apos; structure or their
observable behavior. However, the underlying cause of this privacy risk is not
well understood beyond a handful of anecdotal accounts that suggest overfitting
and influence might play a role.
&lt;/p&gt;
&lt;p&gt;This paper examines the effect that overfitting and influence have on the
ability of an attacker to learn information about the training data from
machine learning models, either through training set membership inference or
attribute inference attacks. Using both formal and empirical analyses, we
illustrate a clear relationship between these factors and the privacy risk that
arises in several popular machine learning algorithms. We find that overfitting
is sufficient to allow an attacker to perform membership inference and, when
the target attribute meets certain conditions about its influence, attribute
inference attacks. Interestingly, our formal analysis also shows that
overfitting is not necessary for these attacks and begins to shed light on what
other factors may be in play. Finally, we explore the connection between
membership inference and attribute inference, showing that there are deep
connections between the two that lead to effective new attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_S/0/1/0/all/0/1&quot;&gt;Samuel Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1&quot;&gt;Matt Fredrikson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomelli_I/0/1/0/all/0/1&quot;&gt;Irene Giacomelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Somesh Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10470">
<title>Attention-Based Models for Text-Dependent Speaker Verification. (arXiv:1710.10470v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10470</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention-based models have recently shown great performance on a range of
tasks, such as speech recognition, machine translation, and image captioning
due to their ability to summarize relevant information that expands through the
entire length of an input sequence. In this paper, we analyze the usage of
attention mechanisms to the problem of sequence summarization in our end-to-end
text-dependent speaker recognition system. We explore different topologies and
their variants of the attention layer, and compare different pooling methods on
the attention weights. Ultimately, we show that attention-based models can
improves the Equal Error Rate (EER) of our speaker verification system by
relatively 14% compared to our non-attention LSTM baseline model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chowdhury_F/0/1/0/all/0/1&quot;&gt;F A Rezaur Rahman Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1&quot;&gt;Ignacio Lopez Moreno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Li Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11586">
<title>Toward Multimodal Image-to-Image Translation. (arXiv:1711.11586v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11586</link>
<description rdf:parseType="Literal">&lt;p&gt;Many image-to-image translation problems are ambiguous, as a single input
image may correspond to multiple possible outputs. In this work, we aim to
model a \emph{distribution} of possible outputs in a conditional generative
modeling setting. The ambiguity of the mapping is distilled in a
low-dimensional latent vector, which can be randomly sampled at test time. A
generator learns to map the given input, combined with this latent code, to the
output. We explicitly encourage the connection between output and the latent
code to be invertible. This helps prevent a many-to-one mapping from the latent
code to the output during training, also known as the problem of mode collapse,
and produces more diverse results. We explore several variants of this approach
by employing different training objectives, network architectures, and methods
of injecting the latent code. Our proposed method encourages bijective
consistency between the latent encoding and output modes. We present a
systematic comparison of our method and other variants on both perceptual
realism and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1&quot;&gt;Oliver Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02195">
<title>Fast spatial inference in the homogeneous Ising model. (arXiv:1712.02195v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02195</link>
<description rdf:parseType="Literal">&lt;p&gt;The Ising model is important in statistical modeling and inference in many
applications, however its normalizing constant, mean number of active vertices
and mean spin interaction are intractable. We provide accurate approximations
that make it possible to calculate these quantities numerically. Simulation
studies indicate good performance when compared to Markov Chain Monte Carlo
methods and at a tiny fraction of the time. The methodology is also used to
perform Bayesian inference in a functional Magnetic Resonance Imaging
activation detection experiment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murua_A/0/1/0/all/0/1&quot;&gt;Alejandro Murua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1&quot;&gt;Ranjan Maitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01587">
<title>SpectralNet: Spectral Clustering using Deep Neural Networks. (arXiv:1801.01587v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is a leading and popular technique in unsupervised data
analysis. Two of its major limitations are scalability and generalization of
the spectral embedding (i.e., out-of-sample-extension). In this paper we
introduce a deep learning approach to spectral clustering that overcomes the
above shortcomings. Our network, which we call SpectralNet, learns a map that
embeds input data points into the eigenspace of their associated graph
Laplacian matrix and subsequently clusters them. We train SpectralNet using a
procedure that involves constrained stochastic optimization. Stochastic
optimization allows it to scale to large datasets, while the constraints, which
are implemented using a special-purpose output layer, allow us to keep the
network output orthogonal. Moreover, the map learned by SpectralNet naturally
generalizes the spectral embedding to unseen data points. To further improve
the quality of the clustering, we replace the standard pairwise Gaussian
affinities with affinities leaned from unlabeled data using a Siamese network.
Additional improvement can be achieved by applying the network to code
representations produced, e.g., by standard autoencoders. Our end-to-end
learning procedure is fully unsupervised. In addition, we apply VC dimension
theory to derive a lower bound on the size of SpectralNet. State-of-the-art
clustering results are reported on the Reuters dataset. Our implementation is
publicly available at https://github.com/kstant0725/SpectralNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Henry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nadler_B/0/1/0/all/0/1&quot;&gt;Boaz Nadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basri_R/0/1/0/all/0/1&quot;&gt;Ronen Basri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04987">
<title>On the Complexity of the Weighted Fused Lasso. (arXiv:1801.04987v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04987</link>
<description rdf:parseType="Literal">&lt;p&gt;The solution path of the 1D fused lasso for an $n$-dimensional input is
piecewise linear with $\mathcal{O}(n)$ segments (Hoefling et al. 2010 and
Tibshirani et al 2011). However, existing proofs of this bound do not hold for
the weighted fused lasso. At the same time, results for the generalized lasso,
of which the weighted fused lasso is a special case, allow $\Omega(3^n)$
segments (Mairal et al. 2012). In this paper, we prove that the number of
segments in the solution path of the weighted fused lasso is
$\mathcal{O}(n^3)$, and for some instances $\Omega(n^2)$. We also give a new,
very simple, proof of the $\mathcal{O}(n)$ bound for the fused lasso.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bento_J/0/1/0/all/0/1&quot;&gt;Jose Bento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1&quot;&gt;Surjyendu Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06934">
<title>On the Iteration Complexity Analysis of Stochastic Primal-Dual Hybrid Gradient Approach with High Probability. (arXiv:1801.06934v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06934</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a stochastic Primal-Dual Hybrid Gradient (PDHG)
approach for solving a wide spectrum of regularized stochastic minimization
problems, where the regularization term is composite with a linear function. It
has been recognized that solving this kind of problem is challenging since the
closed-form solution of the proximal mapping associated with the regularization
term is not available due to the imposed linear composition, and the
per-iteration cost of computing the full gradient of the expected objective
function is extremely high when the number of input data samples is
considerably large.
&lt;/p&gt;
&lt;p&gt;Our new approach overcomes these issues by exploring the special structure of
the regularization term and sampling a few data points at each iteration.
Rather than analyzing the convergence in expectation, we provide the detailed
iteration complexity analysis for the cases of both uniformly and non-uniformly
averaged iterates with high probability. This strongly supports the good
practical performance of the proposed approach. Numerical experiments
demonstrate that the efficiency of stochastic PDHG, which outperforms other
competing algorithms, as expected by the high-probability convergence analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Linbo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianyi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Q/0/1/0/all/0/1&quot;&gt;Qi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xicheng Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09834">
<title>Mixture Proportion Estimation for Positive--Unlabeled Learning via Classifier Dimension Reduction. (arXiv:1801.09834v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09834</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive--unlabeled (PU) learning considers two samples, a positive set $P$
with observations from only one class and an unlabeled set $U$ with
observations from two classes. The goal is to classify observations in $U$.
Class mixture proportion estimation (MPE) in $U$ is a key step in PU learning.
In this paper, we show that PU learning is a generalization of local False
Discovery Rate estimation. Further we show that PU learning MPE can be reduced
to a one--dimensional problem via construction of a classifier trained on the
$P$ and $U$ data sets. These observations enable application of methodology
from the multiple testing literature to the PU learning problem. In particular
we adapt ideas from Storey [2002] and Patra and Sen [2015] to address parameter
identifiability and MPE. We prove consistency of two mixture proportion
estimators using bounds from empirical process theory, develop tuning parameter
free implementations, and demonstrate that they have competitive performance on
simulated waveform data and a protein signaling problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;James P. Long&lt;/a&gt;</dc:creator>
</item></rdf:RDF>