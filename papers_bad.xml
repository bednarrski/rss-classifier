<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07802"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07999"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.00591"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.08127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07777"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07805"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07882"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07995"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08052"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08187"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08219"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08299"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08606"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08607"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.04414"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05714"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07870"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07933"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08130"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08154"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08376"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08438"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08450"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08472"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.09658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.05148"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.08140"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02251"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05693"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04577"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.07802">
<title>Value-aware Quantization for Training and Inference of Neural Networks. (arXiv:1804.07802v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.07802</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel value-aware quantization which applies aggressively
reduced precision to the majority of data while separately handling a small
amount of large data in high precision, which reduces total quantization errors
under very low precision. We present new techniques to apply the proposed
quantization to training and inference. The experiments show that our method
with 3-bit activations (with 2% of large ones) can give the same training
accuracy as full-precision one while offering significant (41.6% and 53.7%)
reductions in the memory cost of activations in ResNet-152 and Inception-v3
compared with the state-of-the-art method. Our experiments also show that deep
networks such as Inception-v3, ResNet-101 and DenseNet-121 can be quantized for
inference with 4-bit weights and activations (with 1% 16-bit data) within 1%
top-1 accuracy drop.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1&quot;&gt;Peter Vajda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07887">
<title>A Cell-Division Search Technique for Inversion with Application to Picture-Discovery and Magnetotellurics. (arXiv:1804.07887v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.07887</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving inverse problems in natural sciences often requires a search pro-
cess to find explanatory models that match collected field data. Inverse
problems are often under-determined meaning that there are many poten- tial
explanatory models for the same data. In such cases using stochastic search,
through providing multiple solutions, can help characterise which model
features that are most persistent and therefore likely to be real.
Unfortunately, in some fields, large parameter spaces can make stochas- tic
search intractable. In this work we improve upon previous work by defining a
compact and expressive representation and search process able to describe and
discover two and three dimensional spatial models. The search process takes
place in stages starting with greedy search, followed by alternating stages of
evolutionary search and a novel model-splitting process inspired by
cell-division. We apply this framework to two prob- lems - magnetotellurics and
picture discovery. We show that our improved representation and search process
is able to produce detailed models with low error residuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_B/0/1/0/all/0/1&quot;&gt;Bradley Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yang Heng Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07999">
<title>Swarm Intelligence: Past, Present and Future. (arXiv:1804.07999v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.07999</link>
<description rdf:parseType="Literal">&lt;p&gt;Many optimization problems in science and engineering are challenging to
solve, and the current trend is to use swarm intelligence (SI) and SI-based
algorithms to tackle such challenging problems. Some significant developments
have been made in recent years, though there are still many open problems in
this area. This paper provides a short but timely analysis about SI-based
algorithms and their links with self-organization. Different characteristics
and properties are analyzed here from both mathematical and qualitative
perspectives. Future research directions are outlined and open questions are
also highlighted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin-She Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_S/0/1/0/all/0/1&quot;&gt;Suash Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fong_S/0/1/0/all/0/1&quot;&gt;Simon Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xingshi He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08150">
<title>Deep Learning in Spiking Neural Networks. (arXiv:1804.08150v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.08150</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning approaches have shown remarkable performance in many areas of
pattern recognition recently. In spite of their power in hierarchical feature
extraction and classification, this type of neural network is computationally
expensive and difficult to implement on hardware for portable devices. In an
other vein of research on neural network architectures, spiking neural networks
(SNNs) have been described as power-efficient models because of their sparse,
spike-based communication framework. SNNs are brain-inspired such that they
seek to mimic the accurate and efficient functionality of the brain. Recent
studies try to take advantages of the both frameworks (deep learning and SNNs)
to develop a deep architecture of SNNs to achieve high performance of recently
proved deep networks while implementing bio-inspired, power-efficient
platforms. Additionally, As the brain process different stimuli patterns
through multi-layer SNNs that are communicating by spike trains via adaptive
synapses, developing artificial deep SNNs can also be very helpful for
understudying the computations done by biological neural circuits. Having both
computational and experimental backgrounds, we are interested in including a
comprehensive summary of recent advances in developing deep SNNs that may
assist computer scientists interested in developing more advanced and efficient
networks and help experimentalists to frame new hypotheses for neural
information processing in the brain using a more realistic model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavanaei_A/0/1/0/all/0/1&quot;&gt;Amirhossein Tavanaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghodrati_M/0/1/0/all/0/1&quot;&gt;Masoud Ghodrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1&quot;&gt;Saeed Reza Kheradpisheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timothee Masquelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony S. Maida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.00591">
<title>The geometry of learning. (arXiv:1605.00591v3 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1605.00591</link>
<description rdf:parseType="Literal">&lt;p&gt;We establish a correspondence between Pavlovian conditioning processes and
fractals. The association strength at a training trial corresponds to a point
in a disconnected set at a given iteration level. In this way, one can
represent a training process as a hopping on a fractal set, instead of the
traditional learning curve as a function of the trial. The main advantage of
this novel perspective is to provide an elegant classification of associative
theories in terms of the geometric features of fractal sets. In particular, the
dimension of fractals can measure the efficiency of conditioning models. We
illustrate the correspondence with the examples of the Hull, Rescorla-Wagner,
and Mackintosh models and show that they are equivalent to a Cantor set. More
generally, conditioning programs are described by the geometry of their
associated fractal, which gives much more information than just its dimension.
We show this in several examples of random fractals and also comment on a
possible relation between our formalism and other &quot;fractal&quot; findings in the
cognitive literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Calcagni_G/0/1/0/all/0/1&quot;&gt;Gianluca Calcagni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.08127">
<title>RIOT: a Stochastic-based Method for Workflow Scheduling in the Cloud. (arXiv:1708.08127v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1708.08127</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud computing provides engineers or scientists a place to run complex
computing tasks. Finding a workflow&apos;s deployment configuration in a cloud
environment is not easy. Traditional workflow scheduling algorithms were based
on some heuristics, e.g. reliability greedy, cost greedy, cost-time balancing,
etc., or more recently, the meta-heuristic methods, such as genetic algorithms.
These methods are very slow and not suitable for rescheduling in the dynamic
cloud environment. This paper introduces RIOT (Randomized Instance Order
Types), a stochastic based method for workflow scheduling. RIOT groups the
tasks in the workflow into virtual machines via a probability model and then
uses an effective surrogate-based method to assess a large amount of potential
scheduling. Experiments in dozens of study cases showed that RIOT executes tens
of times faster than traditional methods while generating comparable results to
other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianfeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1&quot;&gt;Tim Menzies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06834">
<title>Neural Network Based Reinforcement Learning for Audio-Visual Gaze Control in Human-Robot Interaction. (arXiv:1711.06834v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06834</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel neural network-based reinforcement learning
approach for robot gaze control. Our approach enables a robot to learn and to
adapt its gaze control strategy for human-robot interaction neither with the
use of external sensors nor with human supervision. The robot learns to focus
its attention onto groups of people from its own audio-visual experiences,
independently of the number of people, of their positions and of their physical
appearances. In particular, we use a recurrent neural network architecture in
combination with Q-learning to find an optimal action-selection policy; we
pre-train the network using a simulated environment that mimics realistic
scenarios that involve speaking/silent participants, thus avoiding the need of
tedious sessions of a robot interacting with people. Our experimental
evaluation suggests that the proposed method is robust against parameter
estimation, i.e. the parameter values yielded by the method do not have a
decisive impact on the performance. The best results are obtained when both
audio and visual information is jointly used. Experiments with the Nao robot
indicate that our framework is a step forward towards the autonomous learning
of socially acceptable gaze behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masse_B/0/1/0/all/0/1&quot;&gt;Benoit Mass&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesejo_P/0/1/0/all/0/1&quot;&gt;Pablo Mesejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03294">
<title>A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers. (arXiv:1804.03294v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03294</link>
<description rdf:parseType="Literal">&lt;p&gt;Weight pruning methods for deep neural networks (DNNs) have been investigated
recently, but prior work in this area is mainly heuristic, iterative pruning,
thereby lacking guarantees on the weight reduction ratio and convergence time.
To mitigate these limitations, we present a systematic weight pruning framework
of DNNs using the alternating direction method of multipliers (ADMM). We first
formulate the weight pruning problem of DNNs as a nonconvex optimization
problem with combinatorial constraints specifying the sparsity requirements,
and then adopt the ADMM framework for systematic weight pruning. By using ADMM,
the original nonconvex optimization problem is decomposed into two subproblems
that are solved iteratively. One of these subproblems can be solved using
stochastic gradient descent, while the other can be solved analytically. The
proposed ADMM weight pruning method incurs no additional suboptimality besides
that resulting from the nonconvex nature of the original optimization problem.
Furthermore, our approach achieves a fast convergence rate.
&lt;/p&gt;
&lt;p&gt;The weight pruning results are very promising and consistently outperform
prior work. On the LeNet-5 model for the MNIST data set, we achieve 40.2 times
weight reduction without accuracy loss. On the AlexNet model for the ImageNet
data set, we achieve 20 times weight reduction without accuracy loss. When we
focus on the convolutional layer pruning for computation reductions, we can
reduce the total computation by five times compared with prior work (achieving
a total of 13.4 times weight reduction in convolutional layers). A significant
acceleration for DNN training is observed as well, in that we can finish the
whole training process on AlexNet around 80 hours. Our models are released at
https://github.com/KaiqiZhang/admm-pruning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Shaokai Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wujie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fardad_M/0/1/0/all/0/1&quot;&gt;Makan Fardad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07758">
<title>Mapping Images to Psychological Similarity Spaces Using Neural Networks. (arXiv:1804.07758v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07758</link>
<description rdf:parseType="Literal">&lt;p&gt;The cognitive framework of conceptual spaces bridges the gap between symbolic
and subsymbolic AI by proposing an intermediate conceptual layer where
knowledge is represented geometrically. There are two main approaches for
obtaining the dimensions of this conceptual similarity space: using similarity
ratings from psychological experiments and using machine learning techniques.
In this paper, we propose a combination of both approaches by using
psychologically derived similarity ratings to constrain the machine learning
process. This way, a mapping from stimuli to conceptual spaces can be learned
that is both supported by psychological data and allows generalization to
unseen stimuli. The results of a first feasibility study support our proposed
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechberger_L/0/1/0/all/0/1&quot;&gt;Lucas Bechberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kypridemou_E/0/1/0/all/0/1&quot;&gt;Elektra Kypridemou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07777">
<title>The Statistical Model for Ticker, an Adaptive Single-Switch Text-Entry Method for Visually Impaired Users. (arXiv:1804.07777v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.07777</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the statistical model for Ticker [1], a novel
probabilistic stereophonic single-switch text entry method for
visually-impaired users with motor disabilities who rely on single-switch
scanning systems to communicate. All terminology and notation are defined in
[1].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nel_E/0/1/0/all/0/1&quot;&gt;Emli-Mari Nel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristensson_P/0/1/0/all/0/1&quot;&gt;Per Ola Kristensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacKay_D/0/1/0/all/0/1&quot;&gt;David J.C. MacKay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07779">
<title>PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making. (arXiv:1804.07779v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07779</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning and symbolic planning have both been used to build
intelligent autonomous agents. Reinforcement learning relies on learning from
interactions with real world, which often requires an unfeasibly large amount
of experience. Symbolic planning relies on manually crafted symbolic knowledge,
which may not be robust to domain uncertainties and changes. In this paper we
present a unified framework {\em PEORL} that integrates symbolic planning with
hierarchical reinforcement learning (HRL) to cope with decision-making in a
dynamic environment with uncertainties.
&lt;/p&gt;
&lt;p&gt;Symbolic plans are used to guide the agent&apos;s task execution and learning, and
the learned experience is fed back to symbolic knowledge to improve planning.
This method leads to rapid policy search and robust symbolic plans in complex
domains. The framework is tested on benchmark domains of HRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fangkai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1&quot;&gt;Daoming Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gustafson_S/0/1/0/all/0/1&quot;&gt;Steven Gustafson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07805">
<title>Inseparability and Conservative Extensions of Description Logic Ontologies: A Survey. (arXiv:1804.07805v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.07805</link>
<description rdf:parseType="Literal">&lt;p&gt;The question whether an ontology can safely be replaced by another, possibly
simpler, one is fundamental for many ontology engineering and maintenance
tasks. It underpins, for example, ontology versioning, ontology modularization,
forgetting, and knowledge exchange. What safe replacement means depends on the
intended application of the ontology. If, for example, it is used to query
data, then the answers to any relevant ontology-mediated query should be the
same over any relevant data set; if, in contrast, the ontology is used for
conceptual reasoning, then the entailed subsumptions between concept
expressions should coincide. This gives rise to different notions of ontology
inseparability such as query inseparability and concept inseparability, which
generalize corresponding notions of conservative extensions. We survey results
on various notions of inseparability in the context of description logic
ontologies, discussing their applications, useful model-theoretic
characterizations, algorithms for determining whether two ontologies are
inseparable (and, sometimes, for computing the difference between them if they
are not), and the computational complexity of this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botoeva_E/0/1/0/all/0/1&quot;&gt;Elena Botoeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konev_B/0/1/0/all/0/1&quot;&gt;Boris Konev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lutz_C/0/1/0/all/0/1&quot;&gt;Carsten Lutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryzhikov_V/0/1/0/all/0/1&quot;&gt;Vladislav Ryzhikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolter_F/0/1/0/all/0/1&quot;&gt;Frank Wolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakharyaschev_M/0/1/0/all/0/1&quot;&gt;Michael Zakharyaschev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07882">
<title>Dynamic Ensemble Selection VS K-NN: why and when Dynamic Selection obtains higher classification performance?. (arXiv:1804.07882v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.07882</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple classifier systems focus on the combination of classifiers to obtain
better performance than a single robust one. These systems unfold three major
phases: pool generation, selection and integration. One of the most promising
MCS approaches is Dynamic Selection (DS), which relies on finding the most
competent classifier or ensemble of classifiers to predict each test sample.
The majority of the DS techniques are based on the K-Nearest Neighbors (K-NN)
definition, and the quality of the neighborhood has a huge impact on the
performance of DS methods. In this paper, we perform an analysis comparing the
classification results of DS techniques and the K-NN classifier under different
conditions. Experiments are performed on 18 state-of-the-art DS techniques over
30 classification datasets and results show that DS methods present a
significant boost in classification accuracy even though they use the same
neighborhood as the K-NN. The reasons behind the outperformance of DS
techniques over the K-NN classifier reside in the fact that DS techniques can
deal with samples with a high degree of instance hardness (samples that are
located close to the decision border) as opposed to the K-NN. In this paper,
not only we explain why DS techniques achieve higher classification performance
than the K-NN but also when DS should be used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zakane_H/0/1/0/all/0/1&quot;&gt;Hiba H. Zakane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cavalcanti_G/0/1/0/all/0/1&quot;&gt;George D. C. Cavalcanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07946">
<title>Extrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons. (arXiv:1804.07946v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.07946</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose post-processing method for enriching not only word representation
but also its vector space using semantic lexicons, which we call extrofitting.
The method consists of 3 steps as follows: (i) Expanding 1 or more dimension(s)
on all the word vectors, filling with their representative value. (ii)
Transferring semantic knowledge by averaging each representative values of
synonyms and filling them in the expanded dimension(s). These two steps make
representations of the synonyms close together. (iii) Projecting the vector
space using Linear Discriminant Analysis, which eliminates the expanded
dimension(s) with semantic knowledge. When experimenting with GloVe, we find
that our method outperforms Faruqui&apos;s retrofitting on some of word similarity
task. We also report further analysis on our method in respect to word vector
dimensions, vocabulary size as well as other well-known pretrained word vectors
(e.g., Word2Vec, Fasttext).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1&quot;&gt;Hwiyeol Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Stanley Jungkyu Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07995">
<title>Global Convergence Analysis of the Flower Pollination Algorithm: A Discrete-Time Markov Chain Approach. (arXiv:1804.07995v1 [math.OC])</title>
<link>http://arxiv.org/abs/1804.07995</link>
<description rdf:parseType="Literal">&lt;p&gt;Flower pollination algorithm is a recent metaheuristic algorithm for solving
nonlinear global optimization problems. The algorithm has also been extended to
solve multiobjective optimization with promising results. In this work, we
analyze this algorithm mathematically and prove its convergence properties by
using Markov chain theory. By constructing the appropriate transition
probability for a population of flower pollen and using the homogeneity
property, it can be shown that the constructed stochastic sequences can
converge to the optimal set. Under the two proper conditions for convergence,
it is proved that the simplified flower pollination algorithm can indeed
satisfy these convergence conditions and thus the global convergence of this
algorithm can be guaranteed. Numerical experiments are used to demonstrate that
the flower pollination algorithm can converge quickly in practice and can thus
achieve global optimality efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xingshi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin-She Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Karamanoglu_M/0/1/0/all/0/1&quot;&gt;Mehmet Karamanoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08010">
<title>Multi-modal space structure: a new kind of latent correlation for multi-modal entity resolution. (arXiv:1804.08010v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08010</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal data is becoming more common than before because of big data
issues. Finding the semantically equal or similar objects from different data
sources(called entity resolution) is one of the heart problem of multi-modal
task. Current models for solving this problem usually needs much paired data to
find the latent correlation between multi-modal data, which is of high cost. A
new kind latent correlation is proposed in this article. With the correlation,
multi-modal objects can be uniformly represented in a commonly shard space. A
classifying based model is designed for multi-modal entity resolution task.
With the proposed method, the demand of training data can be decreased much.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qibin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1&quot;&gt;Xingchun Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jianjun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaolei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongmei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08032">
<title>A Channel-based Exact Inference Algorithm for Bayesian Networks. (arXiv:1804.08032v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08032</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a new algorithm for exact Bayesian inference that is
based on a recently proposed compositional semantics of Bayesian networks in
terms of channels. The paper concentrates on the ideas behind this algorithm,
involving a linearisation (`stretching&apos;) of the Bayesian network, followed by a
combination of forward state transformation and backward predicate
transformation, while evidence is accumulated along the way. The performance of
a prototype implementation of the algorithm in Python is briefly compared to a
standard implementation (pgmpy): first results show competitive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_B/0/1/0/all/0/1&quot;&gt;Bart Jacobs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08033">
<title>Learning from the experts: From expert systems to machine learned diagnosis models. (arXiv:1804.08033v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08033</link>
<description rdf:parseType="Literal">&lt;p&gt;Expert diagnostic support systems have been extensively studied. The
practical application of these systems in real-world scenarios have been
somewhat limited due to well-understood shortcomings such as extensibility.
More recently, machine learned models for medical diagnosis have gained
momentum since they can learn and generalize patterns found in very large
datasets like electronic health records. These models also have shortcomings.
In particular, there is no easy way to incorporate prior knowledge from
existing literature or experts. In this paper, we present a method to merge
both approaches by using expert systems as generative models that create
simulated data on which models can be learned. We demonstrate that such a
learned model not only preserve the original properties of the expert systems
but also addresses some of their limitations. Furthermore, we show how this
approach can also be used as the starting point to combine expert knowledge
with knowledge extracted from other data sources such as electronic health
records.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravuri_M/0/1/0/all/0/1&quot;&gt;Murali Ravuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anitha Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tso_G/0/1/0/all/0/1&quot;&gt;Geoffrey Tso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1&quot;&gt;Xavier Amatriain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08052">
<title>HeteroMed: Heterogeneous Information Network for Medical Diagnosis. (arXiv:1804.08052v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08052</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent availability of Electronic Health Records (EHR) and great
opportunities they offer for advancing medical informatics, there has been
growing interest in mining EHR for improving quality of care. Disease diagnosis
due to its sensitive nature, huge costs of error, and complexity has become an
increasingly important focus of research in past years. Existing studies model
EHR by capturing co-occurrence of clinical events to learn their latent
embeddings. However, relations among clinical events carry various semantics
and contribute differently to disease diagnosis which gives precedence to a
more advanced modeling of heterogeneous data types and relations in EHR data
than existing solutions. To address these issues, we represent how
high-dimensional EHR data and its rich relationships can be suitably translated
into HeteroMed, a heterogeneous information network for robust medical
diagnosis. Our modeling approach allows for straightforward handling of missing
values and heterogeneity of data. HeteroMed exploits metapaths to capture
higher level and semantically important relations contributing to disease
diagnosis. Furthermore, it employs a joint embedding framework to tailor
clinical event representations to the disease diagnosis goal. To the best of
our knowledge, this is the first study to use Heterogeneous Information Network
for modeling clinical data and disease diagnosis. Experimental results of our
study show superior performance of HeteroMed compared to prior methods in
prediction of exact diagnosis codes and general disease cohorts. Moreover,
HeteroMed outperforms baseline models in capturing similarities of clinical
events which are examined qualitatively through case studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1&quot;&gt;Anahita Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1&quot;&gt;Majid Sarrafzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08062">
<title>Attenuate Locally, Win Globally: An Attenuation-based Framework for Online Stochastic Matching with Timeouts. (arXiv:1804.08062v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1804.08062</link>
<description rdf:parseType="Literal">&lt;p&gt;Online matching problems have garnered significant attention in recent years
due to numerous applications in e-commerce, online advertisements,
ride-sharing, etc. Many of them capture the uncertainty in the real world by
including stochasticity in both the arrival process and the matching process.
The Online Stochastic Matching with Timeouts problem introduced by Bansal, et
al., (Algorithmica, 2012) models matching markets (e.g., E-Bay, Amazon). Buyers
arrive from an independent and identically distributed (i.i.d.) known
distribution on buyer profiles and can be shown a list of items one at a time.
Each buyer has some probability of purchasing each item and a limit (timeout)
on the number of items they can be shown.
&lt;/p&gt;
&lt;p&gt;Bansal et al., (Algorithmica, 2012) gave a 0.12-competitive algorithm which
was improved by Adamczyk, et al., (ESA, 2015) to 0.24. We present an online
attenuation framework that uses an algorithm for offline stochastic matching as
a black box. On the upper bound side, we show that this framework, combined
with a black-box adapted from Bansal et al., (Algorithmica, 2012), yields an
online algorithm which nearly doubles the ratio to 0.46. On the lower bound
side, we show that no algorithm can achieve a ratio better than 0.632 using the
standard LP for this problem. This framework has a high potential for further
improvements since new algorithms for offline stochastic matching can directly
improve the ratio for the online problem.
&lt;/p&gt;
&lt;p&gt;Our online framework also has the potential for a variety of extensions. For
example, we introduce a natural generalization: Online Stochastic Matching with
Two-sided Timeouts in which both online and offline vertices have timeouts. Our
framework provides the first algorithm for this problem achieving a ratio of
0.30. We once again use the algorithm of Adamczyk et al., (ESA, 2015) as a
black-box and plug-it into our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brubach_B/0/1/0/all/0/1&quot;&gt;Brian Brubach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankararaman_K/0/1/0/all/0/1&quot;&gt;Karthik Abinav Sankararaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1&quot;&gt;Aravind Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08096">
<title>Swarm robotics in wireless distributed protocol design for coordinating robots involved in cooperative tasks. (arXiv:1804.08096v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08096</link>
<description rdf:parseType="Literal">&lt;p&gt;The mine detection in an unexplored area is an optimization problem where
multiple mines, randomly distributed throughout an area, need to be discovered
and disarmed in a minimum amount of time. We propose a strategy to explore an
unknown area, using a stigmergy approach based on ants behavior, and a novel
swarm based protocol to recruit and coordinate robots for disarming the mines
cooperatively. Simulation tests are presented to show the effectiveness of our
proposed Ant-based Task Robot Coordination (ATRC) with only the exploration
task and with both exploration and recruiting strategies. Multiple minimization
objectives have been considered: the robots&apos; recruiting time and the overall
area exploration time. We discuss, through simulation, different cases under
different network and field conditions, performed by the robots. The results
have shown that the proposed decentralized approaches enable the swarm of
robots to perform cooperative tasks intelligently without any central control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rango_F/0/1/0/all/0/1&quot;&gt;F. De Rango&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmieri_N/0/1/0/all/0/1&quot;&gt;N. Palmieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;X.S. Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marano_S/0/1/0/all/0/1&quot;&gt;S. Marano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08187">
<title>Advancing Tabu and Restart in Local Search for Maximum Weight Cliques. (arXiv:1804.08187v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08187</link>
<description rdf:parseType="Literal">&lt;p&gt;The tabu and restart are two fundamental strategies for local search. In this
paper, we improve the local search algorithms for solving the Maximum Weight
Clique (MWC) problem by introducing new tabu and restart strategies. Both the
tabu and restart strategies proposed are based on the notion of a local search
scenario, which involves not only a candidate solution but also the tabu status
and unlocking relationship. Compared to the strategy of configuration checking,
our tabu mechanism discourages forming a cycle of unlocking operations. Our new
restart strategy is based on the re-occurrence of a local search scenario
instead of that of a candidate solution. Experimental results show that the
resulting MWC solver outperforms several state-of-the-art solvers on the
DIMACS, BHOSLIB, and two benchmarks from practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Nan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zongjie Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latecki_L/0/1/0/all/0/1&quot;&gt;Longin Jan Latecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kaile Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08219">
<title>Adapted Performance Assessment For Drivers Through Behavioral Advantage. (arXiv:1804.08219v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08219</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential positive impact of autonomous driving and driver assistance
technolo- gies have been a major impetus over the last decade. On the flip
side, it has been a challenging problem to analyze the performance of human
drivers or autonomous driving agents quantitatively. In this work, we propose a
generic method that compares the performance of drivers or autonomous driving
agents even if the environmental conditions are different, by using the driver
behavioral advantage instead of absolute metrics, which efficiently removes the
environmental factors. A concrete application of the method is also presented,
where the performance of more than 100 truck drivers was evaluated and ranked
in terms of fuel efficiency, covering more than 90,000 trips spanning an
average of 300 miles in a variety of driving conditions and environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1&quot;&gt;Dicong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paga_K/0/1/0/all/0/1&quot;&gt;Karthik Paga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08299">
<title>Representational Issues in the Debate on the Standard Model of the Mind. (arXiv:1804.08299v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08299</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we discuss some of the issues concerning the Memory and Content
aspects in the recent debate on the identification of a Standard Model of the
Mind (Laird, Lebiere, and Rosenbloom in press). In particular, we focus on the
representational models concerning the Declarative Memories of current
Cognitive Architectures (CAs). In doing so we outline some of the main problems
affecting the current CAs and suggest that the Conceptual Spaces, a
representational framework developed by Gardenfors, is worth-considering to
address such problems. Finally, we briefly analyze the alternative
representational assumptions employed in the three CAs constituting the current
baseline for the Standard Model (i.e. SOAR, ACT-R and Sigma). In doing so, we
point out the respective differences and discuss their implications in the
light of the analyzed problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chella_A/0/1/0/all/0/1&quot;&gt;Antonio Chella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frixione_M/0/1/0/all/0/1&quot;&gt;Marcello Frixione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lieto_A/0/1/0/all/0/1&quot;&gt;Antonio Lieto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08473">
<title>Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training. (arXiv:1804.08473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.08473</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic generation of natural language from images has attracted extensive
attention. In this paper, we take one step further to investigate generation of
poetic language (with multiple lines) to an image for automatic poetry
creation. This task involves multiple challenges, including discovering poetic
clues from the image (e.g., hope from green), and generating poems to satisfy
both relevance to the image and poeticness in language level. To solve the
above challenges, we formulate the task of poem generation into two correlated
sub-tasks by multi-adversarial training via policy gradient, through which the
cross-modal relevance and poetic language style can be ensured. To extract
poetic clues from images, we propose to learn a deep coupled visual-poetic
embedding, in which the poetic representation from objects, sentiments and
scenes in an image can be jointly learned. Two discriminative networks are
further introduced to guide the poem generation, including a multi-modal
discriminator and a poem-style discriminator. To facilitate the research, we
have collected two poem datasets by human annotators with two distinct
properties: 1) the first human annotated image-to-poem pair dataset (with 8,292
pairs in total), and 2) to-date the largest public English poem corpus dataset
(with 92,265 different poems in total). Extensive experiments are conducted
with 8K images generated with our model, among which 1.5K image are randomly
picked for evaluation. Both objective and subjective evaluations show the
superior performances against the state-of-art methods for poem generation from
images. Turing test carried out with over 500 human subjects, among which 30
evaluators are poetry experts, demonstrates the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jianlong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Makoto P. Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1&quot;&gt;Masatoshi Yoshikawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08606">
<title>Zero-Shot Visual Imitation. (arXiv:1804.08606v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08606</link>
<description rdf:parseType="Literal">&lt;p&gt;The current dominant paradigm for imitation learning relies on strong
supervision of expert actions to learn both &apos;what&apos; and &apos;how&apos; to imitate. We
pursue an alternative paradigm wherein an agent first explores the world
without any expert supervision and then distills its experience into a
goal-conditioned skill policy with a novel forward consistency loss. In our
framework, the role of the expert is only to communicate the goals (i.e., what
to imitate) during inference. The learned policy is then employed to mimic the
expert (i.e., how to imitate) after seeing just a sequence of images
demonstrating the desired task. Our method is &apos;zero-shot&apos; in the sense that the
agent never has access to expert actions during training or for the task
demonstration at inference. We evaluate our zero-shot imitator in two
real-world settings: complex rope manipulation with a Baxter robot and
navigation in previously unseen office environments with a TurtleBot. Through
further experiments in VizDoom simulation, we provide evidence that better
mechanisms for exploration lead to learning a more capable policy which in turn
improves end task performance. Videos, models, and more details are available
at https://pathak22.github.io/zeroshot-imitation/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoudieh_P/0/1/0/all/0/1&quot;&gt;Parsa Mahmoudieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Guanghao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shentu_Y/0/1/0/all/0/1&quot;&gt;Yide Shentu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1&quot;&gt;Evan Shelhamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08607">
<title>Benchmarking projective simulation in navigation problems. (arXiv:1804.08607v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08607</link>
<description rdf:parseType="Literal">&lt;p&gt;Projective simulation (PS) is a model for intelligent agents with a
deliberation capacity that is based on episodic memory. The model has been
shown to provide a flexible framework for constructing reinforcement-learning
agents, and it allows for quantum mechanical generalization, which leads to a
speed-up in deliberation time. PS agents have been applied successfully in the
context of complex skill learning in robotics, and in the design of
state-of-the-art quantum experiments. In this paper, we study the performance
of projective simulation in two benchmarking problems in navigation, namely the
grid world and the mountain car problem. The performance of PS is compared to
standard tabular reinforcement learning approaches, Q-learning and SARSA. Our
comparison demonstrates that the performance of PS and standard learning
approaches are qualitatively and quantitatively similar, while it is much
easier to choose optimal model parameters in case of projective simulation,
with a reduced computational effort of one to two orders of magnitude. Our
results show that the projective simulation model stands out for its simplicity
in terms of the number of model parameters, which makes it simple to set up the
learning agent in unknown task environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnikov_A/0/1/0/all/0/1&quot;&gt;Alexey A. Melnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makmal_A/0/1/0/all/0/1&quot;&gt;Adi Makmal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briegel_H/0/1/0/all/0/1&quot;&gt;Hans J. Briegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.04414">
<title>The Parallel Knowledge Gradient Method for Batch Bayesian Optimization. (arXiv:1606.04414v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1606.04414</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications of black-box optimization, one can evaluate multiple
points simultaneously, e.g. when evaluating the performances of several
different neural network architectures in a parallel computing environment. In
this paper, we develop a novel batch Bayesian optimization algorithm --- the
parallel knowledge gradient method. By construction, this method provides the
one-step Bayes-optimal batch of points to sample. We provide an efficient
strategy for computing this Bayes-optimal batch of points, and we demonstrate
that the parallel knowledge gradient method finds global optima significantly
faster than previous batch Bayesian optimization algorithms on both synthetic
test functions and when tuning hyperparameters of practical machine learning
algorithms, especially when function evaluations are noisy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frazier_P/0/1/0/all/0/1&quot;&gt;Peter I. Frazier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05714">
<title>A Stronger Foundation for Computer Science and P=NP. (arXiv:1708.05714v2 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05714</link>
<description rdf:parseType="Literal">&lt;p&gt;This article describes a Turing machine which can solve for $\beta^{&apos;}$ which
is RE-complete. RE-complete problems are proven to be undecidable by Turing&apos;s
accepted proof on the Entscheidungsproblem. Thus, constructing a machine which
decides over $\beta^{&apos;}$ implies inconsistency in ZFC. We then discover that
unrestricted use of the axiom of substitution can lead to hidden assumptions in
a certain class of proofs by contradiction. These hidden assumptions create an
implied axiom of incompleteness for ZFC. Later, we offer a restriction on the
axiom of substitution by introducing a new axiom which prevents impredicative
tautologies from producing theorems. Our discovery in regards to these
foundational arguments, disproves the SPACE hierarchy theorem which allows us
to solve the P vs NP problem using a TIME-SPACE equivalence oracle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inman_M/0/1/0/all/0/1&quot;&gt;Mark Inman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02195">
<title>Optimality of Approximate Inference Algorithms on Stable Instances. (arXiv:1711.02195v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02195</link>
<description rdf:parseType="Literal">&lt;p&gt;Approximate algorithms for structured prediction problems---such as LP
relaxations and the popular alpha-expansion algorithm (Boykov et al.
2001)---typically far exceed their theoretical performance guarantees on
real-world instances. These algorithms often find solutions that are very close
to optimal. The goal of this paper is to partially explain the performance of
alpha-expansion and an LP relaxation algorithm on MAP inference in
Ferromagnetic Potts models (FPMs). Our main results give stability conditions
under which these two algorithms provably recover the optimal MAP solution.
These theoretical results complement numerous empirical observations of good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lang_H/0/1/0/all/0/1&quot;&gt;Hunter Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vijayaraghavan_A/0/1/0/all/0/1&quot;&gt;Aravindan Vijayaraghavan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05240">
<title>Weakly-supervised Semantic Parsing with Abstract Examples. (arXiv:1711.05240v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05240</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic parsers translate language utterances to programs, but are often
trained from utterance-denotation pairs only. Consequently, parsers must
overcome the problem of spuriousness at training time, where an incorrect
program found at search time accidentally leads to a correct denotation. We
propose that in small well-typed domains, we can semi-automatically generate an
abstract representation for examples that facilitates information sharing
across examples. This alleviates spuriousness, as the probability of randomly
obtaining a correct answer from a program decreases across multiple examples.
We test our approach on CNLVR, a challenging visual reasoning dataset, where
spuriousness is central because denotations are either TRUE or FALSE, and thus
random programs have high probability of leading to a correct denotation. We
develop the first semantic parser for this task and reach 83.5% accuracy, a
15.7% absolute accuracy improvement compared to the best reported accuracy so
far.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1&quot;&gt;Omer Goldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latcinnik_V/0/1/0/all/0/1&quot;&gt;Veronica Latcinnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naveh_U/0/1/0/all/0/1&quot;&gt;Udi Naveh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1&quot;&gt;Amir Globerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01733">
<title>Entropy production rate as a criterion for inconsistency in decision theory. (arXiv:1801.01733v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01733</link>
<description rdf:parseType="Literal">&lt;p&gt;Individual and group decisions are complex, often involving choosing an apt
alternative from a multitude of options. Evaluating pairwise comparisons breaks
down such complex decision problems into tractable ones. Pairwise comparison
matrices (PCMs) are regularly used to solve multiple-criteria decision-making
(MCDM) problems, for example, using Saaty&apos;s analytic hierarchy process (AHP)
framework. However, there are two significant drawbacks of using PCMs. First,
humans evaluate PCMs in an inconsistent manner. Second, not all entries of a
large PCM can be reliably filled by human decision makers. We address these two
issues by first establishing a novel connection between PCMs and
time-irreversible Markov processes. Specifically, we show that every PCM
induces a family of dissipative maximum path entropy random walks (MERW) over
the set of alternatives. We show that only `consistent&apos; PCMs correspond to
detailed balanced MERWs. We identify the non-equilibrium entropy production in
the induced MERWs as a metric of inconsistency of the underlying PCMs. Notably,
the entropy production satisfies all of the recently laid out criteria for
reasonable consistency indices. We also propose an approach to use incompletely
filled PCMs in AHP. Potential future avenues are discussed as well.
&lt;/p&gt;
&lt;p&gt;keywords: analytic hierarchy process, markov chains, maximum entropy
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixit_P/0/1/0/all/0/1&quot;&gt;Purushottam D. Dixit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04397">
<title>Identifiability of Nonparametric Mixture Models and Bayes Optimal Clustering. (arXiv:1802.04397v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04397</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by problems in data clustering, we establish general conditions
under which families of nonparametric mixture models are identifiable by
introducing a novel framework for clustering overfitted \emph{parametric} (i.e.
misspecified) mixture models. These conditions generalize existing conditions
in the literature, and are flexible enough to include for example mixtures of
Gaussian mixtures. In contrast to the recent literature on estimating
nonparametric mixtures, we allow for general nonparametric mixture components,
and instead impose regularity assumptions on the underlying mixing measure. As
our primary application, we apply these results to partition-based clustering,
generalizing the well-known notion of a Bayes optimal partition from classical
model-based clustering to nonparametric settings. Furthermore, this framework
is constructive in that it yields a practical algorithm for learning identified
mixtures, which is illustrated through several examples. The key conceptual
device in the analysis is the convex, metric geometry of probability
distributions on metric spaces and its connection to optimal transport and the
Wasserstein convergence of mixing measures. The result is a flexible framework
for nonparametric clustering with formal consistency guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Aragam_B/0/1/0/all/0/1&quot;&gt;Bryon Aragam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dan_C/0/1/0/all/0/1&quot;&gt;Chen Dan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06288">
<title>ORGaNICs: A Theory of Working Memory in Brains and Machines. (arXiv:1803.06288v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06288</link>
<description rdf:parseType="Literal">&lt;p&gt;Working memory is a cognitive process that is responsible for temporarily
holding and manipulating information. Most of the empirical neuroscience
research on working memory has focused on measuring sustained activity in
prefrontal cortex (PFC) and/or parietal cortex during simple delayed-response
tasks, and most of the models of working memory have been based on neural
integrators. But working memory means much more than just holding a piece of
information online. We describe a new theory of working memory, based on a
recurrent neural circuit that we call ORGaNICs (Oscillatory Recurrent GAted
Neural Integrator Circuits). ORGaNICs are a variety of Long Short Term Memory
units (LSTMs), imported from machine learning and artificial intelligence.
ORGaNICs can be used to explain the complex dynamics of delay-period activity
in prefrontal cortex (PFC) during a working memory task. The theory is
analytically tractable so that we can characterize the dynamics, and the theory
provides a means for reading out information from the dynamically varying
responses at any point in time, in spite of the complex dynamics. ORGaNICs can
be implemented with a biophysical (electrical circuit) model of pyramidal
cells, combined with shunting inhibition via a thalamocortical loop. Although
introduced as a computational theory of working memory, ORGaNICs are also
applicable to models of sensory processing, motor preparation and motor
control. ORGaNICs offer computational advantages compared to other varieties of
LSTMs that are commonly used in AI applications. Consequently, ORGaNICs are a
framework for canonical computation in brains and machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heeger_D/0/1/0/all/0/1&quot;&gt;David J. Heeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_W/0/1/0/all/0/1&quot;&gt;Wayne E. Mackey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09473">
<title>code2vec: Learning Distributed Representations of Code. (arXiv:1803.09473v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09473</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural model for representing snippets of code as continuous
distributed vectors. The main idea is to represent code as a collection of
paths in its abstract syntax tree, and aggregate these paths, in a smart and
scalable way, into a single fixed-length $\textit{code vector}$, which can be
used to predict semantic properties of the snippet.
&lt;/p&gt;
&lt;p&gt;We demonstrate the effectiveness of our approach by using it to predict a
method&apos;s name from the vector representation of its body. We evaluate our
approach by training a model on a dataset of $14$M methods. We show that code
vectors trained on this dataset can predict method names from files that were
completely unobserved during training. Furthermore, we show that our model
learns useful method name vectors that capture semantic similarities,
combinations, and analogies.
&lt;/p&gt;
&lt;p&gt;Comparing previous techniques over the same data set, our approach obtains a
relative improvement of over $75\%$, being the first to successfully predict
method names based on a large, cross-project, corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1&quot;&gt;Uri Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilberstein_M/0/1/0/all/0/1&quot;&gt;Meital Zilberstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1&quot;&gt;Omer Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1&quot;&gt;Eran Yahav&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01508">
<title>The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. (arXiv:1804.01508v7 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01508</link>
<description rdf:parseType="Literal">&lt;p&gt;Although simple individually, artificial neurons provide state-of-the-art
performance when interconnected in deep networks. Unknown to many, there exists
an arguably even simpler and more versatile learning mechanism, namely, the
Tsetlin Automaton. Merely by means of a single integer as memory, it learns the
optimal action in stochastic environments. In this paper, we introduce the
Tsetlin Machine, which solves complex pattern recognition problems with
easy-to-interpret propositional formulas, composed by a collective of Tsetlin
Automata. To eliminate the longstanding problem of vanishing signal-to-noise
ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our
theoretical analysis establishes that the Nash equilibria of the game are
aligned with the propositional formulas that provide optimal pattern
recognition accuracy. This translates to learning without local optima, only
global ones. We argue that the Tsetlin Machine finds the propositional formula
that provides optimal accuracy, with probability arbitrarily close to unity. In
four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks,
SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It
further turns out that the accuracy advantage of the Tsetlin Machine increases
with lack of data. The Tsetlin Machine has a significant computational
performance advantage since both inputs, patterns, and outputs are expressed as
bits, while recognition of patterns relies on bit manipulation. The combination
of accuracy, interpretability, and computational simplicity makes the Tsetlin
Machine a promising tool for a wide range of domains, including safety-critical
medicine. Being the first of its kind, we believe the Tsetlin Machine will
kick-start completely new paths of research, with a potentially significant
impact on the AI field and the applications of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06748">
<title>State-Space Abstractions for Probabilistic Inference: A Systematic Review. (arXiv:1804.06748v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06748</link>
<description rdf:parseType="Literal">&lt;p&gt;Tasks such as social network analysis, human behavior recognition, or
modeling biochemical reactions, can be solved elegantly by using the
probabilistic inference framework. However, standard probabilistic inference
algorithms work at a propositional level, and thus cannot capture the
symmetries and redundancies that are present in these tasks. Algorithms that
exploit those symmetries have been devised in different research fields, for
example by the lifted inference-, multiple object tracking-, and modeling and
simulation-communities. The common idea, that we call state space abstraction,
is to perform inference over compact representations of sets of symmetric
states. Although they are concerned with a similar topic, the relationship
between these approaches has not been investigated systematically. This survey
provides the following contributions. We perform a systematic literature review
to outline the state of the art in probabilistic inference methods exploiting
symmetries. From an initial set of more than 4,000 papers, we identify 116
relevant papers. Furthermore, we provide new high-level categories that
classify the approaches, based on the problem classes the different approaches
can solve. Researchers from different fields that are confronted with a state
space explosion problem in a probabilistic system can use this classification
to identify possible solutions. Finally, based on this conceptualization, we
identify potentials for future research, as some relevant application domains
are not addressed by current approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludtke_S/0/1/0/all/0/1&quot;&gt;Stefan L&amp;#xfc;dtke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Max Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1&quot;&gt;Frank Kr&amp;#xfc;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bader_S/0/1/0/all/0/1&quot;&gt;Sebastian Bader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirste_T/0/1/0/all/0/1&quot;&gt;Thomas Kirste&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06087">
<title>Rafiki: Machine Learning as an Analytics Service System. (arXiv:1804.06087v1 [cs.DB] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1804.06087</link>
<description rdf:parseType="Literal">&lt;p&gt;Big data analytics is gaining massive momentum in the last few years.
Applying machine learning models to big data has become an implicit requirement
or an expectation for most analysis tasks, especially on high-stakes
applications.Typical applications include sentiment analysis against reviews
for analyzing on-line products, image classification in food logging
applications for monitoring user&apos;s daily intake and stock movement prediction.
Extending traditional database systems to support the above analysis is
intriguing but challenging. First, it is almost impossible to implement all
machine learning models in the database engines. Second, expertise knowledge is
required to optimize the training and inference procedures in terms of
efficiency and effectiveness, which imposes heavy burden on the system users.
In this paper, we develop and present a system, called Rafiki, to provide the
training and inference service of machine learning models, and facilitate
complex analytics on top of cloud platforms. Rafiki provides distributed
hyper-parameter tuning for the training service, and online ensemble modeling
for the inference service which trades off between latency and accuracy.
Experimental results confirm the efficiency, effectiveness, scalability and
usability of Rafiki.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jinyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meihui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1&quot;&gt;Teck Khim Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1&quot;&gt;Beng Chin Ooi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07768">
<title>Sampling the Riemann-Theta Boltzmann Machine. (arXiv:1804.07768v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.07768</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that the visible sector probability density function of the
Riemann-Theta Boltzmann machine corresponds to a gaussian mixture model
consisting of an infinite number of component multi-variate gaussians. The
weights of the mixture are given by a discrete multi-variate gaussian over the
hidden state space. This allows us to sample the visible sector density
function in a straight-forward manner. Furthermore, we show that the visible
sector probability density function possesses an affine transform property,
similar to the multi-variate gaussian density.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carrazza_S/0/1/0/all/0/1&quot;&gt;Stefano Carrazza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krefl_D/0/1/0/all/0/1&quot;&gt;Daniel Krefl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07870">
<title>Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size. (arXiv:1804.07870v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07870</link>
<description rdf:parseType="Literal">&lt;p&gt;A key problem in research on adversarial examples is that vulnerability to
adversarial examples is usually measured by running attack algorithms. Because
the attack algorithms are not optimal, the attack algorithms are prone to
overestimating the size of perturbation needed to fool the target model. In
other words, the attack-based methodology provides an upper-bound on the size
of a perturbation that will fool the model, but security guarantees require a
lower bound. CLEVER is a proposed scoring method to estimate a lower bound.
Unfortunately, an estimate of a bound is not a bound. In this report, we show
that gradient masking, a common problem that causes attack methodologies to
provide only a very loose upper bound, causes CLEVER to overestimate the size
of perturbation needed to fool the model. In other words, CLEVER does not
resolve the key problem with the attack-based methodology, because it fails to
provide a lower bound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07891">
<title>A Deep Learning Approach for Air Pollution Forecasting in South Korea Using Encoder-Decoder Networks &amp; LSTM. (arXiv:1804.07891v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07891</link>
<description rdf:parseType="Literal">&lt;p&gt;Tackling air pollution is an imperative problem in South Korea, especially in
urban areas, over the last few years. More specially, South Korea has joined
the ranks of the world&apos;s most polluted countries alongside with other Asian
capitals, such as Beijing or Delhi. Much research is being conducted in
environmental science to evaluate the dangerous impact of particulate matters
on public health. Besides that, deterministic models of air pollutant behavior
are also generated; however, this is both complex and often inaccurate. On the
contrary, deep recurrent neural network reveals potent potential on forecasting
out-comes of time-series data and has become more prevalent. This paper uses
Recurrent Neural Network (RNN) with Long Short-Term Memory units as a framework
for leveraging knowledge from time-series data of air pollution and
meteorological information in Daegu, Seoul, Beijing, and Shenyang.
Additionally, we use encoder-decoder model, which is similar to machine
comprehension problems, as a crucial part of our prediction machine. Finally,
we investigate the prediction accuracy of various configurations. Our
experiments prevent the efficiency of integrating multiple layers of RNN on
prediction model when forecasting far timesteps ahead. This research is a
significant motivation for not only continuing researching on urban air quality
but also help the government leverage that insight to enact beneficial policies
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1&quot;&gt;Tien-Cuong Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1&quot;&gt;Van-Duc Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1&quot;&gt;Sang-Kyun Cha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07933">
<title>Is feature selection secure against training data poisoning?. (arXiv:1804.07933v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07933</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning in adversarial settings is becoming an important task for
application domains where attackers may inject malicious data into the training
set to subvert normal operation of data-driven technologies. Feature selection
has been widely used in machine learning for security applications to improve
generalization and computational efficiency, although it is not clear whether
its use may be beneficial or even counterproductive when training data are
poisoned by intelligent attackers. In this work, we shed light on this issue by
providing a framework to investigate the robustness of popular feature
selection methods, including LASSO, ridge regression and the elastic net. Our
results on malware detection show that feature selection methods can be
significantly compromised under attack (we can reduce LASSO to almost random
choices of feature sets by careful insertion of less than 5% poisoned training
samples), highlighting the need for specific countermeasures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Huang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1&quot;&gt;Battista Biggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_G/0/1/0/all/0/1&quot;&gt;Gavin Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fumera_G/0/1/0/all/0/1&quot;&gt;Giorgio Fumera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eckert_C/0/1/0/all/0/1&quot;&gt;Claudia Eckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1&quot;&gt;Fabio Roli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07944">
<title>Variational Inference In Pachinko Allocation Machines. (arXiv:1804.07944v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;The Pachinko Allocation Machine (PAM) is a deep topic model that allows
representing rich correlation structures among topics by a directed acyclic
graph over topics. Because of the flexibility of the model, however,
approximate inference is very difficult. Perhaps for this reason, only a small
number of potential PAM architectures have been explored in the literature. In
this paper we present an efficient and flexible amortized variational inference
method for PAM, using a deep inference network to parameterize the approximate
posterior distribution in a manner similar to the variational autoencoder. Our
inference method produces more coherent topics than state-of-art inference
methods for PAM while being an order of magnitude faster, which allows
exploration of a wider range of PAM architectures than have previously been
studied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1&quot;&gt;Charles Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08003">
<title>Stability of the Stochastic Gradient Method for an Approximated Large Scale Kernel Machine. (arXiv:1804.08003v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1804.08003</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we measured the stability of stochastic gradient method (SGM)
for learning an approximated Fourier primal support vector machine. The
stability of an algorithm is considered by measuring the generalization error
in terms of the absolute difference between the test and the training error.
Our problem is to learn an approximated kernel function using random Fourier
features for a binary classification problem via online convex optimization
settings. For a convex, Lipschitz continuous and smooth loss function, given
reasonable number of iterations stochastic gradient method is stable. We showed
that with a high probability SGM generalizes well for an approximated kernel
under given assumptions.We empirically verified the theoretical findings for
different parameters using several data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Samareh_A/0/1/0/all/0/1&quot;&gt;Aven Samareh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parizi_M/0/1/0/all/0/1&quot;&gt;Mahshid Salemi Parizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08066">
<title>MQGrad: Reinforcement Learning of Gradient Quantization in Parameter Server. (arXiv:1804.08066v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08066</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most significant bottleneck in training large scale machine
learning models on parameter server (PS) is the communication overhead, because
it needs to frequently exchange the model gradients between the workers and
servers during the training iterations. Gradient quantization has been proposed
as an effective approach to reducing the communication volume. One key issue in
gradient quantization is setting the number of bits for quantizing the
gradients. Small number of bits can significantly reduce the communication
overhead while hurts the gradient accuracies, and vise versa. An ideal
quantization method would dynamically balance the communication overhead and
model accuracy, through adjusting the number bits according to the knowledge
learned from the immediate past training iterations. Existing methods, however,
quantize the gradients either with fixed number of bits, or with predefined
heuristic rules. In this paper we propose a novel adaptive quantization method
within the framework of reinforcement learning. The method, referred to as
MQGrad, formalizes the selection of quantization bits as actions in a Markov
decision process (MDP) where the MDP states records the information collected
from the past optimization iterations (e.g., the sequence of the loss function
values). During the training iterations of a machine learning algorithm, MQGrad
continuously updates the MDP state according to the changes of the loss
function. Based on the information, MDP learns to select the optimal actions
(number of bits) to quantize the gradients. Experimental results based on a
benchmark dataset showed that MQGrad can accelerate the learning of a large
scale deep neural network while keeping its prediction accuracies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1&quot;&gt;Guoxin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wei Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08130">
<title>Sparse Travel Time Estimation from Streaming Data. (arXiv:1804.08130v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.08130</link>
<description rdf:parseType="Literal">&lt;p&gt;We address two shortcomings in online travel time estimation methods for
congested urban traffic. The first shortcoming is related to the determination
of the number of mixture modes, which can change dynamically, within day and
from day to day. The second shortcoming is the wide-spread use of Gaussian
probability densities as mixture components. Gaussian densities fail to capture
the positive skew in travel time distributions and, consequently, large numbers
of mixture components are needed for reasonable fitting accuracy when applied
as mixture components. They also assign positive probabilities to negative
travel times. To address these issues, this paper develops a mixture
distribution with asymmetric components supported on the positive numbers. We
use sparse estimation techniques to ensure parsimonious models. Specifically,
we derive a novel generalization of Gamma mixture densities using
Mittag-Leffler functions, which provides enhanced fitting flexibility and
improved parsimony. In order to accommodate within-day variability and allow
for online implementation of the proposed methodology (i.e., fast computations
on streaming travel time data), we introduce a recursive algorithm which
efficiently updates the fitted distribution whenever new data become available.
Experimental results using real-world travel time data illustrate the efficacy
of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jabari_S/0/1/0/all/0/1&quot;&gt;Saif Eddin Jabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Freris_N/0/1/0/all/0/1&quot;&gt;Nikolaos M. Freris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dilip_D/0/1/0/all/0/1&quot;&gt;Deepthi Mary Dilip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08154">
<title>Local White Matter Architecture Defines Functional Brain Dynamics. (arXiv:1804.08154v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1804.08154</link>
<description rdf:parseType="Literal">&lt;p&gt;Large bundles of myelinated axons, called white matter, anatomically connect
disparate brain regions together and compose the structural core of the human
connectome. We recently proposed a method of measuring the local integrity
along the length of each white matter fascicle, termed the local connectome. If
communication efficiency is fundamentally constrained by the integrity along
the entire length of a white matter bundle, then variability in the functional
dynamics of brain networks should be associated with variability in the local
connectome. We test this prediction using two statistical approaches that are
capable of handling the high dimensionality of data. First, by performing
statistical inference on distance-based correlations, we show that similarity
in the local connectome between individuals is significantly correlated with
similarity in their patterns of functional connectivity. Second, by employing
variable selection using sparse canonical correlation analysis and
cross-validation, we show that segments of the local connectome are predictive
of certain patterns of functional brain dynamics. These results are consistent
with the hypothesis that structural variability along axon bundles constrains
communication between disparate brain regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Choe_Y/0/1/0/all/0/1&quot;&gt;Yo Joong Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balakrishnan_S/0/1/0/all/0/1&quot;&gt;Sivaraman Balakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vettel_J/0/1/0/all/0/1&quot;&gt;Jean M. Vettel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verstynen_T/0/1/0/all/0/1&quot;&gt;Timothy Verstynen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08233">
<title>N-fold Superposition: Improving Neural Networks by Reducing the Noise in Feature Maps. (arXiv:1804.08233v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08233</link>
<description rdf:parseType="Literal">&lt;p&gt;Considering the use of Fully Connected (FC) layer limits the performance of
Convolutional Neural Networks (CNNs), this paper develops a method to improve
the coupling between the convolution layer and the FC layer by reducing the
noise in Feature Maps (FMs). Our approach is divided into three steps. Firstly,
we separate all the FMs into n blocks equally. Then, the weighted summation of
FMs at the same position in all blocks constitutes a new block of FMs. Finally,
we replicate this new block into n copies and concatenate them as the input to
the FC layer. This sharing of FMs could reduce the noise in them apparently and
avert the impact by a particular FM on the specific part weight of hidden
layers, hence preventing the network from overfitting to some extent. Using the
Fermat Lemma, we prove that this method could make the global minima value
range of the loss function wider, by which makes it easier for neural networks
to converge and accelerates the convergence process. This method does not
significantly increase the amounts of network parameters (only a few more
coefficients added), and the experiments demonstrate that this method could
increase the convergence speed and improve the classification performance of
neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chao Gao&lt;/a&gt; (National Digital Switching System Engineering &amp;amp; Technological R&amp;amp;D Center, Zhengzhou, China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08376">
<title>Convolutional capsule network for classification of breast cancer histology images. (arXiv:1804.08376v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.08376</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatization of the diagnosis of any kind of disease is of great importance
and it&apos;s gaining speed as more and more deep learning solutions are applied to
different problems. One of such computer aided systems could be a decision
support too able to accurately differentiate between different types of breast
cancer histological images - normal tissue or carcinoma. In this paper authors
present a deep learning solution, based on convolutional capsule network for
classification of four types of images of breast tissue biopsy when hematoxylin
and eusin staining is applied. The cross-validation accuracy was achieved to be
0.87 with equaly high sensitivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iesmantas_T/0/1/0/all/0/1&quot;&gt;Tomas Iesmantas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alzbutas_R/0/1/0/all/0/1&quot;&gt;Robertas Alzbutas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08396">
<title>Path Planning in Support of Smart Mobility Applications using Generative Adversarial Networks. (arXiv:1804.08396v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08396</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes and evaluates the use of Generative Adversarial Networks
(GANs) for path planning in support of smart mobility applications such as
indoor and outdoor navigation applications, individualized wayfinding for
people with disabilities (e.g., vision impairments, physical disabilities,
etc.), path planning for evacuations, robotic navigations, and path planning
for autonomous vehicles. We propose an architecture based on GANs to recommend
accurate and reliable paths for navigation applications. The proposed system
can use crowd-sourced data to learn the trajectories and infer new ones. The
system provides users with generated paths that help them navigate from their
local environment to reach a desired location. As a use case, we experimented
with the proposed method in support of a wayfinding application in an indoor
environment. Our experiments assert that the generated paths are correct and
reliable. The accuracy of the classification task for the generated paths is up
to 99% and the quality of the generated paths has a mean opinion score of 89%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_M/0/1/0/all/0/1&quot;&gt;Mehdi Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1&quot;&gt;Ala Al-Fuqaha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jun-Seok Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08416">
<title>Learn and Pick Right Nodes to Offload. (arXiv:1804.08416v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1804.08416</link>
<description rdf:parseType="Literal">&lt;p&gt;Task offloading is a promising technology to exploit the benefits of fog
computing. An effective task offloading strategy is needed to utilize the
computation resources efficiently. In this paper, we endeavor to seek an online
task offloading strategy to minimize the long-term latency. In particular, we
formulate a stochastic programming problem, where the expectations of the
system parameters change abruptly at unknown time instants. Meanwhile, we
consider the fact that the queried nodes can only feed back the processing
results after finishing the tasks. We then put forward an effective algorithm
to solve this challenging stochastic programming under the non-stationary
bandit model. We further prove that our proposed algorithm is asymptotically
optimal in a non-stationary fog-enabled network. Numerical simulations are
carried out to corroborate our designs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shengda Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiliang Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08438">
<title>A Spoofing Benchmark for the 2018 Voice Conversion Challenge: Leveraging from Spoofing Countermeasures for Speech Artifact Assessment. (arXiv:1804.08438v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.08438</link>
<description rdf:parseType="Literal">&lt;p&gt;Voice conversion (VC) aims at conversion of speaker characteristic without
altering content. Due to training data limitations and modeling imperfections,
it is difficult to achieve believable speaker mimicry without introducing
processing artifacts; performance assessment of VC, therefore, usually involves
both speaker similarity and quality evaluation by a human panel. As a
time-consuming, expensive, and non-reproducible process, it hinders rapid
prototyping of new VC technology. We address artifact assessment using an
alternative, objective approach leveraging from prior work on spoofing
countermeasures (CMs) for automatic speaker verification. Therein, CMs are used
for rejecting `fake&apos; inputs such as replayed, synthetic or converted speech but
their potential for automatic speech artifact assessment remains unknown. This
study serves to fill that gap. As a supplement to subjective results for the
2018 Voice Conversion Challenge (VCC&apos;18) data, we configure a standard
constant-Q cepstral coefficient CM to quantify the extent of processing
artifacts. Equal error rate (EER) of the CM, a confusability index of VC
samples with real human speech, serves as our artifact measure. Two clusters of
VCC&apos;18 entries are identified: low-quality ones with detectable artifacts (low
EERs), and higher quality ones with less artifacts. None of the VCC&apos;18 systems,
however, is perfect: all EERs are &amp;lt; 30 % (the `ideal&apos; value would be 50 %). Our
preliminary findings suggest potential of CMs outside of their original
application, as a supplemental optimization and benchmarking tool to enhance VC
technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1&quot;&gt;Tomi Kinnunen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toda_T/0/1/0/all/0/1&quot;&gt;Tomoki Toda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saito_D/0/1/0/all/0/1&quot;&gt;Daisuke Saito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Villavicencio_F/0/1/0/all/0/1&quot;&gt;Fernando Villavicencio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08450">
<title>Decorrelated Batch Normalization. (arXiv:1804.08450v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.08450</link>
<description rdf:parseType="Literal">&lt;p&gt;Batch Normalization (BN) is capable of accelerating the training of deep
models by centering and scaling activations within mini-batches. In this work,
we propose Decorrelated Batch Normalization (DBN), which not just centers and
scales activations but whitens them. We explore multiple whitening techniques,
and find that PCA whitening causes a problem we call stochastic axis swapping,
which is detrimental to learning. We show that ZCA whitening does not suffer
from this problem, permitting successful learning. DBN retains the desirable
qualities of BN and further improves BN&apos;s optimization efficiency and
generalization ability. We design comprehensive experiments to show that DBN
can improve the performance of BN on multilayer perceptrons and convolutional
neural networks. Furthermore, we consistently improve the accuracy of residual
networks on CIFAR-10, CIFAR-100, and ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dawei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_B/0/1/0/all/0/1&quot;&gt;Bo Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jia Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08472">
<title>High Dimensional Estimation and Multi-Factor Models. (arXiv:1804.08472v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/1804.08472</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of this paper is to re-investigate the estimation of multiple
factor models by relaxing the convention that the number of factors is small.
We first obtain the collection of all possible factors and we provide a
simultaneous test, security by security, of which factors are significant.
Since the collection of risk factors selected for investigation is large and
highly correlated, we use dimension reduction methods, including the Least
Absolute Shrinkage and Selection Operator (LASSO) and prototype clustering, to
perform the investigation. For comparison with the existing literature, we
compare the multi-factor model&apos;s performance with the Fama-French 5-factor
model. We find that both the Fama-French 5-factor and the multi-factor model
are consistent with the behavior of &quot;large-time scale&quot; security returns. In a
goodness-of-fit test comparing the Fama-French 5-factor with the multi-factor
model, the multi-factor model has a substantially larger adjusted $R^{2}$.
Robustness tests confirm that the multi-factor model provides a reasonable
characterization of security returns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Sumanta Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Jarrow_R/0/1/0/all/0/1&quot;&gt;Robert A. Jarrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wells_M/0/1/0/all/0/1&quot;&gt;Martin T. Wells&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08562">
<title>Spatio-Temporal Neural Networks for Space-Time Series Forecasting and Relations Discovery. (arXiv:1804.08562v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08562</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a dynamical spatio-temporal model formalized as a recurrent
neural network for forecasting time series of spatial processes, i.e. series of
observations sharing temporal and spatial dependencies. The model learns these
dependencies through a structured latent dynamical component, while a decoder
predicts the observations from the latent representations. We consider several
variants of this model, corresponding to different prior hypothesis about the
spatial relations between the series. The model is evaluated and compared to
state-of-the-art baselines, on a variety of forecasting problems representative
of different application areas: epidemiology, geo-spatial statistics and
car-traffic prediction. Besides these evaluations, we also describe experiments
showing the ability of this approach to extract relevant spatial relations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziat_A/0/1/0/all/0/1&quot;&gt;Ali Ziat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delasalles_E/0/1/0/all/0/1&quot;&gt;Edouard Delasalles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1&quot;&gt;Ludovic Denoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1&quot;&gt;Patrick Gallinari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08603">
<title>Towards Learning Sparsely Used Dictionaries with Arbitrary Supports. (arXiv:1804.08603v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08603</link>
<description rdf:parseType="Literal">&lt;p&gt;Dictionary learning is a popular approach for inferring a hidden basis or
dictionary in which data has a sparse representation. Data generated from the
dictionary A (an n by m matrix, with m &amp;gt; n in the over-complete setting) is
given by Y = AX where X is a matrix whose columns have supports chosen from a
distribution over k-sparse vectors, and the non-zero values chosen from a
symmetric distribution. Given Y, the goal is to recover A and X in polynomial
time. Existing algorithms give polytime guarantees for recovering incoherent
dictionaries, under strong distributional assumptions both on the supports of
the columns of X, and on the values of the non-zero entries. In this work, we
study the following question: Can we design efficient algorithms for recovering
dictionaries when the supports of the columns of X are arbitrary?
&lt;/p&gt;
&lt;p&gt;To address this question while circumventing the issue of
non-identifiability, we study a natural semirandom model for dictionary
learning where there are a large number of samples $y=Ax$ with arbitrary
k-sparse supports for x, along with a few samples where the sparse supports are
chosen uniformly at random. While the few samples with random supports ensures
identifiability, the support distribution can look almost arbitrary in
aggregate. Hence existing algorithmic techniques seem to break down as they
make strong assumptions on the supports.
&lt;/p&gt;
&lt;p&gt;Our main contribution is a new polynomial time algorithm for learning
incoherent over-complete dictionaries that works under the semirandom model.
Additionally the same algorithm provides polynomial time guarantees in new
parameter regimes when the supports are fully random. Finally using these
techniques, we also identify a minimal set of conditions on the supports under
which the dictionary can be (information theoretically) recovered from
polynomial samples for almost linear sparsity, i.e., $k=\tilde{O}(n)$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1&quot;&gt;Pranjal Awasthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1&quot;&gt;Aravindan Vijayaraghavan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.09658">
<title>Continuation of Nesterov&apos;s Smoothing for Regression with Structured Sparsity in High-Dimensional Neuroimaging. (arXiv:1605.09658v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.09658</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive models can be used on high-dimensional brain images for diagnosis
of a clinical condition. Spatial regularization through structured sparsity
offers new perspectives in this context and reduces the risk of overfitting the
model while providing interpretable neuroimaging signatures by forcing the
solution to adhere to domain-specific constraints. Total Variation (TV)
enforces spatial smoothness of the solution while segmenting predictive regions
from the background. We consider the problem of minimizing the sum of a smooth
convex loss, a non-smooth convex penalty (whose proximal operator is known) and
a wide range of possible complex, non-smooth convex structured penalties such
as TV or overlapping group Lasso. Existing solvers are either limited in the
functions they can minimize or in their practical capacity to scale to
high-dimensional imaging data. Nesterov&apos;s smoothing technique can be used to
minimize a large number of non-smooth convex structured penalties but
reasonable precision requires a small smoothing parameter, which slows down the
convergence speed. To benefit from the versatility of Nesterov&apos;s smoothing
technique, we propose a first order continuation algorithm, CONESTA, which
automatically generates a sequence of decreasing smoothing parameters. The
generated sequence maintains the optimal convergence speed towards any globally
desired precision. Our main contributions are: To propose an expression of the
duality gap to probe the current distance to the global optimum in order to
adapt the smoothing parameter and the convergence speed. We provide a
convergence rate, which is an improvement over classical proximal gradient
smoothing methods. We demonstrate on both simulated and high-dimensional
structural neuroimaging data that CONESTA significantly outperforms many
state-of-the-art solvers in regard to convergence speed and precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hadj_Selem_F/0/1/0/all/0/1&quot;&gt;Fouad Hadj-Selem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lofstedt_T/0/1/0/all/0/1&quot;&gt;Tommy Lofstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dohmatob_E/0/1/0/all/0/1&quot;&gt;Elvis Dohmatob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frouin_V/0/1/0/all/0/1&quot;&gt;Vincent Frouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubois_M/0/1/0/all/0/1&quot;&gt;Mathieu Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guillemot_V/0/1/0/all/0/1&quot;&gt;Vincent Guillemot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duchesnay_E/0/1/0/all/0/1&quot;&gt;Edouard Duchesnay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.05148">
<title>Discovering Relationships and their Structures Across Disparate Data Modalities. (arXiv:1609.05148v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.05148</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining how certain properties are related to other properties is
fundamental to scientific discovery. As data collection rates accelerate, it is
becoming increasingly difficult, yet ever more important, to determine whether
one property of data (e.g., cloud density) is related to another (e.g., grass
wetness). Only if two properties are related are further investigations into
the geometry of the relationship warranted. While existing approaches can test
whether two properties are related, they may require unfeasibly large sample
sizes in real data scenarios, and do not address how they are related. Our key
insight is that one can adaptively restrict the analysis to the &quot;jointly local&quot;
observations, that is, one can estimate the scales with the most informative
neighbors for determining the existence and geometry of a relationship.
&quot;Multiscale Graph Correlation&quot; (MGC) is a framework that extends global
procedures to be multiscale; consequently, MGC tests typically require far
fewer samples than existing methods for a wide variety of dependence structures
and dimensionalities, while maintaining computational efficiency. Moreover, MGC
provides a simple and elegant multiscale characterization of the potentially
complex latent geometry underlying the relationship. In several real data
applications, MGC uniquely detects the presence and reveals the geometry of the
relationships.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cencheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bridgeford_E/0/1/0/all/0/1&quot;&gt;Eric Bridgeford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maggioni_M/0/1/0/all/0/1&quot;&gt;Mauro Maggioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.08140">
<title>Network classification with applications to brain connectomics. (arXiv:1701.08140v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1701.08140</link>
<description rdf:parseType="Literal">&lt;p&gt;While statistical analysis of a single network has received a lot of
attention in recent years, with a focus on social networks, analysis of a
sample of networks presents its own challenges which require a different set of
analytic tools. Here we study the problem of classification of networks with
labeled nodes, motivated by applications in neuroimaging. Brain networks are
constructed from imaging data to represent functional connectivity between
regions of the brain, and previous work has shown the potential of such
networks to distinguish between various brain disorders, giving rise to a
network classification problem. Existing approaches tend to either treat all
edge weights as a long vector, ignoring the network structure, or focus on
graph topology as represented by summary measures while ignoring the edge
weights. Our goal is to design a classification method that uses both the
individual edge information and the network structure of the data in a
computationally efficient way, and that can produce a parsimonious and
interpretable representation of differences in brain connectivity patterns
between classes. We propose a graph classification method that uses edge
weights as predictors but incorporates the network nature of the data via
penalties that promote sparsity in the number of nodes, in addition to the
usual sparsity penalties that encourage selection of edges. We implement the
method via efficient convex optimization and provide a detailed analysis of
data from two fMRI studies of schizophrenia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arroyo_Relion_J/0/1/0/all/0/1&quot;&gt;Jes&amp;#xfa;s D. Arroyo-Reli&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kessler_D/0/1/0/all/0/1&quot;&gt;Daniel Kessler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Levina_E/0/1/0/all/0/1&quot;&gt;Elizaveta Levina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taylor_S/0/1/0/all/0/1&quot;&gt;Stephan F. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02568">
<title>An Outlyingness Matrix for Multivariate Functional Data Classification. (arXiv:1704.02568v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02568</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of multivariate functional data is an important task in
scientific research. Unlike point-wise data, functional data are usually
classified by their shapes rather than by their scales. We define an
outlyingness matrix by extending directional outlyingness, an effective measure
of the shape variation of curves that combines the direction of outlyingness
with conventional depth. We propose two classifiers based on directional
outlyingness and the outlyingness matrix, respectively. Our classifiers provide
better performance compared with existing depth-based classifiers when applied
on both univariate and multivariate functional data from simulation studies. We
also test our methods on two data problems: speech recognition and gesture
classification, and obtain results that are consistent with the findings from
the simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenlin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Genton_M/0/1/0/all/0/1&quot;&gt;Marc G. Genton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01207">
<title>The $\ell_\infty$ Perturbation of HOSVD and Low Rank Tensor Denoising. (arXiv:1707.01207v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01207</link>
<description rdf:parseType="Literal">&lt;p&gt;The higher order singular value decomposition (HOSVD) of tensors is a
generalization of matrix SVD. The perturbation analysis of HOSVD under random
noise is more delicate than its matrix counterpart. Recent progress has been
made in Richard and Montanari (2014), Zhang and Xia (2017) and Liu et al.
(2017) demonstrating that minimax optimal singular spaces estimation and low
rank tensor recovery in $\ell_2$-norm can be obtained through polynomial time
algorithms. In this paper, we analyze the HOSVD perturbation under Gaussian
noise based on a second order method, which leads to an estimator of singular
vectors with sharp bound in $\ell_\infty$-norm. A low rank tensor denoising
estimator is then proposed which achieves a fast convergence rate
characterizing the entry-wise deviations. The advantages of these
$\ell_\infty$-norm bounds are displayed in applications including high
dimensional clustering and sub-tensor localizations. In addition, the bound
established for HOSVD also elaborates the one-sided spectral perturbation in
$\ell_\infty$-norm for unbalanced (or fat) matrices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xia_D/0/1/0/all/0/1&quot;&gt;Dong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10363">
<title>Diff-DAC: Distributed Actor-Critic for Average Multitask Deep Reinforcement Learning. (arXiv:1710.10363v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10363</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a fully distributed actor-critic algorithm approximated by deep
neural networks, named \textit{Diff-DAC}, with application to single-task and
to average multitask reinforcement learning (MRL). Each agent has access to
data from its local task only, but it aims to learn a policy that performs well
on average for the whole set of tasks. During the learning process, agents
communicate their value-policy parameters to their neighbors, diffusing the
information across the network, so that they converge to a common policy, with
no need for a central node. The method is scalable, since the computational and
communication costs per agent grow with its number of neighbors. We derive
Diff-DAC&apos;s from duality theory and provide novel insights into the standard
actor-critic framework, showing that it is actually an instance of the dual
ascent method that approximates the solution of a linear program. Experiments
suggest that Diff-DAC can outperform the single previous distributed MRL
approach (i.e., Dist-MTLPS) and even the centralized architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macua_S/0/1/0/all/0/1&quot;&gt;Sergio Valcarcel Macua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tukiainen_A/0/1/0/all/0/1&quot;&gt;Aleksi Tukiainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1&quot;&gt;Daniel Garc&amp;#xed;a-Oca&amp;#xf1;a Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldazo_D/0/1/0/all/0/1&quot;&gt;David Baldazo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cote_E/0/1/0/all/0/1&quot;&gt;Enrique Munoz de Cote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zazo_S/0/1/0/all/0/1&quot;&gt;Santiago Zazo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04165">
<title>Temporal Stability in Predictive Process Monitoring. (arXiv:1712.04165v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04165</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive process monitoring is concerned with the analysis of events
produced during the execution of a business process in order to predict as
early as possible the final outcome of an ongoing case. Traditionally,
predictive process monitoring methods are optimized with respect to accuracy.
However, in environments where users make decisions and take actions in
response to the predictions they receive, it is equally important to optimize
the stability of the successive predictions made for each case. To this end,
this paper defines a notion of temporal stability for binary classification
tasks in predictive process monitoring and evaluates existing methods with
respect to both temporal stability and accuracy. We find that methods based on
XGBoost and LSTM neural networks exhibit the highest temporal stability. We
then show that temporal stability can be enhanced by hyperparameter-optimizing
random forests and XGBoost classifiers with respect to inter-run stability.
Finally, we show that time series smoothing techniques can further enhance
temporal stability at the expense of slightly lower accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teinemaa_I/0/1/0/all/0/1&quot;&gt;Irene Teinemaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leontjeva_A/0/1/0/all/0/1&quot;&gt;Anna Leontjeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02251">
<title>Graph Autoencoder-Based Unsupervised Feature Selection with Broad and Local Data Structure Preservation. (arXiv:1801.02251v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02251</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection is a dimensionality reduction technique that selects a
subset of representative features from high dimensional data by eliminating
irrelevant and redundant features. Recently, feature selection combined with
sparse learning has attracted significant attention due to its outstanding
performance compared with traditional feature selection methods that ignores
correlation between features. These works first map data onto a low-dimensional
subspace and then select features by posing a sparsity constraint on the
transformation matrix. However, they are restricted by design to linear data
transformation, a potential drawback given that the underlying correlation
structures of data are often non-linear. To leverage a more sophisticated
embedding, we propose an autoencoder-based unsupervised feature selection
approach that leverages a single-layer autoencoder for a joint framework of
feature selection and manifold learning. More specifically, we enforce column
sparsity on the weight matrix connecting the input layer and the hidden layer,
as in previous work. Additionally, we include spectral graph analysis on the
projected data into the learning process to achieve local data geometry
preservation from the original data space to the low-dimensional feature space.
Extensive experiments are conducted on image, audio, text, and biological data.
The promising experimental results validate the superiority of the proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Siwei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duarte_M/0/1/0/all/0/1&quot;&gt;Marco F.Duarte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03050">
<title>Assessing the effect of advertising expenditures upon sales: a Bayesian structural time series model. (arXiv:1801.03050v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03050</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a robust implementation of the Nerlove--Arrow model using a
Bayesian structural time series model to explain the relationship between
advertising expenditures of a country-wide fast-food franchise network with its
weekly sales. Thanks to the flexibility and modularity of the model, it is well
suited to generalization to other markets or situations. Its Bayesian nature
facilitates incorporating \emph{a priori} information (the manager&apos;s views),
which can be updated with relevant data. This aspect of the model will be used
to present a strategy of budget scheduling across time and channels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gallego_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Gallego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Angulo_P/0/1/0/all/0/1&quot;&gt;Pablo Angulo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suarez_Garcia_P/0/1/0/all/0/1&quot;&gt;Pablo Su&amp;#xe1;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gomez_Ullate_D/0/1/0/all/0/1&quot;&gt;David G&amp;#xf3;mez-Ullate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05693">
<title>Bandit Learning with Positive Externalities. (arXiv:1802.05693v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05693</link>
<description rdf:parseType="Literal">&lt;p&gt;Many platforms are characterized by the fact that future user arrivals are
likely to have preferences similar to users who were satisfied in the past. In
other words, arrivals exhibit positive externalities. We study multiarmed
bandit (MAB) problems with positive externalities. Our model has a finite
number of arms and users are distinguished by the arm(s) they prefer. We model
positive externalities by assuming that the preferred arms of future arrivals
are self-reinforcing based on the experiences of past users. We show that
classical algorithms such as UCB which are optimal in the classical MAB setting
may even exhibit linear regret in the context of positive externalities.
&lt;/p&gt;
&lt;p&gt;We show that there is a fundamental tradeoff: on one hand, the positive
externality allows an algorithm to quickly converge to the &quot;right&quot; population,
on the other hand, this same effect also amplifies the consequences of any
mistakes. We show that this tradeoff calls for a novel algorithmic approach
relative to benchmarks such as UCB and random-explore-then-exploit, which are
not optimal in this setting. We develop explicit lower-bounds to the achievable
regret, with a structure which is quite different that for the standard MAB
settings. We show that the lower-bound is tight by developing an algorithm
which achieves optimal regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Virag Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1&quot;&gt;Jose Blanchet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johari_R/0/1/0/all/0/1&quot;&gt;Ramesh Johari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07756">
<title>Determining the best classifier for predicting the value of a boolean field on a blood donor database using genetic algorithms. (arXiv:1802.07756v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07756</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: Thanks to digitization, we often have access to large databases,
consisting of various fields of information, ranging from numbers to texts and
even boolean values. Such databases lend themselves especially well to machine
learning, classification and big data analysis tasks. We are able to train
classifiers, using already existing data and use them for predicting the values
of a certain field, given that we have information regarding the other fields.
Most specifically, in this study, we look at the Electronic Health Records
(EHRs) that are compiled by hospitals. These EHRs are convenient means of
accessing data of individual patients, but there processing as a whole still
remains a task. However, EHRs that are composed of coherent, well-tabulated
structures lend themselves quite well to the application to machine language,
via the usage of classifiers. In this study, we look at a Blood Transfusion
Service Center Data Set (Data taken from the Blood Transfusion Service Center
in Hsin-Chu City in Taiwan). We used scikit-learn machine learning in python.
From Support Vector Machines(SVM), we use Support Vector Classification(SVC),
from the linear model we import Perceptron. We also used the
K.neighborsclassifier and the decision tree classifiers. Furthermore, we use
the TPOT library to find an optimized pipeline using genetic algorithms. Using
the above classifiers, we score each one of them using k fold cross-validation.
&lt;/p&gt;
&lt;p&gt;Contact: ritabratamaiti@hiretrex.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maiti_R/0/1/0/all/0/1&quot;&gt;Ritabrata Maiti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06561">
<title>Multi-device, Multi-tenant Model Selection with GP-EI. (arXiv:1803.06561v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06561</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization is the core technique behind the emergence of AutoML,
which holds the promise of automatically searching for models and
hyperparameters to make machine learning techniques more accessible. As such
services are moving towards the cloud, we ask -- {\em When multiple AutoML
users share the same computational infrastructure, how should we allocate
resources to maximize the &quot;global happiness&quot; of all users?}
&lt;/p&gt;
&lt;p&gt;We focus on GP-EI, one of the most popular algorithms for automatic model
selection and hyperparameter tuning, and develop a novel multi-device,
multi-tenant extension that is aware of \emph{multiple} computation devices and
multiple users sharing the same set of computation devices. Theoretically,
given $N$ users and $M$ devices, we obtain a regret bound of $O((\text{\bf
{MIU}}(T,K) + M)\frac{N^2}{M})$, where $\text{\bf {MIU}}(T,K)$ refers to the
maximal incremental uncertainty up to time $T$ for the covariance matrix $K$.
Empirically, we evaluate our algorithm on two applications of automatic model
selection, and show that our algorithm significantly outperforms the strategy
of serving users independently. Moreover, when multiple computation devices are
available, we achieve near-linear speedup when the number of users is much
larger than the number of devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1&quot;&gt;Bojan Karlas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Jie Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Ce Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10254">
<title>Disease-Atlas: Navigating Disease Trajectories with Deep Learning. (arXiv:1803.10254v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10254</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint models for longitudinal and time-to-event data are commonly used in
longitudinal studies to forecast disease trajectories over time. While there
are many advantages to joint modeling, the standard forms suffer from
limitations that arise from a fixed model specification, and computational
difficulties when applied to high-dimensional datasets. In this paper, we
propose a deep learning approach to address these limitations, enhancing
existing methods with the inherent flexibility and scalability of deep neural
networks, while retaining the benefits of joint modeling. Using longitudinal
data from two real-world medical datasets, we demonstrate improvements in
performance and scalability, as well as robustness in the presence of
irregularly sampled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Bryan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01955">
<title>Explanations of model predictions with live and breakDown packages. (arXiv:1804.01955v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex models are commonly used in predictive modeling. In this paper we
present R packages that can be used to explain predictions from complex black
box models and attribute parts of these predictions to input features. We
introduce two new approaches and corresponding packages for such attribution,
namely live and breakDown. We also compare their results with existing
implementations of state of the art solutions, namely lime that implements
Locally Interpretable Model-agnostic Explanations and ShapleyR that implements
Shapley values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Staniak_M/0/1/0/all/0/1&quot;&gt;Mateusz Staniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04577">
<title>Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and Some New Implementations. (arXiv:1804.04577v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04577</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we discuss policy iteration methods for approximate solution of
a finite-state discounted Markov decision problem, with a focus on
feature-based aggregation methods and their connection with deep reinforcement
learning schemes. We introduce features of the states of the original problem,
and we formulate a smaller &quot;aggregate&quot; Markov decision problem, whose states
relate to the features. The optimal cost function of the aggregate problem, a
nonlinear function of the features, serves as an architecture for approximation
in value space of the optimal cost function or the cost functions of policies
of the original problem. We discuss properties and possible implementations of
this type of aggregation, including a new approach to approximate policy
iteration. In this approach the policy improvement operation combines
feature-based aggregation with reinforcement learning based on deep neural
networks, which is used to obtain the needed features. We argue that the cost
function of a policy may be approximated much more accurately by the nonlinear
function of the features provided by aggregation, than by the linear function
of the features provided by deep reinforcement learning, thereby potentially
leading to more effective policy improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertsekas_D/0/1/0/all/0/1&quot;&gt;Dimitri P. Bertsekas&lt;/a&gt;</dc:creator>
</item></rdf:RDF>