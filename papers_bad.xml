<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08641"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08650"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08841"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05394"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07863"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08214"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08600"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.01713"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07889"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.08577">
<title>Effective Building Block Design for Deep Convolutional Neural Networks using Search. (arXiv:1801.08577v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.08577</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has shown promising results on many machine learning tasks but
DL models are often complex networks with large number of neurons and layers,
and recently, complex layer structures known as building blocks. Finding the
best deep model requires a combination of finding both the right architecture
and the correct set of parameters appropriate for that architecture. In
addition, this complexity (in terms of layer types, number of neurons, and
number of layers) also present problems with generalization since larger
networks are easier to overfit to the data. In this paper, we propose a search
framework for finding effective architectural building blocks for convolutional
neural networks (CNN). Our approach is much faster at finding models that are
close to state-of-the-art in performance. In addition, the models discovered by
our approach are also smaller than models discovered by similar techniques. We
achieve these twin advantages by designing our search space in such a way that
it searches over a reduced set of state-of-the-art building blocks for CNNs
including residual block, inception block, inception-residual block, ResNeXt
block and many others. We apply this technique to generate models for multiple
image datasets and show that these models achieve performance comparable to
state-of-the-art (and even surpassing the state-of-the-art in one case). We
also show that learned models are transferable between datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_J/0/1/0/all/0/1&quot;&gt;Jayanta K Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiayi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1&quot;&gt;Unmesh Kurup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mohak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08641">
<title>Knowledge Graph Embedding with Multiple Relation Projections. (arXiv:1801.08641v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08641</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs contain rich relational structures of the world, and thus
complement data-driven machine learning in heterogeneous data. One of the most
effective methods in representing knowledge graphs is to embed symbolic
relations and entities into continuous spaces, where relations are
approximately linear translation between projected images of entities in the
relation space. However, state-of-the-art relation projection methods such as
TransR, TransD or TransSparse do not model the correlation between relations,
and thus are not scalable to complex knowledge graphs with thousands of
relations, both in computational demand and in statistical robustness. To this
end we introduce TransF, a novel translation-based method which mitigates the
burden of relation projection by explicitly modeling the basis subspaces of
projection matrices. As a result, TransF is far more light weight than the
existing projection methods, and is robust when facing a high number of
relations. Experimental results on the canonical link prediction task show that
our proposed model outperforms competing rivals by a large margin and achieves
state-of-the-art performance. Especially, TransF improves by 9%/5% in the
head/tail entity prediction task for N-to-1/1-to-N relations over the best
performing translation-based method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1&quot;&gt;Kien Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Truyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08650">
<title>Ontology-based Fuzzy Markup Language Agent for Student and Robot Co-Learning. (arXiv:1801.08650v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08650</link>
<description rdf:parseType="Literal">&lt;p&gt;An intelligent robot agent based on domain ontology, machine learning
mechanism, and Fuzzy Markup Language (FML) for students and robot co-learning
is presented in this paper. The machine-human co-learning model is established
to help various students learn the mathematical concepts based on their
learning ability and performance. Meanwhile, the robot acts as a teacher&apos;s
assistant to co-learn with children in the class. The FML-based knowledge base
and rule base are embedded in the robot so that the teachers can get feedback
from the robot on whether students make progress or not. Next, we inferred
students&apos; learning performance based on learning content&apos;s difficulty and
students&apos; ability, concentration level, as well as teamwork sprit in the class.
Experimental results show that learning with the robot is helpful for
disadvantaged and below-basic children. Moreover, the accuracy of the
intelligent FML-based agent for student learning is increased after machine
learning mechanism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chang-Shing Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mei-Hui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzong-Xiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li-Chung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yung-Ching Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sheng-Chi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1&quot;&gt;Chien-Hsun Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_P/0/1/0/all/0/1&quot;&gt;Pi-Hsia Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubota_N/0/1/0/all/0/1&quot;&gt;Naoyuki Kubota&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08757">
<title>Safe Exploration in Continuous Action Spaces. (arXiv:1801.08757v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08757</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of deploying a reinforcement learning (RL) agent on a
physical system such as a datacenter cooling unit or robot, where critical
constraints must never be violated. We show how to exploit the typically smooth
dynamics of these systems and enable RL algorithms to never violate constraints
during learning. Our technique is to directly add to the policy a safety layer
that analytically solves an action correction formulation per each state. The
novelty of obtaining an elegant closed-form solution is attained due to a
linearized model, learned on past trajectories consisting of arbitrary actions.
This is to mimic the real-world circumstances where data logs were generated
with a behavior policy that is implausible to describe mathematically; such
cases render the known safety-aware off-policy methods inapplicable. We
demonstrate the efficacy of our approach on new representative physics-based
environments, and prevail where reward shaping fails by maintaining zero
constraint violations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_G/0/1/0/all/0/1&quot;&gt;Gal Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvijotham_K/0/1/0/all/0/1&quot;&gt;Krishnamurthy Dvijotham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vecerik_M/0/1/0/all/0/1&quot;&gt;Matej Vecerik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hester_T/0/1/0/all/0/1&quot;&gt;Todd Hester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paduraru_C/0/1/0/all/0/1&quot;&gt;Cosmin Paduraru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tassa_Y/0/1/0/all/0/1&quot;&gt;Yuval Tassa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08829">
<title>Symbol Emergence in Cognitive Developmental Systems: a Survey. (arXiv:1801.08829v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08829</link>
<description rdf:parseType="Literal">&lt;p&gt;Symbol emergence through a robot&apos;s own interactive exploration of the world
without human intervention has been investigated now for several decades.
However, methods that enable a machine to form symbol systems in a robust
bottom-up manner are still missing. Clearly, this shows that we still do not
have an appropriate computational understanding that explains symbol emergence
in biological and artificial systems. Over the years it became more and more
clear that symbol emergence has to be posed as a multi-faceted problem.
Therefore, we will first review the history of the symbol emergence problem in
different fields showing their mutual relations. Then we will describe recent
work and approaches to solve this problem with the aim of providing an
integrative and comprehensive overview of symbol emergence for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1&quot;&gt;Tadahiro Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1&quot;&gt;Emre Ugur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1&quot;&gt;Matej Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamone_L/0/1/0/all/0/1&quot;&gt;Lorenzo Jamone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagai_T/0/1/0/all/0/1&quot;&gt;Takayuki Nagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1&quot;&gt;Benjamin Rosman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuka_T/0/1/0/all/0/1&quot;&gt;Toshihiko Matsuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwahashi_N/0/1/0/all/0/1&quot;&gt;Naoto Iwahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1&quot;&gt;Erhan Oztop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piater_J/0/1/0/all/0/1&quot;&gt;Justus Piater&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worgotter_F/0/1/0/all/0/1&quot;&gt;Florentin W&amp;#xf6;rg&amp;#xf6;tter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08841">
<title>FlashRL: A Reinforcement Learning Platform for Flash Games. (arXiv:1801.08841v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08841</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) is a research area that has blossomed
tremendously in recent years and has shown remarkable potential in among others
successfully playing computer games. However, there only exists a few game
platforms that provide diversity in tasks and state-space needed to advance RL
algorithms. The existing platforms offer RL access to Atari- and a few
web-based games, but no platform fully expose access to Flash games. This is
unfortunate because applying RL to Flash games have potential to push the
research of RL algorithms.
&lt;/p&gt;
&lt;p&gt;This paper introduces the Flash Reinforcement Learning platform (FlashRL)
which attempts to fill this gap by providing an environment for thousands of
Flash games on a novel platform for Flash automation. It opens up easy
experimentation with RL algorithms for Flash games, which has previously been
challenging. The platform shows excellent performance with as little as 5% CPU
utilization on consumer hardware. It shows promising results for novel
reinforcement learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1&quot;&gt;Per-Arne Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05394">
<title>Time Series Segmentation through Automatic Feature Learning. (arXiv:1801.05394v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05394</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet of things (IoT) applications have become increasingly popular in
recent years, with applications ranging from building energy monitoring to
personal health tracking and activity recognition. In order to leverage these
data, automatic knowledge extraction - whereby we map from observations to
interpretable states and transitions - must be done at scale. As such, we have
seen many recent IoT data sets include annotations with a human expert
specifying states, recorded as a set of boundaries and associated labels in a
data sequence. These data can be used to build automatic labeling algorithms
that produce labels as an expert would. Here, we refer to human-specified
boundaries as breakpoints. Traditional changepoint detection methods only look
for statistically-detectable boundaries that are defined as abrupt variations
in the generative parameters of a data sequence. However, we observe that
breakpoints occur on more subtle boundaries that are non-trivial to detect with
these statistical methods. In this work, we propose a new unsupervised
approach, based on deep learning, that outperforms existing techniques and
learns the more subtle, breakpoint boundaries with a high accuracy. Through
extensive experiments on various real-world data sets - including
human-activity sensing data, speech signals, and electroencephalogram (EEG)
activity traces - we demonstrate the effectiveness of our algorithm for
practical applications. Furthermore, we show that our approach achieves
significantly better performance than previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wei-Han Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1&quot;&gt;Jorge Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1&quot;&gt;Bongjun Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Ruby Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07863">
<title>Opinion Dynamics with Varying Susceptibility to Persuasion. (arXiv:1801.07863v1 [cs.SI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1801.07863</link>
<description rdf:parseType="Literal">&lt;p&gt;A long line of work in social psychology has studied variations in people&apos;s
susceptibility to persuasion -- the extent to which they are willing to modify
their opinions on a topic. This body of literature suggests an interesting
perspective on theoretical models of opinion formation by interacting parties
in a network: in addition to considering interventions that directly modify
people&apos;s intrinsic opinions, it is also natural to consider interventions that
modify people&apos;s susceptibility to persuasion. In this work, we adopt a popular
model for social opinion dynamics, and we formalize the opinion maximization
and minimization problems where interventions happen at the level of
susceptibility.
&lt;/p&gt;
&lt;p&gt;We show that modeling interventions at the level of susceptibility lead to an
interesting family of new questions in network opinion dynamics. We find that
the questions are quite different depending on whether there is an overall
budget constraining the number of agents we can target or not. We give a
polynomial-time algorithm for finding the optimal target-set to optimize the
sum of opinions when there are no budget constraints on the size of the
target-set. We show that this problem is NP-hard when there is a budget, and
that the objective function is neither submodular nor supermodular. Finally, we
propose a heuristic for the budgeted opinion optimization and show its efficacy
at finding target-sets that optimize the sum of opinions compared on real world
networks, including a Twitter network with real opinion estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abebe_R/0/1/0/all/0/1&quot;&gt;Rediet Abebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1&quot;&gt;David Parkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsourakakis_C/0/1/0/all/0/1&quot;&gt;Charalampos E. Tsourakakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08214">
<title>Active Neural Localization. (arXiv:1801.08214v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1801.08214</link>
<description rdf:parseType="Literal">&lt;p&gt;Localization is the problem of estimating the location of an autonomous agent
from an observation and a map of the environment. Traditional methods of
localization, which filter the belief based on the observations, are
sub-optimal in the number of steps required, as they do not decide the actions
taken by the agent. We propose &quot;Active Neural Localizer&quot;, a fully
differentiable neural network that learns to localize accurately and
efficiently. The proposed model incorporates ideas of traditional
filtering-based localization methods, by using a structured belief of the state
with multiplicative interactions to propagate belief, and combines it with a
policy model to localize accurately while minimizing the number of steps
required for localization. Active Neural Localizer is trained end-to-end with
reinforcement learning. We use a variety of simulation environments for our
experiments which include random 2D mazes, random mazes in the Doom game engine
and a photo-realistic environment in the Unreal game engine. The results on the
2D environments show the effectiveness of the learned policy in an idealistic
setting while results on the 3D environments demonstrate the model&apos;s capability
of learning the policy and perceptual model jointly from raw-pixel based RGB
observations. We also show that a model trained on random textures in the Doom
environment generalizes well to a photo-realistic office space environment in
the Unreal engine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Chaplot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1&quot;&gt;Emilio Parisotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08570">
<title>Deep Learning in Pharmacogenomics: From Gene Regulation to Patient Stratification. (arXiv:1801.08570v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1801.08570</link>
<description rdf:parseType="Literal">&lt;p&gt;This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kalinin_A/0/1/0/all/0/1&quot;&gt;Alexandr A. Kalinin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Higgins_G/0/1/0/all/0/1&quot;&gt;Gerald A. Higgins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reamaroon_N/0/1/0/all/0/1&quot;&gt;Narathip Reamaroon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Soroushmehr_S/0/1/0/all/0/1&quot;&gt;S.M. Reza Soroushmehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Allyn_Feuer_A/0/1/0/all/0/1&quot;&gt;Ari Allyn-Feuer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dinov_I/0/1/0/all/0/1&quot;&gt;Ivo D. Dinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Najarian_K/0/1/0/all/0/1&quot;&gt;Kayvan Najarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Athey_B/0/1/0/all/0/1&quot;&gt;Brian D. Athey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08600">
<title>Development of ICA and IVA Algorithms with Application to Medical Image Analysis. (arXiv:1801.08600v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08600</link>
<description rdf:parseType="Literal">&lt;p&gt;Independent component analysis (ICA) is a widely used BSS method that can
uniquely achieve source recovery, subject to only scaling and permutation
ambiguities, through the assumption of statistical independence on the part of
the latent sources. Independent vector analysis (IVA) extends the applicability
of ICA by jointly decomposing multiple datasets through the exploitation of the
dependencies across datasets. Though both ICA and IVA algorithms cast in the
maximum likelihood (ML) framework enable the use of all available statistical
information in reality, they often deviate from their theoretical optimality
properties due to improper estimation of the probability density function
(PDF). This motivates the development of flexible ICA and IVA algorithms that
closely adhere to the underlying statistical description of the data. Although
it is attractive minimize the assumptions, important prior information about
the data, such as sparsity, is usually available. If incorporated into the ICA
model, use of this additional information can relax the independence
assumption, resulting in an improvement in the overall separation performance.
Therefore, the development of a unified mathematical framework that can take
into account both statistical independence and sparsity is of great interest.
In this work, we first introduce a flexible ICA algorithm that uses an
effective PDF estimator to accurately capture the underlying statistical
properties of the data. We then discuss several techniques to accurately
estimate the parameters of the multivariate generalized Gaussian distribution,
and how to integrate them into the IVA model. Finally, we provide a
mathematical framework that enables direct control over the influence of
statistical independence and sparsity, and use this framework to develop an
effective ICA algorithm that can jointly exploit these two forms of diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boukouvalas_Z/0/1/0/all/0/1&quot;&gt;Zois Boukouvalas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08639">
<title>Fast binary embeddings, and quantized compressed sensing with structured matrices. (arXiv:1801.08639v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1801.08639</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with two related problems, namely distance-preserving binary
embeddings and quantization for compressed sensing . First, we propose fast
methods to replace points from a subset $\mathcal{X} \subset \mathbb{R}^n$,
associated with the Euclidean metric, with points in the cube $\{\pm 1\}^m$ and
we associate the cube with a pseudo-metric that approximates Euclidean distance
among points in $\mathcal{X}$. Our methods rely on quantizing fast
Johnson-Lindenstrauss embeddings based on bounded orthonormal systems and
partial circulant ensembles, both of which admit fast transforms. Our
quantization methods utilize noise-shaping, and include Sigma-Delta schemes and
distributed noise-shaping schemes. The resulting approximation errors decay
polynomially and exponentially fast in $m$, depending on the embedding method.
This dramatically outperforms the current decay rates associated with binary
embeddings and Hamming distances. Additionally, it is the first such binary
embedding result that applies to fast Johnson-Lindenstrauss maps while
preserving $\ell_2$ norms.
&lt;/p&gt;
&lt;p&gt;Second, we again consider noise-shaping schemes, albeit this time to quantize
compressed sensing measurements arising from bounded orthonormal ensembles and
partial circulant matrices. We show that these methods yield a reconstruction
error that again decays with the number of measurements (and bits), when using
convex optimization for reconstruction. Specifically, for Sigma-Delta schemes,
the error decays polynomially in the number of measurements, and it decays
exponentially for distributed noise-shaping schemes based on beta encoding.
These results are near optimal and the first of their kind dealing with bounded
orthonormal systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1&quot;&gt;Thang Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1&quot;&gt;Rayan Saab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08694">
<title>PDNet: Semantic Segmentation integrated with a Primal-Dual Network for Document binarization. (arXiv:1801.08694v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08694</link>
<description rdf:parseType="Literal">&lt;p&gt;Binarization of digital documents is the task of classifying each pixel in an
image of the document as belonging to the background (parchment/paper) or
foreground (text/ink). Historical documents are often subject to degradations,
that make the task challenging. In the current work a deep neural network
architecture is proposed that combines a fully convolutional network with an
unrolled primal-dual network that can be trained end-to-end in order to achieve
state of the art binarization on four out of seven datasets. Document
binarization is formulated as a energy minimization problem. A fully
convolutional neural network is trained for semantic labeling of pixels to
provide class labeling cost associated with each pixel. This cost estimate is
refined along the edges to compensate for any over or under estimation of the
under represented fore-ground class using a primal-dual approach. We provide
necessary overview on proximal operator that facilitates theoretical
underpinning in order to train a primal-dual network using a gradient descent
algorithm. Numerical instabilities encountered due to the recurrent nature of
primal-dual approach are handled. We provide experimental results on document
binarization competition dataset along with network changes and hyperparameter
tuning required for stability and performance of the network. The network when
pre-trained on synthetic dataset performs better as per the competition
metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ayyalasomayajula_K/0/1/0/all/0/1&quot;&gt;Kalyan Ram Ayyalasomayajula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Malmberg_F/0/1/0/all/0/1&quot;&gt;Filip Malmberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brun_A/0/1/0/all/0/1&quot;&gt;Anders Brun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08788">
<title>Multivariate normal mixture modeling, clustering and classification with the rebmix package. (arXiv:1801.08788v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08788</link>
<description rdf:parseType="Literal">&lt;p&gt;The rebmix package provides R functions for random univariate and
multivariate finite mixture model generation, estimation, clustering and
classification. The paper is focused on multivariate normal mixture models with
unrestricted variance-covariance matrices. The objective is to show how to
generate datasets for a known number of components, numbers of observations and
component parameters, how to estimate the number of components, component
weights and component parameters and how to predict cluster and class
membership based upon a model trained by the REBMIX algorithm. The accompanying
plotting, bootstrapping and other features of the package are dealt with, too.
For demonstration purpose a multivariate normal dataset with unrestricted
variance-covariance matrices is studied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagode_M/0/1/0/all/0/1&quot;&gt;Marko Nagode&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08881">
<title>Correlated Components Analysis --- Extracting Reliable Dimensions in Multivariate Data. (arXiv:1801.08881v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08881</link>
<description rdf:parseType="Literal">&lt;p&gt;How does one find data dimensions that are reliably expressed across
repetitions? For example, in neuroscience one may want to identify combinations
of brain signals that are reliably activated across multiple trials or
subjects. For a clinical assessment with multiple ratings, one may want to
identify an aggregate score that is reliably reproduced across raters. The
approach proposed here --- &quot;correlated components analysis&quot; --- is to identify
components that maximally correlate between repetitions (e.g. trials, subjects,
raters). This can be expressed as the maximization of the ratio of
between-repetition to within-repetition covariance, resulting in a generalized
eigenvalue problem. We show that covariances can be computed efficiently
without explicitly considering all pairs of repetitions, that the result is
equivalent to multi-class linear discriminant analysis for unbiased signals,
and that the approach also maximize reliability, defined as the mean divided by
the deviation across repetitions. We also extend the method to non-linear
components using kernels, discuss regularization to improve numerical
stability, present parametric and non-parametric tests to establish statistical
significance, and provide code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parra_L/0/1/0/all/0/1&quot;&gt;Lucas C. Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haufe_S/0/1/0/all/0/1&quot;&gt;Stefan Haufe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dmochowski_J/0/1/0/all/0/1&quot;&gt;Jacek P. Dmochowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01513">
<title>Beyond Volume: The Impact of Complex Healthcare Data on the Machine Learning Pipeline. (arXiv:1706.01513v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01513</link>
<description rdf:parseType="Literal">&lt;p&gt;From medical charts to national census, healthcare has traditionally operated
under a paper-based paradigm. However, the past decade has marked a long and
arduous transformation bringing healthcare into the digital age. Ranging from
electronic health records, to digitized imaging and laboratory reports, to
public health datasets, today, healthcare now generates an incredible amount of
digital information. Such a wealth of data presents an exciting opportunity for
integrated machine learning solutions to address problems across multiple
facets of healthcare practice and administration. Unfortunately, the ability to
derive accurate and informative insights requires more than the ability to
execute machine learning models. Rather, a deeper understanding of the data on
which the models are run is imperative for their success. While a significant
effort has been undertaken to develop models able to process the volume of data
obtained during the analysis of millions of digitalized patient records, it is
important to remember that volume represents only one aspect of the data. In
fact, drawing on data from an increasingly diverse set of sources, healthcare
data presents an incredibly complex set of attributes that must be accounted
for throughout the machine learning pipeline. This chapter focuses on
highlighting such challenges, and is broken down into three distinct
components, each representing a phase of the pipeline. We begin with attributes
of the data accounted for during preprocessing, then move to considerations
during model building, and end with challenges to the interpretation of model
output. For each component, we present a discussion around data as it relates
to the healthcare domain and offer insight into the challenges each may impose
on the efficiency of machine learning techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_K/0/1/0/all/0/1&quot;&gt;Keith Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faust_L/0/1/0/all/0/1&quot;&gt;Louis Faust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1&quot;&gt;Nitesh V. Chawla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.01713">
<title>Spoken English Intelligibility Remediation with PocketSphinx Alignment and Feature Extraction Improves Substantially over the State of the Art. (arXiv:1709.01713v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1709.01713</link>
<description rdf:parseType="Literal">&lt;p&gt;We use automatic speech recognition to assess spoken English learner
pronunciation based on the authentic intelligibility of the learners&apos; spoken
responses determined from support vector machine (SVM) classifier or deep
learning neural network model predictions of transcription correctness. Using
numeric features produced by PocketSphinx alignment mode and many recognition
passes searching for the substitution and deletion of each expected phoneme and
insertion of unexpected phonemes in sequence, the SVM models achieve 82 percent
agreement with the accuracy of Amazon Mechanical Turk crowdworker
transcriptions, up from 75 percent reported by multiple independent
researchers. Using such features with SVM classifier probability prediction
models can help computer-aided pronunciation teaching (CAPT) systems provide
intelligibility remediation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Brij Mohan Lal Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salsman_J/0/1/0/all/0/1&quot;&gt;James Salsman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03412">
<title>Elastic-net regularized High-dimensional Negative Binomial Regression: Consistency and Weak Signals Detection. (arXiv:1712.03412v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03412</link>
<description rdf:parseType="Literal">&lt;p&gt;We study sparse high-dimensional negative binomial regression problem for
count data regression by showing non-asymptotic merits of the Elastic-net
regularized estimator. With the KKT conditions, we derive two types of
non-asymptotic oracle inequalities for the elastic net estimates of negative
binomial regression by utilizing Compatibility factor and Stabil Condition,
respectively. Based on oracle inequalities we proposed, we firstly show the
sign consistency property of the Elastic-net estimators provided that the
non-zero components in sparse true vector are large than a proper choice of the
weakest signal detection threshold, and the second application is that we give
an oracle inequality for bounding the grouping effect with high probability,
thirdly, under some assumptions of design matrix, we can recover the true
variable set with high probability if the weakest signal detection threshold is
large than 3 times the value of turning parameter, at last, we briefly discuss
the de-biased Elastic-net estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinzhu Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08243">
<title>ConvSCCS: convolutional self-controlled case series model for lagged adverse event detection. (arXiv:1712.08243v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08243</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increased availability of large databases of electronic health
records (EHRs) comes the chance of enhancing health risks screening. Most
post-marketing detections of adverse drug reaction (ADR) rely on physicians&apos;
spontaneous reports, leading to under reporting. To take up this challenge, we
develop a scalable model to estimate the effect of multiple longitudinal
features (drug exposures) on a rare longitudinal outcome. Our procedure is
based on a conditional Poisson model also known as self-controlled case series
(SCCS). We model the intensity of outcomes using a convolution between
exposures and step functions, that are penalized using a combination of
group-Lasso and total-variation. This approach does not require the
specification of precise risk periods, and allows to study in the same model
several exposures at the same time. We illustrate the fact that this approach
improves the state-of-the-art for the estimation of the relative risks both on
simulations and on a cohort of diabetic patients, extracted from the large
French national health insurance database (SNIIRAM), a SQL database built
around medical reimbursements of more than 65 million people. This work has
been done in the context of a research partnership between Ecole Polytechnique
and CNAMTS (in charge of SNIIRAM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morel_M/0/1/0/all/0/1&quot;&gt;Maryan Morel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bacry_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bacry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guilloux_A/0/1/0/all/0/1&quot;&gt;Agathe Guilloux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leroy_F/0/1/0/all/0/1&quot;&gt;Fanny Leroy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07889">
<title>A Theoretical Investigation of Graph Degree as an Unsupervised Normality Measure. (arXiv:1801.07889v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07889</link>
<description rdf:parseType="Literal">&lt;p&gt;For a graph representation of a dataset, a straightforward normality measure
for a sample can be its graph degree. Considering a weighted graph, degree of a
sample corresponds to sum the corresponding row&apos;s values in a similarity
matrix. The measure is intuitive given the abnormal samples are usually rare
and they are dissimilar to the rest of the data. In order to have an in-depth
theoretical understanding, in this manuscript, we investigate the graph degree
in spectral graph clustering based and kernel based point of views and draw
connections to a recent kernel method for the two sample problem. We show that
our analyses guide us to choose fully-connected graphs whose edge weights are
calculated via universal kernels. We show that a simple graph degree based
unsupervised anomaly detection method with the above properties, achieves
higher accuracy compared to other unsupervised anomaly detection methods on
average over 10 widely used datasets. We also provide an extensive analysis on
the effect of the kernel parameter on the method&apos;s accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aytekin_C/0/1/0/all/0/1&quot;&gt;Caglar Aytekin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cricri_F/0/1/0/all/0/1&quot;&gt;Francesco Cricri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksu_E/0/1/0/all/0/1&quot;&gt;Emre Aksu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>