<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06377"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06423"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06462"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06088"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06152"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06191"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06356"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06452"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06533"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06556"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.08697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.01230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.09477"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03698"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.06377">
<title>Progressive Operational Perceptron with Memory. (arXiv:1808.06377v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.06377</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Operational Perceptron (GOP) was proposed to generalize the
linear neuron model in the traditional Multilayer Perceptron (MLP) and this
model can mimic the synaptic connections of the biological neurons that have
nonlinear neurochemical behaviours. Progressive Operational Perceptron (POP) is
a multilayer network composing of GOPs which is formed layer-wise
progressively. In this work, we propose major modifications that can accelerate
as well as augment the progressive learning procedure of POP by incorporating
an information-preserving, linear projection path from the input to the output
layer at each progressive step. The proposed extensions can be interpreted as a
mechanism that provides direct information extracted from the previously
learned layers to the network, hence the term &quot;memory&quot;. This allows the network
to learn deeper architectures with better data representations. An extensive
set of experiments show that the proposed modifications can surpass the
learning capability of the original POPs and other related algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dat Thanh Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1&quot;&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1&quot;&gt;Alexandros Iosifidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06206">
<title>TLR: Transfer Latent Representation for Unsupervised Domain Adaptation. (arXiv:1808.06206v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06206</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation refers to the process of learning prediction models in a
target domain by making use of data from a source domain. Many classic methods
solve the domain adaptation problem by establishing a common latent space,
which may cause the loss of many important properties across both domains. In
this manuscript, we develop a novel method, transfer latent representation
(TLR), to learn a better latent space. Specifically, we design an objective
function based on a simple linear autoencoder to derive the latent
representations of both domains. The encoder in the autoencoder aims to project
the data of both domains into a robust latent space. Besides, the decoder
imposes an additional constraint to reconstruct the original data, which can
preserve the common properties of both domains and reduce the noise that causes
domain shift. Experiments on cross-domain tasks demonstrate the advantages of
TLR over competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1&quot;&gt;Pan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lefei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Ruimin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06316">
<title>Discovering Context Specific Causal Relationships. (arXiv:1808.06316v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.06316</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing need of personalised decision making, such as
personalised medicine and online recommendations, a growing attention has been
paid to the discovery of the context and heterogeneity of causal relationships.
Most existing methods, however, assume a known cause (e.g. a new drug) and
focus on identifying from data the contexts of heterogeneous effects of the
cause (e.g. patient groups with different responses to the new drug). There is
no approach to efficiently detecting directly from observational data context
specific causal relationships, i.e. discovering the causes and their contexts
simultaneously. In this paper, by taking the advantages of highly efficient
decision tree induction and the well established causal inference framework, we
propose the Tree based Context Causal rule discovery (TCC) method, for
efficient exploration of context specific causal relationships from data.
Experiments with both synthetic and real world data sets show that TCC can
effectively discover context specific causal rules from the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Saisai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiuyong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thuc Duy Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06423">
<title>What Stands-in for a Missing Tool? A Prototypical Grounded Knowledge-based Approach to Tool Substitution. (arXiv:1808.06423v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.06423</link>
<description rdf:parseType="Literal">&lt;p&gt;When a robot is operating in a dynamic environment, it cannot be assumed that
a tool required to solve a given task will always be available. In case of a
missing tool, an ideal response would be to find a substitute to complete the
task. In this paper, we present a proof of concept of a grounded
knowledge-based approach to tool substitution. In order to validate the
suitability of a substitute, we conducted experiments involving 22 substitution
scenarios. The substitutes computed by the proposed approach were validated on
the basis of the experts&apos; choices for each scenario. Our evaluation showed, in
63% scenarios, the approach identified exactly the same substitutes as experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thosar_M/0/1/0/all/0/1&quot;&gt;Madhura Thosar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_C/0/1/0/all/0/1&quot;&gt;Christian A. Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zug_S/0/1/0/all/0/1&quot;&gt;Sebastian Zug&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06462">
<title>Cross-Modal Health State Estimation. (arXiv:1808.06462v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.06462</link>
<description rdf:parseType="Literal">&lt;p&gt;Individuals create and consume more diverse data about themselves today than
any time in history. Sources of this data include wearable devices, images,
social media, geospatial information and more. A tremendous opportunity rests
within cross-modal data analysis that leverages existing domain knowledge
methods to understand and guide human health. Especially in chronic diseases,
current medical practice uses a combination of sparse hospital based biological
metrics (blood tests, expensive imaging, etc.) to understand the evolving
health status of an individual. Future health systems must integrate data
created at the individual level to better understand health status perpetually,
especially in a cybernetic framework. In this work we fuse multiple user
created and open source data streams along with established biomedical domain
knowledge to give two types of quantitative state estimates of cardiovascular
health. First, we use wearable devices to calculate cardiorespiratory fitness
(CRF), a known quantitative leading predictor of heart disease which is not
routinely collected in clinical settings. Second, we estimate inherent genetic
traits, living environmental risks, circadian rhythm, and biological metrics
from a diverse dataset. Our experimental results on 24 subjects demonstrate how
multi-modal data can provide personalized health insight. Understanding the
dynamic nature of health status will pave the way for better health based
recommendation engines, better clinical decision making and positive lifestyle
changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_N/0/1/0/all/0/1&quot;&gt;Nitish Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1&quot;&gt;Vaibhav Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putzel_P/0/1/0/all/0/1&quot;&gt;Preston J. Putzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhimaraju_H/0/1/0/all/0/1&quot;&gt;Hari Bhimaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1&quot;&gt;Srikanth Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Ramesh C. Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06570">
<title>Detecting cognitive impairments by agreeing on interpretations of linguistic features. (arXiv:1808.06570v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.06570</link>
<description rdf:parseType="Literal">&lt;p&gt;Linguistic features have shown promising applications for detecting various
cognitive impairments. To improve detection accuracies, increasing the amount
of data or linguistic features have been two applicable approaches. However,
acquiring additional clinical data could be expensive, and hand-carving
features are burdensome. In this paper, we take a third approach, putting
forward Consensus Networks (CN), a framework to diagnose after reaching
agreements between modalities. We divide the linguistic features into
non-overlapping subsets according to their natural categories, let neural
networks (&quot;ePhysicians&quot;) learn low-dimensional representations (&quot;interpretation
vectors&quot;) that agree with each other. These representations are passed into a
neural network classifier, resulting in a framework for assessing cognitive
impairments. In this paper, we also present methods that empirically improve
the performance of CN. Namely, the addition of a noise modality and allowing
gradients to propagate to interpreters while optimizing the classifier. We then
present two ablation studies to illustrate the effectiveness of CN: dividing
subsets in the natural modalities is more beneficial than doing so randomly,
and that models built with consensus settings outperform those without given
the same modalities of features. To understand further what happens in
consensus networks, we visualize the interpretation vectors during training
procedures. They demonstrate symmetry in an aggregate manner. Overall, using
all of the 413 linguistic features, our models significantly outperform
traditional classifiers, which are used by the state-of-the-art papers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zining Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1&quot;&gt;Jekaterina Novikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1&quot;&gt;Frank Rudzicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08508">
<title>Vehicle Traffic Driven Camera Placement for Better Metropolis Security Surveillance. (arXiv:1705.08508v4 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08508</link>
<description rdf:parseType="Literal">&lt;p&gt;Security surveillance is one of the most important issues in smart cities,
especially in an era of terrorism. Deploying a number of (video) cameras is a
common surveillance approach. Given the never-ending power offered by vehicles
to metropolises, exploiting vehicle traffic to design camera placement
strategies could potentially facilitate security surveillance. This article
constitutes the first effort toward building the linkage between vehicle
traffic and security surveillance, which is a critical problem for smart
cities. We expect our study could influence the decision making of surveillance
camera placement, and foster more research of principled ways of security
surveillance beneficial to our physical-world life. Code has been made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yihui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaobo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiapu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mengchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bo An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1&quot;&gt;Xiaohong Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01316">
<title>On Cognitive Preferences and the Plausibility of Rule-based Models. (arXiv:1803.01316v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01316</link>
<description rdf:parseType="Literal">&lt;p&gt;It is conventional wisdom in machine learning and data mining that logical
models such as rule sets are more interpretable than other models, and that
among such rule-based models, simpler models are more interpretable than more
complex ones. In this position paper, we question this latter assumption by
focusing on one particular aspect of interpretability, namely the plausibility
of models. Roughly speaking, we equate the plausibility of a model with the
likeliness that a user accepts it as an explanation for a prediction. In
particular, we argue that, all other things being equal, longer explanations
may be more convincing than shorter ones, and that the predominant bias for
shorter models, which is typically necessary for learning powerful
discriminative models, may not be suitable when it comes to user acceptance of
the learned models. To that end, we first recapitulate evidence for and against
this postulate, and then report the results of an evaluation in a
crowd-sourcing study based on about 3.000 judgments. The results do not reveal
a strong preference for simple rules, whereas we can observe a weak preference
for longer rules in some domains. We then relate these results to well-known
cognitive biases such as the conjunction fallacy, the representative heuristic,
or the recogition heuristic, and investigate their relation to rule length and
plausibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1&quot;&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kliegr_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Kliegr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1&quot;&gt;Heiko Paulheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06088">
<title>Tangent-Normal Adversarial Regularization for Semi-supervised Learning. (arXiv:1808.06088v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06088</link>
<description rdf:parseType="Literal">&lt;p&gt;The ever-increasing size of modern datasets combined with the difficulty of
obtaining label information has made semi-supervised learning of significant
practical importance in modern machine learning applications. Compared with
supervised learning, the key difficulty in semi-supervised learning is how to
make full use of the unlabeled data. In order to utilize manifold information
provided by unlabeled data, we propose a novel regularization called the
tangent-normal adversarial regularization, which is composed by two parts. The
two terms complement with each other and jointly enforce the smoothness along
two different directions that are crucial for semi-supervised learning. One is
applied along the tangent space of the data manifold, aiming to enforce local
invariance of the classifier on the manifold, while the other is performed on
the normal space orthogonal to the tangent space, intending to impose
robustness on the classifier against the noise causing the observed data
deviating from the underlying data manifold. Both of the two regularizers are
achieved by the strategy of virtual adversarial training. Our method has
achieved state-of-the-art performance on semi-supervised learning tasks on both
artificial dataset and FashionMNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingfeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhanxing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06107">
<title>Exact Passive-Aggressive Algorithms for Learning to Rank Using Interval Labels. (arXiv:1808.06107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06107</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose exact passive-aggressive (PA) online algorithms for
learning to rank. The proposed algorithms can be used even when we have
interval labels instead of actual labels for examples. The proposed algorithms
solve a convex optimization problem at every trial. We find exact solution to
those optimization problems to determine the updated parameters. We propose
support class algorithm (SCA) which finds the active constraints using the KKT
conditions of the optimization problems. These active constrains form support
set which determines the set of thresholds that need to be updated. We derive
update rules for PA, PA-I and PA-II. We show that the proposed algorithms
maintain the ordering of the thresholds after every trial. We provide the
mistake bounds of the proposed algorithms in both ideal and general settings.
We also show experimentally that the proposed algorithms successfully learn
accurate classifiers using interval labels as well as exact labels. Proposed
algorithms also do well compared to other approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manwani_N/0/1/0/all/0/1&quot;&gt;Naresh Manwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1&quot;&gt;Mohit Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06152">
<title>On Design of Problem Token Questions in Quality of Experience Surveys. (arXiv:1808.06152v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1808.06152</link>
<description rdf:parseType="Literal">&lt;p&gt;User surveys for Quality of Experience (QoE) are a critical source of
information. In addition to the common &quot;star rating&quot; used to estimate Mean
Opinion Score (MOS), more detailed survey questions (problem tokens) about
specific areas provide valuable insight into the factors impacting QoE. This
paper explores two aspects of the problem token questionnaire design. First, we
study the bias introduced by fixed question order, and second, we study the
challenge of selecting a subset of questions to keep the token set small. Based
on 900,000 calls gathered using a randomized controlled experiment from a live
system, we find that the order bias can be significantly reduced by randomizing
the display order of tokens. The difference in response rate varies based on
token position and display design. It is worth noting that the users respond to
the randomized-order variant at levels that are comparable to the fixed-order
variant. The effective selection of a subset of token questions is achieved by
extracting tokens that provide the highest information gain over user ratings.
This selection is known to be in the class of NP-hard problems. We apply a
well-known greedy submodular maximization method on our dataset to capture 94%
of the information using just 30% of the questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupchup_J/0/1/0/all/0/1&quot;&gt;Jayant Gupchup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beyrami_E/0/1/0/all/0/1&quot;&gt;Ebrahim Beyrami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ellis_M/0/1/0/all/0/1&quot;&gt;Martin Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hosseinkashi_Y/0/1/0/all/0/1&quot;&gt;Yasaman Hosseinkashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johnson_S/0/1/0/all/0/1&quot;&gt;Sam Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cutler_R/0/1/0/all/0/1&quot;&gt;Ross Cutler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06191">
<title>Fourier analysis perspective for sufficient dimension reduction problem. (arXiv:1808.06191v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06191</link>
<description rdf:parseType="Literal">&lt;p&gt;A theory of sufficient dimension reduction (SDR) is developed from an
optimizational perspective. In our formulation of the problem, instead of
dealing with raw data, we assume that our ground truth includes a mapping
${\mathbf f}: {\mathbb R}^n\rightarrow {\mathbb R}^m$ and a probability
distribution function $p$ over ${\mathbb R}^n$, both given analytically. We
formulate SDR as a problem of finding a function ${\mathbf g}: {\mathbb
R}^k\rightarrow {\mathbb R}^m$ and a matrix $P\in {\mathbb R}^{k\times n}$ such
that ${\mathbb E}_{{\mathbf x}\sim p({\mathbf x})} \left|{\mathbf f}({\mathbf
x}) - {\mathbf g}(P{\mathbf x})\right|^2$ is minimal. It turns out that the
latter problem allows a reformulation in the dual space, i.e. instead of
searching for ${\mathbf g}(P{\mathbf x})$ we suggest searching for its Fourier
transform. First, we characterize all tempered distributions that can serve as
the Fourier transform of such functions. The reformulation in the dual space
can be interpreted as a problem of finding a $k$-dimensional linear subspace
$S$ and a tempered distribution ${\mathbf t}$ supported in $S$ such that
${\mathbf t}$ is &quot;close&quot; in a certain sense to the Fourier transform of
${\mathbf f}$.
&lt;/p&gt;
&lt;p&gt;Instead of optimizing over generalized functions with a $k$-dimensional
support, we suggest minimizing over ordinary functions but with an additional
term $R$ that penalizes a strong distortion of the support from any
$k$-dimensional linear subspace. For a specific case of $R$, we develop an
algorithm that can be formulated for functions given in the initial form as
well as for their Fourier transforms. Eventually, we report results of
numerical experiments with a discretized version of the latter algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takhanov_R/0/1/0/all/0/1&quot;&gt;Rustem Takhanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06288">
<title>Multimodal speech synthesis architecture for unsupervised speaker adaptation. (arXiv:1808.06288v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1808.06288</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new architecture for speaker adaptation of
multi-speaker neural-network speech synthesis systems, in which an unseen
speaker&apos;s voice can be built using a relatively small amount of speech data
without transcriptions. This is sometimes called &quot;unsupervised speaker
adaptation&quot;. More specifically, we concatenate the layers to the audio inputs
when performing unsupervised speaker adaptation while we concatenate them to
the text inputs when synthesizing speech from text. Two new training schemes
for the new architecture are also proposed in this paper. These training
schemes are not limited to speech synthesis, other applications are suggested.
Experimental results show that the proposed model not only enables adaptation
to unseen speakers using untranscribed speech but it also improves the
performance of multi-speaker modeling and speaker adaptation using transcribed
audio files.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luong_H/0/1/0/all/0/1&quot;&gt;Hieu-Thi Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06296">
<title>Universal Stagewise Learning for Non-Convex Problems with Convergence on Averaged Solutions. (arXiv:1808.06296v1 [math.OC])</title>
<link>http://arxiv.org/abs/1808.06296</link>
<description rdf:parseType="Literal">&lt;p&gt;Although stochastic gradient descent (\sgd) method and its variants (e.g.,
stochastic momentum methods, \ada) are the choice of algorithms for solving
non-convex problems (especially deep learning), there still remain big gaps
between the theory and the practice with many questions unresolved. For
example, there is still a lack of theories of convergence for {\sgd} that uses
stagewise step size and returns an averaged solution. In addition, theoretical
insights of why adaptive step size of {\ada} could improve non-adaptive step
size of {\sgd} is still missing for non-convex optimization. This paper aims to
address these questions and fill the gap between theory and practice. We
propose a universal stagewise optimization framework for a broad family of
non-smooth non-convex problems with the following key features: (i) each stage
calls a basic algorithm (e.g., {\sgd} or \ada) for a regularized convex problem
that returns an averaged solution; (ii) the step size is decreased in a
stagewise manner; (iii) an averaged solution is returned as the final solution
that is selected from all stagewise averaged solutions with sampling
probabilities {\it increasing} as the stage number. Our theoretical results of
stagewise {\ada} exhibit its adaptive convergence, therefore shed insights on
its faster convergence for problems with sparse stochastic gradients than
stagewise \sgd. To the best of our knowledge, these new results are the first
of their kind for addressing the unresolved issues of existing theories
mentioned earlier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bowen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06314">
<title>A General Framework of Multi-Armed Bandit Processes by Switching Restrictions. (arXiv:1808.06314v1 [math.PR])</title>
<link>http://arxiv.org/abs/1808.06314</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a general framework of multi-armed bandit (MAB) processes
by introducing a type of restrictions on the switches among arms to the arms
evolving in continuous time.
&lt;/p&gt;
&lt;p&gt;The Gittins index process is developed for any single arm subject to the
restrictions on stopping times and then the optimality of the corresponding
Gittins index rule is established. The Gittins indices defined in this paper
are consistent with the ones for MAB processes in continuous time, discrete
time, and semi-Markovian setting so that the new theory covers the classical
models as special cases and also applies to many other situations that have not
yet been touched in the literature. While the proof of the optimality of
Gittins index policies benefits from ideas in the existing theory of MAB
processes in continuous time, new techniques are introduced which drastically
simplifies the proof.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bao_W/0/1/0/all/0/1&quot;&gt;Wenqing Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xiaoqiang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xianyi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06324">
<title>PAC-learning is Undecidable. (arXiv:1808.06324v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06324</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of attempting to learn the mapping between data and labels is the
crux of any machine learning task. It is, therefore, of interest to the machine
learning community on practical as well as theoretical counts to consider the
existence of a test or criterion for deciding the feasibility of attempting to
learn. We investigate the existence of such a criterion in the setting of
PAC-learning, basing the feasibility solely on whether the mapping to be learnt
lends itself to approximation by a given class of hypothesis functions. We show
that no such criterion exists, exposing a fundamental limitation in the
decidability of learning. In other words, we prove that testing for
PAC-learnability is undecidable in the Turing sense. We also briefly discuss
some of the probable implications of this result to the current practice of
machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatraman_S/0/1/0/all/0/1&quot;&gt;Sairaam Venkatraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1&quot;&gt;S Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarma_R/0/1/0/all/0/1&quot;&gt;R Raghunatha Sarma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06329">
<title>The Mismatch Principle: Statistical Learning Under Large Model Uncertainties. (arXiv:1808.06329v1 [math.ST])</title>
<link>http://arxiv.org/abs/1808.06329</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the learning capacity of empirical risk minimization with regard to
the squared loss and a convex hypothesis class consisting of linear functions.
While these types of estimators were originally designed for noisy linear
regression problems, it recently turned out that they are in fact capable of
handling considerably more complicated situations, involving highly non-linear
distortions. This work intends to provide a comprehensive explanation of this
somewhat astonishing phenomenon. At the heart of our analysis stands the
mismatch principle, which is a simple, yet generic recipe to establish
theoretical error bounds for empirical risk minimization. The scope of our
results is fairly general, permitting arbitrary sub-Gaussian input-output
pairs, possibly with strongly correlated feature variables. Noteworthy, the
mismatch principle also generalizes to a certain extent the classical
orthogonality principle for ordinary least squares. This adaption allows us to
investigate problem setups of recent interest, most importantly,
high-dimensional parameter regimes and non-linear observation processes. In
particular, our theoretical framework is applied to various scenarios of
practical relevance, such as single-index models, variable selection, and
strongly correlated designs. We thereby demonstrate the key purpose of the
mismatch principle, that is, learning (semi-)parametric output rules under
large model uncertainties and misspecifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Genzel_M/0/1/0/all/0/1&quot;&gt;Martin Genzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kutyniok_G/0/1/0/all/0/1&quot;&gt;Gitta Kutyniok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06347">
<title>A Distribution Similarity Based Regularizer for Learning Bayesian Networks. (arXiv:1808.06347v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06347</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic graphical models compactly represent joint distributions by
decomposing them into factors over subsets of random variables. In Bayesian
networks, the factors are conditional probability distributions. For many
problems, common information exists among those factors. Adding similarity
restrictions can be viewed as imposing prior knowledge for model
regularization. With proper restrictions, learned models usually generalize
better. In this work, we study methods that exploit such high-level
similarities to regularize the learning process and apply them to the task of
modeling the wave propagation in inhomogeneous media. We propose a novel
distribution-based penalization approach that encourages similar conditional
probability distribution rather than force the parameters to be similar
explicitly. We show in experiment that our proposed algorithm solves the
modeling wave propagation problem, which other baseline methods are not able to
solve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1&quot;&gt;Weirui Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenyi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06356">
<title>Causal Discovery by Telling Apart Parents and Children. (arXiv:1808.06356v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06356</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of inferring the directed, causal graph from
observational data, assuming no hidden confounders. We take an information
theoretic approach, and make three main contributions.
&lt;/p&gt;
&lt;p&gt;First, we show how through algorithmic information theory we can obtain SCI,
a highly robust, effective and computationally efficient test for conditional
independence---and show it outperforms the state of the art when applied in
constraint-based inference methods such as stable PC.
&lt;/p&gt;
&lt;p&gt;Second, building upon on SCI, we show how to tell apart the parents and
children of a given node based on the algorithmic Markov condition. We give the
Climb algorithm to efficiently discover the directed, causal Markov
blanket---and show it is at least as accurate as inferring the global network,
while being much more efficient.
&lt;/p&gt;
&lt;p&gt;Last, but not least, we detail how we can use the Climb score to direct those
edges that state of the art causal discovery algorithms based on PC or GES
leave undirected---and show this improves their precision, recall and F1 scores
by up to 20%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marx_A/0/1/0/all/0/1&quot;&gt;Alexander Marx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vreeken_J/0/1/0/all/0/1&quot;&gt;Jilles Vreeken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06452">
<title>Reproducible evaluation of classification methods in Alzheimer&apos;s disease: framework and application to MRI and PET data. (arXiv:1808.06452v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06452</link>
<description rdf:parseType="Literal">&lt;p&gt;A large number of papers have introduced novel machine learning and feature
extraction methods for automatic classification of AD. However, they are
difficult to reproduce because key components of the validation are often not
readily available. These components include selected participants and input
data, image preprocessing and cross-validation procedures. The performance of
the different approaches is also difficult to compare objectively. In
particular, it is often difficult to assess which part of the method provides a
real improvement, if any. We propose a framework for reproducible and objective
classification experiments in AD using three publicly available datasets (ADNI,
AIBL and OASIS). The framework comprises: i) automatic conversion of the three
datasets into BIDS format, ii) a modular set of preprocessing pipelines,
feature extraction and classification methods, together with an evaluation
framework, that provide a baseline for benchmarking the different components.
We demonstrate the use of the framework for a large-scale evaluation on 1960
participants using T1 MRI and FDG PET data. In this evaluation, we assess the
influence of different modalities, preprocessing, feature types, classifiers,
training set sizes and datasets. Performances were in line with the
state-of-the-art. FDG PET outperformed T1 MRI for all classification tasks. No
difference in performance was found for the use of different atlases, image
smoothing, partial volume correction of FDG PET images, or feature type. Linear
SVM and L2-logistic regression resulted in similar performance and both
outperformed random forests. The classification performance increased along
with the number of subjects used for training. Classifiers trained on ADNI
generalized well to AIBL and OASIS. All the code of the framework and the
experiments is publicly available at:
https://gitlab.icm-institute.org/aramislab/AD-ML.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samper_Gonzalez_J/0/1/0/all/0/1&quot;&gt;Jorge Samper-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgos_N/0/1/0/all/0/1&quot;&gt;Ninon Burgos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottani_S/0/1/0/all/0/1&quot;&gt;Simona Bottani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fontanella_S/0/1/0/all/0/1&quot;&gt;Sabrina Fontanella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pascal Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcoux_A/0/1/0/all/0/1&quot;&gt;Arnaud Marcoux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Routier_A/0/1/0/all/0/1&quot;&gt;Alexandre Routier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillon_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Guillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacci_M/0/1/0/all/0/1&quot;&gt;Michael Bacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Junhao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertrand_A/0/1/0/all/0/1&quot;&gt;Anne Bertrand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertin_H/0/1/0/all/0/1&quot;&gt;Hugo Bertin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habert_M/0/1/0/all/0/1&quot;&gt;Marie-Odile Habert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durrleman_S/0/1/0/all/0/1&quot;&gt;Stanley Durrleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evgeniou_T/0/1/0/all/0/1&quot;&gt;Theodoros Evgeniou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colliot_O/0/1/0/all/0/1&quot;&gt;Olivier Colliot&lt;/a&gt;, for the &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Initiative_A/0/1/0/all/0/1&quot;&gt;Alzheimer&amp;#x27;s Disease Neuroimaging Initiative&lt;/a&gt;, the &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biomarkers_A/0/1/0/all/0/1&quot;&gt;Australian Imaging Biomarkers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ageing_L/0/1/0/all/0/1&quot;&gt;Lifestyle flagship study of ageing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06470">
<title>predictSLUMS: A new model for identifying and predicting informal settlements and slums in cities from street intersections using machine learning. (arXiv:1808.06470v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.06470</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying current and future informal regions within cities remains a
crucial issue for policymakers and governments in developing countries. The
delineation process of identifying such regions in cities requires a lot of
resources. While there are various studies that identify informal settlements
based on satellite image classification, relying on both supervised or
unsupervised machine learning approaches, these models either require multiple
input data to function or need further development with regards to precision.
In this paper, we introduce a novel method for identifying and predicting
informal settlements using only street intersections data, regardless of the
variation of urban form, number of floors, materials used for construction or
street width. With such minimal input data, we attempt to provide planners and
policy-makers with a pragmatic tool that can aid in identifying informal zones
in cities. The algorithm of the model is based on spatial statistics and a
machine learning approach, using Multinomial Logistic Regression (MNL) and
Artificial Neural Networks (ANN). The proposed model relies on defining
informal settlements based on two ubiquitous characteristics that these regions
tend to be filled in with smaller subdivided lots of housing relative to the
formal areas within the local context, and the paucity of services and
infrastructure within the boundary of these settlements that require relatively
bigger lots. We applied the model in five major cities in Egypt and India that
have spatial structures in which informality is present. These cities are
Greater Cairo, Alexandria, Hurghada and Minya in Egypt, and Mumbai in India.
The predictSLUMS model shows high validity and accuracy for identifying and
predicting informality within the same city the model was trained on or in
different ones of a similar context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mohamed R. Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titheridge_H/0/1/0/all/0/1&quot;&gt;Helena Titheridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haworth_J/0/1/0/all/0/1&quot;&gt;James Haworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06475">
<title>The perceived quality of process discovery tools. (arXiv:1808.06475v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.06475</link>
<description rdf:parseType="Literal">&lt;p&gt;Process discovery has seen a rise in popularity in the last decade for both
researchers and businesses. Recent developments mainly focused on the power and
the functionalities of the discovery algorithm. While continuous improvement of
these functional aspects is very important, non-functional aspects such as
visualization and usability are often overlooked. However, these aspects are
considered valuable for end-users and play an important part in the experience
of these end-users when working with a process discovery tool. A questionnaire
has been sent out to give end-users the opportunity to voice their opinion on
available process discovery tools and about the state of process discovery as a
domain in general. The results of 66 respondents are presented and compared
with the answers of 63 respondents that were contacted through one particular
software vendor&apos;s employee and customer base (i.e., Celonis).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bru_F/0/1/0/all/0/1&quot;&gt;Francis Bru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claes_J/0/1/0/all/0/1&quot;&gt;Jan Claes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06533">
<title>Spatial Filtering for Brain Computer Interfaces: A Comparison between the Common Spatial Pattern and Its Variant. (arXiv:1808.06533v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1808.06533</link>
<description rdf:parseType="Literal">&lt;p&gt;The electroencephalogram (EEG) is the most popular form of input for brain
computer interfaces (BCIs). However, it can be easily contaminated by various
artifacts and noise, e.g., eye blink, muscle activities, powerline noise, etc.
Therefore, the EEG signals are often filtered both spatially and temporally to
increase the signal-to-noise ratio before they are fed into a machine learning
algorithm for recognition. This paper considers spatial filtering,
particularly, the common spatial pattern (CSP) filters for EEG classification.
In binary classification, CSP seeks a set of filters to maximize the variance
for one class while minimizing it for the other. We first introduce the
traditional solution, and then a new solution based on a slightly different
objective function. We performed comprehensive experiments on motor imagery to
compare the two approaches, and found that generally the traditional CSP
solution still gives better results. We also showed that adding regularization
to the covariance matrices can improve the final classification performance, no
matter which objective function is used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongrui Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06536">
<title>Study of Set-Membership Adaptive Kernel Algorithms. (arXiv:1808.06536v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1808.06536</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last decade, a considerable research effort has been devoted to
developing adaptive algorithms based on kernel functions. One of the main
features of these algorithms is that they form a family of universal
approximation techniques, solving problems with nonlinearities elegantly. In
this paper, we present data-selective adaptive kernel normalized least-mean
square (KNLMS) algorithms that can increase their learning rate and reduce
their computational complexity. In fact, these methods deal with kernel
expansions, creating a growing structure also known as the dictionary, whose
size depends on the number of observations and their innovation. The algorithms
described herein use an adaptive step-size to accelerate the learning and can
offer an excellent tradeoff between convergence speed and steady state, which
allows them to solve nonlinear filtering and estimation problems with a large
number of parameters without requiring a large computational cost. The
data-selective update scheme also limits the number of operations performed and
the size of the dictionary created by the kernel expansion, saving
computational resources and dealing with one of the major problems of kernel
adaptive algorithms. A statistical analysis is carried out along with a
computational complexity analysis of the proposed algorithms. Simulations show
that the proposed KNLMS algorithms outperform existing algorithms in examples
of nonlinear system identification and prediction of a time series originating
from a nonlinear difference equation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Flores_A/0/1/0/all/0/1&quot;&gt;A. Flores&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lamare_R/0/1/0/all/0/1&quot;&gt;R. C. de Lamare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06537">
<title>Ricean K-factor Estimation based on Channel Quality Indicator in OFDM Systems using Neural Network. (arXiv:1808.06537v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1808.06537</link>
<description rdf:parseType="Literal">&lt;p&gt;Ricean channel model is widely used in wireless communications to
characterize the channels with a line-of-sight path. The Ricean K factor,
defined as the ratio of direct path and scattered paths, provides a good
indication of the link quality. Most existing works estimate K factor based on
either maximum-likelihood criterion or higher-order moments, and the existing
works are targeted at K-factor estimation at receiver side. In this work, a
novel approach is proposed. Cast as a classification problem, the estimation of
K factor by neural network provides high accuracy. Moreover, the proposed
K-factor estimation is done at transmitter side for transmit processing, thus
saving the limited feedback bandwidth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06556">
<title>Triangle Lasso for Simultaneous Clustering and Optimization in Graph Datasets. (arXiv:1808.06556v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06556</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, network lasso has drawn many attentions due to its remarkable
performance on simultaneous clustering and optimization. However, it usually
suffers from the imperfect data (noise, missing values etc), and yields
sub-optimal solutions. The reason is that it finds the similar instances
according to their features directly, which is usually impacted by the
imperfect data, and thus returns sub-optimal results. In this paper, we propose
triangle lasso to avoid its disadvantage. Triangle lasso finds the similar
instances according to their neighbours. If two instances have many common
neighbours, they tend to become similar. Although some instances are profiled
by the imperfect data, it is still able to find the similar counterparts.
Furthermore, we develop an efficient algorithm based on Alternating Direction
Method of Multipliers (ADMM) to obtain a moderately accurate solution. In
addition, we present a dual method to obtain the accurate solution with the low
additional time consumption. We demonstrate through extensive numerical
experiments that triangle lasso is robust to the imperfect data. It usually
yields a better performance than the state-of-the-art method when performing
data analysis tasks in practical scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yawei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1&quot;&gt;En Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinzhong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianping Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06560">
<title>Multi-View Graph Embedding Using Randomized Shortest Paths. (arXiv:1808.06560v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06560</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data sets often provide multiple types of information about the
same set of entities. This data is well represented by multi-view graphs, which
consist of several distinct sets of edges over the same nodes. These can be
used to analyze how entities interact from different viewpoints. Combining
multiple views improves the quality of inferences drawn from the underlying
data, which has increased interest in developing efficient multi-view graph
embedding methods. We propose an algorithm, C-RSP, that generates a common (C)
embedding of a multi-view graph using Randomized Shortest Paths (RSP). This
algorithm generates a dissimilarity measure between nodes by minimizing the
expected cost of a random walk between any two nodes across all views of a
multi-view graph, in doing so encoding both the local and global structure of
the graph. We test C-RSP on both real and synthetic data and show that it
outperforms benchmark algorithms at embedding and clustering tasks while
remaining computationally efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gamage_A/0/1/0/all/0/1&quot;&gt;Anuththari Gamage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rappaport_B/0/1/0/all/0/1&quot;&gt;Brian Rappaport&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aeron_S/0/1/0/all/0/1&quot;&gt;Shuchin Aeron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06576">
<title>Peptide-Spectra Matching from Weak Supervision. (arXiv:1808.06576v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1808.06576</link>
<description rdf:parseType="Literal">&lt;p&gt;As in many other scientific domains, we face a fundamental problem when using
machine learning to identify proteins from mass spectrometry data: large ground
truth datasets mapping inputs to correct outputs are extremely difficult to
obtain. Instead, we have access to imperfect hand-coded models crafted by
domain experts. In this paper, we apply deep neural networks to an important
step of the protein identification problem, the pairing of mass spectra with
short sequences of amino acids called peptides. We train our model to
differentiate between top scoring results from a state-of-the art classical
system and hard-negative second and third place results. Our resulting model is
much better at identifying peptides with spectra than the model used to
generate its training data. In particular, we achieve a 43% improvement over
standard matching methods and a 10% improvement over a combination of the
matching method and an industry standard cross-spectra reranking tool.
Importantly, in a more difficult experimental regime that reflects current
challenges facing biologists, our advantage over the previous state-of-the-art
grows to 15% even after reranking. We believe this approach will generalize to
other challenging scientific problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Schoenholz_S/0/1/0/all/0/1&quot;&gt;Samuel S. Schoenholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hackett_S/0/1/0/all/0/1&quot;&gt;Sean Hackett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deming_L/0/1/0/all/0/1&quot;&gt;Laura Deming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Melamud_E/0/1/0/all/0/1&quot;&gt;Eugene Melamud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jaitly_N/0/1/0/all/0/1&quot;&gt;Navdeep Jaitly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+McAllister_F/0/1/0/all/0/1&quot;&gt;Fiona McAllister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+OBrien_J/0/1/0/all/0/1&quot;&gt;Jonathon O&amp;#x27;Brien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dahl_G/0/1/0/all/0/1&quot;&gt;George Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bennett_B/0/1/0/all/0/1&quot;&gt;Bryson Bennett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kohler_D/0/1/0/all/0/1&quot;&gt;Daphne Kohler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06581">
<title>The Deconfounded Recommender: A Causal Inference Approach to Recommendation. (arXiv:1808.06581v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.06581</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of a recommender system is to show its users items that they will
like. In forming its prediction, the recommender system tries to answer: &quot;what
would the rating be if we &apos;forced&apos; the user to watch the movie?&quot; This is a
question about an intervention in the world, a causal question, and so
traditional recommender systems are doing causal inference from observational
data. This paper develops a causal inference approach to recommendation.
Traditional recommenders are likely biased by unobserved confounders, variables
that affect both the &quot;treatment assignments&quot; (which movies the users watch) and
the &quot;outcomes&quot; (how they rate them). We develop the deconfounded recommender, a
strategy to leverage classical recommendation models for causal predictions.
The deconfounded recommender uses Poisson factorization on which movies users
watched to infer latent confounders in the data; it then augments common
recommendation models to correct for potential confounding bias. The
deconfounded recommender improves recommendation and it enjoys stable
performance against interventions on test sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Dawen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1&quot;&gt;Laurent Charlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.08697">
<title>Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via Truncated Rayleigh Flow. (arXiv:1604.08697v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1604.08697</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse generalized eigenvalue problem (GEP) plays a pivotal role in a large
family of high-dimensional statistical models, including sparse Fisher&apos;s
discriminant analysis, canonical correlation analysis, and sufficient dimension
reduction. Sparse GEP involves solving a non-convex optimization problem. Most
existing methods and theory in the context of specific statistical models that
are special cases of the sparse GEP require restrictive structural assumptions
on the input matrices. In this paper, we propose a two-stage computational
framework to solve the sparse GEP. At the first stage, we solve a convex
relaxation of the sparse GEP. Taking the solution as an initial value, we then
exploit a nonconvex optimization perspective and propose the truncated Rayleigh
flow method (Rifle) to estimate the leading generalized eigenvector. We show
that Rifle converges linearly to a solution with the optimal statistical rate
of convergence for many statistical models. Theoretically, our method
significantly improves upon the existing literature by eliminating structural
assumptions on the input matrices for both stages. To achieve this, our
analysis involves two key ingredients: (i) a new analysis of the gradient based
method on nonconvex objective functions, and (ii) a fine-grained
characterization of the evolution of sparsity patterns along the solution path.
Thorough numerical studies are provided to validate the theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kean Ming Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.01230">
<title>Bayesian Optical Flow with Uncertainty Quantification. (arXiv:1611.01230v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.01230</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical flow refers to the visual motion observed between two consecutive
images. Since the degree of freedom is typically much larger than the
constraints imposed by the image observations, the straightforward formulation
of optical flow as an inverse problem is ill-posed. Standard approaches to
determine optical flow rely on formulating and solving an optimization problem
that contains both a data fidelity term and a regularization term, the latter
effectively resolves the otherwise ill-posedness of the inverse problem. In
this work, we depart from the deterministic formalism, and instead treat
optical flow as a statistical inverse problem. We discuss how a classical
optical flow solution can be interpreted as a point estimate in this more
general framework. The statistical approach, whose &quot;solution&quot; is a distribution
of flow fields, which we refer to as Bayesian optical flow, allows not only
&quot;point&quot; estimates (e.g., the computation of average flow field), but also
statistical estimates (e.g., quantification of uncertainty) that are beyond any
standard method for optical flow. As application, we benchmark Bayesian optical
flow together with uncertainty quantification using several types of prescribed
ground-truth flow fields and images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jie Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Quevedo_F/0/1/0/all/0/1&quot;&gt;Fernando J. Quevedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bollt_E/0/1/0/all/0/1&quot;&gt;Erik Bollt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.09477">
<title>A Compressive Sensing Approach to Community Detection with Applications. (arXiv:1708.09477v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1708.09477</link>
<description rdf:parseType="Literal">&lt;p&gt;The community detection problem for graphs asks one to partition the n
vertices V of a graph G into k communities, or clusters, such that there are
many intracluster edges and few intercluster edges. Of course this is
equivalent to finding a permutation matrix P such that, if A denotes the
adjacency matrix of G, then PAP^T is approximately block diagonal. As there are
k^n possible partitions of n vertices into k subsets, directly determining the
optimal clustering is clearly infeasible. Instead one seeks to solve a more
tractable approximation to the clustering problem. In this paper we reformulate
the community detection problem via sparse solution of a linear system
associated with the Laplacian of a graph G and then develop a two-stage
approach based on a thresholding technique and a compressive sensing algorithm
to find a sparse solution which corresponds to the community containing a
vertex of interest in G. Crucially, our approach results in an algorithm which
is able to find a single cluster of size n_0 in O(nlog(n)n_0) operations and
all k clusters in fewer than O(n^2ln(n)) operations. This is a marked
improvement over the classic spectral clustering algorithm, which is unable to
find a single cluster at a time and takes approximately O(n^3) operations to
find all k clusters. Moreover, we are able to provide robust guarantees of
success for the case where G is drawn at random from the Stochastic Block
Model, a popular model for graphs with clusters. Extensive numerical results
are also provided, showing the efficacy of our algorithm on both synthetic and
real-world data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_M/0/1/0/all/0/1&quot;&gt;Ming-Jun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mckenzie_D/0/1/0/all/0/1&quot;&gt;Daniel Mckenzie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09001">
<title>One-to-one Mapping between Stimulus and Neural State: Memory and Classification. (arXiv:1805.09001v3 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09001</link>
<description rdf:parseType="Literal">&lt;p&gt;Synaptic strength can be seen as probability to propagate impulse, and
function could exist from propagation activity to synaptic strength. If the
function satisfies constraints such as continuity and monotonicity, neural
network under external stimulus would always go to fixed point, and there could
be one-one-one mapping between external stimulus and synaptic strength at fixed
point. In other words, neural network &quot;memorizes&quot; external stimulus in its
synapses. A biological classifier is proposed to utilize this mapping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Sizhong Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00877">
<title>Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization. (arXiv:1806.00877v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00877</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of single-agent reinforcement learning, multi-agent
reinforcement learning (MARL) remains challenging due to complex interactions
between agents. Motivated by decentralized applications such as sensor
networks, swarm robotics, and power grids, we study policy evaluation in MARL,
where agents with jointly observed state-action pairs and private local rewards
collaborate to learn the value of a given policy.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a double averaging scheme, where each agent
iteratively performs averaging over both space and time to incorporate
neighboring gradient information and local reward information, respectively. We
prove that the proposed algorithm converges to the optimal solution at a global
geometric rate. In particular, such an algorithm is built upon a primal-dual
reformulation of the mean squared projected Bellman error minimization problem,
which gives rise to a decentralized convex-concave saddle-point problem. To the
best of our knowledge, the proposed double averaging primal-dual optimization
algorithm is the first to achieve fast finite-time convergence on decentralized
convex-concave saddle-point problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wai_H/0/1/0/all/0/1&quot;&gt;Hoi-To Wai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mingyi Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01085">
<title>One-Class Kernel Spectral Regression for Outlier Detection. (arXiv:1807.01085v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01085</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces a new efficient nonlinear one-class classifier
formulated as the Rayleigh quotient criterion optimisation. The method,
operating in a reproducing kernel Hilbert subspace, minimises the scatter of
target distribution along an optimal projection direction while at the same
time keeping projections of positive observations distant from the mean of the
negative class. We provide a graph embedding view of the problem which can then
be solved efficiently using the spectral regression approach. In this sense,
unlike previous similar methods which often require costly eigen-computations
of dense matrices, the proposed approach casts the problem under consideration
into a regression framework which is computationally more efficient. In
particular, it is shown that the dominant complexity of the proposed method is
the complexity of computing the kernel matrix. Additional appealing
characteristics of the proposed one-class classifier are: 1-the ability to be
trained in an incremental fashion (allowing for application in streaming data
scenarios while also reducing the computational complexity in a non-streaming
operation mode); 2-being unsupervised, but providing the option for refining
the solution using negative training examples, when available; Last but not
least, 3-the use of the kernel trick which facilitates a nonlinear mapping of
the data into a high-dimensional feature space to seek better solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arashloo_S/0/1/0/all/0/1&quot;&gt;Shervin Rahimzadeh Arashloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1&quot;&gt;Josef Kittler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03513">
<title>Automatic trajectory recognition in Active Target Time Projection Chambers data by means of hierarchical clustering. (arXiv:1807.03513v3 [physics.ins-det] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03513</link>
<description rdf:parseType="Literal">&lt;p&gt;The automatic reconstruction of three-dimensional particle tracks from Active
Target Time Projection Chambers data can be a challenging task, especially in
the presence of noise. In this article, we propose a non-parametric algorithm
that is based on the idea of clustering point triplets instead of the original
points. We define an appropriate distance measure on point triplets and then
apply a single-link hierarchical clustering on the triplets. Compared to
parametric approaches like RANSAC or the Hough transform, the new algorithm has
the advantage of potentially finding trajectories even of shapes that are not
known beforehand. This feature is particularly important in low-energy nuclear
physics experiments with Active Targets operating inside a magnetic field. The
algorithm has been validated using data from experiments performed with the
Active Target Time Projection Chamber developed at the National Superconducting
Cyclotron Laboratory (NSCL).The results demonstrate the capability of the
algorithm to identify and isolate particle tracks that describe non-analytical
trajectories. For curved tracks, the vertex detection recall was 86\% and the
precision 94\%. For straight tracks, the vertex detection recall was 96\% and
the precision 98\%. In the case of a test set containing only straight linear
tracks, the algorithm performed better than an iterative Hough transform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dalitz_C/0/1/0/all/0/1&quot;&gt;Christoph Dalitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ayyad_Y/0/1/0/all/0/1&quot;&gt;Yassid Ayyad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wilberg_J/0/1/0/all/0/1&quot;&gt;Jens Wilberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Aymans_L/0/1/0/all/0/1&quot;&gt;Lukas Aymans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bazin_D/0/1/0/all/0/1&quot;&gt;Daniel Bazin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mittig_W/0/1/0/all/0/1&quot;&gt;Wolfgang Mittig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11156">
<title>Transformationally Identical and Invariant Convolutional Neural Networks by Combining Symmetric Operations or Input Vectors. (arXiv:1807.11156v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11156</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformationally invariant processors constructed by transformed input
vectors or operators have been suggested and applied to many applications. In
this study, transformationally identical processing based on combining results
of all sub-processes with corresponding transformations at one of the
processing steps or at the beginning step were found to be equivalent for a
given condition. This property can be applied to most convolutional neural
network (CNN) systems. Specifically, a transformationally identical CNN can be
constructed by arranging internally symmetric operations in parallel with the
same transformation family that includes a flatten layer with weights sharing
among their corresponding transformation elements. Other transformationally
identical CNNs can be constructed by averaging transformed input vectors of the
family at the input layer followed by an ordinary CNN process or by a set of
symmetric operations. Interestingly, we found that both types of
transformationally identical CNN systems are mathematically equivalent by
either applying an averaging operation to corresponding elements of all
sub-channels before the activation function or without using a non-linear
activation function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1&quot;&gt;ShihChung B. Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1&quot;&gt;Matthew T. Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_S/0/1/0/all/0/1&quot;&gt;Seong K. Mun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03698">
<title>BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear Regressions. (arXiv:1808.03698v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03698</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a new machine learning (ML) model for nonlinear
regression called Boosting Smooth Transition Regression Trees (BooST). The main
advantage of the BooST model is that it estimates the derivatives (partial
effects) of very general nonlinear models, providing more interpretation about
the mapping between the covariates and the dependent variable than other tree
based models, such as Random Forests. We provide some asymptotic theory that
shows consistency of the partial derivative estimates and we present some
examples on both simulated and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fonseca_Y/0/1/0/all/0/1&quot;&gt;Yuri Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Medeiros_M/0/1/0/all/0/1&quot;&gt;Marcelo Medeiros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vasconcelos_G/0/1/0/all/0/1&quot;&gt;Gabriel Vasconcelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veiga_A/0/1/0/all/0/1&quot;&gt;Alvaro Veiga&lt;/a&gt;</dc:creator>
</item></rdf:RDF>