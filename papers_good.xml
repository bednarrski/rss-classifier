<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09355"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09786"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09496"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09613"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09657"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09701"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.07269"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04394"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07894"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.05543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09411"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09484"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09501"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09653"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1607.07819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02269"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07483"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.09355">
<title>Scoring Lexical Entailment with a Supervised Directional Similarity Network. (arXiv:1805.09355v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.09355</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Supervised Directional Similarity Network (SDSN), a novel
neural architecture for learning task-specific transformation functions on top
of general-purpose word embeddings. Relying on only a limited amount of
supervision from task-specific scores on a subset of the vocabulary, our
architecture is able to generalise and transform a general-purpose
distributional vector space to model the relation of lexical entailment.
Experiments show excellent performance on scoring graded lexical entailment,
raising the state-of-the-art on the HyperLex dataset by approximately 25%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1&quot;&gt;Marek Rei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerz_D/0/1/0/all/0/1&quot;&gt;Daniela Gerz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1&quot;&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09692">
<title>Been There, Done That: Meta-Learning with Episodic Recall. (arXiv:1805.09692v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.09692</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-learning agents excel at rapidly learning new tasks from open-ended task
distributions; yet, they forget what they learn about each task as soon as the
next begins. When tasks reoccur - as they do in natural environments -
metalearning agents must explore again instead of immediately exploiting
previously discovered solutions. We propose a formalism for generating
open-ended yet repetitious environments, then develop a meta-learning
architecture for solving these environments. This architecture melds the
standard LSTM working memory with a differentiable neural episodic memory. We
explore the capabilities of agents with this episodic LSTM in five
meta-learning environments with reoccurring tasks, ranging from bandits to
navigation and stochastic sequential decision problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ritter_S/0/1/0/all/0/1&quot;&gt;Samuel Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jane X. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kurth_Nelson_Z/0/1/0/all/0/1&quot;&gt;Zeb Kurth-Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jayakumar_S/0/1/0/all/0/1&quot;&gt;Siddhant M. Jayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botvinick_M/0/1/0/all/0/1&quot;&gt;Matthew Botvinick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09712">
<title>Autonomously and Simultaneously Refining Deep Neural Network Parameters by Generative Adversarial Networks. (arXiv:1805.09712v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.09712</link>
<description rdf:parseType="Literal">&lt;p&gt;The choice of parameters, and the design of the network architecture are
important factors affecting the performance of deep neural networks. However,
there has not been much work on developing an established and systematic way of
building the structure and choosing the parameters of a neural network, and
this task heavily depends on trial and error and empirical results. Considering
that there are many design and parameter choices, such as the number of neurons
in each layer, the type of activation function, the choice of using drop out or
not, it is very hard to cover every configuration, and find the optimal
structure. In this paper, we propose a novel and systematic method that
autonomously and simultaneously optimizes multiple parameters of any given deep
neural network by using a generative adversarial network (GAN). In our proposed
approach, two different models compete and improve each other progressively
with a GAN-based strategy. Our proposed approach can be used to autonomously
refine the parameters, and improve the accuracy of different deep neural
network architectures. Without loss of generality, the proposed method has been
tested with three different neural network architectures, and three very
different datasets and applications. The results show that the presented
approach can simultaneously and successfully optimize multiple neural network
parameters, and achieve increased accuracy in all three scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakillioglu_B/0/1/0/all/0/1&quot;&gt;Burak Kakillioglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yantao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velipasalar_S/0/1/0/all/0/1&quot;&gt;Senem Velipasalar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09786">
<title>Hyperbolic Attention Networks. (arXiv:1805.09786v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.09786</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce hyperbolic attention networks to endow neural networks with
enough capacity to match the complexity of data with hierarchical and power-law
structure. A few recent approaches have successfully demonstrated the benefits
of imposing hyperbolic geometry on the parameters of shallow networks. We
extend this line of work by imposing hyperbolic geometry on the activations of
neural networks. This allows us to exploit hyperbolic geometry to reason about
embeddings produced by deep networks. We achieve this by re-expressing the
ubiquitous mechanism of soft attention in terms of operations defined for
hyperboloid and Klein models. Our method shows improvements in terms of
generalization on neural machine translation, learning on graphs and visual
question answering tasks while keeping the neural representations compact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1&quot;&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1&quot;&gt;Misha Denil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1&quot;&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razavi_A/0/1/0/all/0/1&quot;&gt;Ali Razavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermann_K/0/1/0/all/0/1&quot;&gt;Karl Moritz Hermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1&quot;&gt;Peter Battaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bapst_V/0/1/0/all/0/1&quot;&gt;Victor Bapst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raposo_D/0/1/0/all/0/1&quot;&gt;David Raposo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1&quot;&gt;Adam Santoro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1&quot;&gt;Nando de Freitas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09791">
<title>Multi-Task Zipping via Layer-wise Neuron Sharing. (arXiv:1805.09791v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.09791</link>
<description rdf:parseType="Literal">&lt;p&gt;Future mobile devices are anticipated to perceive, understand and react to
the world on their own by running multiple correlated deep neural networks
on-device. Yet the complexity of these neural networks needs to be trimmed down
both withinmodel and cross-model to fit in mobile storage and memory. Previous
studies focus on squeezing the redundancy within a single neural network. In
this work, we aim to reduce the redundancy across multiple models. We propose
Multi-Task Zipping (MTZ), a framework to automatically merge correlated,
pre-trained deep neural networks for cross-model compression. Central in MTZ is
a layer-wise neuron sharing and incoming weight updating scheme that induces a
minimal change in the error function. MTZ inherits information from each model
and demands light retraining to re-boost the accuracy of individual tasks.
Evaluations show that MTZ is able to fully merge the hidden layers of two
VGG-16 networks with a 3.18% increase in the test error averaged on ImageNet
and CelebA, or share 39.61% parameters between the two networks with &amp;lt; 0.5%
increase in the test errors for both tasks. The number of iterations to retrain
the combined network is at least 17.8x lower than that of training a single
VGG-16 network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoxi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zimu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1&quot;&gt;Lothar Thiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05225">
<title>RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition. (arXiv:1805.05225v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05225</link>
<description rdf:parseType="Literal">&lt;p&gt;We compare the fast training and decoding speed of RETURNN of attention
models for translation, due to fast CUDA LSTM kernels, and a fast pure
TensorFlow beam search decoder. We show that a layer-wise pretraining scheme
for recurrent attention models gives over 1% BLEU improvement absolute and it
allows to train deeper recurrent encoder networks. Promising preliminary
results on max. expected BLEU training are presented. We are able to train
state-of-the-art models for translation and end-to-end models for speech
recognition and show results on WMT 2017 and Switchboard. The flexibility of
RETURNN allows a fast research feedback loop to experiment with alternative
architectures, and its generality allows to use it on a wide range of
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1&quot;&gt;Albert Zeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkhouli_T/0/1/0/all/0/1&quot;&gt;Tamer Alkhouli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1&quot;&gt;Hermann Ney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09496">
<title>Intelligent Trainer for Model-Based Reinforcement Learning. (arXiv:1805.09496v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09496</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based deep reinforcement learning (DRL) algorithm uses the sampled data
from a real environment to learn the underlying system dynamics to construct an
approximate cyber environment. By using the synthesized data generated from the
cyber environment to train the target controller, the training cost can be
reduced significantly. In current research, issues such as the applicability of
approximate model and the strategy to sample and train from the real and cyber
environment have not been fully investigated. To address these issues, we
propose to utilize an intelligent trainer to properly use the approximate model
and control the sampling and training procedure in the model-based DRL. To do
so, we package the training process of a model-based DRL as a standard RL
environment, and design an RL trainer to control the training process. The
trainer has three control actions: the first action controls where to sample in
the real and cyber environment; the second action determines how many data
should be sampled from the cyber environment and the third action controls how
many times the cyber data should be used to train the target controller. These
actions would be controlled manually if without the trainer. The proposed
framework is evaluated on five different tasks of OpenAI gym and the test
results show that the proposed trainer achieves significant better performance
than a fixed parameter model-based RL baseline algorithm. In addition, we
compare the performance of the intelligent trainer to a random trainer and
prove that the intelligent trainer can indeed learn on the fly. The proposed
training framework can be extended to more control actions with more
sophisticated trainer design to further reduce the tweak cost of model-based RL
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Linsen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yonggang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_K/0/1/0/all/0/1&quot;&gt;Kyle Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09613">
<title>A0C: Alpha Zero in Continuous Action Space. (arXiv:1805.09613v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.09613</link>
<description rdf:parseType="Literal">&lt;p&gt;A core novelty of Alpha Zero is the interleaving of tree search and deep
learning, which has proven very successful in board games like Chess, Shogi and
Go. These games have a discrete action space. However, many real-world
reinforcement learning domains have continuous action spaces, for example in
robotic control, navigation and self-driving cars. This paper presents the
necessary theoretical extensions of Alpha Zero to deal with continuous action
space. We also provide some preliminary experiments on the Pendulum swing-up
task, empirically showing the feasibility of our approach. Thereby, this work
provides a first step towards the application of iterated search and learning
in domains with a continuous action space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moerland_T/0/1/0/all/0/1&quot;&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Broekens_J/0/1/0/all/0/1&quot;&gt;Joost Broekens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Plaat_A/0/1/0/all/0/1&quot;&gt;Aske Plaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jonker_C/0/1/0/all/0/1&quot;&gt;Catholijn M. Jonker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09622">
<title>SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels. (arXiv:1805.09622v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.09622</link>
<description rdf:parseType="Literal">&lt;p&gt;We present SOSELETO (SOurce SELEction for Target Optimization), a new method
for exploiting a source dataset to solve a classification problem on a target
dataset. SOSELETO is based on the following simple intuition: some source
examples are more informative than others for the target problem. To capture
this intuition, source samples are each given weights; these weights are solved
for jointly with the source and target classification problems via a bilevel
optimization scheme. The target therefore gets to choose the source samples
which are most informative for its own classification task. Furthermore, the
bilevel nature of the optimization acts as a kind of regularization on the
target, mitigating overfitting. SOSELETO may be applied to both classic
transfer learning, as well as the problem of training on datasets with noisy
labels; we show state of the art results on both of these problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1&quot;&gt;Or Litany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_D/0/1/0/all/0/1&quot;&gt;Daniel Freedman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09655">
<title>Global-Locally Self-Attentive Dialogue State Tracker. (arXiv:1805.09655v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.09655</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialogue state tracking, which estimates user goals and requests given the
dialogue context, is an essential part of task-oriented dialogue systems. In
this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker
(GLAD), which learns representations of the user utterance and previous system
actions with global-local modules. Our model uses global modules to share
parameters between estimators for different types (called slots) of dialogue
states, and uses local modules to learn slot-specific features. We show that
this significantly improves tracking of rare states and achieves
state-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD
obtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ,
outperforming prior work by 3.7% and 5.5%. On DSTC2, our model obtains 74.5%
joint goal accuracy and 97.5% request accuracy, outperforming prior work by
1.1% and 1.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1&quot;&gt;Victor Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09657">
<title>Learning compositionally through attentive guidance. (arXiv:1805.09657v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.09657</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Attentive Guidance (AG), a new mechanism to
direct a sequence to sequence model equipped with attention to find more
compositional solutions that generalise even in cases where the training and
testing distribution strongly diverge. We test AG on two tasks, devised
precisely to asses the composi- tional capabilities of neural models and show
how vanilla sequence to sequence models with attention overfit the training
distribution, while the guided versions come up with compositional solutions
that, in some cases, fit the training and testing distributions equally well.
AG is a simple and intuitive method to provide a learning bias to a sequence to
sequence model without the need of including extra components, that we believe
allows to inject a component in the training process which is also present in
human learning: guidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1&quot;&gt;Dieuwke Hupkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Anand Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korrel_K/0/1/0/all/0/1&quot;&gt;Kris Korrel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1&quot;&gt;German Kruszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1&quot;&gt;Elia Bruni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09701">
<title>R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering. (arXiv:1805.09701v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.09701</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Visual Question Answering (VQA) has emerged as one of the most
significant tasks in multimodal learning as it requires understanding both
visual and textual modalities. Existing methods mainly rely on extracting image
and question features to learn their joint feature embedding via multimodal
fusion or attention mechanism. Some recent studies utilize external
VQA-independent models to detect candidate entities or attributes in images,
which serve as semantic knowledge complementary to the VQA task. However, these
candidate entities or attributes might be unrelated to the VQA task and have
limited semantic capacities. To better utilize semantic knowledge in images, we
propose a novel framework to learn visual relation facts for VQA. Specifically,
we build up a Relation-VQA (R-VQA) dataset based on the Visual Genome dataset
via a semantic similarity module, in which each data consists of an image, a
corresponding question, a correct answer and a supporting relation fact. A
well-defined relation detector is then adopted to predict visual
question-related relation facts. We further propose a multi-step attention
model composed of visual attention and semantic attention sequentially to
extract related visual knowledge and semantic knowledge. We conduct
comprehensive experiments on the two benchmark datasets, demonstrating that our
model achieves state-of-the-art performance and verifying the benefit of
considering visual relation facts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09801">
<title>Meta-Gradient Reinforcement Learning. (arXiv:1805.09801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09801</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of reinforcement learning algorithms is to estimate and/or optimise
the value function. However, unlike supervised learning, no teacher or oracle
is available to provide the true value function. Instead, the majority of
reinforcement learning algorithms estimate and/or optimise a proxy for the
value function. This proxy is typically based on a sampled and bootstrapped
approximation to the true value function, known as a return. The particular
choice of return is one of the chief components determining the nature of the
algorithm: the rate at which future rewards are discounted; when and how values
should be bootstrapped; or even the nature of the rewards themselves. It is
well-known that these decisions are crucial to the overall success of RL
algorithms. We discuss a gradient-based meta-learning algorithm that is able to
adapt the nature of the return, online, whilst interacting and learning from
the environment. When applied to 57 games on the Atari 2600 environment over
200 million frames, our algorithm achieved a new state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhongwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1&quot;&gt;Hado van Hasselt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1&quot;&gt;David Silver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.07269">
<title>Explanation in Artificial Intelligence: Insights from the Social Sciences. (arXiv:1706.07269v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.07269</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been a recent resurgence in the area of explainable artificial
intelligence as researchers and practitioners seek to make their algorithms
more understandable. Much of this research is focused on explicitly explaining
decisions or actions to a human observer, and it should not be controversial to
say that looking at how humans explain to each other can serve as a useful
starting point for explanation in artificial intelligence. However, it is fair
to say that most work in explainable artificial intelligence uses only the
researchers&apos; intuition of what constitutes a `good&apos; explanation. There exists
vast and valuable bodies of research in philosophy, psychology, and cognitive
science of how people define, generate, select, evaluate, and present
explanations, which argues that people employ certain cognitive biases and
social expectations towards the explanation process. This paper argues that the
field of explainable artificial intelligence should build on this existing
research, and reviews relevant papers from philosophy, cognitive
psychology/science, and social psychology, which study these topics. It draws
out some important findings, and discusses ways that these can be infused with
work on explainable artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1&quot;&gt;Tim Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11577">
<title>Learning Depthwise Graph Convolution from Data Manifold. (arXiv:1710.11577v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11577</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolution Neural Network (CNN) has gained tremendous success in computer
vision tasks with its outstanding ability to capture the local latent features.
Recently, there has been an increasing interest in extending convolution
operations to the non-Euclidean geometry. Although various types of convolution
operations have been proposed for graphs or manifolds, their connections with
traditional convolution over grid-structured data are not well-understood. In
this paper, we show that depthwise separable convolution can be successfully
generalized for the unification of both graph-based and grid-based convolution
methods. Based on this insight we propose a novel Depthwise Separable Graph
Convolution (DSGC) approach which is compatible with the tradition convolution
network and subsumes existing convolution methods as special cases. It is
equipped with the combined strengths in model expressiveness, compatibility
(relatively small number of parameters), modularity and computational
efficiency in training. Extensive experiments show the outstanding performance
of DSGC in comparison with strong baselines on multi-domain benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_G/0/1/0/all/0/1&quot;&gt;Guokun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04394">
<title>M-Walk: Learning to Walk in Graph with Monte Carlo Tree Search. (arXiv:1802.04394v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04394</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to walk over a graph towards a target node for a given input query
and a source node is an important problem in applications such as knowledge
base completion (KBC). It can be formulated as a reinforcement learning (RL)
problem with a known state transition model. To overcome the challenge of
sparse reward, we develop a graph-walking agent called M-Walk, which consists
of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS).
The RNN encodes the state (i.e., history of the walked path) and maps it
separately to a policy, a state value and state-action Q-values. In order to
effectively train the agent from sparse reward, we combine MCTS with the neural
policy to generate trajectories yielding more positive rewards. From these
trajectories, the network is improved in an off-policy manner using Q-learning,
which modifies the RNN policy via parameter sharing. Our proposed RL algorithm
repeatedly applies this policy-improvement step to learn the entire model. At
test time, MCTS is again combined with the neural policy to predict the target
node. Experimental results on several graph-walking benchmarks show that M-Walk
is able to learn better policies than other RL-based methods, which are mainly
based on policy gradients. M-Walk also outperforms traditional KBC baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yelong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianshu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Sen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuqing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05800">
<title>Tree-CNN: A Hierarchical Deep Convolutional Neural Network for Incremental Learning. (arXiv:1802.05800v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05800</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Convolutional Neural Networks (CNNs) have shown remarkable
performance in many computer vision tasks such as object recognition and
detection. However, complex training issues, such as `catastrophic forgetting&apos;
and hyper-parameter tuning, make incremental learning in CNNs a difficult
challenge. In this paper, we propose a hierarchical deep neural network, with
CNNs at multiple levels, and a corresponding training method for incremental
learning. The network grows in a tree-like manner to accommodate the new
classes of data without losing the ability to identify the previously trained
classes. The proposed network was tested on CIFAR-100 and reported 60.46%
accuracy and 20% reduction in training effort as compared to retraining final
layers of a deep network. The network organizes the incoming classes of data
into feature-driven super-classes and improves upon existing hierarchical CNN
models by adding the capability of self-growth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1&quot;&gt;Deboleena Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1&quot;&gt;Priyadarshini Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07047">
<title>Cell Selection with Deep Reinforcement Learning in Sparse Mobile Crowdsensing. (arXiv:1804.07047v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07047</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse Mobile CrowdSensing (MCS) is a novel MCS paradigm where data inference
is incorporated into the MCS process for reducing sensing costs while its
quality is guaranteed. Since the sensed data from different cells (sub-areas)
of the target sensing area will probably lead to diverse levels of inference
data quality, cell selection (i.e., choose which cells of the target area to
collect sensed data from participants) is a critical issue that will impact the
total amount of data that requires to be collected (i.e., data collection
costs) for ensuring a certain level of quality. To address this issue, this
paper proposes a Deep Reinforcement learning based Cell selection mechanism for
Sparse MCS, called DR-Cell. First, we properly model the key concepts in
reinforcement learning including state, action, and reward, and then propose to
use a deep recurrent Q-network for learning the Q-function that can help decide
which cell is a better choice under a certain state during cell selection.
Furthermore, we leverage the transfer learning techniques to reduce the amount
of data required for training the Q-function if there are multiple correlated
MCS tasks that need to be conducted in the same target area. Experiments on
various real-life sensing datasets verify the effectiveness of DR-Cell over the
state-of-the-art cell selection mechanisms in Sparse MCS by reducing up to 15%
of sensed cells with the same data inference quality guarantee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1&quot;&gt;En Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongjian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04007">
<title>A Unified Knowledge Representation and Context-aware Recommender System in Internet of Things. (arXiv:1805.04007v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04007</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the rapidly developing Internet of Things (IoT), numerous and diverse
physical devices, Edge devices, Cloud infrastructure, and their quality of
service requirements (QoS), need to be represented within a unified
specification in order to enable rapid IoT application development, monitoring,
and dynamic reconfiguration. But heterogeneities among different configuration
knowledge representation models pose limitations for acquisition, discovery and
curation of configuration knowledge for coordinated IoT applications. This
paper proposes a unified data model to represent IoT resource configuration
knowledge artifacts. It also proposes IoT-CANE (Context-Aware recommendatioN
systEm) to facilitate incremental knowledge acquisition and declarative context
driven knowledge recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1&quot;&gt;Awa Alqahtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solaiman_E/0/1/0/all/0/1&quot;&gt;Ellis Solaiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_C/0/1/0/all/0/1&quot;&gt;Charith Perera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1&quot;&gt;Prem Prakash Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benatallah_B/0/1/0/all/0/1&quot;&gt;Boualem Benatallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1&quot;&gt;Rajiv Ranjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07894">
<title>Generative Adversarial Examples. (arXiv:1805.07894v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07894</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples are typically constructed by perturbing an existing data
point, and current defense methods are focused on guarding against this type of
attack. In this paper, we propose a new class of adversarial examples that are
synthesized entirely from scratch using a conditional generative model. We
first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to
model the class-conditional distribution over inputs. Then, conditioned on a
desired class, we search over the AC-GAN latent space to find images that are
likely under the generative model and are misclassified by a target classifier.
We demonstrate through human evaluation that this new kind of adversarial
inputs, which we call Generative Adversarial Examples, are legitimate and
belong to the desired class. Our empirical results on the MNIST, SVHN, and
CelebA datasets show that generative adversarial examples can easily bypass
strong adversarial training and certified defense methods which can foil
existing adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_R/0/1/0/all/0/1&quot;&gt;Rui Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kushman_N/0/1/0/all/0/1&quot;&gt;Nate Kushman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.05543">
<title>Coordinating Collaborative Chat in Massive Open Online Courses. (arXiv:1704.05543v1 [cs.CY] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1704.05543</link>
<description rdf:parseType="Literal">&lt;p&gt;An earlier study of a collaborative chat intervention in a Massive Open
Online Course (MOOC) identified negative effects on attrition stemming from a
requirement for students to be matched with exactly one partner prior to
beginning the activity. That study raised questions about how to orchestrate a
collaborative chat intervention in a MOOC context in order to provide the
benefit of synchronous social engagement without the coordination difficulties.
In this paper we present a careful analysis of an intervention designed to
overcome coordination difficulties by welcoming students into the chat on a
rolling basis as they arrive rather than requiring them to be matched with a
partner before beginning. The results suggest the most positive impact when
experiencing a chat with exactly one partner rather than more or less. A
qualitative analysis of the chat data reveals differential experiences between
these configurations that suggests a potential explanation for the effect and
raises questions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1&quot;&gt;Gaurav Singh Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayanan_S/0/1/0/all/0/1&quot;&gt;Sreecharan Sankaranarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1&quot;&gt;Carolyn Penstein Ros&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09366">
<title>Semi-supervised classification by reaching consensus among modalities. (arXiv:1805.09366v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09366</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces transductive consensus network (TCNs), as an extension
of a consensus network (CN), for semi-supervised learning. TCN does multi-modal
classification based on a few available labels by urging the {\em
interpretations} of different modalities to resemble each other. We formulate
the multi-modal, semi-supervised learning problem, put forward TCN for
multi-modal semi-supervised learning task, and its several variants. To
understand the mechanisms of TCN, we formulate the {\em similarity} of the
interpretations as the negative relative Jensen-Shannon divergence, and show
that a consensus state beneficial for classification desires a stable but not
perfect similarity between the interpretations. We show the performances of TCN
are better than best benchmark algorithms given only 20 and 80 labeled samples
on Bank Marketing and the DementiaBank dataset respectively, and align with
their performances given more labeled samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zining Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1&quot;&gt;Jekaterina Novikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1&quot;&gt;Frank Rudzicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09370">
<title>Towards Robust Training of Neural Networks by Regularizing Adversarial Gradients. (arXiv:1805.09370v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09370</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, neural networks have demonstrated outstanding effectiveness
in a large amount of applications.However, recent works have shown that neural
networks are susceptible to adversarial examples, indicating possible flaws
intrinsic to the network structures. To address this problem and improve the
robustness of neural networks, we investigate the fundamental mechanisms behind
adversarial examples and propose a novel robust training method via regulating
adversarial gradients. The regulation effectively squeezes the adversarial
gradients of neural networks and significantly increases the difficulty of
adversarial example generation.Without any adversarial example involved, the
robust training method could generate naturally robust networks, which are
near-immune to various types of adversarial examples. Experiments show the
naturally robust networks can achieve optimal accuracy against Fast Gradient
Sign Method (FGSM) and C\&amp;amp;W attacks on MNIST, Cifar10, and Google Speech
Command dataset. Moreover, our proposed method also provides neural networks
with consistent robustness against transferable attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fuxun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zirui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09411">
<title>A Generalized Active Learning Approach for Unsupervised Anomaly Detection. (arXiv:1805.09411v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.09411</link>
<description rdf:parseType="Literal">&lt;p&gt;This work formalizes the new framework for anomaly detection, called active
anomaly detection. This framework has, in practice, the same cost of
unsupervised anomaly detection but with the possibility of much better results.
We show that unsupervised anomaly detection is an undecidable problem and that
a prior needs to be assumed for the anomalies probability distribution in order
to have performance guarantees. Finally, we also present a new layer that can
be attached to any deep learning model designed for unsupervised anomaly
detection to transform it into an active anomaly detection method, presenting
results on both synthetic and real anomaly detection datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pimentel_T/0/1/0/all/0/1&quot;&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Monteiro_M/0/1/0/all/0/1&quot;&gt;Marianne Monteiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Viana_J/0/1/0/all/0/1&quot;&gt;Juliano Viana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veloso_A/0/1/0/all/0/1&quot;&gt;Adriano Veloso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ziviani_N/0/1/0/all/0/1&quot;&gt;Nivio Ziviani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09458">
<title>Evading the Adversary in Invariant Representation. (arXiv:1805.09458v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09458</link>
<description rdf:parseType="Literal">&lt;p&gt;Representations of data that are invariant to changes in specified nuisance
factors are useful for a wide range of problems: removing potential bias in
prediction problems, controlling the effects of known confounders, and
disentangling meaningful factors of variation. Unfortunately, learning
representations that exhibit invariance to arbitrary nuisance factors yet
remain useful for other tasks is challenging. Existing approaches cast the
trade-off between task performance and invariance in an adversarial way, using
an iterative minimax optimization. We show that adversarial training is
unnecessary and sometimes counter-productive by casting invariant
representation learning for various tasks as a single information-theoretic
objective that can be directly optimized. We demonstrate that this approach
matches or exceeds performance of state-of-the-art adversarial approaches for
learning fair representations and for generative modeling with controllable
transformations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1&quot;&gt;Daniel Moyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shuyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brekelmans_R/0/1/0/all/0/1&quot;&gt;Rob Brekelmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1&quot;&gt;Aram Galstyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09461">
<title>Deep Reinforcement Learning For Sequence to Sequence Models. (arXiv:1805.09461v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09461</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, sequence-to-sequence (seq2seq) models are used in a variety
of tasks from machine translation, headline generation, text summarization,
speech to text, to image caption generation. The underlying framework of all
these models are usually a deep neural network which contains an encoder and
decoder. The encoder processes the input data and a decoder receives the output
of the encoder and generates the final output. Although simply using an
encoder/decoder model would, most of the time, produce better result than
traditional methods on the above-mentioned tasks, researchers proposed
additional improvements over these sequence to sequence models, like using an
attention-based model over the input, pointer-generation models, and
self-attention models. However, all these seq2seq models suffer from two common
problems: 1) exposure bias and 2) inconsistency between train/test measurement.
Recently a completely fresh point of view emerged in solving these two problems
in seq2seq models by using methods in Reinforcement Learning (RL). In these new
researches, we try to look at the seq2seq problems from the RL point of view
and we try to come up with a formulation that could combine the power of RL
methods in decision-making and sequence to sequence models in remembering long
memories. In this paper, we will summarize some of the most recent frameworks
that combines concepts from RL world to the deep neural network area and
explain how these two areas could benefit from each other in solving complex
seq2seq tasks. In the end, we will provide insights on some of the problems of
the current existing models and how we can improve them with better RL models.
We also provide the source code for implementing most of the models that will
be discussed in this paper on the complex task of abstractive text
summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keneshloo_Y/0/1/0/all/0/1&quot;&gt;Yaser Keneshloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1&quot;&gt;Tian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandan K. Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1&quot;&gt;Naren Ramakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09470">
<title>Taming Convergence for Asynchronous Stochastic Gradient Descent with Unbounded Delay in Non-Convex Learning. (arXiv:1805.09470v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09470</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the convergence performance of asynchronous stochastic gradient
descent method (Async-SGD) has received increasing attention in recent years
due to their foundational role in machine learning. To date, however, most of
the existing works are restricted to either bounded gradient delays or convex
settings. In this paper, we focus on Async-SGD and its variant Async-SGDI
(which uses increasing batch size) for non-convex optimization problems with
unbounded gradient delays. We prove $o(1/\sqrt{k})$ convergence rate for
Async-SGD and $o(1/k)$ for Async-SGDI. Also, a unifying sufficient condition
for Async-SGD&apos;s convergence is established, which includes two major gradient
delay models in the literature as special cases and yields a new delay model
not considered thus far.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09484">
<title>Multi-Level Deep Cascade Trees for Conversion Rate Prediction. (arXiv:1805.09484v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09484</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing effective and efficient recommendation methods is very challenging
for modern e-commerce platforms (e.g. Taobao). Generally speaking, two
essential modules named &quot;Click-Through Rate Prediction&quot; (CTR) and &quot;Conversion
Rate Prediction&quot; (CVR) are included, where CVR module is a crucial factor that
affects the final purchasing volume directly. However, it is indeed very
challenging due to its sparseness nature. In this paper, we tackle this problem
by proposing multi-Level Deep Cascade Trees (ldcTree), which is a novel
decision tree ensemble approach. It leverages deep cascade structures by
stacking Gradient Boosting Decision Trees (GBDT) to effectively learn feature
representation. In addition, we propose to utilize the cross-entropy in each
tree of the preceding GBDT as the input feature representation for next level
GBDT, which has a clear explanation, i.e., a traversal from root to leaf nodes
in the next level GBDT corresponds to the combination of certain traversals in
the preceding GBDT. The deep cascade structure and the combination rule enable
the proposed ldcTree to have a stronger distributed feature representation
ability. Moreover, inspired by ensemble learning, we propose an Ensemble
ldcTree (E-ldcTree) to encourage the model&apos;s diversity and enhance the
representation ability further. Finally, we propose an improved Feature
learning method based on EldcTree (F-EldcTree) for taking adequate use of weak
and strong correlation features identified by pre-trained GBDT models.
Experimental results on off-line dataset and online deployment demonstrate the
effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Quan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Keping Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1&quot;&gt;Taiwei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1&quot;&gt;Fuyu Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Pipei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09501">
<title>AutoAugment: Learning Augmentation Policies from Data. (arXiv:1805.09501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.09501</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we take a closer look at data augmentation for images, and
describe a simple procedure called AutoAugment to search for improved data
augmentation policies. Our key insight is to create a search space of data
augmentation policies, evaluating the quality of a particular policy directly
on the dataset of interest. In our implementation, we have designed a search
space where a policy consists of many sub-policies, one of which is randomly
chosen for each image in each mini-batch. A sub-policy consists of two
operations, each operation being an image processing function such as
translation, rotation, or shearing, and the probabilities and magnitudes with
which the functions are applied. We use a search algorithm to find the best
policy such that the neural network yields the highest validation accuracy on a
target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10,
CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain
a Top-1 accuracy of 83.54%. On CIFAR-10, we achieve an error rate of 1.48%,
which is 0.65% better than the previous state-of-the-art. On reduced data
settings, AutoAugment performs comparably to semi-supervised methods without
using any unlabeled examples. Finally, policies learned from one dataset can be
transferred to work well on other similar datasets. For example, the policy
learned on ImageNet allows us to achieve state-of-the-art accuracy on the fine
grained visual classification dataset Stanford Cars, without fine-tuning
weights pre-trained on additional data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1&quot;&gt;Ekin D. Cubuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1&quot;&gt;Barret Zoph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mane_D/0/1/0/all/0/1&quot;&gt;Dandelion Mane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1&quot;&gt;Vijay Vasudevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09621">
<title>Backpropagation with N-D Vector-Valued Neurons Using Arbitrary Bilinear Products. (arXiv:1805.09621v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09621</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector-valued neural learning has emerged as a promising direction in deep
learning recently. Traditionally, training data for neural networks (NNs) are
formulated as a vector of scalars; however, its performance may not be optimal
since associations among adjacent scalars are not modeled. In this paper, we
propose a new vector neural architecture called the Arbitrary BIlinear Product
Neural Network (ABIPNN), which processes information as vectors in each neuron,
and the feedforward projections are defined using arbitrary bilinear products.
Such bilinear products can include circular convolution, seven-dimensional
vector product, skew circular convolution, reversed- time circular convolution,
or other new products not seen in previous work. As a proof-of-concept, we
apply our proposed network to multispectral image denoising and singing voice
sepa- ration. Experimental results show that ABIPNN gains substantial
improvements when compared to conventional NNs, suggesting that associations
are learned during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhe-Cheng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_T/0/1/0/all/0/1&quot;&gt;Tak-Shing T. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jyh-Shing R. Jang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09653">
<title>Uncertainty-Aware Attention for Reliable Interpretation and Prediction. (arXiv:1805.09653v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.09653</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention mechanism is effective in both focusing the deep learning models on
relevant features and interpreting them. However, attentions may be unreliable
since the networks that generate them are often trained in a weakly-supervised
manner. To overcome this limitation, we introduce the notion of input-dependent
uncertainty to the attention mechanism, such that it generates attention for
each feature with varying degrees of noise based on the given input, to learn
larger variance on instances it is uncertain about. We learn this
Uncertainty-aware Attention (UA) mechanism using variational inference, and
validate it on various risk prediction tasks from electronic health records on
which our model significantly outperforms existing attention models. The
analysis of the learned attentions shows that our model generates attentions
that comply with clinicians&apos; interpretation, and provide richer interpretation
via learned variance. Further evaluation of both the accuracy of the
uncertainty calibration and the prediction performance with &quot;I don&apos;t know&quot;
decision show that UA yields networks with high reliability as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Jay Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hae Beom Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Saehoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang Joon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;Eunho Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09733">
<title>Towards Robust Evaluations of Continual Learning. (arXiv:1805.09733v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.09733</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning experiments used in current deep learning papers do not
faithfully assess fundamental challenges of learning continually, masking
weak-points of the suggested approaches instead. We study gaps in such existing
evaluations, proposing essential experimental evaluations that are more
representative of continual learning&apos;s challenges, and suggest a
re-prioritization of research efforts in the field. We show that current
approaches fail with our new evaluations and, to analyse these failures, we
propose a variational loss which unifies many existing solutions to continual
learning under a Bayesian framing, as either &apos;prior-focused&apos; or
&apos;likelihood-focused&apos;. We show that while prior-focused approaches such as EWC
and VCL perform well on existing evaluations, they perform dramatically worse
when compared to likelihood-focused approaches on other simple tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Farquhar_S/0/1/0/all/0/1&quot;&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yarin Gal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09785">
<title>Entropy and mutual information in models of deep neural networks. (arXiv:1805.09785v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.09785</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine a class of deep learning models with a tractable method to compute
information-theoretic quantities. Our contributions are three-fold: (i) We show
how entropies and mutual informations can be derived from heuristic statistical
physics methods, under the assumption that weight matrices are independent and
orthogonally-invariant. (ii) We extend particular cases in which this result is
known to be rigorously exact by providing a proof for two-layers networks with
Gaussian random weights, using the recently introduced adaptive interpolation
method. (iii) We propose an experiment framework with generative models of
synthetic datasets, on which we train deep neural networks with a weight
constraint designed so that the assumption in (i) is verified during learning.
We study the behavior of entropies and mutual informations throughout learning
and conclude that, in the proposed setting, the relationship between
compression and generalization remains elusive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabrie_M/0/1/0/all/0/1&quot;&gt;Marylou Gabri&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manoel_A/0/1/0/all/0/1&quot;&gt;Andre Manoel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luneau_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Luneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbier_J/0/1/0/all/0/1&quot;&gt;Jean Barbier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1&quot;&gt;Nicolas Macris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krzakala_F/0/1/0/all/0/1&quot;&gt;Florent Krzakala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zdeborova_L/0/1/0/all/0/1&quot;&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1607.07819">
<title>Approximation by Combinations of ReLU and Squared ReLU Ridge Functions with $ \ell^1 $ and $ \ell^0 $ Controls. (arXiv:1607.07819v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1607.07819</link>
<description rdf:parseType="Literal">&lt;p&gt;We establish $ L^{\infty} $ and $ L^2 $ error bounds for functions of many
variables that are approximated by linear combinations of ReLU (rectified
linear unit) and squared ReLU ridge functions with $ \ell^1 $ and $ \ell^0 $
controls on their inner and outer parameters. With the squared ReLU ridge
function, we show that the $ L^2 $ approximation error is inversely
proportional to the inner layer $ \ell^0 $ sparsity and it need only be
sublinear in the outer layer $ \ell^0 $ sparsity. Our constructions are
obtained using a variant of the Jones-Barron probabilistic method, which can be
interpreted as either stratified sampling with proportionate allocation or
two-stage cluster sampling. We also provide companion error lower bounds that
reveal near optimality of our constructions. Despite the sparsity assumptions,
we showcase the richness and flexibility of these ridge combinations by
defining a large family of functions, in terms of certain spectral conditions,
that are particularly well approximated by them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1&quot;&gt;Jason M. Klusowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barron_A/0/1/0/all/0/1&quot;&gt;Andrew R. Barron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09170">
<title>Realistic Evaluation of Deep Semi-Supervised Learning Algorithms. (arXiv:1804.09170v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09170</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning (SSL) provides a powerful framework for leveraging
unlabeled data when labels are limited or expensive to obtain. SSL algorithms
based on deep neural networks have recently proven successful on standard
benchmark tasks. However, we argue that these benchmarks fail to address many
issues that these algorithms would face in real-world applications. After
creating a unified reimplementation of various widely-used SSL techniques, we
test them in a suite of experiments designed to address these issues. We find
that the performance of simple baselines which do not use unlabeled data is
often underreported, that SSL methods differ in sensitivity to the amount of
labeled and unlabeled data, and that performance can degrade substantially when
the unlabeled dataset contains out-of-class examples. To help guide SSL
research towards real-world applicability, we make our unified reimplemention
and evaluation platform publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliver_A/0/1/0/all/0/1&quot;&gt;Avital Oliver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odena_A/0/1/0/all/0/1&quot;&gt;Augustus Odena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1&quot;&gt;Ekin D. Cubuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian J. Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02269">
<title>Incorporating Privileged Information to Unsupervised Anomaly Detection. (arXiv:1805.02269v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02269</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new unsupervised anomaly detection ensemble called SPI which
can harness privileged information - data available only for training examples
but not for (future) test examples. Our ideas build on the Learning Using
Privileged Information (LUPI) paradigm pioneered by Vapnik et al. [19,17],
which we extend to unsupervised learning and in particular to anomaly
detection. SPI (for Spotting anomalies with Privileged Information) constructs
a number of frames/fragments of knowledge (i.e., density estimates) in the
privileged space and transfers them to the anomaly scoring space through
&quot;imitation&quot; functions that use only the partial information available for test
examples. Our generalization of the LUPI paradigm to unsupervised anomaly
detection shepherds the field in several key directions, including (i) domain
knowledge-augmented detection using expert annotations as PI, (ii) fast
detection using computationally-demanding data as PI, and (iii) early detection
using &quot;historical future&quot; data as PI. Through extensive experiments on
simulated and real datasets, we show that augmenting privileged information to
anomaly detection significantly improves detection performance. We also
demonstrate the promise of SPI under all three settings (i-iii); with PI
capturing expert knowledge, computationally expensive features, and future data
on three real world detection tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1&quot;&gt;Shubhranshu Shekhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07483">
<title>Tell Me Something New: A New Framework for Asynchronous Parallel Learning. (arXiv:1805.07483v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07483</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach for parallel computation in the context of
machine learning that we call &quot;Tell Me Something New&quot; (TMSN). This approach
involves a set of independent workers that use broadcast to update each other
when they observe &quot;something new&quot;. TMSN does not require synchronization or a
head node and is highly resilient against failing machines or laggards. We
demonstrate the utility of TMSN by applying it to learning boosted trees. We
show that our implementation is 10 times faster than XGBoost and LightGBM on
the splice-site prediction problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alafate_J/0/1/0/all/0/1&quot;&gt;Julaiti Alafate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1&quot;&gt;Yoav Freund&lt;/a&gt;</dc:creator>
</item></rdf:RDF>