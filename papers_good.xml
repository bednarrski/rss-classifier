<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04890"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04656"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04659"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04775"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04778"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04950"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.04849">
<title>The unreasonable effectiveness of the forget gate. (arXiv:1804.04849v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.04849</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the success of the gated recurrent unit, a natural question is whether
all the gates of the long short-term memory (LSTM) network are necessary.
Previous research has shown that the forget gate is one of the most important
gates in the LSTM. Here we show that a forget-gate-only version of the LSTM
with chrono-initialized biases, not only provides computational savings but
outperforms the standard LSTM on multiple benchmark datasets and competes with
some of the best contemporary models. Our proposed network, the JANET, achieves
accuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the
standard LSTM which yields accuracies of 98.5% and 91%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westhuizen_J/0/1/0/all/0/1&quot;&gt;Jos van der Westhuizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1&quot;&gt;Joan Lasenby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04890">
<title>Neural Trajectory Analysis of Recurrent Neural Network In Handwriting Synthesis. (arXiv:1804.04890v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.04890</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are capable of learning to generate highly
realistic, online handwritings in a wide variety of styles from a given text
sequence. Furthermore, the networks can generate handwritings in the style of a
particular writer when the network states are primed with a real sequence of
pen movements from the writer. However, how populations of neurons in the RNN
collectively achieve such performance still remains poorly understood. To
tackle this problem, we investigated learned representations in RNNs by
extracting low-dimensional, neural trajectories that summarize the activity of
a population of neurons in the network during individual syntheses of
handwritings. The neural trajectories show that different writing styles are
encoded in different subspaces inside an internal space of the network. Within
each subspace, different characters of the same style are represented as
different state dynamics. These results demonstrate the effectiveness of
analyzing the neural trajectory for intuitive understanding of how the RNNs
work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charbonneau_K/0/1/0/all/0/1&quot;&gt;Kristof B. Charbonneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shouno_O/0/1/0/all/0/1&quot;&gt;Osamu Shouno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04946">
<title>Affective Recommendation System for Tourists by Using Emotion Generating Calculations. (arXiv:1804.04946v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1804.04946</link>
<description rdf:parseType="Literal">&lt;p&gt;An emotion orientated intelligent interface consists of Emotion Generating
Calculations (EGC) and Mental State Transition Network (MSTN). We have
developed the Android EGC application software which the agent works to
evaluate the feelings in the conversation. In this paper, we develop the
tourist information system which can estimate the user&apos;s feelings at the
sightseeing spot. The system can recommend the sightseeing spot and the local
food corresponded to the user&apos;s feeling. The system calculates the
recommendation list by the estimate function which consists of Google search
results, the important degree of a term at the sightseeing website, and the the
aroused emotion by EGC. In order to show the effectiveness, this paper
describes the experimental results for some situations during Hiroshima
sightseeing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tachibana_I/0/1/0/all/0/1&quot;&gt;Issei Tachibana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00043">
<title>Unsupervised Machine Translation Using Monolingual Corpora Only. (arXiv:1711.00043v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00043</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine translation has recently achieved impressive performance thanks to
recent advances in deep learning and the availability of large-scale parallel
corpora. There have been numerous attempts to extend these successes to
low-resource language pairs, yet requiring tens of thousands of parallel
sentences. In this work, we take this research direction to the extreme and
investigate whether it is possible to learn to translate even without any
parallel data. We propose a model that takes sentences from monolingual corpora
in two different languages and maps them into the same latent space. By
learning to reconstruct in both languages from this shared feature space, the
model effectively learns to translate without using any labeled data. We
demonstrate our model on two widely used datasets and two language pairs,
reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French
datasets, without using even a single parallel sentence at training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lample_G/0/1/0/all/0/1&quot;&gt;Guillaume Lample&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1&quot;&gt;Alexis Conneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1&quot;&gt;Ludovic Denoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1&quot;&gt;Marc&amp;#x27;Aurelio Ranzato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04656">
<title>3D G-CNNs for Pulmonary Nodule Detection. (arXiv:1804.04656v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04656</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) require a large amount of annotated data
to learn from, which is often difficult to obtain in the medical domain. In
this paper we show that the sample complexity of CNNs can be significantly
improved by using 3D roto-translation group convolutions (G-Convs) instead of
the more conventional translational convolutions. These 3D G-CNNs were applied
to the problem of false positive reduction for pulmonary nodule detection, and
proved to be substantially more effective in terms of performance, sensitivity
to malignant nodules, and speed of convergence compared to a strong and
comparable baseline architecture with regular convolutions, data augmentation
and a similar number of parameters. For every dataset size tested, the G-CNN
achieved a FROC score close to the CNN trained on ten times more data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winkels_M/0/1/0/all/0/1&quot;&gt;Marysia Winkels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco S. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04659">
<title>Asynchronous Parallel Sampling Gradient Boosting Decision Tree. (arXiv:1804.04659v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04659</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of big data technology, Gradient Boosting Decision Tree,
i.e. GBDT, becomes one of the most important machine learning algorithms for
its accurate output. However, the training process of GBDT needs a lot of
computational resources and time. In order to accelerate the training process
of GBDT, the asynchronous parallel sampling gradient boosting decision tree,
abbr. asynch-SGBDT is proposed in this paper. Via introducing sampling, we
adapt the numerical optimization process of traditional GBDT training process
into stochastic optimization process and use asynchronous parallel stochastic
gradient descent to accelerate the GBDT training process. Meanwhile, the
theoretical analysis of asynch-SGBDT is provided by us in this paper.
Experimental results show that GBDT training process could be accelerated by
asynch-SGBDT. Our asynchronous parallel strategy achieves an almost linear
speedup, especially for high-dimensional sparse datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daning_C/0/1/0/all/0/1&quot;&gt;Cheng Daning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fen_X/0/1/0/all/0/1&quot;&gt;Xia Fen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shigang_L/0/1/0/all/0/1&quot;&gt;Li Shigang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yunquan_Z/0/1/0/all/0/1&quot;&gt;Zhang Yunquan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04758">
<title>MOVI: A Model-Free Approach to Dynamic Fleet Management. (arXiv:1804.04758v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04758</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern vehicle fleets, e.g., for ridesharing platforms and taxi companies,
can reduce passengers&apos; waiting times by proactively dispatching vehicles to
locations where pickup requests are anticipated in the future. Yet it is
unclear how to best do this: optimal dispatching requires optimizing over
several sources of uncertainty, including vehicles&apos; travel times to their
dispatched locations, as well as coordinating between vehicles so that they do
not attempt to pick up the same passenger. While prior works have developed
models for this uncertainty and used them to optimize dispatch policies, in
this work we introduce a model-free approach. Specifically, we propose MOVI, a
Deep Q-network (DQN)-based framework that directly learns the optimal vehicle
dispatch policy. Since DQNs scale poorly with a large number of possible
dispatches, we streamline our DQN training and suppose that each individual
vehicle independently learns its own optimal policy, ensuring scalability at
the cost of less coordination between vehicles. We then formulate a centralized
receding-horizon control (RHC) policy to compare with our DQN policies. To
compare these policies, we design and build MOVI as a large-scale realistic
simulator based on 15 million taxi trip records that simulates policy-agnostic
responses to dispatch decisions. We show that the DQN dispatch policy reduces
the number of unserviced requests by 76% compared to without dispatch and 20%
compared to the RHC approach, emphasizing the benefits of a model-free approach
and suggesting that there is limited value to coordinating vehicle actions.
This finding may help to explain the success of ridesharing platforms, for
which drivers make individual decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oda_T/0/1/0/all/0/1&quot;&gt;Takuma Oda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1&quot;&gt;Carlee Joe-Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04775">
<title>Distribution Regression Network. (arXiv:1804.04775v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04775</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce our Distribution Regression Network (DRN) which performs
regression from input probability distributions to output probability
distributions. Compared to existing methods, DRN learns with fewer model
parameters and easily extends to multiple input and multiple output
distributions. On synthetic and real-world datasets, DRN performs similarly or
better than the state-of-the-art. Furthermore, DRN generalizes the conventional
multilayer perceptron (MLP). In the framework of MLP, each node encodes a real
number, whereas in DRN, each node encodes a probability distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_C/0/1/0/all/0/1&quot;&gt;Connie Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hwee Kuan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1&quot;&gt;Teck Khim Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04778">
<title>Understanding Community Structure in Layered Neural Networks. (arXiv:1804.04778v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04778</link>
<description rdf:parseType="Literal">&lt;p&gt;A layered neural network is now one of the most common choices for the
prediction of high-dimensional practical data sets, where the relationship
between input and output data is complex and cannot be represented well by
simple conventional models. Its effectiveness is shown in various tasks,
however, the lack of interpretability of the trained result by a layered neural
network has limited its application area.
&lt;/p&gt;
&lt;p&gt;In our previous studies, we proposed methods for extracting a simplified
global structure of a trained layered neural network by classifying the units
into communities according to their connection patterns with adjacent layers.
These methods provided us with knowledge about the strength of the relationship
between communities from the existence of bundled connections, which are
determined by threshold processing of the connection ratio between pairs of
communities.
&lt;/p&gt;
&lt;p&gt;However, it has been difficult to understand the role of each community
quantitatively by observing the modular structure. We could only know to which
sets of the input and output dimensions each community was mainly connected, by
tracing the bundled connections from the community to the input and output
layers. Another problem is that the finally obtained modular structure is
changed greatly depending on the setting of the threshold hyperparameter used
for determining bundled connections.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a new method for interpreting quantitatively the
role of each community in inference, by defining the effect of each input
dimension on a community, and the effect of a community on each output
dimension. We show experimentally that our proposed method can reveal the role
of each part of a layered neural network by applying the neural networks to
three types of data sets, extracting communities from the trained network, and
applying the proposed method to the community structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1&quot;&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hiramatsu_K/0/1/0/all/0/1&quot;&gt;Kaoru Hiramatsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kashino_K/0/1/0/all/0/1&quot;&gt;Kunio Kashino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04791">
<title>Fast, Parameter free Outlier Identification for Robust PCA. (arXiv:1804.04791v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04791</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust PCA, the problem of PCA in the presence of outliers has been
extensively investigated in the last few years. Here we focus on Robust PCA in
the column sparse outlier model. The existing methods for column sparse outlier
model assumes either the knowledge of the dimension of the lower dimensional
subspace or the fraction of outliers in the system. However in many
applications knowledge of these parameters is not available. Motivated by this
we propose a parameter free outlier identification method for robust PCA which
a) does not require the knowledge of outlier fraction, b) does not require the
knowledge of the dimension of the underlying subspace, c) is computationally
simple and fast. Further, analytical guarantees are derived for outlier
identification and the performance of the algorithm is compared with the
existing state of the art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Menon_V/0/1/0/all/0/1&quot;&gt;Vishnu Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kalyani_S/0/1/0/all/0/1&quot;&gt;Sheetal Kalyani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04950">
<title>DeepFM: An End-to-End Wide &amp; Deep Learning Framework for CTR Prediction. (arXiv:1804.04950v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1804.04950</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning sophisticated feature interactions behind user behaviors is critical
in maximizing CTR for recommender systems. Despite great progress, existing
methods have a strong bias towards low- or high-order interactions, or rely on
expertise feature engineering. In this paper, we show that it is possible to
derive an end-to-end learning model that emphasizes both low- and high-order
feature interactions. The proposed framework, DeepFM, combines the power of
factorization machines for recommendation and deep learning for feature
learning in a new neural network architecture. Compared to the latest Wide &amp;amp;
Deep model from Google, DeepFM has a shared raw feature input to both its
&quot;wide&quot; and &quot;deep&quot; components, with no need of feature engineering besides raw
features. DeepFM, as a general learning framework, can incorporate various
network architectures in its deep component. In this paper, we study two
instances of DeepFM where its &quot;deep&quot; component is DNN and PNN respectively, for
which we denote as DeepFM-D and DeepFM-P. Comprehensive experiments are
conducted to demonstrate the effectiveness of DeepFM-D and DeepFM-P over the
existing models for CTR prediction, on both benchmark data and commercial data.
We conduct online A/B test in Huawei App Market, which reveals that DeepFM-D
leads to more than 10% improvement of click-through rate in the production
environment, compared to a well-engineered LR model. We also covered related
practice in deploying our framework in Huawei App Market.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Huifeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Ruiming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunming Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiuqiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Dong&lt;/a&gt;</dc:creator>
</item></rdf:RDF>