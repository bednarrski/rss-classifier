<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06490"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06742"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05583"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.01467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03749"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1703.06490">
<title>Generating Multi-label Discrete Patient Records using Generative Adversarial Networks. (arXiv:1703.06490v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06490</link>
<description rdf:parseType="Literal">&lt;p&gt;Access to electronic health record (EHR) data has motivated computational
advances in medical research. However, various concerns, particularly over
privacy, can limit access to and collaborative use of EHR data. Sharing
synthetic EHR data could mitigate risk. In this paper, we propose a new
approach, medical Generative Adversarial Network (medGAN), to generate
realistic synthetic patient records. Based on input real patient records,
medGAN can generate high-dimensional discrete variables (e.g., binary and count
features) via a combination of an autoencoder and generative adversarial
networks. We also propose minibatch averaging to efficiently avoid mode
collapse, and increase the learning efficiency with batch normalization and
shortcut connections. To demonstrate feasibility, we showed that medGAN
generates synthetic patient records that achieve comparable performance to real
data on many experiments including distribution statistics, predictive modeling
tasks and a medical expert review. We also empirically observe a limited
privacy risk in both identity and attribute disclosure using medGAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswal_S/0/1/0/all/0/1&quot;&gt;Siddharth Biswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malin_B/0/1/0/all/0/1&quot;&gt;Bradley Malin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duke_J/0/1/0/all/0/1&quot;&gt;Jon Duke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_W/0/1/0/all/0/1&quot;&gt;Walter F. Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04016">
<title>Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution. (arXiv:1801.04016v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04016</link>
<description rdf:parseType="Literal">&lt;p&gt;Current machine learning systems operate, almost exclusively, in a
statistical, or model-free mode, which entails severe theoretical limits on
their power and performance. Such systems cannot reason about interventions and
retrospection and, therefore, cannot serve as the basis for strong AI. To
achieve human level intelligence, learning machines need the guidance of a
model of reality, similar to the ones used in causal inference tasks. To
demonstrate the essential role of such models, I will present a summary of
seven tasks which are beyond reach of current machine learning systems and
which have been accomplished using the tools of causal modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pearl_J/0/1/0/all/0/1&quot;&gt;Judea Pearl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04055">
<title>A3T: Adversarially Augmented Adversarial Training. (arXiv:1801.04055v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04055</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research showed that deep neural networks are highly sensitive to
so-called adversarial perturbations, which are tiny perturbations of the input
data purposely designed to fool a machine learning classifier. Most
classification models, including deep learning models, are highly vulnerable to
adversarial attacks. In this work, we investigate a procedure to improve
adversarial robustness of deep neural networks through enforcing representation
invariance. The idea is to train the classifier jointly with a discriminator
attached to one of its hidden layer and trained to filter the adversarial
noise. We perform preliminary experiments to test the viability of the approach
and to compare it to other standard adversarial training methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erraqabi_A/0/1/0/all/0/1&quot;&gt;Akram Erraqabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1&quot;&gt;Aristide Baratin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1&quot;&gt;Simon Lacoste-Julien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04062">
<title>MINE: Mutual Information Neural Estimation. (arXiv:1801.04062v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04062</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that the estimation of the mutual information between high
dimensional continuous random variables is achievable by gradient descent over
neural networks. This paper presents a Mutual Information Neural Estimator
(MINE) that is linearly scalable in dimensionality as well as in sample size.
MINE is back-propable and we prove that it is strongly consistent. We
illustrate a handful of applications in which MINE is succesfully applied to
enhance the property of generative models in both unsupervised and supervised
settings. We apply our framework to estimate the information bottleneck, and
apply it in tasks related to supervised classification problems. Our results
demonstrate substantial added flexibility and improvement in these settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belghazi_I/0/1/0/all/0/1&quot;&gt;Ishmael Belghazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1&quot;&gt;Sai Rajeswar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1&quot;&gt;Aristide Baratin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04159">
<title>Can Who-Edits-What Predict Edit Survival?. (arXiv:1801.04159v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1801.04159</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet has enabled the emergence of massive online collaborative
projects. As the number of contributors to these projects grows, it becomes
increasingly important to understand and predict whether the edits that users
make will eventually impact the project positively. Existing solutions either
rely on a user reputation system or consist of a highly-specialized predictor
tailored to a specific peer-production system. In this work, we explore a
different point in the solution space, which does not involve any content-based
feature of the edits. To this end, we formulate a statistical model of edit
outcomes. We view each edit as a game between the editor and the component of
the project. We posit that the probability of a positive outcome is a function
of the editor&apos;s skill, of the difficulty of editing the component and of a
user-component interaction term. Our model is broadly applicable, as it only
requires observing data about who makes an edit, what the edit affects and
whether the edit survives or not. Then, we consider Wikipedia and the Linux
kernel, two examples of large-scale collaborative projects, and we seek to
understand whether this simple model can effectively predict edit survival: in
both cases, we provide a positive answer. Our approach significantly
outperforms those based solely on user reputation and bridges the gap with
specialized predictors that use content-based features. Furthermore, inspecting
the model parameters enables us to discover interesting structure in the data.
Our method is simple to implement, computationally inexpensive, and it produces
interpretable results; as such, we believe that it is a valuable tool to
analyze collaborative systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yardim_A/0/1/0/all/0/1&quot;&gt;Ali Batuhan Yard&amp;#x131;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kristof_V/0/1/0/all/0/1&quot;&gt;Victor Kristof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maystre_L/0/1/0/all/0/1&quot;&gt;Lucas Maystre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grossglauser_M/0/1/0/all/0/1&quot;&gt;Matthias Grossglauser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04211">
<title>Deep Learning for Sampling from Arbitrary Probability Distributions. (arXiv:1801.04211v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04211</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a fully connected neural network model to map samples
from a uniform distribution to samples of any explicitly known probability
density function. During the training, the Jensen-Shannon divergence between
the distribution of the model&apos;s output and the target distribution is
minimized. We experimentally demonstrate that our model converges towards the
desired state. It provides an alternative to existing sampling methods such as
inversion sampling, rejection sampling, Gaussian mixture models and
Markov-Chain-Monte-Carlo. Our model has high sampling efficiency and is easily
applied to any probability distribution, without the need of further analytical
or numerical calculations. It can produce correlated samples, such that the
output distribution converges faster towards the target than for independent
samples. But it is also able to produce independent samples, if single values
are fed into the network and the input values are independent as well. We focus
on one-dimensional sampling, but additionally illustrate a two-dimensional
example with a target distribution of dependent variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horger_F/0/1/0/all/0/1&quot;&gt;Felix Horger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wurfl_T/0/1/0/all/0/1&quot;&gt;Tobias W&amp;#xfc;rfl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christlein_V/0/1/0/all/0/1&quot;&gt;Vincent Christlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06742">
<title>Twin Networks: Matching the Future for Sequence Generation. (arXiv:1708.06742v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06742</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple technique for encouraging generative RNNs to plan ahead.
We train a &quot;backward&quot; recurrent network to generate a given sequence in reverse
order, and we encourage states of the forward model to predict cotemporal
states of the backward model. The backward network is used only during
training, and plays no role during sampling or inference. We hypothesize that
our approach eases modeling of long-term dependencies by implicitly forcing the
forward states to hold information about the longer-term future (as contained
in the backward states). We show empirically that our approach achieves 9%
relative improvement for a speech recognition task, and achieves significant
improvement on a COCO caption generation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serdyuk_D/0/1/0/all/0/1&quot;&gt;Dmitriy Serdyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_N/0/1/0/all/0/1&quot;&gt;Nan Rosemary Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1&quot;&gt;Alessandro Sordoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1&quot;&gt;Adam Trischler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Chris Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05583">
<title>Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification. (arXiv:1709.05583v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05583</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have transformed several artificial intelligence
research areas including computer vision, speech recognition, and natural
language processing. However, recent studies demonstrated that DNNs are
vulnerable to adversarial manipulations at testing time. Specifically, suppose
we have a testing example, whose label can be correctly predicted by a DNN
classifier. An attacker can add a small carefully crafted noise to the testing
example such that the DNN classifier predicts an incorrect label, where the
crafted testing example is called adversarial example. Such attacks are called
evasion attacks. Evasion attacks are one of the biggest challenges for
deploying DNNs in safety and security critical applications such as
self-driving cars. In this work, we develop new methods to defend against
evasion attacks. Our key observation is that adversarial examples are close to
the classification boundary. Therefore, we propose region-based classification
to be robust to adversarial examples. For a benign/adversarial testing example,
we ensemble information in a hypercube centered at the example to predict its
label. In contrast, traditional classifiers are point-based classification,
i.e., given a testing example, the classifier predicts its label based on the
testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets
demonstrate that our region-based classification can significantly mitigate
evasion attacks without sacrificing classification accuracy on benign examples.
Specifically, our region-based classification achieves the same classification
accuracy on testing benign examples as point-based classification, but our
region-based classification is significantly more robust than point-based
classification to various evasion attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.01467">
<title>Mean-field theory of input dimensionality reduction in unsupervised deep neural networks. (arXiv:1710.01467v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.01467</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks as powerful tools are widely used in various domains.
However, the nature of computations at each layer of the deep networks is far
from being well understood. Increasing the interpretability of deep neural
networks is thus important. Here, we construct a mean-field framework to
understand how compact representations are developed across layers, not only in
deterministic random deep networks but also in generative deep networks where
network parameters are learned from input data. Our theory shows that the deep
computation implements a dimensionality reduction while maintaining a finite
level of weak correlations between neurons for possible feature extraction.
This work may pave the way for understanding how a sensory hierarchy works in
general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haiping Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01401">
<title>Demystifying MMD GANs. (arXiv:1801.01401v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01401</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the training and performance of generative adversarial
networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs.
As our main theoretical contribution, we clarify the situation with bias in GAN
loss functions raised by recent work: we show that gradient estimators used in
the optimization process for both MMD GANs and Wasserstein GANs are unbiased,
but learning a discriminator based on samples leads to biased gradients for the
generator parameters. We also discuss the issue of kernel choice for the MMD
critic, and characterize the kernel corresponding to the energy distance used
for the Cramer GAN critic. Being an integral probability metric, the MMD
benefits from training strategies recently developed for Wasserstein GANs. In
experiments, the MMD GAN is able to employ a smaller critic network than the
Wasserstein GAN, resulting in a simpler and faster-training algorithm with
matching performance. We also propose an improved measure of GAN convergence,
the Kernel Inception Distance, and show how to use it to dynamically adapt
learning rates during GAN training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Binkowski_M/0/1/0/all/0/1&quot;&gt;Miko&amp;#x142;aj Bi&amp;#x144;kowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1&quot;&gt;Dougal J. Sutherland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1&quot;&gt;Michael Arbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03329">
<title>Weakly Supervised One-Shot Detection with Attention Siamese Networks. (arXiv:1801.03329v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of weakly supervised one-shot detection. In this task,
we attempt to perform a detection task over a set of unseen classes, when
training only using weak binary labels that indicate the existence of a class
instance in a given example. The model is conditioned on a single exemplar of
an unseen class and a target example that may or may not contain an instance of
the same class as the exemplar. A similarity map is computed by using a Siamese
neural network to map the exemplar and regions of the target example to a
latent representation space and then computing cosine similarity scores between
representations. An attention mechanism weights different regions in the target
example, and enables learning of the one-shot detection task using the weaker
labels alone. The model can be applied to detection tasks from different
domains, including computer vision object detection. We evaluate our attention
Siamese networks on a one-shot detection task from the audio domain, where it
detects audio keywords in spoken utterances. Our model considerably outperforms
a baseline approach and yields a 42.6% average precision for detection across
10 unseen classes. Moreover, architectural developments from computer vision
object detection models such as a region proposal network can be incorporated
into the model architecture, and results show that performance is expected to
improve by doing so.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keren_G/0/1/0/all/0/1&quot;&gt;Gil Keren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmitt_M/0/1/0/all/0/1&quot;&gt;Maximilian Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kehrenberg_T/0/1/0/all/0/1&quot;&gt;Thomas Kehrenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03749">
<title>Improved asynchronous parallel optimization analysis for stochastic incremental methods. (arXiv:1801.03749v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03749</link>
<description rdf:parseType="Literal">&lt;p&gt;As datasets continue to increase in size and multi-core computer
architectures are developed, asynchronous parallel optimization algorithms
become more and more essential to the field of Machine Learning. Unfortunately,
conducting the theoretical analysis asynchronous methods is difficult, notably
due to the introduction of delay and inconsistency in inherently sequential
algorithms. Handling these issues often requires resorting to simplifying but
unrealistic assumptions. Through a novel perspective, we revisit and clarify a
subtle but important technical issue present in a large fraction of the recent
convergence rate proofs for asynchronous parallel optimization algorithms, and
propose a simplification of the recently introduced &quot;perturbed iterate&quot;
framework that resolves it. We demonstrate the usefulness of our new framework
by analyzing three distinct asynchronous parallel incremental optimization
algorithms: Hogwild (asynchronous SGD), KROMAGNON (asynchronous SVRG) and
ASAGA, a novel asynchronous parallel version of the incremental gradient
algorithm SAGA that enjoys fast linear convergence rates. We are able to both
remove problematic assumptions and obtain better theoretical results. Notably,
we prove that ASAGA and KROMAGNON can obtain a theoretical linear speedup on
multi-core systems even without sparsity assumptions. We present results of an
implementation on a 40-core architecture illustrating the practical speedups as
well as the hardware overhead. Finally, we investigate the overlap constant, an
ill-understood but central quantity for the theoretical analysis of
asynchronous parallel algorithms. We find that it encompasses much more
complexity than suggested in previous work, and often is order-of-magnitude
bigger than traditionally thought.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Leblond_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Leblond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pedregosa_F/0/1/0/all/0/1&quot;&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lacoste_Julien_S/0/1/0/all/0/1&quot;&gt;Simon Lacoste-Julien&lt;/a&gt;</dc:creator>
</item></rdf:RDF>