<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00053"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00284"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1507.01239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00154"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00228"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00392"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00425"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00462"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00614"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00743"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01729"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07163"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00807"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00002"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00042"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00051"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00099"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00297"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00400"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00516"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00260"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.00725"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06370"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.00053">
<title>Task-Driven Convolutional Recurrent Models of the Visual System. (arXiv:1807.00053v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1807.00053</link>
<description rdf:parseType="Literal">&lt;p&gt;Feed-forward convolutional neural networks (CNNs) are currently
state-of-the-art for object classification tasks such as ImageNet. Further,
they are quantitatively accurate models of temporally-averaged responses of
neurons in the primate brain&apos;s visual system. However, biological visual
systems have two ubiquitous architectural features not shared with typical
CNNs: local recurrence within cortical areas, and long-range feedback from
downstream areas to upstream areas. Here we explored the role of recurrence in
improving classification performance. We found that standard forms of
recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the
ImageNet task. In contrast, custom cells that incorporated two structural
features, bypassing and gating, were able to boost task accuracy substantially.
We extended these design principles in an automated search over thousands of
model architectures, which identified novel local recurrent cells and
long-range feedback connections useful for object recognition. Moreover, these
task-optimized ConvRNNs explained the dynamics of neural activity in the
primate visual system better than feedforward networks, suggesting a role for
the brain&apos;s recurrent connections in performing difficult visual behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nayebi_A/0/1/0/all/0/1&quot;&gt;Aran Nayebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bear_D/0/1/0/all/0/1&quot;&gt;Daniel Bear&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kubilius_J/0/1/0/all/0/1&quot;&gt;Jonas Kubilius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kar_K/0/1/0/all/0/1&quot;&gt;Kohitij Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ganguli_S/0/1/0/all/0/1&quot;&gt;Surya Ganguli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sussillo_D/0/1/0/all/0/1&quot;&gt;David Sussillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+DiCarlo_J/0/1/0/all/0/1&quot;&gt;James J. DiCarlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yamins_D/0/1/0/all/0/1&quot;&gt;Daniel L. K. Yamins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00188">
<title>Benchmarking the Hill-Valley Evolutionary Algorithm for the GECCO 2018 Competition on Niching Methods Multimodal Optimization. (arXiv:1807.00188v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.00188</link>
<description rdf:parseType="Literal">&lt;p&gt;This report presents benchmarking results of the latest version of the
Hill-Valley Evolutionary Algorithm (HillVallEA) on the CEC2013 niching
benchmark suite. The benchmarking follows restrictions required by the GECCO
2018 competition on Niching methods for Multimodal Optimization. In particular,
no problem dependent parameter tuning is performed. A number of adjustments
have been made to original publication of HillVallEA that are discussed in this
report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maree_S/0/1/0/all/0/1&quot;&gt;S.C. Maree&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1&quot;&gt;T. Alderliesten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thierens_D/0/1/0/all/0/1&quot;&gt;D. Thierens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1&quot;&gt;P.A.N. Bosman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00284">
<title>Autonomous Deep Learning: A Genetic DCNN Designer for Image Classification. (arXiv:1807.00284v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00284</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the breakthrough success of deep convolutional
neural networks (DCNNs) in image classification and other vision applications.
Although freeing users from the troublesome handcrafted feature extraction by
providing a uniform feature extraction-classification framework, DCNNs still
require a handcrafted design of their architectures. In this paper, we propose
the genetic DCNN designer, an autonomous learning algorithm can generate a DCNN
architecture automatically based on the data available for a specific image
classification problem. We first partition a DCNN into multiple stacked meta
convolutional blocks and fully connected blocks, each containing the operations
of convolution, pooling, fully connection, batch normalization, activation and
drop out, and thus convert the architecture into an integer vector. Then, we
use refined evolutionary operations, including selection, mutation and
crossover to evolve a population of DCNN architectures. Our results on the
MNIST, Fashion-MNIST, EMNISTDigit, EMNIST-Letter, CIFAR10 and CIFAR100 datasets
suggest that the proposed genetic DCNN designer is able to produce
automatically DCNN architectures, whose performance is comparable to, if not
better than, that of stateof- the-art DCNN models
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Benteng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1507.01239">
<title>Experiments on Parallel Training of Deep Neural Network using Model Averaging. (arXiv:1507.01239v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1507.01239</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we apply model averaging to parallel training of deep neural
network (DNN). Parallelization is done in a model averaging manner. Data is
partitioned and distributed to different nodes for local model updates, and
model averaging across nodes is done every few minibatches. We use multiple
GPUs for data parallelization, and Message Passing Interface (MPI) for
communication between nodes, which allows us to perform model averaging
frequently without losing much time on communication. We investigate the
effectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) and
Restricted Boltzmann Machine (RBM) pretraining for parallel training in
model-averaging framework, and explore the best setups in term of different
learning rate schedules, averaging frequencies and minibatch sizes. It is shown
that NG-SGD and RBM pretraining benefits parameter-averaging based model
training. On the 300h Switchboard dataset, a 9.3 times speedup is achieved
using 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracy
loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoyu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00049">
<title>AI in Game Playing: Sokoban Solver. (arXiv:1807.00049v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00049</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence is becoming instrumental in a variety of
applications. Games serve as a good breeding ground for trying and testing
these algorithms in a sandbox with simpler constraints in comparison to real
life. In this project, we aim to develop an AI agent that can solve the
classical Japanese game of Sokoban using various algorithms and heuristics and
compare their performances through standard metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesan_A/0/1/0/all/0/1&quot;&gt;Anand Venkatesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Atishay Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grewal_R/0/1/0/all/0/1&quot;&gt;Rakesh Grewal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00082">
<title>Amanuensis: The Programmer&apos;s Apprentice. (arXiv:1807.00082v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1807.00082</link>
<description rdf:parseType="Literal">&lt;p&gt;This document provides an overview of the material covered in a course taught
at Stanford in the spring quarter of 2018. The course draws upon insight from
cognitive and systems neuroscience to implement hybrid connectionist and
symbolic reasoning systems that leverage and extend the state of the art in
machine learning by integrating human and machine intelligence. As a concrete
example we focus on digital assistants that learn from continuous dialog with
an expert software engineer while providing initial value as powerful
analytical, computational and mathematical savants. Over time these savants
learn cognitive strategies (domain-relevant problem solving skills) and develop
intuitions (heuristics and the experience necessary for applying them) by
learning from their expert associates. By doing so these savants elevate their
innate analytical skills allowing them to partner on an equal footing as
versatile collaborators - effectively serving as cognitive extensions and
digital prostheses, thereby amplifying and emulating their human partner&apos;s
conceptually-flexible thinking patterns and enabling improved access to and
control over powerful computing resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dean_T/0/1/0/all/0/1&quot;&gt;Thomas Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chiang_M/0/1/0/all/0/1&quot;&gt;Maurice Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gomez_M/0/1/0/all/0/1&quot;&gt;Marcus Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gruver_N/0/1/0/all/0/1&quot;&gt;Nate Gruver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hindy_Y/0/1/0/all/0/1&quot;&gt;Yousef Hindy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lam_M/0/1/0/all/0/1&quot;&gt;Michelle Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Peter Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sanchez_S/0/1/0/all/0/1&quot;&gt;Sophia Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Saxena_R/0/1/0/all/0/1&quot;&gt;Rohun Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Michael Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lucy Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Catherine Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00154">
<title>AI in Education needs interpretable machine learning: Lessons from Open Learner Modelling. (arXiv:1807.00154v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00154</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability of the underlying AI representations is a key raison
d&apos;\^{e}tre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring
Systems (ITS) research. OLMs provide tools for &apos;opening&apos; up the AI models of
learners&apos; cognition and emotions for the purpose of supporting human learning
and teaching. Over thirty years of research in ITS (also known as AI in
Education) produced important work, which informs about how AI can be used in
Education to best effects and, through the OLM research, what are the necessary
considerations to make it interpretable and explainable for the benefit of
learning. We argue that this work can provide a valuable starting point for a
framework of interpretable AI, and as such is of relevance to the application
of both knowledge-based and machine learning systems in other high-stakes
contexts, beyond education.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conati_C/0/1/0/all/0/1&quot;&gt;Cristina Conati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porayska_Pomsta_K/0/1/0/all/0/1&quot;&gt;Kaska Porayska-Pomsta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavrikis_M/0/1/0/all/0/1&quot;&gt;Manolis Mavrikis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00228">
<title>Embedding Models for Episodic Memory. (arXiv:1807.00228v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00228</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years a number of large-scale triple-oriented knowledge graphs have
been generated and various models have been proposed to perform learning in
those graphs. Most knowledge graphs are static and reflect the world in its
current state. In reality, of course, the state of the world is changing: a
healthy person becomes diagnosed with a disease and a new president is
inaugurated. In this paper, we extend models for static knowledge graphs to
temporal knowledge graphs. This enables us to store episodic data and to
generalize to new facts (inductive learning). We generalize leading learning
models for static knowledge graphs (i.e., Tucker, RESCAL, HolE, ComplEx,
DistMult) to temporal knowledge graphs. In particular, we introduce a new
tensor model, ConT, with superior generalization performance. The performances
of all proposed models are analyzed on two different datasets: the Global
Database of Events, Language, and Tone (GDELT) and the database for Integrated
Conflict Early Warning System (ICEWS). We argue that temporal knowledge graph
embeddings might be models also for cognitive episodic memory (facts we
remember and can recollect) and that a semantic memory (current facts we know)
can be generated from episodic memory by a marginalization operation. We
validate this episodic-to-semantic projection hypothesis with the ICEWS
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daxberger_E/0/1/0/all/0/1&quot;&gt;Erik Daxberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00340">
<title>Towards Adversarial Training with Moderate Performance Improvement for Neural Network Classification. (arXiv:1807.00340v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00340</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been demonstrated that deep neural networks are prone to noisy
examples particular adversarial samples during inference process. The gap
between robust deep learning systems in real world applications and vulnerable
neural networks is still large. Current adversarial training strategies improve
the robustness against adversarial samples. However, these methods lead to
accuracy reduction when the input examples are clean thus hinders the
practicability. In this paper, we investigate an approach that protects the
neural network classification from the adversarial samples and improves its
accuracy when the input examples are clean. We demonstrate the versatility and
effectiveness of our proposed approach on a variety of different networks and
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1&quot;&gt;Xinhan Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Pengqian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1&quot;&gt;Meng Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00366">
<title>Beyond Winning and Losing: Modeling Human Motivations and Behaviors Using Inverse Reinforcement Learning. (arXiv:1807.00366v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00366</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, reinforcement learning (RL) methods have been applied to
model gameplay with great success, achieving super-human performance in various
environments, such as Atari, Go, and Poker. However, those studies mostly focus
on winning the game and have largely ignored the rich and complex human
motivations, which are essential for understanding different players&apos; diverse
behaviors. In this paper, we present a novel method called Multi-Motivation
Behavior Modeling (MMBM) that takes the multifaceted human motivations into
consideration and models the underlying value structure of the players using
inverse RL. Our approach does not require the access to the dynamic of the
system, making it feasible to model complex interactive environments such as
massively multiplayer online games. MMBM is tested on the World of Warcraft
Avatar History dataset, which recorded over 70,000 users&apos; gameplay spanning
three years period. Our model reveals the significant difference of value
structures among different player groups. Using the results of motivation
modeling, we also predict and explain their diverse gameplay behaviors and
provide a quantitative assessment of how the redesign of the game environment
impacts players&apos; behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tongfang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sam Xianjun Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00392">
<title>Gradient Reversal Against Discrimination. (arXiv:1807.00392v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00392</link>
<description rdf:parseType="Literal">&lt;p&gt;No methods currently exist for making arbitrary neural networks fair. In this
work we introduce GRAD, a new and simplified method to producing fair neural
networks that can be used for auto-encoding fair representations or directly
with predictive networks. It is easy to implement and add to existing
architectures, has only one (insensitive) hyper-parameter, and provides
improved individual and group fairness. We use the flexibility of GRAD to
demonstrate multi-attribute protection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raff_E/0/1/0/all/0/1&quot;&gt;Edward Raff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sylvester_J/0/1/0/all/0/1&quot;&gt;Jared Sylvester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00412">
<title>Learning to Drive in a Day. (arXiv:1807.00412v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00412</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate the first application of deep reinforcement learning to
autonomous driving. From randomly initialised parameters, our model is able to
learn a policy for lane following in a handful of training episodes using a
single monocular image as input. We provide a general and easy to obtain
reward: the distance travelled by the vehicle without the safety driver taking
control. We use a continuous, model-free deep reinforcement learning algorithm,
with all exploration and optimisation performed on-vehicle. This demonstrates a
new framework for autonomous driving which moves away from reliance on defined
logical rules, mapping, and direct supervision. We discuss the challenges and
opportunities to scale this approach to a broader range of autonomous driving
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1&quot;&gt;Alex Kendall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawke_J/0/1/0/all/0/1&quot;&gt;Jeffrey Hawke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janz_D/0/1/0/all/0/1&quot;&gt;David Janz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazur_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Mazur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reda_D/0/1/0/all/0/1&quot;&gt;Daniele Reda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_J/0/1/0/all/0/1&quot;&gt;John-Mark Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_V/0/1/0/all/0/1&quot;&gt;Vinh-Dieu Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1&quot;&gt;Alex Bewley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Amar Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00425">
<title>Dynamic Prediction Length for Time Series with Sequence to Sequence Networks. (arXiv:1807.00425v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00425</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks and sequence to sequence models require a
predetermined length for prediction output length. Our model addresses this by
allowing the network to predict a variable length output in inference. A new
loss function with a tailored gradient computation is developed that trades off
prediction accuracy and output length. The model utilizes a function to
determine whether a particular output at a time should be evaluated or not
given a predetermined threshold. We evaluate the model on the problem of
predicting the prices of securities. We find that the model makes longer
predictions for more stable securities and it naturally balances prediction
accuracy and length.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harmon_M/0/1/0/all/0/1&quot;&gt;Mark Harmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00462">
<title>ColdRoute: Effective Routing of Cold Questions in Stack Exchange Sites. (arXiv:1807.00462v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00462</link>
<description rdf:parseType="Literal">&lt;p&gt;Routing questions in Community Question Answer services (CQAs) such as Stack
Exchange sites is a well-studied problem. Yet, cold-start -- a phenomena
observed when a new question is posted is not well addressed by existing
approaches. Additionally, cold questions posted by new askers present
significant challenges to state-of-the-art approaches. We propose ColdRoute to
address these challenges. ColdRoute is able to handle the task of routing cold
questions posted by new or existing askers to matching experts. Specifically,
we use Factorization Machines on the one-hot encoding of critical features such
as question tags and compare our approach to well-studied techniques such as
CQARank and semantic matching (LDA, BoW, and Doc2Vec). Using data from eight
stack exchange sites, we are able to improve upon the routing metrics
(Precision$@1$, Accuracy, MRR) over the state-of-the-art models such as
semantic matching by $159.5\%$,$31.84\%$, and $40.36\%$ for cold questions
posted by existing askers, and $123.1\%$, $27.03\%$, and $34.81\%$ for cold
questions posted by new askers respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Aniket Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1&quot;&gt;Srinivasan Parthasarathy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00564">
<title>Inference, Learning, and Population Size: Projectivity for SRL Models. (arXiv:1807.00564v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00564</link>
<description rdf:parseType="Literal">&lt;p&gt;A subtle difference between propositional and relational data is that in many
relational models, marginal probabilities depend on the population or domain
size. This paper connects the dependence on population size to the classic
notion of projectivity from statistical theory: Projectivity implies that
relational predictions are robust with respect to changes in domain size. We
discuss projectivity for a number of common SRL systems, and identify syntactic
fragments that are guaranteed to yield projective models. The syntactic
conditions are restrictive, which suggests that projectivity is difficult to
achieve in SRL, and care must be taken when working with different domain
sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_M/0/1/0/all/0/1&quot;&gt;Manfred Jaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1&quot;&gt;Oliver Schulte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00614">
<title>Knowledge Compilation with Continuous Random Variables and its Application in Hybrid Probabilistic Logic Programming. (arXiv:1807.00614v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00614</link>
<description rdf:parseType="Literal">&lt;p&gt;In probabilistic reasoning, the traditionally discrete domain has been
elevated to the hybrid domain encompassing additionally continuous random
variables. Inference in the hybrid domain, however, usually necessitates to
condone trade-offs on either the inference on discrete or continuous random
variables. We introduce a novel approach based on weighted model integration
and algebraic model counting that circumvents these trade-offs. We then show
how it supports knowledge compilation and exact probabilistic inference.
Moreover, we introduce the hybrid probabilistic logic programming language
HAL-ProbLog, an extension of ProbLog, to which we apply our inference approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martires_P/0/1/0/all/0/1&quot;&gt;Pedro Zuidberg Dos Martires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dries_A/0/1/0/all/0/1&quot;&gt;Anton Dries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1&quot;&gt;Luc De Raedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00703">
<title>Introducing the Simulated Flying Shapes and Simulated Planar Manipulator Datasets. (arXiv:1807.00703v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00703</link>
<description rdf:parseType="Literal">&lt;p&gt;We release two artificial datasets, Simulated Flying Shapes and Simulated
Planar Manipulator that allow to test the learning ability of video processing
systems. In particular, the dataset is meant as a tool which allows to easily
assess the sanity of deep neural network models that aim to encode, reconstruct
or predict video frame sequences. The datasets each consist of 90000 videos.
The Simulated Flying Shapes dataset comprises scenes showing two objects of
equal shape (rectangle, triangle and circle) and size in which one object
approaches its counterpart. The Simulated Planar Manipulator shows a 3-DOF
planar manipulator that executes a pick-and-place task in which it has to place
a size-varying circle on a squared platform. Different from other widely used
datasets such as moving MNIST [1], [2], the two presented datasets involve
goal-oriented tasks (e.g. the manipulator grasping an object and placing it on
a platform), rather than showing random movements. This makes our datasets more
suitable for testing prediction capabilities and the learning of sophisticated
motions by a machine learning model. This technical document aims at providing
an introduction into the usage of both datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rothfuss_J/0/1/0/all/0/1&quot;&gt;Jonas Rothfuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1&quot;&gt;Eren Erdal Aksoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asfour_T/0/1/0/all/0/1&quot;&gt;Tamim Asfour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00737">
<title>Improving Goal-Oriented Visual Dialog Agents via Advanced Recurrent Nets with Tempered Policy Gradient. (arXiv:1807.00737v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00737</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning goal-oriented dialogues by means of deep reinforcement learning has
recently become a popular research topic. However, training text-generating
agents efficiently is still a considerable challenge. Commonly used
policy-based dialogue agents often end up focusing on simple utterances and
suboptimal policies. To mitigate this problem, we propose a class of novel
temperature-based extensions for policy gradient methods, which are referred to
as Tempered Policy Gradients (TPGs). These methods encourage exploration with
different temperature control strategies. We derive three variations of the
TPGs and show their superior performance on a recently published AI-testbed,
i.e., the GuessWhat?! game. On the testbed, we achieve significant improvements
with two innovations. The first one is an extension of the state-of-the-art
solutions with Seq2Seq and Memory Network structures that leads to an
improvement of 9%. The second one is the application of our newly developed TPG
methods, which improves the performance additionally by around 5% and, even
more importantly, helps produce more convincing utterances. TPG can easily be
applied to any goal-oriented dialogue systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00743">
<title>Fusing First-order Knowledge Compilation and the Lifted Junction Tree Algorithm. (arXiv:1807.00743v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00743</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard approaches for inference in probabilistic formalisms with
first-order constructs include lifted variable elimination (LVE) for single
queries as well as first-order knowledge compilation (FOKC) based on weighted
model counting. To handle multiple queries efficiently, the lifted junction
tree algorithm (LJT) uses a first-order cluster representation of a model and
LVE as a subroutine in its computations. For certain inputs, the
implementations of LVE and, as a result, LJT ground parts of a model where FOKC
has a lifted run. The purpose of this paper is to prepare LJT as a backbone for
lifted inference and to use any exact inference algorithm as subroutine. Using
FOKC in LJT allows us to compute answers faster than LJT, LVE, and FOKC for
certain inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_T/0/1/0/all/0/1&quot;&gt;Tanya Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1&quot;&gt;Ralf M&amp;#xf6;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00744">
<title>Preventing Unnecessary Groundings in the Lifted Dynamic Junction Tree Algorithm. (arXiv:1807.00744v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00744</link>
<description rdf:parseType="Literal">&lt;p&gt;The lifted dynamic junction tree algorithm (LDJT) efficiently answers
filtering and prediction queries for probabilistic relational temporal models
by building and then reusing a first-order cluster representation of a
knowledge base for multiple queries and time steps. Unfortunately, a non-ideal
elimination order can lead to groundings even though a lifted run is possible
for a model. We extend LDJT (i) to identify unnecessary groundings while
proceeding in time and (ii) to prevent groundings by delaying eliminations
through changes in a temporal first-order cluster representation. The extended
version of LDJT answers multiple temporal queries orders of magnitude faster
than the original version.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrke_M/0/1/0/all/0/1&quot;&gt;Marcel Gehrke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_T/0/1/0/all/0/1&quot;&gt;Tanya Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1&quot;&gt;Ralf M&amp;#xf6;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00751">
<title>Understanding the Effectiveness of Lipschitz Constraint in Training of GANs via Gradient Analysis. (arXiv:1807.00751v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00751</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to bring a new perspective for understanding GANs, by delving
deeply into the key factors that lead to the failure and success of training of
GANs. Specifically, (i) we study the value surface of the optimal
discriminative function, from which we show that the fundamental source of
failure in training of GANs stems from the unwarranted gradient direction; (ii)
we show that Lipschitz constraint is not always necessary for evaluating
Wasserstein distance, and we further demonstrate that, without Lipschitz
constraint, Wasserstein GAN may also fail in the same way as other GANs; (iii)
we theoretically show that Lipschitz constraint is generally a powerful tool to
guarantee meaningful gradient directions and we further propose a generalized
family of GAN formulations based on Lipschitz constraint, where Wasserstein GAN
is a special case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhiming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lantao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00780">
<title>Ambient Hidden Space of Generative Adversarial Networks. (arXiv:1807.00780v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00780</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial models are powerful tools to model structure in
complex distributions for a variety of tasks. Current techniques for learning
generative models require an access to samples which have high quality, and
advanced generative models are applied to generate samples from noisy training
data through ambient modules. However, the modules are only practical for the
output space of the generator, and their application in the hidden space is not
well studied. In this paper, we extend the ambient module to the hidden space
of the generator, and provide the uniqueness condition and the corresponding
strategy for the ambient hidden generator in the adversarial training process.
We report the practicality of the proposed method on the benchmark dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1&quot;&gt;Xinhan Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Pengqian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1&quot;&gt;Meng Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01729">
<title>Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x)) Alternative. (arXiv:1708.01729v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01729</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we mathematically study several GAN related topics,
including Inception score, label smoothing, gradient vanishing and the
-log(D(x)) alternative.
&lt;/p&gt;
&lt;p&gt;---
&lt;/p&gt;
&lt;p&gt;An advanced version is included in &lt;a href=&quot;/abs/1703.02000&quot;&gt;arXiv:1703.02000&lt;/a&gt; &quot;Activation Maximization
Generative Adversarial Nets&quot;.
&lt;/p&gt;
&lt;p&gt;Please refer Section 6 in &lt;a href=&quot;/abs/1703.02000&quot;&gt;1703.02000&lt;/a&gt; for detailed analysis on Inception
Score, and refer its appendix for the discussions on Label Smoothing, Gradient
Vanishing and -log(D(x)) Alternative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhiming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06744">
<title>Learning to Organize Knowledge and Answer Questions with N-Gram Machines. (arXiv:1711.06744v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06744</link>
<description rdf:parseType="Literal">&lt;p&gt;Though deep neural networks have great success in natural language
processing, they are limited at more knowledge intensive AI tasks, such as
open-domain Question Answering (QA). Existing end-to-end deep QA models need to
process the entire text after observing the question, and therefore their
complexity in responding a question is linear in the text size. This is
prohibitive for practical tasks such as QA from Wikipedia, a novel, or the Web.
We propose to solve this scalability issue by using symbolic meaning
representations, which can be indexed and retrieved efficiently with complexity
that is independent of the text size. We apply our approach, called the N-Gram
Machine (NGM), to three representative tasks. First as proof-of-concept, we
demonstrate that NGM successfully solves the bAbI tasks of synthetic text.
Second, we show that NGM scales to large corpus by experimenting on &quot;life-long
bAbI&quot;, a special version of bAbI that contains millions of sentences. Lastly on
the WikiMovies dataset, we use NGM to induce latent structure (i.e. schema) and
answer questions from natural language Wikipedia text, with only QA pairs as
weak supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiazhong Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1&quot;&gt;William W. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1&quot;&gt;Ni Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07163">
<title>Dynamic Neural Program Embedding for Program Repair. (arXiv:1711.07163v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07163</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural program embeddings have shown much promise recently for a variety of
program analysis tasks, including program synthesis, program repair, fault
localization, etc. However, most existing program embeddings are based on
syntactic features of programs, such as raw token sequences or abstract syntax
trees. Unlike images and text, a program has an unambiguous semantic meaning
that can be difficult to capture by only considering its syntax (i.e.
syntactically similar pro- grams can exhibit vastly different run-time
behavior), which makes syntax-based program embeddings fundamentally limited.
This paper proposes a novel semantic program embedding that is learned from
program execution traces. Our key insight is that program states expressed as
sequential tuples of live variable values not only captures program semantics
more precisely, but also offer a more natural fit for Recurrent Neural Networks
to model. We evaluate different syntactic and semantic program embeddings on
predicting the types of errors that students make in their submissions to an
introductory programming class and two exercises on the CodeHunt education
platform. Evaluation results show that our new semantic program embedding
significantly outperforms the syntactic program embeddings based on token
sequences and abstract syntax trees. In addition, we augment a search-based
program repair system with the predictions obtained from our se- mantic
embedding, and show that search efficiency is also significantly improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhendong Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00807">
<title>Learning Semantic Sentence Embeddings using Pair-wise Discriminator. (arXiv:1806.00807v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00807</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a method for obtaining sentence-level embeddings.
While the problem of securing word-level embeddings is very well studied, we
propose a novel method for obtaining sentence-level embeddings. This is
obtained by a simple method in the context of solving the paraphrase generation
task. If we use a sequential encoder-decoder model for generating paraphrase,
we would like the generated paraphrase to be semantically close to the original
sentence. One way to ensure this is by adding constraints for true paraphrase
embeddings to be close and unrelated paraphrase candidate sentence embeddings
to be far. This is ensured by using a sequential pair-wise discriminator that
shares weights with the encoder that is trained with a suitable loss function.
Our loss function penalizes paraphrase sentence embedding distances from being
too large. This loss is used in combination with a sequential encoder-decoder
network. We also validated our method by evaluating the obtained embeddings for
a sentiment analysis task. The proposed method results in semantic embeddings
and outperforms the state-of-the-art on the paraphrase generation and sentiment
analysis task on standard datasets. These results are also shown to be
statistically significant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1&quot;&gt;Badri N. Patro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1&quot;&gt;Vinod K. Kurmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sandeep Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1&quot;&gt;Vinay P. Namboodiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00002">
<title>Single Index Latent Variable Models for Network Topology Inference. (arXiv:1807.00002v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00002</link>
<description rdf:parseType="Literal">&lt;p&gt;A semi-parametric, non-linear regression model in the presence of latent
variables is applied towards learning network graph structure. These latent
variables can correspond to unmodeled phenomena or unmeasured agents in a
complex system of interacting entities. This formulation jointly estimates
non-linearities in the underlying data generation, the direct interactions
between measured entities, and the indirect effects of unmeasured processes on
the observed data. The learning is posed as regularized empirical risk
minimization. Details of the algorithm for learning the model are outlined.
Experiments demonstrate the performance of the learned model on real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jonathan Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moura_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; M.F. Moura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00042">
<title>Neural Networks Trained to Solve Differential Equations Learn General Representations. (arXiv:1807.00042v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00042</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a technique based on the singular vector canonical correlation
analysis (SVCCA) for measuring the generality of neural network layers across a
continuously-parametrized set of tasks. We illustrate this method by studying
generality in neural networks trained to solve parametrized boundary value
problems based on the Poisson partial differential equation. We find that the
first hidden layer is general, and that deeper layers are successively more
specific. Next, we validate our method against an existing technique that
measures layer generality using transfer learning experiments. We find
excellent agreement between the two methods, and note that our method is much
faster, particularly for continuously-parametrized problems. Finally, we
visualize the general representations of the first layers, and interpret them
as generalized coordinates over the input domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Magill_M/0/1/0/all/0/1&quot;&gt;Martin Magill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qureshi_F/0/1/0/all/0/1&quot;&gt;Faisal Qureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haan_H/0/1/0/all/0/1&quot;&gt;Hendrick W. de Haan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00051">
<title>Adversarial Examples in Deep Learning: Characterization and Divergence. (arXiv:1807.00051v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00051</link>
<description rdf:parseType="Literal">&lt;p&gt;The burgeoning success of deep learning has raised the security and privacy
concerns as more and more tasks are accompanied with sensitive data.
Adversarial attacks in deep learning have emerged as one of the dominating
security threat to a range of mission-critical deep learning systems and
applications. This paper takes a holistic and principled approach to perform
statistical characterization of adversarial examples in deep learning. We
provide a general formulation of adversarial examples and elaborate on the
basic principle for adversarial attack algorithm design. We introduce easy and
hard categorization of adversarial attacks to analyze the effectiveness of
adversarial examples in terms of attack success rate, degree of change in
adversarial perturbation, average entropy of prediction qualities, and fraction
of adversarial examples that lead to successful attacks. We conduct extensive
experimental study on adversarial behavior in easy and hard attacks under deep
learning models with different hyperparameters and different deep learning
frameworks. We show that the same adversarial attack behaves differently under
different hyperparameters and across different frameworks due to the different
features learned under different deep learning model training process. Our
statistical characterization with strong empirical evidence provides a
transformative enlightenment on mitigation strategies towards effective
countermeasures against present and future adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wenqi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Ling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truex_S/0/1/0/all/0/1&quot;&gt;Stacey Truex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gursoy_M/0/1/0/all/0/1&quot;&gt;Mehmet Emre Gursoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00095">
<title>Probabilistic Bisection with Spatial Metamodels. (arXiv:1807.00095v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00095</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic Bisection Algorithm performs root finding based on knowledge
acquired from noisy oracle responses. We consider the generalized PBA setting
(G-PBA) where the statistical distribution of the oracle is unknown and
location-dependent, so that model inference and Bayesian knowledge updating
must be performed simultaneously. To this end, we propose to leverage the
spatial structure of a typical oracle by constructing a statistical surrogate
for the underlying logistic regression step. We investigate several
non-parametric surrogates, including Binomial Gaussian Processes (B-GP),
Polynomial, Kernel, and Spline Logistic Regression. In parallel, we develop
sampling policies that adaptively balance learning the oracle distribution and
learning the root. One of our proposals mimics active learning with B-GPs and
provides a novel look-ahead predictive variance formula. The resulting gains of
our Spatial PBA algorithm relative to earlier G-PBA models are illustrated with
synthetic examples and a challenging stochastic root finding problem from
Bermudan option pricing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Sergio Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ludkovski_M/0/1/0/all/0/1&quot;&gt;Mike Ludkovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00099">
<title>Title Generation for Web Tables. (arXiv:1807.00099v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.00099</link>
<description rdf:parseType="Literal">&lt;p&gt;Descriptive titles provide crucial context for interpreting tables that are
extracted from web pages and are a key component of table-based web
applications. Prior approaches have attempted to produce titles by selecting
existing text snippets associated with the table. These approaches, however,
are limited by their dependence on suitable titles existing a priori. In our
user study, we observe that the relevant information for the title tends to be
scattered across the page, and often---more than 80% of time---does not appear
verbatim anywhere in the page. We propose instead the application of a
sequence-to-sequence neural network model as a more generalizable means of
generating high-quality titles. This is accomplished by extracting many text
snippets that have potentially relevant information to the table, encoding them
into an input sequence, and using both copy and generation mechanisms in the
decoder to balance relevance and readability of the generated title. We
validate this approach with human evaluation on sample web tables and report
that while sequence models with only a copy mechanism or only a generation
mechanism are easily outperformed by simple selection-based baselines, the
model with both capabilities outperforms them all, approaching the quality of
crowdsourced titles while training on fewer than ten thousand examples. To the
best of our knowledge, the proposed technique is the first to consider
text-generation methods for table titles, and establishes a new state of the
art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hancock_B/0/1/0/all/0/1&quot;&gt;Braden Hancock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hongrae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00199">
<title>Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction. (arXiv:1807.00199v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00199</link>
<description rdf:parseType="Literal">&lt;p&gt;Recidivism prediction scores are used across the USA to determine sentencing
and supervision for hundreds of thousands of inmates. One such generator of
recidivism prediction scores is Northpointe&apos;s Correctional Offender Management
Profiling for Alternative Sanctions (COMPAS) score, used in states like
California and Florida, which past research has shown to be biased against
black inmates according to certain measures of fairness. To counteract this
racial bias, we present an adversarially-trained neural network that predicts
recidivism and is trained to remove racial bias. When comparing the results of
our model to COMPAS, we gain predictive accuracy and get closer to achieving
two out of three measures of fairness: parity and equality of odds. Our model
can be generalized to any prediction and demographic. This piece of research
contributes an example of scientific replication and simplification in a
high-stakes real-world application like recidivism prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadsworth_C/0/1/0/all/0/1&quot;&gt;Christina Wadsworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_F/0/1/0/all/0/1&quot;&gt;Francesca Vera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1&quot;&gt;Chris Piech&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00297">
<title>Exponential Convergence of the Deep Neural Network Approximation for Analytic Functions. (arXiv:1807.00297v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00297</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove that for analytic functions in low dimension, the convergence rate
of the deep neural network approximation is exponential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1&quot;&gt;Weinan E&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingcan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00400">
<title>Antithetic and Monte Carlo kernel estimators for partial rankings. (arXiv:1807.00400v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00400</link>
<description rdf:parseType="Literal">&lt;p&gt;In the modern age, rankings data is ubiquitous and it is useful for a variety
of applications such as recommender systems, multi-object tracking and
preference learning. However, most rankings data encountered in the real world
is incomplete, which forbids the direct application of existing modelling tools
for complete rankings. Our contribution is a novel way to extend kernel methods
for complete rankings to partial rankings, via consistent Monte Carlo
estimators of Gram matrices.
&lt;/p&gt;
&lt;p&gt;These Monte Carlo kernel estimators are based on extending kernel mean
embeddings to the embedding of a set of full rankings consistent with an
observed partial ranking. They form a computationally tractable alternative to
previous approaches for partial rankings data.
&lt;/p&gt;
&lt;p&gt;We also present a novel variance reduction scheme based on an antithetic
variate construction between permutations to obtain an improved estimator. An
overview of the existing kernels and metrics for permutations is also provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lomeli_M/0/1/0/all/0/1&quot;&gt;Maria Lomeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1&quot;&gt;Mark Rowland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00458">
<title>Adversarial Perturbations Against Real-Time Video Classification Systems. (arXiv:1807.00458v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00458</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has demonstrated the brittleness of machine learning systems
to adversarial perturbations. However, the studies have been mostly limited to
perturbations on images and more generally, classification that does not deal
with temporally varying inputs. In this paper we ask &quot;Are adversarial
perturbations possible in real-time video classification systems and if so,
what properties must they satisfy?&quot; Such systems find application in
surveillance applications, smart vehicles, and smart elderly care and thus,
misclassification could be particularly harmful (e.g., a mishap at an elderly
care facility may be missed). We show that accounting for temporal structure is
key to generating adversarial examples in such systems. We exploit recent
advances in generative adversarial network (GAN) architectures to account for
temporal correlations and generate adversarial samples that can cause
misclassification rates of over 80% for targeted activities. More importantly,
the samples also leave other activities largely unaffected making them
extremely stealthy. Finally, we also surprisingly find that in many scenarios,
the same perturbation can be applied to every frame in a video clip that makes
the adversary&apos;s ability to achieve misclassification relatively easy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shasha Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neupane_A/0/1/0/all/0/1&quot;&gt;Ajaya Neupane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Sujoy Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chengyu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1&quot;&gt;Srikanth V. Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1&quot;&gt;Amit K. Roy Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swami_A/0/1/0/all/0/1&quot;&gt;Ananthram Swami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00516">
<title>Balanced Distribution Adaptation for Transfer Learning. (arXiv:1807.00516v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00516</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning has achieved promising results by leveraging knowledge from
the source domain to annotate the target domain which has few or none labels.
Existing methods often seek to minimize the distribution divergence between
domains, such as the marginal distribution, the conditional distribution or
both. However, these two distances are often treated equally in existing
algorithms, which will result in poor performance in real applications.
Moreover, existing methods usually assume that the dataset is balanced, which
also limits their performances on imbalanced tasks that are quite common in
real problems. To tackle the distribution adaptation problem, in this paper, we
propose a novel transfer learning approach, named as Balanced Distribution
\underline{A}daptation~(BDA), which can adaptively leverage the importance of
the marginal and conditional distribution discrepancies, and several existing
methods can be treated as special cases of BDA. Based on BDA, we also propose a
novel Weighted Balanced Distribution Adaptation~(W-BDA) algorithm to tackle the
class imbalance issue in transfer learning. W-BDA not only considers the
distribution adaptation between domains but also adaptively changes the weight
of each class. To evaluate the proposed methods, we conduct extensive
experiments on several transfer learning tasks, which demonstrate the
effectiveness of our proposed algorithms over several state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1&quot;&gt;Shuji Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wenjie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00560">
<title>weight-importance sparse training in keyword spotting. (arXiv:1807.00560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00560</link>
<description rdf:parseType="Literal">&lt;p&gt;Large size models are implemented in recently ASR system to deal with complex
speech recognition problems. The num- ber of parameters in these models makes
them hard to deploy, especially on some resource-short devices such as car
tablet. Besides this, at most of time, ASR system is used to deal with
real-time problem such as keyword spotting (KWS). It is contradictory to the
fact that large model requires long com- putation time. To deal with this
problem, we apply some sparse algo- rithms to reduces number of parameters in
some widely used models, Deep Neural Network (DNN) KWS, which requires real
short computation time. We can prune more than 90 % even 95% of parameters in
the model with tiny effect decline. And the sparse model performs better than
baseline models which has same order number of parameters. Besides this, sparse
algorithm can lead us to find rational model size au- tomatically for certain
problem without concerning choosing an original model size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1&quot;&gt;Sihao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1&quot;&gt;Fan Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Min Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jue Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00734">
<title>The relativistic discriminator: a key element missing from standard GAN. (arXiv:1807.00734v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00734</link>
<description rdf:parseType="Literal">&lt;p&gt;In standard generative adversarial network (SGAN), the discriminator
estimates the probability that the input data is real. The generator is trained
to increase the probability that fake data is real. We argue that it should
also simultaneously decrease the probability that real data is real because 1)
this would account for a priori knowledge that half of the data in the
mini-batch is fake, 2) this would be observed with divergence minimization, and
3) in optimal settings, SGAN would be equivalent to integral probability metric
(IPM) GANs.
&lt;/p&gt;
&lt;p&gt;We show that this property can be induced by using a relativistic
discriminator which estimate the probability that the given real data is more
realistic than a randomly sampled fake data. We also present a variant in which
the discriminator estimate the probability that the given real data is more
realistic than fake data, on average. We generalize both approaches to
non-standard GAN loss functions and we refer to them respectively as
Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that
IPM-based GANs are a subset of RGANs which use the identity function.
&lt;/p&gt;
&lt;p&gt;Empirically, we observe that 1) RGANs and RaGANs are significantly more
stable and generate higher quality data samples than their non-relativistic
counterparts, 2) Standard RaGAN with gradient penalty generate data of better
quality than WGAN-GP while only requiring a single discriminator update per
generator update (reducing the time taken for reaching the state-of-the-art by
400%), and 3) RaGANs are able to generate plausible high resolutions images
(256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these
images are of significantly better quality than the ones generated by WGAN-GP
and SGAN with spectral normalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jolicoeur_Martineau_A/0/1/0/all/0/1&quot;&gt;Alexia Jolicoeur-Martineau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00745">
<title>Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data. (arXiv:1807.00745v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00745</link>
<description rdf:parseType="Literal">&lt;p&gt;Manually labeled corpora are expensive to create and often not available for
low-resource languages or domains. Automatic labeling approaches are an
alternative way to obtain labeled data in a quicker and cheaper way. However,
these labels often contain more errors which can deteriorate a classifier&apos;s
performance when trained on this data. We propose a noise layer that is added
to a neural network architecture. This allows modeling the noise and train on a
combination of clean and noisy data. We show that in a low-resource NER task we
can improve performance by up to 35% by using additional, noisy data and
handling the noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1&quot;&gt;Michael A. Hedderich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1&quot;&gt;Dietrich Klakow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00752">
<title>Waveform to Single Sinusoid Regression to Estimate the F0 Contour from Noisy Speech Using Recurrent Deep Neural Networks. (arXiv:1807.00752v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1807.00752</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental frequency (F0) represents pitch in speech that determines
prosodic characteristics of speech and is needed in various tasks for speech
analysis and synthesis. Despite decades of research on this topic, F0
estimation at low signal-to-noise ratios (SNRs) in unexpected noise conditions
remains difficult. This work proposes a new approach to noise robust F0
estimation using a recurrent neural network (RNN) trained in a supervised
manner. Recent studies employ deep neural networks (DNNs) for F0 tracking as a
frame-by-frame classification task into quantised frequency states but we
propose waveform-to-sinusoid regression instead to achieve both noise
robustness and accurate estimation with increased frequency resolution.
&lt;/p&gt;
&lt;p&gt;Experimental results with PTDB-TUG corpus contaminated by additive noise
(NOISEX-92) demonstrate that the proposed method improves gross pitch error
(GPE) rate and fine pitch error (FPE) by more than 35 % at SNRs between -10 dB
and +10 dB compared with well-known noise robust F0 tracker, PEFAC.
Furthermore, the proposed method also outperforms state-of-the-art DNN-based
approaches by more than 15 % in terms of both FPE and GPE rate over the
preceding SNR range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kato_A/0/1/0/all/0/1&quot;&gt;Akihiro Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1&quot;&gt;Tomi Kinnunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00260">
<title>Deep Asymmetric Multi-task Feature Learning. (arXiv:1708.00260v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00260</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can
learn deep representations shared across multiple tasks while effectively
preventing negative transfer that may happen in the feature sharing process.
Specifically, we introduce an asymmetric autoencoder term that allows reliable
predictors for the easy tasks to have high contribution to the feature learning
while suppressing the influences of unreliable predictors for more difficult
tasks. This allows the learning of less noisy representations, and enables
unreliable predictors to exploit knowledge from the reliable predictors via the
shared latent features. Such asymmetric knowledge transfer through shared
features is also more scalable and efficient than inter-task asymmetric
transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for
multitask learning and image classification, on which it significantly
outperforms existing symmetric and asymmetric multitask learning models, by
effectively preventing negative transfer in deep feature learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hae Beom Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;Eunho Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.00725">
<title>Learning hard quantum distributions with variational autoencoders. (arXiv:1710.00725v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1710.00725</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying general quantum many-body systems is one of the major challenges in
modern physics because it requires an amount of computational resources that
scales exponentially with the size of the system.Simulating the evolution of a
state, or even storing its description, rapidly becomes intractable for exact
classical algorithms. Recently, machine learning techniques, in the form of
restricted Boltzmann machines, have been proposed as a way to efficiently
represent certain quantum states with applications in state tomography and
ground state estimation. Here, we introduce a new representation of states
based on variational autoencoders. Variational autoencoders are a type of
generative model in the form of a neural network. We probe the power of this
representation by encoding probability distributions associated with states
from different classes. Our simulations show that deep networks give a better
representation for states that are hard to sample from, while providing no
benefit for random states. This suggests that the probability distributions
associated to hard quantum states might have a compositional structure that can
be exploited by layered neural networks. Specifically, we consider the
learnability of a class of quantum states introduced by Fefferman and Umans.
Such states are provably hard to sample for classical computers, but not for
quantum ones, under plausible computational complexity assumptions. The good
level of compression achieved for hard states suggests these methods can be
suitable for characterising states of the size expected in first generation
quantum hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rocchetto_A/0/1/0/all/0/1&quot;&gt;Andrea Rocchetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Grant_E/0/1/0/all/0/1&quot;&gt;Edward Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Strelchuk_S/0/1/0/all/0/1&quot;&gt;Sergii Strelchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1&quot;&gt;Giuseppe Carleo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Severini_S/0/1/0/all/0/1&quot;&gt;Simone Severini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01528">
<title>The Matrix Calculus You Need For Deep Learning. (arXiv:1802.01528v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01528</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is an attempt to explain all the matrix calculus you need in order
to understand the training of deep neural networks. We assume no math knowledge
beyond what you learned in calculus 1, and provide links to help you refresh
the necessary math where needed. Note that you do not need to understand this
material before you start learning to train and use deep learning in practice;
rather, this material is for those who are already familiar with the basics of
neural networks, and wish to deepen their understanding of the underlying math.
Don&apos;t worry if you get stuck at some point along the way---just go back and
reread the previous section, and try writing down and working through some
examples. And if you&apos;re still stuck, we&apos;re happy to answer your questions in
the Theory category at forums.fast.ai. Note: There is a reference section at
the end of the paper summarizing all the key matrix calculus rules and
terminology discussed here. See related articles at &lt;a href=&quot;http://explained.ai&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parr_T/0/1/0/all/0/1&quot;&gt;Terence Parr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_J/0/1/0/all/0/1&quot;&gt;Jeremy Howard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05965">
<title>MaxGain: Regularisation of Neural Networks by Constraining Activation Magnitudes. (arXiv:1804.05965v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05965</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective regularisation of neural networks is essential to combat
overfitting due to the large number of parameters involved. We present an
empirical analogue to the Lipschitz constant of a feed-forward neural network,
which we refer to as the maximum gain. We hypothesise that constraining the
gain of a network will have a regularising effect, similar to how constraining
the Lipschitz constant of a network has been shown to improve generalisation. A
simple algorithm is provided that involves rescaling the weight matrix of each
layer after each parameter update. We conduct a series of studies on common
benchmark datasets, and also a novel dataset that we introduce to enable easier
significance testing for experiments using convolutional networks. Performance
on these datasets compares favourably with other common regularisation
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gouk_H/0/1/0/all/0/1&quot;&gt;Henry Gouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pfahringer_B/0/1/0/all/0/1&quot;&gt;Bernhard Pfahringer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frank_E/0/1/0/all/0/1&quot;&gt;Eibe Frank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cree_M/0/1/0/all/0/1&quot;&gt;Michael Cree&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06370">
<title>Progress &amp; Compress: A scalable framework for continual learning. (arXiv:1805.06370v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06370</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a conceptually simple and scalable framework for continual
learning domains where tasks are learned sequentially. Our method is constant
in the number of parameters and is designed to preserve performance on
previously encountered tasks while accelerating learning progress on subsequent
problems. This is achieved by training a network with two components: A
knowledge base, capable of solving previously encountered problems, which is
connected to an active column that is employed to efficiently learn the current
task. After learning a new task, the active column is distilled into the
knowledge base, taking care to protect any previously acquired skills. This
cycle of active learning (progression) followed by consolidation (compression)
requires no architecture growth, no access to or storing of previous data or
tasks, and no task-specific parameters. We demonstrate the progress &amp;amp; compress
approach on sequential classification of handwritten alphabets as well as two
reinforcement learning domains: Atari games and 3D maze navigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schwarz_J/0/1/0/all/0/1&quot;&gt;Jonathan Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luketina_J/0/1/0/all/0/1&quot;&gt;Jelena Luketina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Czarnecki_W/0/1/0/all/0/1&quot;&gt;Wojciech M. Czarnecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grabska_Barwinska_A/0/1/0/all/0/1&quot;&gt;Agnieszka Grabska-Barwinska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hadsell_R/0/1/0/all/0/1&quot;&gt;Raia Hadsell&lt;/a&gt;</dc:creator>
</item></rdf:RDF>