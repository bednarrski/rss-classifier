<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01183"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01763"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01741"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02240"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1710.11160">
<title>Exponential improvements for quantum-accessible reinforcement learning. (arXiv:1710.11160v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11160</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum computers can offer dramatic improvements over classical devices for
data analysis tasks such as prediction and classification. However, less is
known about the advantages that quantum computers may bring in the setting of
reinforcement learning, where learning is achieved via interaction with a task
environment. Here, we consider a special case of reinforcement learning, where
the task environment allows quantum access. In addition, we impose certain
&quot;naturalness&quot; conditions on the task environment, which rule out the kinds of
oracle problems that are studied in quantum query complexity (and for which
quantum speedups are well-known). Within this framework of quantum-accessible
reinforcement learning environments, we demonstrate that quantum agents can
achieve exponential improvements in learning efficiency, surpassing previous
results that showed only quadratic improvements. A key step in the proof is to
construct task environments that encode well-known oracle problems, such as
Simon&apos;s problem and Recursive Fourier Sampling, while satisfying the above
&quot;naturalness&quot; conditions for reinforcement learning. Our results suggest that
quantum agents may perform well in certain game-playing scenarios, where the
game has recursive structure, and the agent can learn by playing against
itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1&quot;&gt;Vedran Dunjko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi-Kai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xingyao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1&quot;&gt;Jacob M. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06896">
<title>A Multi-task Selected Learning Approach for Solving New Type 3D Bin Packing Problem. (arXiv:1804.06896v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06896</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a new type of 3D bin packing problem (BPP), in which a
number of cuboid-shaped items must be put into a bin one by one orthogonally.
The objective is to find a way to place these items that can minimize the
surface area of the bin. This problem is based on the fact that there is no
fixed-sized bin in many real business scenarios and the cost of a bin is
proportional to its surface area. Based on previous research on 3D BPP, the
surface area is determined by the sequence, spatial locations and orientations
of items. It is a new NP-hard combinatorial optimization problem on
unfixed-sized bin packing, for which we propose a multi-task framework based on
Selected Learning, generating the sequence and orientations of items packed
into the bin simultaneously. During training steps, Selected Learning chooses
one of loss functions derived from Deep Reinforcement Learning and Supervised
Learning corresponding to the training procedure. Numerical results show that
the method proposed significantly outperforms Lego baselines by a substantial
gain of 7.52%. Moreover, we produce large scale 3D Bin Packing order data set
for studying bin packing problems and will release it to the research
community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haoyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1&quot;&gt;Lu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yu Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kenny Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaodong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinghui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jiangwen Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01183">
<title>Quantified Markov Logic Networks. (arXiv:1807.01183v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01183</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov Logic Networks (MLNs) are well-suited for expressing statistics such
as &quot;with high probability a smoker knows another smoker&quot; but not for expressing
statements such as &quot;there is a smoker who knows most other smokers&quot;, which is
necessary for modeling, e.g. influencers in social networks. To overcome this
shortcoming, we study quantified MLNs which generalize MLNs by introducing
statistical universal quantifiers, allowing to express also the latter type of
statistics in a principled way. Our main technical contribution is to show that
the standard reasoning tasks in quantified MLNs, maximum a posteriori and
marginal inference, can be reduced to their respective MLN counterparts in
polynomial time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_Basulto_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Guti&amp;#xe9;rrez-Basulto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jean Christoph Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzelka_O/0/1/0/all/0/1&quot;&gt;Ondrej Kuzelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01763">
<title>Seq2RDF: An end-to-end application for deriving Triples from Natural Language Text. (arXiv:1807.01763v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01763</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an end-to-end approach that takes unstructured textual input and
generates structured output compliant with a given vocabulary. Inspired by
recent successes in neural machine translation, we treat the triples within a
given knowledge graph as an independent graph language and propose an
encoder-decoder framework with an attention mechanism that leverages knowledge
graph embeddings. Our model learns the mapping from natural language text to
triple representation in the form of subject-predicate-object using the
selected knowledge graph vocabulary. Experiments on three different data sets
show that we achieve competitive F1-Measures over the baselines using our
simple yet effective approach. A demo video is included.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tongtao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1&quot;&gt;Deborah L. McGuinness&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01741">
<title>Logical Semantics and Commonsense Knowledge: Where Did we Go Wrong, and How to Go Forward, Again. (arXiv:1808.01741v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01741</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that logical semantics might have faltered due to its failure in
distinguishing between two fundamentally very different types of concepts:
ontological concepts, that should be types in a strongly-typed ontology, and
logical concepts, that are predicates corresponding to properties of and
relations between objects of various ontological types. We will then show that
accounting for these differences amounts to the integration of lexical and
compositional semantics in one coherent framework, and to an embedding in our
logical semantics of a strongly-typed ontology that reflects our commonsense
view of the world and the way we talk about it in ordinary language. We will
show that in such a framework a number of challenges in natural language
semantics can be adequately and systematically treated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1&quot;&gt;Walid S. Saba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10668">
<title>On the overfly algorithm in deep learning of neural networks. (arXiv:1807.10668v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10668</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we investigate the supervised backpropagation training of
multilayer neural networks from a dynamical systems point of view. We discuss
some links with the qualitative theory of differential equations and introduce
the overfly algorithm to tackle the local minima problem. Our approach is based
on the existence of first integrals of the generalised gradient system with
build-in dissipation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsygvintsev_A/0/1/0/all/0/1&quot;&gt;Alexei Tsygvintsev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02240">
<title>Speeding Up Distributed Gradient Descent by Utilizing Non-persistent Stragglers. (arXiv:1808.02240v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1808.02240</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed gradient descent (DGD) is an efficient way of implementing
gradient descent (GD), especially for large data sets, by dividing the
computation tasks into smaller subtasks and assigning to different computing
servers (CSs) to be executed in parallel. In standard parallel execution,
per-iteration waiting time is limited by the execution time of the straggling
servers. Coded DGD techniques have been introduced recently, which can tolerate
straggling servers via assigning redundant computation tasks to the CSs. In
most of the existing DGD schemes, either with coded computation or coded
communication, the non-straggling CSs transmit one message per iteration once
they complete all their assigned computation tasks. However, although the
straggling servers cannot complete all their assigned tasks, they are often
able to complete a certain portion of them. In this paper, we allow multiple
transmissions from each CS at each iteration in order to make sure a maximum
number of completed computations can be reported to the aggregating server
(AS), including the straggling servers. We numerically show that the average
completion time per iteration can be reduced significantly by slightly
increasing the communication load per server.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1&quot;&gt;Emre Ozfatura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1&quot;&gt;Sennur Ulukus&lt;/a&gt;</dc:creator>
</item></rdf:RDF>