<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09227"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09356"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09820"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09099"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09681"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09853"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09932"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10056"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07131"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09080"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09111"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09134"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09138"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09151"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09327"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09522"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09672"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10122"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.00670"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02548"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06677"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10116"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.09227">
<title>A General Dichotomy of Evolutionary Algorithms on Monotone Functions. (arXiv:1803.09227v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09227</link>
<description rdf:parseType="Literal">&lt;p&gt;It is known that the evolutionary algorithm $(1+1)$-EA with mutation rate
$c/n$ optimises every monotone function efficiently if $c&amp;lt;1$, and needs
exponential time on the some monotone functions (HotTopic functions) if $c\geq
2.2$. We study the same question for a large variety of algorithms,
particularly for $(1+\lambda)$-EA, $(\mu+1)$-EA, $(\mu+1)$-GA, their fast
counterparts like fast $(1+1)$-EA, and for $(1+(\lambda,\lambda))$-GA. We find
that all considered mutation-based algorithms show a similar dichotomy for
HotTopic functions, or even for all monotone functions. For the
$(1+(\lambda,\lambda))$-GA, this dichotomy is in the parameter $c\gamma$, which
is the expected number of bit flips in an individual after mutation and
crossover, neglecting selection. For the fast algorithms, the dichotomy is in
$m_2/m_1$, where $m_1$ and $m_2$ are the first and second falling moment of the
number of bit flips. Surprisingly, the range of efficient parameters is not
affected by either population size $\mu$ nor by the offspring population size
$\lambda$.
&lt;/p&gt;
&lt;p&gt;The picture changes completely if crossover is allowed. The genetic
algorithms $(\mu+1)$-GA and fast $(\mu+1)$-GA are efficient for arbitrary
mutations strengths if $\mu$ is large enough.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengler_J/0/1/0/all/0/1&quot;&gt;Johannes Lengler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09254">
<title>A theory of the phenomenology of Multipopulation Genetic Algorithm with an application to the Ising model. (arXiv:1803.09254v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09254</link>
<description rdf:parseType="Literal">&lt;p&gt;Genetic algorithm (GA) is a stochastic metaheuristic process consisting on
the evolution of a population of candidate solutions for a given optimization
problem. By extension, multipopulation genetic algorithm (MPGA) aims for
efficiency by evolving many populations, or islands, in parallel and performing
migrations between them periodically. The connectivity between islands
constrains the directions of migration and characterizes MPGA as a dynamic
process over a network. As such, predicting the evolution of the quality of the
solutions is a difficult challenge, implying in the waste of computer resources
and energy when the parameters are inadequate. By using models derived from
statistical mechanics, this work aims to estimate equations for the study of
dynamics in relation to the connectivity in MPGA. To illustrate the importance
of understanding MPGA, we show its application as an efficient alternative to
the thermalization phase of Metropolis-Hastings algorithm applied to the Ising
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resende_B/0/1/0/all/0/1&quot;&gt;Bruno Messias Farias de Resende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morais_B/0/1/0/all/0/1&quot;&gt;Bruno Well Dantas Morais&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09356">
<title>Neural Nets via Forward State Transformation and Backward Loss Transformation. (arXiv:1803.09356v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09356</link>
<description rdf:parseType="Literal">&lt;p&gt;This article studies (multilayer perceptron) neural networks with an emphasis
on the transformations involved --- both forward and backward --- in order to
develop a semantical/logical perspective that is in line with standard program
semantics. The common two-pass neural network training algorithms make this
viewpoint particularly fitting. In the forward direction, neural networks act
as state transformers. In the reverse direction, however, neural networks
change losses of outputs to losses of inputs, thereby acting like a
(real-valued) predicate transformer. In this way, backpropagation is functorial
by construction, as shown earlier in recent other work. We illustrate this
perspective by training a simple instance of a neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_B/0/1/0/all/0/1&quot;&gt;Bart Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprunger_D/0/1/0/all/0/1&quot;&gt;David Sprunger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09574">
<title>Long short-term memory and Learning-to-learn in networks of spiking neurons. (arXiv:1803.09574v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09574</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks of spiking neurons (SNNs) are frequently studied as models for
networks of neurons in the brain, but also as paradigm for novel energy
efficient computing hardware. In principle they are especially suitable for
computations in the temporal domain, such as speech processing, because their
computations are carried out via events in time and space. But so far they have
been lacking the capability to preserve information for longer time spans
during a computation, until it is updated or needed - like a register of a
digital computer. This function is provided to artificial neural networks
through Long Short-Term Memory (LSTM) units. We show here that SNNs attain
similar capabilities if one includes adapting neurons in the network.
Adaptation denotes an increase of the firing threshold of a neuron after
preceding firing. A substantial fraction of neurons in the neocortex of rodents
and humans has been found to be adapting. It turns out that if adapting neurons
are integrated in a suitable manner into the architecture of SNNs, the
performance of these enhanced SNNs, which we call LSNNs, for computation in the
temporal domain approaches that of artificial neural networks with LSTM-units.
In addition, the computing and learning capabilities of LSNNs can be
substantially enhanced through learning-to-learn (L2L) methods from machine
learning, that have so far been applied primarily to LSTM networks and
apparently never to SSNs.
&lt;/p&gt;
&lt;p&gt;This preliminary report on arXiv will be replaced by a more detailed version
in about a month.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1&quot;&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salaj_D/0/1/0/all/0/1&quot;&gt;Darjan Salaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramoney_A/0/1/0/all/0/1&quot;&gt;Anand Subramoney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legenstein_R/0/1/0/all/0/1&quot;&gt;Robert Legenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1&quot;&gt;Wolfgang Maass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09820">
<title>A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay. (arXiv:1803.09820v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09820</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep learning has produced dazzling successes for applications of
image, speech, and video processing in the past few years, most trainings are
with suboptimal hyper-parameters, requiring unnecessarily long training times.
Setting the hyper-parameters remains a black art that requires years of
experience to acquire. This report proposes several efficient ways to set the
hyper-parameters that significantly reduce training time and improves
performance. Specifically, this report shows how to examine the training
validation/test loss function for subtle clues of underfitting and overfitting
and suggests guidelines for moving toward the optimal balance point. Then it
discusses how to increase/decrease the learning rate/momentum to speed up
training. Our experiments show that it is crucial to balance every manner of
regularization for each dataset and architecture. Weight decay is used as a
sample regularizer to show how its optimal value is tightly coupled with the
learning rates and momentums.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1&quot;&gt;Leslie N. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08983">
<title>Automated Evaluation of Out-of-Context Errors. (arXiv:1803.08983v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.08983</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new approach to evaluate computational models for the task of
text understanding by the means of out-of-context error detection. Through the
novel design of our automated modification process, existing large-scale data
sources can be adopted for a vast number of text understanding tasks. The data
is thereby altered on a semantic level, allowing models to be tested against a
challenging set of modified text passages that require to comprise a broader
narrative discourse. Our newly introduced task targets actual real-world
problems of transcription and translation systems by inserting authentic
out-of-context errors. The automated modification process is applied to the
2016 TEDTalk corpus. Entirely automating the process allows the adoption of
complete datasets at low cost, facilitating supervised learning procedures and
deeper networks to be trained and tested. To evaluate the quality of the
modification algorithm a language model and a supervised binary classification
model are trained and tested on the altered dataset. A human baseline
evaluation is examined to compare the results with human performance. The
outcome of the evaluation task indicates the difficulty to detect semantic
errors for machine-learning algorithms and humans, showing that the errors
cannot be identified when limited to a single sentence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1&quot;&gt;Patrick Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1&quot;&gt;Jan Niehues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1&quot;&gt;Alex Waibel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09099">
<title>A Resourceful Reframing of Behavior Trees. (arXiv:1803.09099v1 [cs.PL])</title>
<link>http://arxiv.org/abs/1803.09099</link>
<description rdf:parseType="Literal">&lt;p&gt;Designers of autonomous agents, whether in physical or virtual environments,
need to express nondeterminisim, failure, and parallelism in behaviors, as well
as accounting for synchronous coordination between agents. Behavior Trees are a
semi-formalism deployed widely for this purpose in the games industry, but with
challenges to scalability, reasoning, and reuse of common sub-behaviors.
&lt;/p&gt;
&lt;p&gt;We present an alternative formulation of behavior trees through a language
design perspective, giving a formal operational semantics, type system, and
corresponding implementation. We express specifications for atomic behaviors as
linear logic formulas describing how they transform the environment, and our
type system uses linear sequent calculus to derive a compositional type
assignment to behavior tree expressions. These types expose the conditions
required for behaviors to succeed and allow abstraction over parameters to
behaviors, enabling the development of behavior &quot;building blocks&quot; amenable to
compositional reasoning and reuse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martens_C/0/1/0/all/0/1&quot;&gt;Chris Martens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butler_E/0/1/0/all/0/1&quot;&gt;Eric Butler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osborn_J/0/1/0/all/0/1&quot;&gt;Joseph C. Osborn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09156">
<title>An Overview of Vulnerabilities of Voice Controlled Systems. (arXiv:1803.09156v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1803.09156</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last few years, a rapidly increasing number of Internet-of-Things
(IoT) systems that adopt voice as the primary user input have emerged. These
systems have been shown to be vulnerable to various types of voice spoofing
attacks. However, how exactly these techniques differ or relate to each other
has not been extensively studied. In this paper, we provide a survey of recent
attack and defense techniques for voice controlled systems and propose a
classification of these techniques. We also discuss the need for a universal
defense strategy that protects a system from various types of attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poellabauer_C/0/1/0/all/0/1&quot;&gt;Christian Poellabauer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09211">
<title>Bernoulli Embeddings for Graphs. (arXiv:1803.09211v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09211</link>
<description rdf:parseType="Literal">&lt;p&gt;Just as semantic hashing can accelerate information retrieval, binary valued
embeddings can significantly reduce latency in the retrieval of graphical data.
We introduce a simple but effective model for learning such binary vectors for
nodes in a graph. By imagining the embeddings as independent coin flips of
varying bias, continuous optimization techniques can be applied to the
approximate expected loss. Embeddings optimized in this fashion consistently
outperform the quantization of both spectral graph embeddings and various
learned real-valued embeddings, on both ranking and pre-ranking tasks for a
variety of datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1&quot;&gt;Vinith Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1&quot;&gt;Sumit Bhatia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09535">
<title>Connectionist Recommendation in the Wild. (arXiv:1803.09535v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09535</link>
<description rdf:parseType="Literal">&lt;p&gt;The aggregate behaviors of users can collectively encode deep semantic
information about the objects with which they interact. In this paper, we
demonstrate novel ways in which the synthesis of these data can illuminate the
terrain of users&apos; environment and support them in their decision making and
wayfinding. A novel application of Recurrent Neural Networks and skip-gram
models, approaches popularized by their application to modeling language, are
brought to bear on student university enrollment sequences to create vector
representations of courses and map out traversals across them. We present
demonstrations of how scrutability from these neural networks can be gained and
how the combination of these techniques can be seen as an evolution of content
tagging and a means for a recommender to balance user preferences inferred from
data with those explicitly specified. From validation of the models to the
development of a UI, we discuss additional requisite functionality informed by
the results of a field study leading to the ultimate deployment of the system
at a university.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pardos_Z/0/1/0/all/0/1&quot;&gt;Zachary A. Pardos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zihao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Weijie Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09681">
<title>I/O Logic in HOL --- First Steps. (arXiv:1803.09681v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09681</link>
<description rdf:parseType="Literal">&lt;p&gt;A semantical embedding of input/output logic in classical higher-order logic
is presented. This embedding enables the mechanisation and automation of
reasoning tasks in input/output logic with off-the-shelf higher-order theorem
provers and proof assistants. The key idea for the solution presented here
results from the analysis of an inaccurate previous embedding attempt, which we
will discuss as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1&quot;&gt;Christoph Benzm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parent_X/0/1/0/all/0/1&quot;&gt;Xavier Parent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09702">
<title>HAMLET: Interpretable Human And Machine co-LEarning Technique. (arXiv:1803.09702v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09702</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient label acquisition processes are key to obtaining robust
classifiers. However, data labeling is often challenging and subject to high
levels of label noise. This can arise even when classification targets are well
defined, if instances to be labeled are more difficult than the prototypes used
to define the class, leading to disagreements among the expert community.
&lt;/p&gt;
&lt;p&gt;Here, we enable efficient training of deep neural networks. From
low-confidence labels, we iteratively improve their quality by simultaneous
learning of machines and experts. We call it Human And Machine co-LEarning
Technique (HAMLET). Throughout the process, experts become more consistent,
while the algorithm provides them with explainable feedback for confirmation.
HAMLET uses a neural embedding function and a memory module filled with diverse
reference embeddings from different classes. Its output includes classification
labels and highly relevant reference embeddings as explanation.
&lt;/p&gt;
&lt;p&gt;We took the study of brain monitoring at intensive care unit (ICU) as an
application of HAMLET on continuous electroencephalography (cEEG) data.
Although cEEG monitoring yields large volumes of data, labeling costs and
difficulty make it hard to build a classifier. Additionally, while experts
agree on the labels of clear-cut examples of cEEG patterns, labeling many
real-world cEEG data can be extremely challenging. Thus, a large minority of
sequences might be mislabeled. HAMLET has shown significant performance gain
against deep learning and other baselines, increasing accuracy from 7.03% to
68.75% on challenging inputs. Besides improved performance, clinical experts
confirmed the interpretability of those reference embeddings in helping
explaining the classification results by HAMLET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deiss_O/0/1/0/all/0/1&quot;&gt;Olivier Deiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswal_S/0/1/0/all/0/1&quot;&gt;Siddharth Biswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haoqi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westover_M/0/1/0/all/0/1&quot;&gt;M. Brandon Westover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09789">
<title>On Chatbots Exhibiting Goal-Directed Autonomy in Dynamic Environments. (arXiv:1803.09789v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09789</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversation interfaces (CIs), or chatbots, are a popular form of intelligent
agents that engage humans in task-oriented or informal conversation. In this
position paper and demonstration, we argue that chatbots working in dynamic
environments, like with sensor data, can not only serve as a promising platform
to research issues at the intersection of learning, reasoning, representation
and execution for goal-directed autonomy; but also handle non-trivial business
applications. We explore the underlying issues in the context of Water Advisor,
a preliminary multi-modal conversation system that can access and explain water
quality data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Biplav Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09853">
<title>Generative Design in Minecraft (GDMC), Settlement Generation Competition. (arXiv:1803.09853v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09853</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the settlement generation competition for Minecraft,
the first part of the Generative Design in Minecraft challenge. The settlement
generation competition is about creating Artificial Intelligence (AI) agents
that can produce functional, aesthetically appealing and believable settlements
adapted to a given Minecraft map - ideally at a level that can compete with
human created designs. The aim of the competition is to advance procedural
content generation for games, especially in overcoming the challenges of
adaptive and holistic PCG. The paper introduces the technical details of the
challenge, but mostly focuses on what challenges this competition provides and
why they are scientifically relevant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salge_C/0/1/0/all/0/1&quot;&gt;Christoph Salge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1&quot;&gt;Michael Cerny Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canaan_R/0/1/0/all/0/1&quot;&gt;Rodrigo Canaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09932">
<title>Image Semantic Transformation: Faster, Lighter and Stronger. (arXiv:1803.09932v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.09932</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Image-Semantic-Transformation-Reconstruction-Circle(ISTRC) model,
a novel and powerful method using facenet&apos;s Euclidean latent space to
understand the images. As the name suggests, ISTRC construct the circle, able
to perfectly reconstruct images. One powerful Euclidean latent space embedded
in ISTRC is FaceNet&apos;s last layer with the power of distinguishing and
understanding images. Our model will reconstruct the images and manipulate
Euclidean latent vectors to achieve semantic transformations and semantic
images arthimetic calculations. In this paper, we show that ISTRC performs 10
high-level semantic transformations like &quot;Male and female&quot;,&quot;add smile&quot;,&quot;open
mouth&quot;, &quot;deduct beard or add mustache&quot;, &quot;bigger/smaller nose&quot;, &quot;make older and
younger&quot;, &quot;bigger lips&quot;, &quot;bigger eyes&quot;, &quot;bigger/smaller mouths&quot; and &quot;more
attractive&quot;. It just takes 3 hours(GTX 1080) to train the models of 10 semantic
transformations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dasong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianbo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10056">
<title>Automated Speed and Lane Change Decision Making using Deep Reinforcement Learning. (arXiv:1803.10056v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.10056</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a method, based on deep reinforcement learning, for
automatically generating a general purpose decision making function. A Deep
Q-Network agent was trained in a simulated environment to handle speed and lane
change decisions for a truck-trailer combination. In a highway driving case, it
is shown that the method produced an agent that matched or surpassed the
performance of a commonly used reference model. To demonstrate the generality
of the method, the exact same algorithm was also tested by training it for an
overtaking case on a road with oncoming traffic. Furthermore, a novel way of
applying a convolutional neural network to high level input that represents
interchangeable objects is also introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoel_C/0/1/0/all/0/1&quot;&gt;Carl-Johan Hoel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolff_K/0/1/0/all/0/1&quot;&gt;Krister Wolff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laine_L/0/1/0/all/0/1&quot;&gt;Leo Laine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07131">
<title>CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise. (arXiv:1711.07131v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07131</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of learning image classification models
with label noise. Existing approaches depending on human supervision are
generally not scalable as manually identifying correct or incorrect labels is
time-consuming, whereas approaches not relying on human supervision are
scalable but less effective. To reduce the amount of human supervision for
label noise cleaning, we introduce CleanNet, a joint neural embedding network,
which only requires a fraction of the classes being manually verified to
provide the knowledge of label noise that can be transferred to other classes.
We further integrate CleanNet and conventional convolutional neural network
classifier into one framework for image classification learning. We demonstrate
the effectiveness of the proposed algorithm on both of the label noise
detection task and the image classification on noisy data task on several
large-scale datasets. Experimental results show that CleanNet can reduce label
noise detection error rate on held-out classes where no human supervision
available by 41.5% compared to current weakly supervised methods. It also
achieves 47% of the performance gain of verifying all images with only 3.2%
images verified on an image classification task. Source code and dataset will
be available at kuanghuei.github.io/CleanNetProject.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-Huei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaodong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linjun Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03781">
<title>NestedNet: Learning Nested Sparse Structures in Deep Neural Networks. (arXiv:1712.03781v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03781</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there have been increasing demands to construct compact deep
architectures to remove unnecessary redundancy and to improve the inference
speed. While many recent works focus on reducing the redundancy by eliminating
unneeded weight parameters, it is not possible to apply a single deep
architecture for multiple devices with different resources. When a new device
or circumstantial condition requires a new deep architecture, it is necessary
to construct and train a new network from scratch. In this work, we propose a
novel deep learning framework, called a nested sparse network, which exploits
an n-in-1-type nested structure in a neural network. A nested sparse network
consists of multiple levels of networks with a different sparsity ratio
associated with each level, and higher level networks share parameters with
lower level networks to enable stable nested learning. The proposed framework
realizes a resource-aware versatile architecture as the same network can meet
diverse resource requirements. Moreover, the proposed nested network can learn
different forms of knowledge in its internal networks at different levels,
enabling multiple tasks using a single network, such as coarse-to-fine
hierarchical classification. In order to train the proposed nested sparse
network, we propose efficient weight connection learning and channel and layer
scheduling strategies. We evaluate our network in multiple tasks, including
adaptive deep compression, knowledge distillation, and learning class
hierarchy, and demonstrate that nested sparse networks perform competitively,
but more efficiently, compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Eunwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_C/0/1/0/all/0/1&quot;&gt;Chanho Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Songhwai Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05044">
<title>Learning to Explore with Meta-Policy Gradient. (arXiv:1803.05044v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05044</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of off-policy learning, including deep Q-learning and deep
deterministic policy gradient (DDPG), critically depends on the choice of the
exploration policy. Existing exploration methods are mostly based on adding
noise to the on-going actor policy and can only explore \emph{local} regions
close to what the actor policy dictates. In this work, we develop a simple
meta-policy gradient algorithm that allows us to adaptively learn the
exploration policy in DDPG. Our algorithm allows us to train flexible
exploration behaviors that are independent of the actor policy, yielding a
\emph{global exploration} that significantly speeds up the learning process.
With an extensive study, we show that our method significantly improves the
sample-efficiency of DDPG on a variety of reinforcement learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianbing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jian Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08037">
<title>Similar Elements and Metric Labeling on Complete Graphs. (arXiv:1803.08037v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08037</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a problem that involves finding similar elements in a collection
of sets. The problem is motivated by applications in machine learning and
pattern recognition. We formulate the similar elements problem as an
optimization and give an efficient approximation algorithm that finds a
solution within a factor of 2 of the optimal. The similar elements problem is a
special case of the metric labeling problem and we also give an efficient
2-approximation algorithm for the metric labeling problem on complete graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felzenszwalb_P/0/1/0/all/0/1&quot;&gt;Pedro F. Felzenszwalb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08978">
<title>Broad Learning for Healthcare. (arXiv:1803.08978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.08978</link>
<description rdf:parseType="Literal">&lt;p&gt;A broad spectrum of data from different modalities are generated in the
healthcare domain every day, including scalar data (e.g., clinical measures
collected at hospitals), tensor data (e.g., neuroimages analyzed by research
institutes), graph data (e.g., brain connectivity networks), and sequence data
(e.g., digital footprints recorded on smart sensors). Capability for modeling
information from these heterogeneous data sources is potentially transformative
for investigating disease mechanisms and for informing therapeutic
interventions.
&lt;/p&gt;
&lt;p&gt;Our works in this thesis attempt to facilitate healthcare applications in the
setting of broad learning which focuses on fusing heterogeneous data sources
for a variety of synergistic knowledge discovery and machine learning tasks. We
are generally interested in computer-aided diagnosis, precision medicine, and
mobile health by creating accurate user profiles which include important
biomarkers, brain connectivity patterns, and latent representations. In
particular, our works involve four different data mining problems with
application to the healthcare domain: multi-view feature selection, subgraph
pattern mining, brain network embedding, and multi-view sequence prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bokai Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08996">
<title>Pattern Analysis with Layered Self-Organizing Maps. (arXiv:1803.08996v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.08996</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper defines a new learning architecture, Layered Self-Organizing Maps
(LSOMs), that uses the SOM and supervised-SOM learning algorithms. The
architecture is validated with the MNIST database of hand-written digit images.
LSOMs are similar to convolutional neural nets (covnets) in the way they sample
data, but different in the way they represent features and learn. LSOMs analyze
(or generate) image patches with maps of exemplars determined by the SOM
learning algorithm rather than feature maps from filter-banks learned via
backprop.
&lt;/p&gt;
&lt;p&gt;LSOMs provide an alternative to features derived from covnets. Multi-layer
LSOMs are trained bottom-up, without the use of backprop and therefore may be
of interest as a model of the visual cortex. The results show organization at
multiple levels. The algorithm appears to be resource efficient in learning,
classifying and generating images. Although LSOMs can be used for
classification, their validation accuracy for these exploratory runs was well
below the state of the art. The goal of this article is to define the
architecture and display the structures resulting from its application to the
MNIST images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedlander_D/0/1/0/all/0/1&quot;&gt;David Friedlander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09050">
<title>Learning to Reweight Examples for Robust Deep Learning. (arXiv:1803.09050v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09050</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have been shown to be very powerful modeling tools for
many supervised learning tasks involving complex input patterns. However, they
can also easily overfit to training set biases and label noises. In addition to
various regularizers, example reweighting algorithms are popular solutions to
these problems, but they require careful tuning of additional hyperparameters,
such as example mining schedules and regularization hyperparameters. In
contrast to past reweighting methods, which typically consist of functions of
the cost value of each example, in this work we propose a novel meta-learning
algorithm that learns to assign weights to training examples based on their
gradient directions. To determine the example weights, our method performs a
meta gradient descent step on the current mini-batch example weights (which are
initialized from zero) to minimize the loss on a clean unbiased validation set.
Our proposed method can be easily implemented on any type of deep network, does
not require any additional hyperparameter tuning, and achieves impressive
performance on class imbalance and corrupted label problems where only a small
amount of clean validation data is available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengye Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenyuan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09080">
<title>AAANE: Attention-based Adversarial Autoencoder for Multi-scale Network Embedding. (arXiv:1803.09080v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09080</link>
<description rdf:parseType="Literal">&lt;p&gt;Network embedding represents nodes in a continuous vector space and preserves
structure information from the Network. Existing methods usually adopt a
&quot;one-size-fits-all&quot; approach when concerning multi-scale structure information,
such as first- and second-order proximity of nodes, ignoring the fact that
different scales play different roles in the embedding learning. In this paper,
we propose an Attention-based Adversarial Autoencoder Network Embedding(AAANE)
framework, which promotes the collaboration of different scales and lets them
vote for robust representations. The proposed AAANE consists of two components:
1) Attention-based autoencoder effectively capture the highly non-linear
network structure, which can de-emphasize irrelevant scales during training. 2)
An adversarial regularization guides the autoencoder learn robust
representations by matching the posterior distribution of the latent embeddings
to given prior distribution. This is the first attempt to introduce attention
mechanisms to multi-scale network embedding. Experimental results on real-world
networks show that our learned attention parameters are different for every
network and the proposed approach outperforms existing state-of-the-art
approaches for network embedding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_L/0/1/0/all/0/1&quot;&gt;Lei Sang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shengsheng Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xindong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09111">
<title>Learning architectures based on quantum entanglement: a simple matrix product state algorithm for image recognition. (arXiv:1803.09111v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09111</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a fundamental, but still elusive question whether methods based on
quantum mechanics, in particular on quantum entanglement, can be used for
classical information processing and machine learning. Even partial answer to
this question would bring important insights to both fields of both machine
learning and quantum mechanics. In this work, we implement simple numerical
experiments, related to pattern/images classification, in which we represent
the classifiers by quantum matrix product states (MPS). Classical machine
learning algorithm is then applied to these quantum states. We explicitly show
how quantum features (i.e., single-site and bipartite entanglement) can emerge
in such represented images; entanglement characterizes here the importance of
data, and this information can be practically used to improve the learning
procedures. Thanks to the low demands on the dimensions and number of the
unitary matrices, necessary to construct the MPS, we expect such numerical
experiments could open new paths in classical machine learning, and shed at
same time lights on generic quantum simulations/computations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuhan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lewenstein_M/0/1/0/all/0/1&quot;&gt;Maciej Lewenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ran_S/0/1/0/all/0/1&quot;&gt;Shi-Ju Ran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09123">
<title>Equation Embeddings. (arXiv:1803.09123v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09123</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an unsupervised approach for discovering semantic representations
of mathematical equations. Equations are challenging to analyze because each is
unique, or nearly unique. Our method, which we call equation embeddings, finds
good representations of equations by using the representations of their
surrounding words. We used equation embeddings to analyze four collections of
scientific articles from the arXiv, covering four computer science domains
(NLP, IR, AI, and ML) and $\sim$98.5k equations. Quantitatively, we found that
equation embeddings provide better models when compared to existing word
embedding approaches. Qualitatively, we found that equation embeddings provide
coherent semantic representations of equations and can capture semantic
similarity to other equations and to words.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krstovski_K/0/1/0/all/0/1&quot;&gt;Kriste Krstovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09134">
<title>Characterizing Diseases and disorders in Gay Users&apos; tweets. (arXiv:1803.09134v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1803.09134</link>
<description rdf:parseType="Literal">&lt;p&gt;A lack of information exists about the health issues of lesbian, gay,
bisexual, transgender, and queer (LGBTQ) people who are often excluded from
national demographic assessments, health studies, and clinical trials. As a
result, medical experts and researchers lack a holistic understanding of the
health disparities facing these populations. Fortunately, publicly available
social media data such as Twitter data can be utilized to support the decisions
of public health policy makers and managers with respect to LGBTQ people. This
research employs a computational approach to collect tweets from gay users on
health-related topics and model these topics. To determine the nature of
health-related information shared by men who have sex with men on Twitter, we
collected thousands of tweets from 177 active users. We sampled these tweets
using a framework that can be applied to other LGBTQ sub-populations in future
research. We found 11 diseases in 7 categories based on ICD 10 that are in line
with the published studies and official reports.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_F/0/1/0/all/0/1&quot;&gt;Frank Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1&quot;&gt;Amir Karami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitzie_V/0/1/0/all/0/1&quot;&gt;Vanessa Kitzie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09138">
<title>Posterior Concentration for Sparse Deep Learning. (arXiv:1803.09138v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09138</link>
<description rdf:parseType="Literal">&lt;p&gt;Spike-and-Slab Deep Learning (SS-DL) is a fully Bayesian alternative to
Dropout for improving generalizability of deep ReLU networks. This new type of
regularization enables provable recovery of smooth input-output maps with
unknown levels of smoothness. Indeed, we show that the posterior distribution
concentrates at the near minimax rate for $\alpha$-H\&quot;older smooth maps,
performing as well as if we knew the smoothness level $\alpha$ ahead of time.
Our result sheds light on architecture design for deep neural networks, namely
the choice of depth, width and sparsity level. These network attributes
typically depend on unknown smoothness in order to be optimal. We obviate this
constraint with the fully Bayes construction. As an aside, we show that SS-DL
does not overfit in the sense that the posterior concentrates on smaller
networks with fewer (up to the optimal number of) nodes and links. Our results
provide new theoretical justifications for deep ReLU networks from a Bayesian
point of view.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polson_N/0/1/0/all/0/1&quot;&gt;Nicholas Polson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rockova_V/0/1/0/all/0/1&quot;&gt;Veronika Rockova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09151">
<title>Natural Gradients in Practice: Non-Conjugate Variational Inference in Gaussian Process Models. (arXiv:1803.09151v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09151</link>
<description rdf:parseType="Literal">&lt;p&gt;The natural gradient method has been used effectively in conjugate Gaussian
process models, but the non-conjugate case has been largely unexplored. We
examine how natural gradients can be used in non-conjugate stochastic settings,
together with hyperparameter learning. We conclude that the natural gradient
can significantly improve performance in terms of wall-clock time. For
ill-conditioned posteriors the benefit of the natural gradient method is
especially pronounced, and we demonstrate a practical setting where ordinary
gradients are unusable. We show how natural gradients can be computed
efficiently and automatically in any parameterization, using automatic
differentiation. Our code is integrated into the GPflow package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salimbeni_H/0/1/0/all/0/1&quot;&gt;Hugh Salimbeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eleftheriadis_S/0/1/0/all/0/1&quot;&gt;Stefanos Eleftheriadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hensman_J/0/1/0/all/0/1&quot;&gt;James Hensman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09180">
<title>Unsupervised Domain Adaptation: from Simulation Engine to the RealWorld. (arXiv:1803.09180v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.09180</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale labeled training datasets have enabled deep neural networks to
excel on a wide range of benchmark vision tasks. However, in many applications
it is prohibitively expensive or time-consuming to obtain large quantities of
labeled data. To cope with limited labeled training data, many have attempted
to directly apply models trained on a large-scale labeled source domain to
another sparsely labeled target domain. Unfortunately, direct transfer across
domains often performs poorly due to domain shift and dataset bias. Domain
adaptation is the machine learning paradigm that aims to learn a model from a
source domain that can perform well on a different (but related) target domain.
In this paper, we summarize and compare the latest unsupervised domain
adaptation methods in computer vision applications. We classify the non-deep
approaches into sample re-weighting and intermediate subspace transformation
categories, while the deep strategy includes discrepancy-based methods,
adversarial generative models, adversarial discriminative models and
reconstruction-based methods. We also discuss some potential directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1&quot;&gt;Sanjit A. Seshia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09327">
<title>Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization. (arXiv:1803.09327v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09327</link>
<description rdf:parseType="Literal">&lt;p&gt;Vanishing and exploding gradients are two of the main obstacles in training
deep neural networks, especially in capturing long range dependencies in
recurrent neural networks~(RNNs). In this paper, we present an efficient
parametrization of the transition matrix of an RNN that allows us to stabilize
the gradients that arise in its training. Specifically, we parameterize the
transition matrix by its singular value decomposition(SVD), which allows us to
explicitly track and control its singular values. We attain efficiency by using
tools that are common in numerical linear algebra, namely Householder
reflectors for representing the orthogonal matrices that arise in the SVD. By
explicitly controlling the singular values, our proposed Spectral-RNN method
allows us to easily solve the exploding gradient problem and we observe that it
empirically solves the vanishing gradient issue to a large extent. We note that
the SVD parameterization can be used for any rectangular weight matrix, hence
it can be easily extended to any deep neural network, such as a multi-layer
perceptron. Theoretically, we demonstrate that our parameterization does not
lose any expressive power, and show how it controls generalization of RNN for
the classification task. %, and show how it potentially makes the optimization
process easier. Our extensive experimental results also demonstrate that the
proposed framework converges faster, and has good generalization, especially in
capturing long range dependencies, as shown on the synthetic addition and copy
tasks, as well as on MNIST and Penn Tree Bank data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1&quot;&gt;Qi Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09383">
<title>On the Performance of Preconditioned Stochastic Gradient Descent. (arXiv:1803.09383v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09383</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the performance of preconditioned stochastic gradient
descent (PSGD), which can be regarded as an enhance stochastic Newton method
with the ability to handle gradient noise and non-convexity at the same time.
We have improved the implementation of PSGD, unrevealed its relationship to
equilibrated stochastic gradient descent (ESGD) and batch normalization, and
provided a software package (https://github.com/lixilinx/psgd_tf) implemented
in Tensorflow to compare variations of PSGD and stochastic gradient descent
(SGD) on a wide range of benchmark problems with commonly used neural network
models, e.g., convolutional and recurrent neural networks. Comparison results
clearly demonstrate the advantages of PSGD in terms of convergence speeds and
generalization performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi-Lin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09386">
<title>A Systematic Comparison of Deep Learning Architectures in an Autonomous Vehicle. (arXiv:1803.09386v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09386</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-driving technology is advancing rapidly, largely due to recent
developments in deep learning algorithms. To date, however, there has been no
systematic comparison of how different deep learning architectures perform at
such tasks, or an attempt to determine a correlation between classification
performance and performance in an actual vehicle. Here, we introduce the first
controlled comparison of seven contemporary deep-learning architectures in an
end-to-end autonomous driving task. We use a simple and affordable platform
consisting of of an off-the-shelf, remotely operated vehicle, a GPU equipped
computer and an indoor foam-rubber racetrack. We compare a fully-connected
network, a 2-layer CNN, AlexNet, VGG-16, Inception-V3, ResNet-26, and LSTM and
report the number of laps they are able to successfully complete without
crashing while traversing an indoor racetrack under identical testing
conditions. Based on these tests, AlexNet completed the most laps without
crashing out of all networks, and ResNet-26 is the most &apos;efficient&apos;
architecture examined, with respect to the number of laps completed relative to
the number of parameters. We also observe whether spatial, color, or temporal
features - or some combination - are more important for such tasks. Finally, we
show that validation loss/accuracy is not sufficiently indicative of the
model&apos;s performance even when employed in a real vehicle with a simple task,
emphasizing the need for greater accessibility to research platforms within the
self-driving community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teti_M/0/1/0/all/0/1&quot;&gt;Michael Teti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barenholtz_E/0/1/0/all/0/1&quot;&gt;Elan Barenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_S/0/1/0/all/0/1&quot;&gt;Shawn Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_W/0/1/0/all/0/1&quot;&gt;William Hahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09522">
<title>A Provably Correct Algorithm for Deep Learning that Actually Works. (arXiv:1803.09522v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09522</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a layer-by-layer algorithm for training deep convolutional
networks, where each step involves gradient updates for a two layer network
followed by a simple clustering algorithm. Our algorithm stems from a deep
generative model that generates mages level by level, where lower resolution
images correspond to latent semantic classes. We analyze the convergence rate
of our algorithm assuming that the data is indeed generated according to this
model (as well as additional assumptions). While we do not pretend to claim
that the assumptions are realistic for natural images, we do believe that they
capture some true properties of real data. Furthermore, we show that our
algorithm actually works in practice (on the CIFAR dataset), achieving results
in the same ballpark as that of vanilla convolutional neural networks that are
being trained by stochastic gradient descent. Finally, our proof techniques may
be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1&quot;&gt;Eran Malach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1&quot;&gt;Shai Shalev-Shwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09672">
<title>On the Intrinsic Dimensionality of Face Representation. (arXiv:1803.09672v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.09672</link>
<description rdf:parseType="Literal">&lt;p&gt;The two underlying factors that determine the efficacy of face
representations are, the embedding function to represent a face image and the
dimensionality of the representation, e.g. the number of features. While the
design of the embedding function has been well studied, relatively little is
known about the compactness of such representations. For instance, what is the
minimal number of degrees of freedom or intrinsic dimensionality of a given
face representation? Can we find a mapping from the ambient representation to
this minimal intrinsic space that retains it&apos;s full utility? This paper
addresses both of these questions. Given a face representation, (1) we leverage
intrinsic geodesic distances induced by a neighborhood graph to empirically
estimate it&apos;s intrinsic dimensionality, (2) develop a neural network based
non-linear mapping that transforms the ambient representation to the minimal
intrinsic space of that dimensionality, and (3) validate the veracity of the
mapping through face matching in the intrinsic space. Experiments on benchmark
face datasets (LFW, IJB-A, IJB-B, PCSO and CASIA) indicate that, (1) the
intrinsic dimensionality of deep neural network representation is significantly
lower than the dimensionality of the ambient features. For instance, Facenet&apos;s
128-d representation has an intrinsic dimensionality in the range of 9-12, and
(2) the neural network based mapping is able to provide face representations of
significantly lower dimensionality while being as discriminative (TAR @ 0.1%
FAR of 84.67%, 90.40% at 10 and 20 dimensions, respectively vs 95.50% at 128
ambient dimension on the LFW dataset) as the corresponding ambient
representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Sixue Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1&quot;&gt;Vishnu Naresh Boddeti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anil K. Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09733">
<title>Convolutional Attribute Embedding and Cross-Domain Representations for Domain Transfer Learning. (arXiv:1803.09733v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09733</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of transfer learning with the attribute
data. In the transfer learning problem, we want to leverage the data of the
auxiliary and the target domains to build an effective model for the
classification problem in the target domain. Meanwhile, the attributes are
naturally stable cross different domains. This strongly motives us to learn
effective domain transfer attribute representations. To this end, we proposed
to embed the attributes of the data to a common space by using the powerful
convolutional neural network (CNN) model. The convolutional representations of
the data points are mapped to the corresponding attributes so that they can be
effective embedding of the attributes. We also represent the data of different
domains by a domain-independent CNN, ant a domain-specific CNN, and combine
their outputs with the attribute embedding to build the classification model.
An joint learning framework is constructed to minimize the classification
errors, the attribute mapping error, the mismatching of the domain-independent
representations cross different domains, and to encourage the the neighborhood
smoothness of representations in the target domain. The minimization problem is
solved by an iterative algorithm based on gradient descent. Experiments over
benchmark data sets of person re-identification, bankruptcy prediction, and
spam email detection, show the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1&quot;&gt;Fang Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10016">
<title>Cross-validation in high-dimensional spaces: a lifeline for least-squares models and multi-class LDA. (arXiv:1803.10016v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10016</link>
<description rdf:parseType="Literal">&lt;p&gt;Least-squares models such as linear regression and Linear Discriminant
Analysis (LDA) are amongst the most popular statistical learning techniques.
However, since their computation time increases cubically with the number of
features, they are inefficient in high-dimensional neuroimaging datasets.
Fortunately, for k-fold cross-validation, an analytical approach has been
developed that yields the exact cross-validated predictions in least-squares
models without explicitly training the model. Its computation time grows with
the number of test samples. Here, this approach is systematically investigated
in the context of cross-validation and permutation testing. LDA is used
exemplarily but results hold for all other least-squares methods. Furthermore,
a non-trivial extension to multi-class LDA is formally derived. The analytical
approach is evaluated using complexity calculations, simulations, and
permutation testing of an EEG/MEG dataset. Depending on the ratio between
features and samples, the analytical approach is up to 10,000x faster than the
standard approach (retraining the model on each training set). This allows for
a fast cross-validation of least-squares models and multi-class LDA in
high-dimensional data, with obvious applications in multi-dimensional datasets,
Representational Similarity Analysis, and permutation testing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treder_M/0/1/0/all/0/1&quot;&gt;Matthias S. Treder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10082">
<title>Efficient parametrization of multi-domain deep neural networks. (arXiv:1803.10082v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.10082</link>
<description rdf:parseType="Literal">&lt;p&gt;A practical limitation of deep neural networks is their high degree of
specialization to a single task and visual domain. Recently, inspired by the
successes of transfer learning, several authors have proposed to learn instead
universal, fixed feature extractors that, used as the first stage of any deep
network, work well for several tasks and domains simultaneously. Nevertheless,
such universal features are still somewhat inferior to specialized networks.
&lt;/p&gt;
&lt;p&gt;To overcome this limitation, in this paper we propose to consider instead
universal parametric families of neural networks, which still contain
specialized problem-specific models, but differing only by a small number of
parameters. We study different designs for such parametrizations, including
series and parallel residual adapters, joint adapter compression, and parameter
allocations, and empirically identify the ones that yield the highest
compression. We show that, in order to maximize performance, it is necessary to
adapt both shallow and deep layers of a deep network, but the required changes
are very small. We also show that these universal parametrization are very
effective for transfer learning, where they outperform traditional fine-tuning
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebuffi_S/0/1/0/all/0/1&quot;&gt;Sylvestre-Alvise Rebuffi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1&quot;&gt;Hakan Bilen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10122">
<title>World Models. (arXiv:1803.10122v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10122</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore building generative neural network models of popular reinforcement
learning environments. Our world model can be trained quickly in an
unsupervised manner to learn a compressed spatial and temporal representation
of the environment. By using features extracted from the world model as inputs
to an agent, we can train a very compact and simple policy that can solve the
required task. We can even train our agent entirely inside of its own
hallucinated dream generated by its world model, and transfer this policy back
into the actual environment.
&lt;/p&gt;
&lt;p&gt;An interactive version of this paper is available at
$\href{https://worldmodels.github.io/}{\mathtt{https://worldmodels.github.io}}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1&quot;&gt;David Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10123">
<title>Bayesian Gradient Descent: Online Variational Bayes Learning with Increased Robustness to Catastrophic Forgetting and Weight Pruning. (arXiv:1803.10123v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest a novel approach for the estimation of the posterior distribution
of the weights of a neural network, using an online version of the variational
Bayes method. Having a confidence measure of the weights allows to combat
several shortcomings of neural networks, such as their parameter redundancy,
and their notorious vulnerability to the change of input distribution
(&quot;catastrophic forgetting&quot;). Specifically, We show that this approach helps
alleviate the catastrophic forgetting phenomenon - even without the knowledge
of when the tasks are been switched. Furthermore, it improves the robustness of
the network to weight pruning - even without re-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeno_C/0/1/0/all/0/1&quot;&gt;Chen Zeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Golan_I/0/1/0/all/0/1&quot;&gt;Itay Golan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoffer_E/0/1/0/all/0/1&quot;&gt;Elad Hoffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.00670">
<title>Variational Inference: A Review for Statisticians. (arXiv:1601.00670v8 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1601.00670</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the core problems of modern statistics is to approximate
difficult-to-compute probability densities. This problem is especially
important in Bayesian statistics, which frames all inference about unknown
quantities as a calculation involving the posterior density. In this paper, we
review variational inference (VI), a method from machine learning that
approximates probability densities through optimization. VI has been used in
many applications and tends to be faster than classical methods, such as Markov
chain Monte Carlo sampling. The idea behind VI is to first posit a family of
densities and then to find the member of that family which is close to the
target. Closeness is measured by Kullback-Leibler divergence. We review the
ideas behind mean-field variational inference, discuss the special case of VI
applied to exponential family models, present a full example with a Bayesian
mixture of Gaussians, and derive a variant that uses stochastic optimization to
scale up to massive data. We discuss modern research in VI and highlight
important open problems. VI is powerful, but it is not yet well understood. Our
hope in writing this paper is to catalyze statistical research on this class of
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kucukelbir_A/0/1/0/all/0/1&quot;&gt;Alp Kucukelbir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McAuliffe_J/0/1/0/all/0/1&quot;&gt;Jon D. McAuliffe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02444">
<title>Global optimality conditions for deep neural networks. (arXiv:1707.02444v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02444</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the error landscape of deep linear and nonlinear neural networks
with the squared error loss. Minimizing the loss of a deep linear neural
network is a nonconvex problem, and despite recent progress, our understanding
of this loss surface is still incomplete. For deep linear networks, we present
necessary and sufficient conditions for a critical point of the risk function
to be a global minimum. Surprisingly, our conditions provide an efficiently
checkable test for global optimality, while such tests are typically
intractable in nonconvex optimization. We further extend these results to deep
nonlinear neural networks and prove similar sufficient conditions for global
optimality, albeit in a more limited function space setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1&quot;&gt;Chulhee Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1&quot;&gt;Suvrit Sra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02548">
<title>Predicting Hurricane Trajectories using a Recurrent Neural Network. (arXiv:1802.02548v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02548</link>
<description rdf:parseType="Literal">&lt;p&gt;Hurricanes are cyclones circulating about a defined center whose closed wind
speeds exceed 75 mph originating over tropical and subtropical waters. At
landfall, hurricanes can result in severe disasters. The accuracy of predicting
their trajectory paths is critical to reduce economic loss and save human
lives. Given the complexity and nonlinearity of weather data, a recurrent
neural network (RNN) could be beneficial in modeling hurricane behavior. We
propose the application of a fully connected RNN to predict the trajectory of
hurricanes. We employed the RNN over a fine grid to reduce typical truncation
errors. We utilized their latitude, longitude, wind speed, and pressure
publicly provided by the National Hurricane Center (NOAA) to predict the
trajectory of a hurricane at 6-hour intervals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemany_S/0/1/0/all/0/1&quot;&gt;Sheila Alemany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltran_J/0/1/0/all/0/1&quot;&gt;Jonathan Beltran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Adrian Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganzfried_S/0/1/0/all/0/1&quot;&gt;Sam Ganzfried&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06677">
<title>Degeneration in VAE: in the Light of Fisher Information Loss. (arXiv:1802.06677v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06677</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational Autoencoder (VAE) is one of the most popular generative models,
and enormous advances have been explored in recent years. Due to the increasing
complexity of the raw data and the model architecture, deep networks are needed
in VAE models while few works discuss their impacts. According to our
observation, VAE does not always benefit from deeper architecture: 1) Deeper
encoder makes VAE learn more comprehensible latent representations, while
results in blurry reconstruction samples; 2) Deeper decoder ensures more
high-quality generations, while the latent representations become abstruse; 3)
When encoder and decoder both go deeper, abstruse latent representation occurs
with blurry reconstruction samples at same time. In this paper, we deduce a
Fisher information measure for the corresponding analysis. With such measure,
we demonstrate that information loss is ineluctable in feed-forward networks
and causes the previous three types of degeneration, especially when the
network goes deeper. We also demonstrate that skip connections benefit the
preservation of information amount, thus propose a VAE enhanced by skip
connections, named SCVAE. In the experiments, SCVAE is shown to mitigate the
information loss and to achieve a promising performance in both encoding and
decoding tasks. Moreover, SCVAE can be adaptive to other state-of-the-art
variants of VAE for further amelioration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor W. Tsang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10116">
<title>Generalized Byzantine-tolerant SGD. (arXiv:1802.10116v3 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1802.10116</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose three new robust aggregation rules for distributed synchronous
Stochastic Gradient Descent~(SGD) under a general Byzantine failure model. The
attackers can arbitrarily manipulate the data transferred between the servers
and the workers in the parameter server~(PS) architecture. We prove the
Byzantine resilience properties of these aggregation rules. Empirical analysis
shows that the proposed techniques outperform current approaches for realistic
use cases and Byzantine attack scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1&quot;&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_I/0/1/0/all/0/1&quot;&gt;Indranil Gupta&lt;/a&gt;</dc:creator>
</item></rdf:RDF>