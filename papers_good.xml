<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06217"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06492"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06511"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06022"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06117"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06394"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.00309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05052"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08402"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07963"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10079"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05240"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.05979">
<title>Optimizing Deep Neural Network Architecture: A Tabu Search Based Approach. (arXiv:1808.05979v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05979</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of Feedforward neural network (FNN) fully de-pends upon the
selection of architecture and training algorithm. FNN architecture can be
tweaked using several parameters, such as the number of hidden layers, number
of hidden neurons at each hidden layer and number of connections between
layers. There may be exponential combinations for these architectural
attributes which may be unmanageable manually, so it requires an algorithm
which can automatically design an optimal architecture with high generalization
ability. Numerous optimization algorithms have been utilized for FNN
architecture determination. This paper proposes a new methodology which can
work on the estimation of hidden layers and their respective neurons for FNN.
This work combines the advantages of Tabu search (TS) and Gradient descent with
momentum backpropagation (GDM) training algorithm to demonstrate how Tabu
search can automatically select the best architecture from the populated
architectures based on minimum testing error criteria. The proposed approach
has been tested on four classification benchmark dataset of different size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1&quot;&gt;Tarun Kumar Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_K/0/1/0/all/0/1&quot;&gt;Khalid Raza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06622">
<title>Learning recurrent dynamics in spiking networks. (arXiv:1803.06622v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06622</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking activity of neurons engaged in learning and performing a task show
complex spatiotemporal dynamics. While the output of recurrent network models
can learn to perform various tasks, the possible range of recurrent dynamics
that emerge after learning remains unknown. Here we show that modifying the
recurrent connectivity with a recursive least squares algorithm provides
sufficient flexibility for synaptic and spiking rate dynamics of spiking
networks to produce a wide range of spatiotemporal activity. We apply the
training method to learn arbitrary firing patterns, stabilize irregular spiking
activity of a balanced network, and reproduce the heterogeneous spiking rate
patterns of cortical neurons engaged in motor planning and movement. We
identify sufficient conditions for successful learning, characterize two types
of learning errors, and assess the network capacity. Our findings show that
synaptically-coupled recurrent spiking networks possess a vast computational
capability that can support the diverse activity patterns in the brain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Christopher Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chow_C/0/1/0/all/0/1&quot;&gt;Carson Chow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06217">
<title>Let CONAN tell you a story: Procedural quest generation. (arXiv:1808.06217v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.06217</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes an engine for the Creation Of Novel Adventure Narrative
(CONAN), which is a procedural quest generator. It uses a planning approach to
story generation. The engine is tested on its ability to create quests, which
are sets of actions that must be performed in order to achieve a certain goal,
usually for a reward. The engine takes in a world description represented as a
set of facts, including characters, locations, and items, and generates quests
according to the state of the world and the preferences of the characters. We
evaluate quests through the classification of the motivations behind the
quests, based on the sequences of actions required to complete the quests. We
also compare different world descriptions and analyze the difference in
motivations for the quests produced by the engine. Compared against human
structural quest analysis, the current engine was found to be able to replicate
the quest structures found in commercial video game quests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breault_V/0/1/0/all/0/1&quot;&gt;Vincent Breault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellet_S/0/1/0/all/0/1&quot;&gt;Sebastien Ouellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_J/0/1/0/all/0/1&quot;&gt;Jim Davies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06244">
<title>XL-NBT: A Cross-lingual Neural Belief Tracking Framework. (arXiv:1808.06244v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.06244</link>
<description rdf:parseType="Literal">&lt;p&gt;Task-oriented dialog systems are becoming pervasive, and many companies
heavily rely on them to complement human agents for customer service in call
centers. With globalization, the need for providing cross-lingual customer
support becomes more urgent than ever. However, cross-lingual support poses
great challenges---it requires a large amount of additional annotated data from
native speakers. In order to bypass the expensive human annotation and achieve
the first step towards the ultimate goal of building a universal dialog
management system, we set out to build a cross-lingual state tracking framework
without requiring any human labor. Specifically, we assume that there exists a
source language with dialog belief tracking annotations while having no access
to any form of dialogue data for the other target languages. Then, we pre-train
a state tracker for the source language as a teacher, which is able to exploit
easy-to-access parallel data and distill its own knowledge to the student state
tracker in target languages. In this paper, we specifically discuss two
different types of common parallel resources (bilingual corpus and bilingual
dictionary) and design different strategies to realize our transfer learning
framework. Experimentally, we successfully use English state tracker as the
teacher to transfer its knowledge to both Italian and German trackers and
achieve promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianshu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xifeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06474">
<title>A study on speech enhancement using exponent-only floating point quantized neural network (EOFP-QNN). (arXiv:1808.06474v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1808.06474</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous studies have investigated the effectiveness of neural network
quantization on pattern classification tasks. The present study, for the first
time, investigated the performance of speech enhancement (a regression task in
speech processing) using a novel exponent-only floating-point quantized neural
network (EOFP-QNN). The proposed EOFP-QNN consists of two stages:
mantissa-quantization and exponent-quantization. In the mantissa-quantization
stage, EOFP-QNN learns how to quantize the mantissa bits of the model
parameters while preserving the regression accuracy in the least mantissa
precision. In the exponent-quantization stage, the exponent part of the
parameters is further quantized without any additional performance degradation.
We evaluated the proposed EOFP quantization technique on two types of neural
networks, namely, bidirectional long short-term memory (BLSTM) and fully
convolutional neural network (FCN), on a speech enhancement task. Experimental
results showed that the model sizes can be significantly reduced (the model
sizes of the quantized BLSTM and FCN models were only 18.75% and 21.89%,
respectively, compared to those of the original models) while maintaining a
satisfactory speech-enhancement performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yi-Te Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Chen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuo_T/0/1/0/all/0/1&quot;&gt;Tei-Wei Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06492">
<title>Benchmarking Automatic Machine Learning Frameworks. (arXiv:1808.06492v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06492</link>
<description rdf:parseType="Literal">&lt;p&gt;AutoML serves as the bridge between varying levels of expertise when
designing machine learning systems and expedites the data science process. A
wide range of techniques is taken to address this, however there does not exist
an objective comparison of these techniques. We present a benchmark of current
open source AutoML solutions using open source datasets. We test auto-sklearn,
TPOT, auto_ml, and H2O&apos;s AutoML solution against a compiled set of regression
and classification datasets sourced from OpenML and find that auto-sklearn
performs the best across classification datasets and TPOT performs the best
across regression datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaji_A/0/1/0/all/0/1&quot;&gt;Adithya Balaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_A/0/1/0/all/0/1&quot;&gt;Alexander Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06497">
<title>Learning to Dialogue via Complex Hindsight Experience Replay. (arXiv:1808.06497v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.06497</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning methods have been used for learning dialogue policies
from the experience of conversations. However, learning an effective dialogue
policy frequently requires prohibitively many conversations. This is partly
because of the sparse rewards in dialogues, and the relatively small number of
successful dialogues in early learning phase. Hindsight experience replay (HER)
enables an agent to learn from failure, but the vanilla HER is inapplicable to
dialogue domains due to dialogue goals being implicit (c.f., explicit goals in
manipulation tasks). In this work, we develop two complex HER methods providing
different trade-offs between complexity and performance. Experiments were
conducted using a realistic user simulator. Results suggest that our HER
methods perform better than standard and prioritized experience replay methods
(as applied to deep Q-networks) in learning rate, and that our two complex HER
methods can be combined to produce the best performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Keting Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoping Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06511">
<title>State-of-the-art Chinese Word Segmentation with Bi-LSTMs. (arXiv:1808.06511v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.06511</link>
<description rdf:parseType="Literal">&lt;p&gt;A wide variety of neural-network architectures have been proposed for the
task of Chinese word segmentation. Surprisingly, we find that a bidirectional
LSTM model, when combined with standard deep learning techniques and best
practices, can achieve better accuracy on many of the popular datasets as
compared to models based on more complex neural-network architectures.
Furthermore, our error analysis shows that out-of-vocabulary words remain
challenging for neural-network models, and many of the remaining errors are
unlikely to be fixed through architecture changes. Instead, more effort should
be made on exploring resources for further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Ji Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1&quot;&gt;Kuzman Ganchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1&quot;&gt;David Weiss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05638">
<title>Improving Search through A3C Reinforcement Learning based Conversational Agent. (arXiv:1709.05638v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05638</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a reinforcement learning based search assistant which can assist
users through a set of actions and sequence of interactions to enable them
realize their intent. Our approach caters to subjective search where the user
is seeking digital assets such as images which is fundamentally different from
the tasks which have objective and limited search modalities. Labeled
conversational data is generally not available in such search tasks and
training the agent through human interactions can be time consuming. We propose
a stochastic virtual user which impersonates a real user and can be used to
sample user behavior efficiently to train the agent which accelerates the
bootstrapping of the agent. We develop A3C algorithm based context preserving
architecture which enables the agent to provide contextual assistance to the
user. We compare the A3C agent with Q-learning and evaluate its performance on
average rewards and state values it obtains with the virtual user in validation
episodes. Our experiments show that the agent learns to achieve higher rewards
and better states.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1&quot;&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1&quot;&gt;Aarushi Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1&quot;&gt;Shagun Sodhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00560">
<title>Interpretable Deep Convolutional Neural Networks via Meta-learning. (arXiv:1802.00560v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00560</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model&apos;s outputs. The recent movement
for &quot;algorithmic fairness&quot; also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02560">
<title>Belief likelihood function for generalised logistic regression. (arXiv:1808.02560v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1808.02560</link>
<description rdf:parseType="Literal">&lt;p&gt;The notion of belief likelihood function of repeated trials is introduced,
whenever the uncertainty for individual trials is encoded by a belief measure
(a finite random set). This generalises the traditional likelihood function,
and provides a natural setting for belief inference from statistical data.
Factorisation results are proven for the case in which conjunctive or
disjunctive combination are employed, leading to analytical expressions for the
lower and upper likelihoods of `sharp&apos; samples in the case of Bernoulli trials,
and to the formulation of a generalised logistic regression framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06021">
<title>What do the US West Coast Public Libraries Post on Twitter?. (arXiv:1808.06021v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.06021</link>
<description rdf:parseType="Literal">&lt;p&gt;Twitter has provided a great opportunity for public libraries to disseminate
information for a variety of purposes. Twitter data have been applied in
different domains such as health, politics, and history. There are thousands of
public libraries in the US, but no study has yet investigated the content of
their social media posts like tweets to find their interests. Moreover,
traditional content analysis of Twitter content is not an efficient task for
exploring thousands of tweets. Therefore, there is a need for automatic methods
to overcome the limitations of manual methods. This paper proposes a
computational approach to collecting and analyzing using Twitter Application
Programming Interfaces (API) and investigates more than 138,000 tweets from 48
US west coast libraries using topic modeling. We found 20 topics and assigned
them to five categories including public relations, book, event, training, and
social good. Our results show that the US west coast libraries are more
interested in using Twitter for public relations and book-related events. This
research has both practical and theoretical applications for libraries as well
as other organizations to explore social media actives of their customer and
themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1&quot;&gt;Amir Karami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1&quot;&gt;Matthew Collins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06022">
<title>Characterizing Transgender Health Issues in Twitter. (arXiv:1808.06022v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.06022</link>
<description rdf:parseType="Literal">&lt;p&gt;Although there are millions of transgender people in the world, a lack of
information exists about their health issues. This issue has consequences for
the medical field, which only has a nascent understanding of how to identify
and meet this population&apos;s health-related needs. Social media sites like
Twitter provide new opportunities for transgender people to overcome these
barriers by sharing their personal health experiences. Our research employs a
computational framework to collect tweets from self-identified transgender
users, detect those that are health-related, and identify their information
needs. This framework is significant because it provides a macro-scale
perspective on an issue that lacks investigation at national or demographic
levels. Our findings identified 54 distinct health-related topics that we
grouped into 7 broader categories. Further, we found both linguistic and
topical differences in the health-related information shared by transgender men
(TM) as com-pared to transgender women (TW). These findings can help inform
medical and policy-based strategies for health interventions within transgender
communities. Also, our proposed approach can inform the development of
computational strategies to identify the health-related information needs of
other marginalized populations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1&quot;&gt;Amir Karami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_F/0/1/0/all/0/1&quot;&gt;Frank Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitzie_V/0/1/0/all/0/1&quot;&gt;Vanessa L. Kitzie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06117">
<title>Effect of secular trend in drug effectiveness study in real world data. (arXiv:1808.06117v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06117</link>
<description rdf:parseType="Literal">&lt;p&gt;We discovered secular trend bias in a drug effectiveness study for a recently
approved drug. We compared treatment outcomes between patients who received the
newly approved drug and patients exposed to the standard treatment. All
patients diagnosed after the new drug&apos;s approval date were considered. We built
a machine learning causal inference model to determine patient subpopulations
likely to respond better to the newly approved drug. After identifying the
presence of secular trend bias in our data, we attempted to adjust for the bias
in two different ways. First, we matched patients on the number of days from
the new drug&apos;s approval date that the patient&apos;s treatment (new or standard)
began. Second, we included a covariate in the model for the number of days
between the date of approval of the new drug and the treatment (new or
standard) start date. Neither approach completely mitigated the bias. Residual
bias we attribute to differences in patient disease severity or other
unmeasured patient characteristics. Had we not identified the secular trend
bias in our data, the causal inference model would have been interpreted
without consideration for this underlying bias. Being aware of, testing for,
and handling potential bias in the data is essential to diminish the
uncertainty in AI modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alford_S/0/1/0/all/0/1&quot;&gt;Sharon Hensley Alford&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1&quot;&gt;Piyush Madan&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahatma_S/0/1/0/all/0/1&quot;&gt;Shilpa Mahatma&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buleje_I/0/1/0/all/0/1&quot;&gt;Italo Buleje&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yanyan Han&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1&quot;&gt;Fang Lu&lt;/a&gt; (2) ((1) IBM Watson, (2) IBM Research)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06275">
<title>Applying Machine Learning To Maize Traits Prediction. (arXiv:1808.06275v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06275</link>
<description rdf:parseType="Literal">&lt;p&gt;Heterosis is the improved or increased function of any biological quality in
a hybrid offspring. We have studied yet the largest maize SNP dataset for
traits prediction. We develop linear and non-linear models which consider
relationships between different hybrids as well as other effect. Specially
designed model proved to be efficient and robust in prediction maize&apos;s traits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Binbin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xupeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06394">
<title>Faster Support Vector Machines. (arXiv:1808.06394v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06394</link>
<description rdf:parseType="Literal">&lt;p&gt;The time complexity of support vector machines (SVMs) prohibits training on
huge data sets with millions of samples. Recently, multilevel approaches to
train SVMs have been developed to allow for time efficient training on huge
data sets. While regular SVMs perform the entire training in one - time
consuming - optimization step, multilevel SVMs first build a hierarchy of
problems decreasing in size that resemble the original problem and then train
an SVM model for each hierarchy level benefiting from the solved models of
previous levels. We present a faster multilevel support vector machine that
uses a label propagation algorithm to construct the problem hierarchy.
Extensive experiments show that our new algorithm achieves speed-ups up to two
orders of magnitude while having similar or better classification quality over
state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlag_S/0/1/0/all/0/1&quot;&gt;Sebastian Schlag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1&quot;&gt;Matthias Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1&quot;&gt;Christian Schulz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06444">
<title>Synthetic Patient Generation: A Deep Learning Approach Using Variational Autoencoders. (arXiv:1808.06444v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06444</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence in healthcare is a new and exciting frontier and the
possibilities are endless. With deep learning approaches beating human
performances in many areas, the logical next step is to attempt their
application in the health space. For these and other Machine Learning
approaches to produce good results and have their potential realized, the need
for, and importance of, large amounts of accurate data is second to none. This
is a challenge faced by many industries and more so in the healthcare space. We
present an approach of using Variational Autoencoders (VAE&apos;s) as an approach to
generating more data for training deeper networks, as well as uncovering
underlying patterns in diagnoses and the patients suffering from them. By
training a VAE, on available data, it was able to learn the latent distribution
of the patient features given the diagnosis. It is then possible, after
training, to sample from the learnt latent distribution to generate new
accurate patient records given the patient diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salim_A/0/1/0/all/0/1&quot;&gt;Ally Salim Jr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06503">
<title>Collaborative Pressure Ulcer Prevention: An Automated Skin Damage and Pressure Ulcer Assessment Tool for Nursing Professionals, Patients, Family Members and Carers. (arXiv:1808.06503v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.06503</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the Pressure Ulcers Online Website, which is a first
step solution towards a new and innovative platform for helping people to
detect, understand and manage pressure ulcers. It outlines the reasons why the
project has been developed and provides a central point of contact for pressure
ulcer analysis and ongoing research. Using state-of-the-art technologies in
convolutional neural networks and transfer learning along with end-to-end web
technologies, this platform allows pressure ulcers to be analysed and findings
to be reported. As the system evolves through collaborative partnerships,
future versions will provide decision support functions to describe the complex
characteristics of pressure ulcers along with information on wound care across
multiple user boundaries. This project is therefore intended to raise awareness
and support for people suffering with or providing care for pressure ulcers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_P/0/1/0/all/0/1&quot;&gt;Paul Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalmers_C/0/1/0/all/0/1&quot;&gt;Carl Chalmers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tully_D/0/1/0/all/0/1&quot;&gt;David Tully&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06508">
<title>Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies. (arXiv:1808.06508v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06508</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent behaviour in the real-world requires the ability to acquire new
knowledge from an ongoing sequence of experiences while preserving and reusing
past knowledge. We propose a novel algorithm for unsupervised representation
learning from piece-wise stationary visual data: Variational Autoencoder with
Shared Embeddings (VASE). Based on the Minimum Description Length principle,
VASE automatically detects shifts in the data distribution and allocates spare
representational capacity to new knowledge, while simultaneously protecting
previously learnt representations from catastrophic forgetting. Our approach
encourages the learnt representations to be disentangled, which imparts a
number of desirable properties: VASE can deal sensibly with ambiguous inputs,
it can enhance its own representations through imagination-based exploration,
and most importantly, it exhibits semantically meaningful sharing of latents
between different datasets. Compared to baselines with entangled
representations, our approach is able to reason beyond surface-level statistics
and perform semantically meaningful cross-domain inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eccles_T/0/1/0/all/0/1&quot;&gt;Tom Eccles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthey_L/0/1/0/all/0/1&quot;&gt;Loic Matthey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgess_C/0/1/0/all/0/1&quot;&gt;Christopher P. Burgess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watters_N/0/1/0/all/0/1&quot;&gt;Nick Watters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerchner_A/0/1/0/all/0/1&quot;&gt;Alexander Lerchner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Higgins_I/0/1/0/all/0/1&quot;&gt;Irina Higgins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06573">
<title>A Semi-Supervised and Inductive Embedding Model for Churn Prediction of Large-Scale Mobile Games. (arXiv:1808.06573v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06573</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile gaming has emerged as a promising market with billion-dollar revenues.
A variety of mobile game platforms and services have been developed around the
world. One critical challenge for these platforms and services is to understand
user churn behavior in mobile games. Successful churn prediction will benefit
many stakeholders such as game developers and platform operators. In this
paper, we present the first large-scale churn prediction solution for mobile
games. In view of the common limitations of the state-of-the-art methods built
upon traditional machine learning models, we devise a novel semi-supervised and
inductive embedding model that jointly learns the prediction function and the
embedding function for user-app relationships. We model these two functions by
deep neural networks with a unique edge embedding technique that is able to
capture both contextual information and relationship dynamics. We also design a
novel attributed random walk technique that takes into consideration both
topological adjacency and attributes similarities. To evaluate the performance
of our solution, we collect the real-world data from a commercial mobile gaming
platform that includes tens of thousands of games and hundreds of millions of
user-app interactions. The experimental results with this data demonstrate the
superiority of our proposed model against existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Muhe Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xidao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yong Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duffield_N/0/1/0/all/0/1&quot;&gt;Nick Duffield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Na Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10102">
<title>Structural Conditions for Projection-Cost Preservation via Randomized Matrix Multiplication. (arXiv:1705.10102v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10102</link>
<description rdf:parseType="Literal">&lt;p&gt;Projection-cost preservation is a low-rank approximation guarantee which
ensures that the cost of any rank-$k$ projection can be preserved using a
smaller sketch of the original data matrix. We present a general structural
result outlining four sufficient conditions to achieve projection-cost
preservation. These conditions can be satisfied using tools from the Randomized
Linear Algebra literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chowdhury_A/0/1/0/all/0/1&quot;&gt;Agniva Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiasen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Drineas_P/0/1/0/all/0/1&quot;&gt;Petros Drineas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.00309">
<title>Variance Regularizing Adversarial Learning. (arXiv:1707.00309v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.00309</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel approach for training adversarial models by replacing
the discriminator score with a bi-modal Gaussian distribution over the
real/fake indicator variables. In order to do this, we train the Gaussian
classifier to match the target bi-modal distribution implicitly through
meta-adversarial training. We hypothesize that this approach ensures a non-zero
gradient to the generator, even in the limit of a perfect classifier. We test
our method against standard benchmark image datasets as well as show the
classifier output distribution is smooth and has overlap between the real and
fake modes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grewal_K/0/1/0/all/0/1&quot;&gt;Karan Grewal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05052">
<title>Machine Learning: Basic Principles. (arXiv:1805.05052v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05052</link>
<description rdf:parseType="Literal">&lt;p&gt;This tutorial is based on the lecture notes for the courses &quot;Machine
Learning: Basic Principles&quot; and &quot;Artificial Intelligence&quot;, which I have
(co-)taught since 2015 at Aalto University. The aim is to provide an accessible
introduction to some of the main concepts and methods within machine learning.
Many of the current systems which are considered as (artificially) intelligent
are based on combinations of few basic machine learning methods. After
formalizing the main building blocks of a machine learning problem, some
popular algorithmic design patterns formachine learning methods are discussed
in some detail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08402">
<title>Adapted Deep Embeddings: A Synthesis of Methods for $k$-Shot Inductive Transfer Learning. (arXiv:1805.08402v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08402</link>
<description rdf:parseType="Literal">&lt;p&gt;The focus in machine learning has branched beyond training classifiers on a
single task to investigating how previously acquired knowledge in a source
domain can be leveraged to facilitate learning in a related target domain,
known as inductive transfer learning. Three active lines of research have
independently explored transfer learning using neural networks. In weight
transfer, a model trained on the source domain is used as an initialization
point for a network to be trained on the target domain. In deep metric
learning, the source domain is used to construct an embedding that captures
class structure in both the source and target domains. In few-shot learning,
the focus is on generalizing well in the target domain based on a limited
number of labeled examples. We compare state-of-the-art methods from these
three paradigms and also explore hybrid adapted-embedding methods that use
limited target-domain data to fine tune embeddings constructed from
source-domain data. We conduct a systematic comparison of methods in a variety
of domains, varying the number of labeled instances available in the target
domain ($k$), as well as the number of target-domain classes. We reach three
principal conclusions: (1) Deep embeddings are far superior, compared to weight
transfer, as a starting point for inter-domain transfer or model re-use (2) Our
hybrid methods robustly outperform every few-shot learning and every deep
metric learning method previously proposed, with a mean error reduction of 30%
over state-of-the-art. (3) Among loss functions for discovering embeddings, the
histogram loss (Ustinova &amp;amp; Lempitsky, 2016) is most robust. We hope our results
will motivate a unification of research in weight transfer, deep metric
learning, and few-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_T/0/1/0/all/0/1&quot;&gt;Tyler R. Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ridgeway_K/0/1/0/all/0/1&quot;&gt;Karl Ridgeway&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1&quot;&gt;Michael C. Mozer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07963">
<title>Deep Transfer Learning for Cross-domain Activity Recognition. (arXiv:1807.07963v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07963</link>
<description rdf:parseType="Literal">&lt;p&gt;Human activity recognition plays an important role in people&apos;s daily life.
However, it is often expensive and time-consuming to acquire sufficient labeled
activity data. To solve this problem, transfer learning leverages the labeled
samples from the source domain to annotate the target domain which has few or
none labels. Unfortunately, when there are several source domains available, it
is difficult to select the right source domains for transfer. The right source
domain means that it has the most similar properties with the target domain,
thus their similarity is higher, which can facilitate transfer learning.
Choosing the right source domain helps the algorithm perform well and prevents
the negative transfer. In this paper, we propose an effective Unsupervised
Source Selection algorithm for Activity Recognition (USSAR). USSAR is able to
select the most similar $K$ source domains from a list of available domains.
After this, we propose an effective Transfer Neural Network to perform
knowledge transfer for Activity Recognition (TNNAR). TNNAR could capture both
the time and spatial relationship between activities while transferring
knowledge. Experiments on three public activity recognition datasets
demonstrate that: 1) The USSAR algorithm is effective in selecting the best
source domains. 2) The TNNAR method can reach high accuracy when performing
activity knowledge transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_V/0/1/0/all/0/1&quot;&gt;Vincent W. Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Meiyu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10079">
<title>Automatic Detection of Node-Replication Attack in Vehicular Ad-hoc Networks. (arXiv:1807.10079v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10079</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in smart cities applications enforce security threads such as
node replication attacks. Such attack is take place when the attacker plants a
replicated network node within the network. Vehicular Ad hoc networks are
connecting sensors that have limited resources and required the response time
to be as low as possible. In this type networks, traditional detection
algorithms of node replication attacks are not efficient. In this paper, we
propose an initial idea to apply a newly adapted statistical methodology that
can detect node replication attacks with high performance as compared to
state-of-the-art techniques. We provide a sufficient description of this
methodology and a road-map for testing and experiment its performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamil_M/0/1/0/all/0/1&quot;&gt;Mohammed GH. I. AL Zamil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01204">
<title>Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data. (arXiv:1808.01204v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01204</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have many successful applications, while much less
theoretical understanding has been gained. Towards bridging this gap, we study
the problem of learning a two-layer overparameterized ReLU neural network for
multi-class classification via stochastic gradient descent (SGD) from random
initialization. In the overparameterized setting, when the data comes from
mixtures of well-separated distributions, we prove that SGD learns a network
with a small generalization error, albeit the network has enough capacity to
fit arbitrary labels. Furthermore, the analysis provides interesting insights
into several aspects of learning neural networks and can be verified based on
empirical studies on synthetic data and on the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03314">
<title>Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. (arXiv:1808.03314v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03314</link>
<description rdf:parseType="Literal">&lt;p&gt;Because of their effectiveness in broad practical applications, LSTM networks
have received a wealth of coverage in scientific journals, technical blogs, and
implementation guides. However, in most articles, the inference formulas for
the LSTM network and its parent, RNN, are stated axiomatically, while the
training formulas are omitted altogether. In addition, the technique of
&quot;unrolling&quot; an RNN is routinely presented without justification throughout the
literature. The goal of this paper is to explain the essential RNN and LSTM
fundamentals in a single document. Drawing from concepts in signal processing,
we formally derive the canonical RNN formulation from differential equations.
We then propose and prove a precise statement, which yields the RNN unrolling
technique. We also review the difficulties with training the standard RNN and
address them by transforming the RNN into the &quot;Vanilla LSTM&quot; network through a
series of logical arguments. We provide all equations pertaining to the LSTM
system together with detailed descriptions of its constituent entities. Albeit
unconventional, our choice of notation and the method for presenting the LSTM
system emphasizes ease of understanding. As part of the analysis, we identify
new opportunities to enrich the LSTM system and incorporate these extensions
into the Vanilla LSTM network, producing the most general LSTM variant to date.
The target reader has already been exposed to RNNs and LSTM networks through
numerous available resources and is open to an alternative pedagogical
approach. A Machine Learning practitioner seeking guidance for implementing our
new augmented LSTM model in software for experimentation and research will find
the insights and derivations in this tutorial valuable as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherstinsky_A/0/1/0/all/0/1&quot;&gt;Alex Sherstinsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05240">
<title>Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks. (arXiv:1808.05240v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05240</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantized deep neural networks (QDNNs) are attractive due to their much lower
memory storage and faster inference speed than their regular full precision
counterparts. To maintain the same performance level especially at low
bit-widths, QDNNs must be retrained. Their training involves piecewise constant
activation functions and discrete weights, hence mathematical challenges arise.
We introduce the notion of coarse derivative and propose the blended coarse
gradient descent (BCGD) algorithm, for training fully quantized neural
networks. Coarse gradient is generally not a gradient of any function but an
artificial ascent direction. The weight update of BCGD goes by coarse gradient
correction of a weighted average of the full precision weights and their
quantization (the so-called blending), which yields sufficient descent in the
objective value and thus accelerates the training. Our experiments demonstrate
that this simple blending technique is very effective for quantization at
extremely low bit-width such as binarization. In full quantization of ResNet-18
for ImageNet classification task, BCGD gives 64.36% top-1 accuracy with binary
weights across all layers and 4-bit adaptive activation. If the weights in the
first and last layers are kept in full precision, this number increases to
65.46%. As theoretical justification, we provide the convergence analysis of
coarse gradient descent for a two-layer neural network model with Gaussian
input data, and prove that the expected coarse gradient correlates positively
with the underlying true gradient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Penghang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1&quot;&gt;Stanley Osher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yingyong Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Jack Xin&lt;/a&gt;</dc:creator>
</item></rdf:RDF>