<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09226"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08919"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09245"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04908"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08925"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08993"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09089"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09142"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05502"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.09226">
<title>HyperNets and their application to learning spatial transformations. (arXiv:1807.09226v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.09226</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a conceptual framework for higher-order artificial
neural networks. The idea of higher-order networks arises naturally when a
model is required to learn some group of transformations, every element of
which is well-approximated by a traditional feedforward network. Thus the group
as a whole can be represented as a hyper network. One of typical examples of
such groups is spatial transformations. We show that the proposed framework,
which we call HyperNets, is able to deal with at least two basic spatial
transformations of images: rotation and affine transformation. We show that
HyperNets are able not only to generalize rotation and affine transformation,
but also to compensate the rotation of images bringing them into canonical
forms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potapov_A/0/1/0/all/0/1&quot;&gt;Alexey Potapov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shcherbakov_O/0/1/0/all/0/1&quot;&gt;Oleg Shcherbakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhdanov_I/0/1/0/all/0/1&quot;&gt;Innokentii Zhdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodionov_S/0/1/0/all/0/1&quot;&gt;Sergey Rodionov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skorobogatko_N/0/1/0/all/0/1&quot;&gt;Nikolai Skorobogatko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08919">
<title>The Variational Homoencoder: Learning to learn high capacity generative models from few examples. (arXiv:1807.08919v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08919</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical Bayesian methods can unify many related tasks (e.g. k-shot
classification, conditional and unconditional generation) as inference within a
single generative model. However, when this generative model is expressed as a
powerful neural network such as a PixelCNN, we show that existing learning
techniques typically fail to effectively use latent variables. To address this,
we develop a modification of the Variational Autoencoder in which encoded
observations are decoded to new elements from the same class. This technique,
which we call a Variational Homoencoder (VHE), produces a hierarchical latent
variable model which better utilises latent variables. We use the VHE framework
to learn a hierarchical PixelCNN on the Omniglot dataset, which outperforms all
existing models on test set likelihood and achieves strong performance on
one-shot generation and classification tasks. We additionally validate the VHE
on natural images from the YouTube Faces database. Finally, we develop
extensions of the model that apply to richer dataset structures such as
factorial and hierarchical categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1&quot;&gt;Luke B. Hewitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nye_M/0/1/0/all/0/1&quot;&gt;Maxwell I. Nye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gane_A/0/1/0/all/0/1&quot;&gt;Andreea Gane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08970">
<title>Computational speedups using small quantum devices. (arXiv:1807.08970v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1807.08970</link>
<description rdf:parseType="Literal">&lt;p&gt;Suppose we have a small quantum computer with only M qubits. Can such a
device genuinely speed up certain algorithms, even when the problem size is
much larger than M? Here we answer this question to the affirmative. We present
a hybrid quantum-classical algorithm to solve 3SAT problems involving n&amp;lt;&amp;lt;M
variables that significantly speeds up its fully classical counterpart. This
question may be relevant in view of the current quest to build small quantum
computers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1&quot;&gt;Vedran Dunjko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yimin Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cirac_J/0/1/0/all/0/1&quot;&gt;J. Ignacio Cirac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09161">
<title>An argument in favor of strong scaling for deep neural networks with small datasets. (arXiv:1807.09161v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1807.09161</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, with the popularization of deep learning frameworks and
large datasets, researchers have started parallelizing their models in order to
train faster. This is crucially important, because they typically explore many
hyperparameters in order to find the best ones for their applications. This
process is time consuming and, consequently, speeding up training improves
productivity. One approach to parallelize deep learning models followed by many
researchers is based on weak scaling. The minibatches increase in size as new
GPUs are added to the system. In addition, new learning rates schedules have
been proposed to fix optimization issues that occur with large minibatch sizes.
In this paper, however, we show that the recommendations provided by recent
work do not apply to models that lack large datasets. In fact, we argument in
favor of using strong scaling for achieving reliable performance in such cases.
We evaluated our approach with up to 32 GPUs and show that weak scaling not
only does not have the same accuracy as the sequential model, it also fails to
converge most of time. Meanwhile, strong scaling has good scalability while
having exactly the same accuracy of a sequential implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_R/0/1/0/all/0/1&quot;&gt;Renato L. de F. Cunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_E/0/1/0/all/0/1&quot;&gt;Eduardo R. Rodrigues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viana_M/0/1/0/all/0/1&quot;&gt;Matheus Palhares Viana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1&quot;&gt;Dario Augusto Borges Oliveira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09245">
<title>Visual Dynamics: Stochastic Future Generation via Layered Cross Convolutional Networks. (arXiv:1807.09245v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.09245</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of synthesizing a number of likely future frames from a
single input image. In contrast to traditional methods that have tackled this
problem in a deterministic or non-parametric way, we propose to model future
frames in a probabilistic manner. Our probabilistic model makes it possible for
us to sample and synthesize many possible future frames from a single input
image. To synthesize realistic movement of objects, we propose a novel network
structure, namely a Cross Convolutional Network; this network encodes image and
motion information as feature maps and convolutional kernels, respectively. In
experiments, our model performs well on synthetic data, such as 2D shapes and
animated game sprites, and on real-world video frames. We present analyses of
the learned network representations, showing it is implicitly learning a
compact encoding of object appearance and motion. We also demonstrate a few of
its applications, including visual analogy-making and video extrapolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tianfan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouman_K/0/1/0/all/0/1&quot;&gt;Katherine L. Bouman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1&quot;&gt;William T. Freeman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04908">
<title>Emergence of Grounded Compositional Language in Multi-Agent Populations. (arXiv:1703.04908v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04908</link>
<description rdf:parseType="Literal">&lt;p&gt;By capturing statistical patterns in large corpora, machine learning has
enabled significant advances in natural language processing, including in
machine translation, question answering, and sentiment analysis. However, for
agents to intelligently interact with humans, simply capturing the statistical
patterns is insufficient. In this paper we investigate if, and how, grounded
compositional language can emerge as a means to achieve goals in multi-agent
populations. Towards this end, we propose a multi-agent learning environment
and learning methods that bring about emergence of a basic compositional
language. This language is represented as streams of abstract discrete symbols
uttered by agents over time, but nonetheless has a coherent structure that
possesses a defined vocabulary and syntax. We also observe emergence of
non-verbal communication such as pointing and guiding when language
communication is unavailable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08825">
<title>Hierarchical Classification using Binary Data. (arXiv:1807.08825v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08825</link>
<description rdf:parseType="Literal">&lt;p&gt;In classification problems, especially those that categorize data into a
large number of classes, the classes often naturally follow a hierarchical
structure. That is, some classes are likely to share similar structures and
features. Those characteristics can be captured by considering a hierarchical
relationship among the class labels. Here, we extend a recent simple
classification approach on binary data in order to efficiently classify
hierarchical data. In certain settings, specifically, when some classes are
significantly easier to identify than others, we showcase computational and
accuracy advantages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molitor_D/0/1/0/all/0/1&quot;&gt;Denali Molitor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1&quot;&gt;Deanna Needell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08925">
<title>Anomaly detection in static networks using egonets. (arXiv:1807.08925v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1807.08925</link>
<description rdf:parseType="Literal">&lt;p&gt;Network data has rapidly emerged as an important and active area of
statistical methodology. In this paper we consider the problem of anomaly
detection in networks. Given a large background network, we seek to detect
whether there is a small anomalous subgraph present in the network, and if such
a subgraph is present, which nodes constitute the subgraph. We propose an
inferential tool based on egonets to answer this question. The proposed method
is computationally efficient and naturally amenable to parallel computing, and
easily extends to a wide variety of network models. We demonstrate through
simulation studies that the egonet method works well under a wide variety of
network models. We obtain some fascinating empirical results by applying the
egonet method on several well-studied benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Srijan Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08993">
<title>Deep-CLASS at ISIC Machine Learning Challenge 2018. (arXiv:1807.08993v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08993</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reports the method and evaluation results of MedAusbild team for
ISIC challenge task. Since early 2017, our team has worked on melanoma
classification [1][6], and has employed deep learning since beginning of 2018
[7]. Deep learning helps researchers absolutely to treat and detect diseases by
analyzing medical data (e.g., medical images). One of the representative models
among the various deep-learning models is a convolutional neural network (CNN).
Although our team has an experience with segmentation and classification of
benign and malignant skin-lesions, we have participated in the task 3 of ISIC
Challenge 2018 for classification of seven skin diseases, explained in this
paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasiri_S/0/1/0/all/0/1&quot;&gt;Sara Nasiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Matthias Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helsper_J/0/1/0/all/0/1&quot;&gt;Julien Helsper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1&quot;&gt;Madjid Fathi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09089">
<title>Decision Variance in Online Learning. (arXiv:1807.09089v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.09089</link>
<description rdf:parseType="Literal">&lt;p&gt;Online learning has classically focused on the expected behaviour of learning
policies. Recently, risk-averse online learning has gained much attention. In
this paper, a risk-averse multi-armed bandit problem where the performance of
policies is measured based on the mean-variance of the rewards is studied. The
variance of the rewards depends on the variance of the underlying processes as
well as the variance of the player&apos;s decisions. The performance of two existing
policies is analyzed and new fundamental limitations on risk-averse learning is
established. In particular, it is shown that although an $\mathcal{O}(\log T)$
distribution-dependent regret in time $T$ is achievable (similar to the
risk-neutral setting), the worst-case (i.e. minimax) regret is lower bounded by
$\Omega(T)$ (in contrast to the $\Omega(\sqrt{T})$ lower bound in the
risk-neutral setting). The lower bound results are even stronger in the sense
that they are proven for the case of online learning with full feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vakili_S/0/1/0/all/0/1&quot;&gt;Sattar Vakili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boukouvalas_A/0/1/0/all/0/1&quot;&gt;Alexis Boukouvalas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09097">
<title>Algorithm Selection for Collaborative Filtering: the influence of graph metafeatures and multicriteria metatargets. (arXiv:1807.09097v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.09097</link>
<description rdf:parseType="Literal">&lt;p&gt;To select the best algorithm for a new problem is an expensive and difficult
task. However, there are automatic solutions to address this problem: using
Metalearning, which takes advantage of problem characteristics (i.e.
metafeatures), one is able to predict the relative performance of algorithms.
In the Collaborative Filtering scope, recent works have proposed diverse
metafeatures describing several dimensions of this problem. Despite interesting
and effective findings, it is still unknown whether these are the most
effective metafeatures. Hence, this work proposes a new set of graph
metafeatures, which approach the Collaborative Filtering problem from a Graph
Theory perspective. Furthermore, in order to understand whether metafeatures
from multiple dimensions are a better fit, we investigate the effects of
comprehensive metafeatures. These metafeatures are a selection of the best
metafeatures from all existing Collaborative Filtering metafeatures. The impact
of the most representative metafeatures is investigated in a controlled
experimental setup. Another contribution we present is the use of a
Pareto-Efficient ranking procedure to create multicriteria metatargets. These
new rankings of algorithms, which take into account multiple evaluation
measures, allow to explore the algorithm selection problem in a fairer and more
detailed way. According to the experimental results, the graph metafeatures are
a good alternative to related work metafeatures. However, the results have
shown that the feature selection procedure used to create the comprehensive
metafeatures is is not effective, since there is no gain in predictive
performance. Finally, an extensive metaknowledge analysis was conducted to
identify the most influential metafeatures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_T/0/1/0/all/0/1&quot;&gt;Tiago Cunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1&quot;&gt;Carlos Soares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; C.P.L.F. de Carvalho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09119">
<title>Sleep Staging by Modeling Sleep Stage Transitions using Deep CRF. (arXiv:1807.09119v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1807.09119</link>
<description rdf:parseType="Literal">&lt;p&gt;Sleep plays a vital role in human health, both mental and physical. Sleep
disorders like sleep apnea are increasing in prevalence, with the rapid
increase in factors like obesity. Sleep apnea is most commonly treated with
Continuous Positive Air Pressure (CPAP) therapy, which maintains the
appropriate pressure to ensure continuous airflow. It is widely accepted that
in addition to preventing air passage collapse, increase in deep and REM sleep
stages would be good metrics for how well the CPAP therapy is working in
improving sleep health. Presently, however, there is no mechanism to easily
detect a patient&apos;s sleep stages from CPAP flow data alone. We propose, for the
first time, an automated sleep staging model based only on the flow signal.
&lt;/p&gt;
&lt;p&gt;Recently deep neural networks have shown high accuracy on sleep staging by
eliminating handcrafted features. However, these methods focus exclusively on
extracting informative features from the input signal, without paying much
attention to the dynamics of sleep stages in the output sequence. We propose an
end-to-end framework that uses a deep convolution-recurrent neural networks to
extract high-level features from raw flow signal and then uses a structured
output layer based on a conditional random field to model the temporal
transition structure of the sleep stages. We improve upon the previous methods
by 10% using our model, that can be augmented to the previous sleep staging
deep learning methods. We also show that our method can be used to accurately
track sleep metrics like sleep efficiency calculated from sleep stages that can
be deployed for monitoring the response of CPAP therapy on sleep apnea
patients. Apart from the technical contributions, we expect this study to
motivate new research questions in sleep science, especially towards the
understanding of sleep architecture trajectory among patients under CPAP
therapy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aggarwal_K/0/1/0/all/0/1&quot;&gt;Karan Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khadanga_S/0/1/0/all/0/1&quot;&gt;Swaraj Khadanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq R. Joty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kazaglis_L/0/1/0/all/0/1&quot;&gt;Louis Kazaglis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Srivastava_J/0/1/0/all/0/1&quot;&gt;Jaideep Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09142">
<title>Recurrent Neural Networks for Long and Short-Term Sequential Recommendation. (arXiv:1807.09142v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.09142</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems objectives can be broadly characterized as modeling user
preferences over short-or long-term time horizon. A large body of previous
research studied long-term recommendation through dimensionality reduction
techniques applied to the historical user-item interactions. A recently
introduced session-based recommendation setting highlighted the importance of
modeling short-term user preferences. In this task, Recurrent Neural Networks
(RNN) have shown to be successful at capturing the nuances of user&apos;s
interactions within a short time window. In this paper, we evaluate RNN-based
models on both short-term and long-term recommendation tasks. Our experimental
results suggest that RNNs are capable of predicting immediate as well as
distant user interactions. We also find the best performing configuration to be
a stacked RNN with layer normalization and tied item embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villatel_K/0/1/0/all/0/1&quot;&gt;Kiewan Villatel&lt;/a&gt; (SEQUEL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnova_E/0/1/0/all/0/1&quot;&gt;Elena Smirnova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mary_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;mie Mary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1&quot;&gt;Philippe Preux&lt;/a&gt; (SEQUEL)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09200">
<title>Self-Paced Learning with Adaptive Deep Visual Embeddings. (arXiv:1807.09200v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.09200</link>
<description rdf:parseType="Literal">&lt;p&gt;Selecting the most appropriate data examples to present a deep neural network
(DNN) at different stages of training is an unsolved challenge. Though
practitioners typically ignore this problem, a non-trivial data scheduling
method may result in a significant improvement in both convergence and
generalization performance. In this paper, we introduce Self-Paced Learning
with Adaptive Deep Visual Embeddings (SPL-ADVisE), a novel end-to-end training
protocol that unites self-paced learning (SPL) and deep metric learning (DML).
We leverage the Magnet Loss to train an embedding convolutional neural network
(CNN) to learn a salient representation space. The student CNN classifier
dynamically selects similar instance-level training examples to form a
mini-batch, where the easiness from the cross-entropy loss and the true
diverseness of examples from the learned metric space serve as sample
importance priors. To demonstrate the effectiveness of SPL-ADVisE, we use deep
CNN architectures for the task of supervised image classification on several
coarse- and fine-grained visual recognition datasets. Results show that, across
all datasets, the proposed method converges faster and reaches a higher final
accuracy than other SPL variants, particularly on fine-grained classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thangarasa_V/0/1/0/all/0/1&quot;&gt;Vithursan Thangarasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01473">
<title>Deep linear neural networks with arbitrary loss: All local minima are global. (arXiv:1712.01473v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01473</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider deep linear networks with arbitrary convex differentiable loss.
We provide a short and elementary proof of the fact that all local minima are
global minima if the hidden layers are either 1) at least as wide as the input
layer, or 2) at least as wide as the output layer. This result is the strongest
possible in the following sense: If the loss is convex and Lipschitz but not
differentiable then deep linear networks can have sub-optimal local minima.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurent_T/0/1/0/all/0/1&quot;&gt;Thomas Laurent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brecht_J/0/1/0/all/0/1&quot;&gt;James von Brecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10158">
<title>Learning to Classify from Impure Samples with High-Dimensional Data. (arXiv:1801.10158v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10158</link>
<description rdf:parseType="Literal">&lt;p&gt;A persistent challenge in practical classification tasks is that labeled
training sets are not always available. In particle physics, this challenge is
surmounted by the use of simulations. These simulations accurately reproduce
most features of data, but cannot be trusted to capture all of the complex
correlations exploitable by modern machine learning methods. Recent work in
weakly supervised learning has shown that simple, low-dimensional classifiers
can be trained using only the impure mixtures present in data. Here, we
demonstrate that complex, high-dimensional classifiers can also be trained on
impure mixtures using weak supervision techniques, with performance comparable
to what could be achieved with pure samples. Using weak supervision will
therefore allow us to avoid relying exclusively on simulations for
high-dimensional classification. This work opens the door to a new regime
whereby complex models are trained directly on data, providing direct access to
probe the underlying physics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Komiske_P/0/1/0/all/0/1&quot;&gt;Patrick T. Komiske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Metodiev_E/0/1/0/all/0/1&quot;&gt;Eric M. Metodiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Nachman_B/0/1/0/all/0/1&quot;&gt;Benjamin Nachman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Schwartz_M/0/1/0/all/0/1&quot;&gt;Matthew D. Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09081">
<title>Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution. (arXiv:1804.09081v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09081</link>
<description rdf:parseType="Literal">&lt;p&gt;Architecture search aims at automatically finding neural architectures that
are competitive with architectures designed by human experts. While recent
approaches have achieved state-of-the-art predictive performance for image
recognition, they are problematic under resource constraints for two reasons:
(1) the neural architectures found are solely optimized for high predictive
performance, without penalizing excessive resource consumption; (2) most
architecture search methods require vast computational resources. We address
the first shortcoming by proposing LEMONADE, an evolutionary algorithm for
multi-objective architecture search that allows approximating the Pareto-front
of architectures under multiple objectives, such as predictive performance and
number of parameters, in a single run of the method. We address the second
shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which
generates children networks that are warmstarted with the predictive
performance of their trained parents. This is accomplished by using
(approximate) network morphism operators for generating children. The
combination of these two contributions allows finding models that are on par or
even outperform different-sized NASNets, MobileNets, MobileNets V2 and Wide
Residual Networks on CIFAR-10 and ImageNet64x64 within only one week on eight
GPUs, which is about 20-40x less compute power than previous architecture
search methods that yield state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elsken_T/0/1/0/all/0/1&quot;&gt;Thomas Elsken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Metzen_J/0/1/0/all/0/1&quot;&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05502">
<title>Nonlinear Dimensionality Reduction for Discriminative Analytics of Multiple Datasets. (arXiv:1805.05502v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05502</link>
<description rdf:parseType="Literal">&lt;p&gt;Principal component analysis (PCA) is widely used for feature extraction and
dimensionality reduction, with documented merits in diverse tasks involving
high-dimensional data. Standard PCA copes with one dataset at a time, but it is
challenged when it comes to analyzing multiple datasets jointly. In certain
data science settings however, one is often interested in extracting the most
discriminative information from one dataset of particular interest (a.k.a.
target data) relative to the other(s) (a.k.a. background data). To this end,
this paper puts forth a novel approach, termed discriminative (d) PCA, for such
discriminative analytics of multiple datasets. Under certain conditions, dPCA
is proved to be least-squares optimal in recovering the component vector unique
to the target data relative to background data. To account for nonlinear data
correlations, (linear) dPCA models for one or multiple background datasets are
generalized through kernel-based learning. Interestingly, all dPCA variants
admit an analytical solution obtainable with a single (generalized) eigenvalue
decomposition. Finally, corroborating dimensionality reduction tests using both
synthetic and real datasets are provided to validate the effectiveness of the
proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item></rdf:RDF>