<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06699"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06540"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06696"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06763"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06906"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06630"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06657"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06945"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05936"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06538"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.06699">
<title>Adaptive Neural Trees. (arXiv:1807.06699v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.06699</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks and decision trees operate on largely separate
paradigms; typically, the former performs representation learning with
pre-specified architectures, while the latter is characterised by learning
hierarchies over pre-specified features with data-driven architectures. We
unite the two via adaptive neural trees (ANTs), a model that incorporates
representation learning into edges, routing functions and leaf nodes of a
decision tree, along with a backpropagation-based training algorithm that
adaptively grows the architecture from primitive modules (e.g., convolutional
layers). We demonstrate that, whilst achieving over 99% and 90% accuracy on
MNIST and CIFAR-10 datasets, ANTs benefit from (i) faster inference via
conditional computation, (ii) increased interpretability via hierarchical
clustering e.g. learning meaningful class associations, such as separating
natural vs. man-made objects, and (iii) a mechanism to adapt the architecture
to the size and complexity of the training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arulkumaran_K/0/1/0/all/0/1&quot;&gt;Kai Arulkumaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Criminisi_A/0/1/0/all/0/1&quot;&gt;Antonio Criminisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1&quot;&gt;Aditya Nori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06540">
<title>Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning. (arXiv:1807.06540v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.06540</link>
<description rdf:parseType="Literal">&lt;p&gt;We found an easy and quick post-learning method named &quot;Icing on the Cake&quot; to
enhance a classification performance in deep learning. The method is that we
train only the final classifier again after an ordinary training is done.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konno_T/0/1/0/all/0/1&quot;&gt;Tomohiko Konno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwazume_M/0/1/0/all/0/1&quot;&gt;Michiaki Iwazume&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06696">
<title>Integrating Algorithmic Planning and Deep Learning for Partially Observable Navigation. (arXiv:1807.06696v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.06696</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to take a novel approach to robot system design where each
building block of a larger system is represented as a differentiable program,
i.e. a deep neural network. This representation allows for integrating
algorithmic planning and deep learning in a principled manner, and thus combine
the benefits of model-free and model-based methods. We apply the proposed
approach to a challenging partially observable robot navigation task. The robot
must navigate to a goal in a previously unseen 3-D environment without knowing
its initial location, and instead relying on a 2-D floor map and visual
observations from an onboard camera. We introduce the Navigation Networks
(NavNets) that encode state estimation, planning and acting in a single,
end-to-end trainable recurrent neural network. In preliminary simulation
experiments we successfully trained navigation networks to solve the
challenging partially observable navigation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkus_P/0/1/0/all/0/1&quot;&gt;Peter Karkus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wee Sun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06734">
<title>Generating Levels That Teach Mechanics. (arXiv:1807.06734v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.06734</link>
<description rdf:parseType="Literal">&lt;p&gt;The automatic generation of game tutorials is a challenging AI problem. While
it is possible to generate annotations and instructions that explain to the
player how the game is played, this paper focuses on generating a gameplay
experience that introduces the player to a game mechanic. It evolves small
levels for the Mario AI Framework that can only be beaten by an agent that
knows how to perform specific actions in the game. It uses variations of a
perfect A* agent that are limited in various ways, such as not being able to
jump high or see enemies, to test how failing to do certain actions can stop
the player from beating the level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1&quot;&gt;Michael Cerny Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1&quot;&gt;Ahmed Khalifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barros_G/0/1/0/all/0/1&quot;&gt;Gabriella A.B. Barros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nealen_A/0/1/0/all/0/1&quot;&gt;Andy Nealen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06763">
<title>General Value Function Networks. (arXiv:1807.06763v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06763</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we show that restricting the representation-layer of a
Recurrent Neural Network (RNN) improves accuracy and reduces the depth of
recursive training procedures in partially observable domains. Artificial
Neural Networks have been shown to learn useful state representations for
high-dimensional visual and continuous control domains. If the the tasks at
hand exhibits long depends back in time, these instantaneous feed-forward
approaches are augmented with recurrent connections and trained with Back-prop
Through Time (BPTT). This unrolled training can become computationally
prohibitive if the dependency structure is long, and while recent work on LSTMs
and GRUs has improved upon naive training strategies, there is still room for
improvements in computational efficiency and parameter sensitivity. In this
paper we explore a simple modification to the classic RNN structure:
restricting the state to be comprised of multi-step General Value Function
predictions. We formulate an architecture called General Value Function
Networks (GVFNs), and corresponding objective that generalizes beyond previous
approaches. We show that our GVFNs are significantly more robust to train, and
facilitate accurate prediction with no gradients needed back-in-time in domains
with substantial long-term dependences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlegel_M/0/1/0/all/0/1&quot;&gt;Matthew Schlegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Adam White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patterson_A/0/1/0/all/0/1&quot;&gt;Andrew Patterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1&quot;&gt;Martha White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06906">
<title>Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search. (arXiv:1807.06906v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06906</link>
<description rdf:parseType="Literal">&lt;p&gt;While existing work on neural architecture search (NAS) tunes hyperparameters
in a separate post-processing step, we demonstrate that architectural choices
and other hyperparameter settings interact in a way that can render this
separation suboptimal. Likewise, we demonstrate that the common practice of
using very few epochs during the main NAS and much larger numbers of epochs
during a post-processing step is inefficient due to little correlation in the
relative rankings for these two training regimes. To combat both of these
problems, we propose to use a recent combination of Bayesian optimization and
Hyperband for efficient joint neural architecture and hyperparameter search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1&quot;&gt;Arber Zela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1&quot;&gt;Aaron Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falkner_S/0/1/0/all/0/1&quot;&gt;Stefan Falkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06978">
<title>Improving Explainable Recommendations with Synthetic Reviews. (arXiv:1807.06978v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.06978</link>
<description rdf:parseType="Literal">&lt;p&gt;An important task for a recommender system to provide interpretable
explanations for the user. This is important for the credibility of the system.
Current interpretable recommender systems tend to focus on certain features
known to be important to the user and offer their explanations in a structured
form. It is well known that user generated reviews and feedback from reviewers
have strong leverage over the users&apos; decisions. On the other hand, recent text
generation works have been shown to generate text of similar quality to human
written text, and we aim to show that generated text can be successfully used
to explain recommendations.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a framework consisting of popular review-oriented
generation models aiming to create personalised explanations for
recommendations. The interpretations are generated at both character and word
levels. We build a dataset containing reviewers&apos; feedback from the Amazon books
review dataset. Our cross-domain experiments are designed to bridge from
natural language processing to the recommender system domain. Besides language
model evaluation methods, we employ DeepCoNN, a novel review-oriented
recommender system using a deep neural network, to evaluate the recommendation
performance of generated reviews by root mean square error (RMSE). We
demonstrate that the synthetic personalised reviews have better recommendation
performance than human written reviews. To our knowledge, this presents the
first machine-generated natural language explanations for rating prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1&quot;&gt;Sixun Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1&quot;&gt;Aonghus Lawlor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_F/0/1/0/all/0/1&quot;&gt;Felipe Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolog_P/0/1/0/all/0/1&quot;&gt;Peter Dolog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07024">
<title>Fast Model-Selection through Adapting Design of Experiments Maximizing Information Gain. (arXiv:1807.07024v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1807.07024</link>
<description rdf:parseType="Literal">&lt;p&gt;To perform model-selection efficiently, we must run informative experiments.
Here, we extend a seminal method for designing Bayesian optimal experiments
that maximize the information gained from data collected. We introduce two
computational improvements: a search algorithm from artificial intelligence and
a sampling procedure shrinking the space of possible experiments to evaluate.
We collected data for five different experimental designs and show that
experiments optimized for information gain make model-selection faster, and
cheaper, as compared to the designs chosen by a pool of expert
experimentalists. Our procedure is general and can be applied iteratively to
lab, field and online experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balietti_S/0/1/0/all/0/1&quot;&gt;Stefano Balietti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klein_B/0/1/0/all/0/1&quot;&gt;Brennan Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Riedl_C/0/1/0/all/0/1&quot;&gt;Christoph Riedl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00215">
<title>Internal node bagging. (arXiv:1805.00215v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00215</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel view to understand how dropout works as an inexplicit
ensemble learning method, which doesn&apos;t point out how many and which nodes to
learn a certain feature. We propose a new training method named internal node
bagging, it explicitly forces a group of nodes to learn a certain feature in
training time, and combine those nodes to be one node in inference time. It
means we can use much more parameters to improve model&apos;s fitting ability in
training time while keeping model small in inference time. We test our method
on several benchmark datasets and find it performs significantly better than
dropout on small models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1&quot;&gt;Shun Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06397">
<title>Expressing Linear Orders Requires Exponential-Size DNNFs. (arXiv:1807.06397v2 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06397</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that any DNNF circuit that expresses the set of linear orders over a
set of $n$ candidates must be of size $2^{\Omega(n)}$. Moreover, we show that
there exist DNNF circuits of size $2^{O(n)}$ expressing linear orders over $n$
candidates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haan_R/0/1/0/all/0/1&quot;&gt;Ronald de Haan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06630">
<title>Expressive power of outer product manifolds on feed-forward neural networks. (arXiv:1807.06630v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06630</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical neural networks are exponentially more efficient than their
corresponding &quot;shallow&quot; counterpart with the same expressive power, but involve
huge number of parameters and require tedious amounts of training. Our main
idea is to mathematically understand and describe the hierarchical structure of
feedforward neural networks by reparametrization invariant Riemannian metrics.
By computing or approximating the tangent subspace, we better utilize the
original network via sparse representations that enables switching to shallow
networks after a very early training stage. Our experiments show that the
proposed approximation of the metric improves and sometimes even surpasses the
achievable performance of the original network significantly even after a few
epochs of training the original feedforward network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daroczy_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Dar&amp;#xf3;czy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aleksziev_R/0/1/0/all/0/1&quot;&gt;Rita Aleksziev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benczur_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Bencz&amp;#xfa;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06657">
<title>Airline Passenger Name Record Generation using Generative Adversarial Networks. (arXiv:1807.06657v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06657</link>
<description rdf:parseType="Literal">&lt;p&gt;Passenger Name Records (PNRs) are at the heart of the travel industry.
Created when an itinerary is booked, they contain travel and passenger
information. It is usual for airlines and other actors in the industry to
inter-exchange and access each other&apos;s PNR, creating the challenge of using
them without infringing data ownership laws. To address this difficulty, we
propose a method to generate realistic synthetic PNRs using Generative
Adversarial Networks (GANs). Unlike other GAN applications, PNRs consist of
categorical and numerical features with missing/NaN values, which makes the use
of GANs challenging. We propose a solution based on Cram\&apos;{e}r GANs,
categorical feature embedding and a Cross-Net architecture. The method was
tested on a real PNR dataset, and evaluated in terms of distribution matching,
memorization, and performance of predictive models for two real business
problems: client segmentation and passenger nationality prediction. Results
show that the generated data matches well with the real PNRs without memorizing
them, and that it can be used to train models for real business applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottini_A/0/1/0/all/0/1&quot;&gt;Alejandro Mottini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lheritier_A/0/1/0/all/0/1&quot;&gt;Alix Lheritier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acuna_Agost_R/0/1/0/all/0/1&quot;&gt;Rodrigo Acuna-Agost&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06689">
<title>Efficient Deep Learning on Multi-Source Private Data. (arXiv:1807.06689v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06689</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models benefit from large and diverse datasets. Using such
datasets, however, often requires trusting a centralized data aggregator. For
sensitive applications like healthcare and finance this is undesirable as it
could compromise patient privacy or divulge trade secrets. Recent advances in
secure and privacy-preserving computation, including trusted hardware enclaves
and differential privacy, offer a way for mutually distrusting parties to
efficiently train a machine learning model without revealing the training data.
In this work, we introduce Myelin, a deep learning framework which combines
these privacy-preservation primitives, and use it to establish a baseline level
of performance for fully private machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hynes_N/0/1/0/all/0/1&quot;&gt;Nick Hynes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Raymond Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06752">
<title>Gradient Band-based Adversarial Training for Generalized Attack Immunity of A3C Path Finding. (arXiv:1807.06752v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06752</link>
<description rdf:parseType="Literal">&lt;p&gt;As adversarial attacks pose a serious threat to the security of AI system in
practice, such attacks have been extensively studied in the context of computer
vision applications. However, few attentions have been paid to the adversarial
research on automatic path finding. In this paper, we show dominant adversarial
examples are effective when targeting A3C path finding, and design a Common
Dominant Adversarial Examples Generation Method (CDG) to generate dominant
adversarial examples against any given map. In addition, we propose Gradient
Band-based Adversarial Training, which trained with a single randomly choose
dominant adversarial example without taking any modification, to realize the
&quot;1:N&quot; attack immunity for generalized dominant adversarial examples. Extensive
experimental results show that, the lowest generation precision for CDG
algorithm is 91.91%, and the lowest immune precision for Gradient Band-based
Adversarial Training is 93.89%, which can prove that our method can realize the
generalized attack immunity of A3C path finding with a high confidence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1&quot;&gt;Wenjia Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yingxiao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06819">
<title>Self-supervised Knowledge Distillation Using Singular Value Decomposition. (arXiv:1807.06819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.06819</link>
<description rdf:parseType="Literal">&lt;p&gt;To solve deep neural network (DNN)&apos;s huge training dataset and its high
computation issue, so-called teacher-student (T-S) DNN which transfers the
knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN
has limited range of use, and the knowledge of T-DNN is insufficiently
transferred to S-DNN. To improve the quality of the transferred knowledge from
T-DNN, we propose a new knowledge distillation using singular value
decomposition (SVD). In addition, we define a knowledge transfer as a
self-supervised task and suggest a way to continuously receive information from
T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of
the T-DNN can be up to 1.1\% better than the T-DNN in terms of classification
accuracy. Also assuming the same computational cost, our S-DNN outperforms the
S-DNN driven by the state-of-the-art distillation with a performance advantage
of 1.79\%. code is available on https://github.com/sseung0703/SSKD\_SVD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dae Ha Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Byung Cheol Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06945">
<title>Cyclostationary Statistical Models and Algorithms for Anomaly Detection Using Multi-Modal Data. (arXiv:1807.06945v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1807.06945</link>
<description rdf:parseType="Literal">&lt;p&gt;A framework is proposed to detect anomalies in multi-modal data. A deep
neural network-based object detector is employed to extract counts of objects
and sub-events from the data. A cyclostationary model is proposed to model
regular patterns of behavior in the count sequences. The anomaly detection
problem is formulated as a problem of detecting deviations from learned
cyclostationary behavior. Sequential algorithms are proposed to detect
anomalies using the proposed model. The proposed algorithms are shown to be
asymptotically efficient in a well-defined sense. The developed algorithms are
applied to a multi-modal data consisting of CCTV imagery and social media posts
to detect a 5K run in New York City.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banerjee_T/0/1/0/all/0/1&quot;&gt;Taposh Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Whipps_G/0/1/0/all/0/1&quot;&gt;Gene Whipps&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gurram_P/0/1/0/all/0/1&quot;&gt;Prudhvi Gurram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06972">
<title>Data-Efficient Weakly Supervised Learning for Low-Resource Audio Event Detection Using Deep Learning. (arXiv:1807.06972v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1807.06972</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to perform audio event detection under the common
constraint that only limited training data are available. In training a deep
learning system to perform audio event detection, two practical problems arise.
Firstly, most datasets are &apos;weakly labelled&apos; having only a list of events
present in each recording without any temporal information for training.
Secondly, deep neural networks need a very large amount of labelled training
data to achieve good quality performance, yet in practice it is difficult to
collect enough samples for most classes of interest. In this paper, we propose
a data-efficient training of a stacked convolutional and recurrent neural
network. This neural network is trained in a multi instance learning setting
for which we introduce a new loss function that leads to improved training
compared to the usual approaches for weakly supervised learning. We
successfully test our approach on a low-resource dataset that lacks temporal
labels, for bird vocalisation detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morfi_V/0/1/0/all/0/1&quot;&gt;Veronica Morfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10574">
<title>Decoupled Parallel Backpropagation with Convergence Guarantee. (arXiv:1804.10574v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10574</link>
<description rdf:parseType="Literal">&lt;p&gt;Backpropagation algorithm is indispensable for the training of feedforward
neural networks. It requires propagating error gradients sequentially from the
output layer all the way back to the input layer. The backward locking in
backpropagation algorithm constrains us from updating network layers in
parallel and fully leveraging the computing resources. Recently, several
algorithms have been proposed for breaking the backward locking. However, their
performances degrade seriously when networks are deep. In this paper, we
propose decoupled parallel backpropagation algorithm for deep learning
optimization with convergence guarantee. Firstly, we decouple the
backpropagation algorithm using delayed gradients, and show that the backward
locking is removed when we split the networks into multiple modules. Then, we
utilize decoupled parallel backpropagation in two stochastic methods and prove
that our method guarantees convergence to critical points for the non-convex
problem. Finally, we perform experiments for training deep convolutional neural
networks on benchmark datasets. The experimental results not only confirm our
theoretical analysis, but also demonstrate that the proposed method can achieve
significant speedup without loss of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05936">
<title>Variational Inference: A Unified Framework of Generative Models and Some Revelations. (arXiv:1807.05936v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05936</link>
<description rdf:parseType="Literal">&lt;p&gt;We reinterpreting the variational inference in a new perspective. Via this
way, we can easily prove that EM algorithm, VAE, GAN, AAE, ALI(BiGAN) are all
special cases of variational inference. The proof also reveals the loss of
standard GAN is incomplete and it explains why we need to train GAN cautiously.
From that, we find out a regularization term to improve stability of GAN
training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jianlin Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06538">
<title>Pseudo-Feature Generation for Imbalanced Data Analysis in Deep Learning. (arXiv:1807.06538v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06538</link>
<description rdf:parseType="Literal">&lt;p&gt;We generate pseudo-features by multivariate probability distributions
obtained from feature maps in a low layer of trained deep neural networks.
Then, we virtually augment the data of minor classes by the pseudo-features in
order to overcome imbalanced data problems. Because all the wild data are
imbalanced, the proposed method has the possibility to improve the ability of
DNN in a broad range of problems
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konno_T/0/1/0/all/0/1&quot;&gt;Tomohiko Konno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwazume_M/0/1/0/all/0/1&quot;&gt;Michiaki Iwazume&lt;/a&gt;</dc:creator>
</item></rdf:RDF>