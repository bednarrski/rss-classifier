<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07120"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06504"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06511"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06593"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06610"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06879"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06297"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06523"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06657"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06660"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1508.04409"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07359"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05010"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1708.07120">
<title>Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. (arXiv:1708.07120v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07120</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we describe a phenomenon, which we named &quot;super-convergence&quot;,
where neural networks can be trained an order of magnitude faster than with
standard training methods. The existence of super-convergence is relevant to
understanding why deep networks generalize well. One of the key elements of
super-convergence is training with one learning rate cycle and a large maximum
learning rate. A primary insight that allows super-convergence training is that
large learning rates regularize the training, hence requiring a reduction of
all other forms of regularization in order to preserve an optimal
regularization balance. We also derive a simplification of the Hessian Free
optimization method to compute an estimate of the optimal learning rate.
Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet
datasets, and resnet, wide-resnet, densenet, and inception architectures. In
addition, we show that super-convergence provides a greater boost in
performance relative to standard training when the amount of labeled training
data is limited. The architectures and code to replicate the figures in this
paper are available at github.com/lnsmith54/super-convergence. See
&lt;a href=&quot;http://www.fast.ai/2018/04/30/dawnbench-fastai/&quot;&gt;this http URL&lt;/a&gt; for an application of
super-convergence to win the DAWNBench challenge (see
https://dawn.cs.stanford.edu/benchmark/).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1&quot;&gt;Leslie N. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topin_N/0/1/0/all/0/1&quot;&gt;Nicholay Topin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06502">
<title>First Experiments with Neural Translation of Informal to Formal Mathematics. (arXiv:1805.06502v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.06502</link>
<description rdf:parseType="Literal">&lt;p&gt;We report on our first experiments to train deep neural networks that
automatically translate informalized $\LaTeX{}$-written Mizar texts into the
formal Mizar language. Using Luong et al.&apos;s neural machine translation model
(NMT), we tested our aligned informal-formal corpora against various
hyperparameters and evaluated their results. Our experiments show that NMT is
able to generate correct Mizar statements on more than 60 percent of the
inference data, indicating that formalization through artificial neural network
is a promising approach for automated formalization of mathematics. We present
several case studies to illustrate our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1&quot;&gt;Cezary Kaliszyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1&quot;&gt;Josef Urban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06504">
<title>Analogical Reasoning on Chinese Morphological and Semantic Relations. (arXiv:1805.06504v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.06504</link>
<description rdf:parseType="Literal">&lt;p&gt;Analogical reasoning is effective in capturing linguistic regularities. This
paper proposes an analogical reasoning task on Chinese. After delving into
Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28
explicit semantic relations. A big and balanced dataset CA8 is then built for
this task, including 17813 questions. Furthermore, we systematically explore
the influences of vector representations, context features, and corpora on
analogical reasoning. With the experiments, CA8 is proved to be a reliable
benchmark for evaluating Chinese word embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Renfen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wensi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06511">
<title>Improving End-of-turn Detection in Spoken Dialogues by Detecting Speaker Intentions as a Secondary Task. (arXiv:1805.06511v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.06511</link>
<description rdf:parseType="Literal">&lt;p&gt;This work focuses on the use of acoustic cues for modeling turn-taking in
dyadic spoken dialogues. Previous work has shown that speaker intentions (e.g.,
asking a question, uttering a backchannel, etc.) can influence turn-taking
behavior and are good predictors of turn-transitions in spoken dialogues.
However, speaker intentions are not readily available for use by automated
systems at run-time; making it difficult to use this information to anticipate
a turn-transition. To this end, we propose a multi-task neural approach for
predicting turn- transitions and speaker intentions simultaneously. Our results
show that adding the auxiliary task of speaker intention prediction improves
the performance of turn-transition prediction in spoken dialogues, without
relying on additional input features during run-time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aldeneh_Z/0/1/0/all/0/1&quot;&gt;Zakaria Aldeneh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Dimitriadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Provost_E/0/1/0/all/0/1&quot;&gt;Emily Mower Provost&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06525">
<title>Text classification based on ensemble extreme learning machine. (arXiv:1805.06525v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1805.06525</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach based on cost-sensitive ensemble
weighted extreme learning machine; we call this approach AE1-WELM. We apply
this approach to text classification. AE1-WELM is an algorithm including
balanced and imbalanced multiclassification for text classification. Weighted
ELM assigning the different weights to the different samples improves the
classification accuracy to a certain extent, but weighted ELM considers the
differences between samples in the different categories only and ignores the
differences between samples within the same categories. We measure the
importance of the documents by the sample information entropy, and generate
cost-sensitive matrix and factor based on the document importance, then embed
the cost-sensitive weighted ELM into the AdaBoost.M1 framework seamlessly.
Vector space model(VSM) text representation produces the high dimensions and
sparse features which increase the burden of ELM. To overcome this problem, we
develop a text classification framework combining the word vector and AE1-WELM.
The experimental results show that our method provides an accurate, reliable
and effective solution for text classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1&quot;&gt;Peilun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ju Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06593">
<title>Cross-Target Stance Classification with Self-Attention Networks. (arXiv:1805.06593v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.06593</link>
<description rdf:parseType="Literal">&lt;p&gt;In stance classification, the target on which the stance is made defines the
boundary of the task, and a classifier is usually trained for prediction on the
same target. In this work, we explore the potential for generalizing
classifiers between different targets, and propose a neural model that can
apply what has been learned from a source target to a destination target. We
show that our model can find useful information shared between relevant targets
which improves generalization in certain scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paris_C/0/1/0/all/0/1&quot;&gt;Cecile Paris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1&quot;&gt;Surya Nepal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1&quot;&gt;Ross Sparks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06610">
<title>A Formulation of Recursive Self-Improvement and Its Possible Efficiency. (arXiv:1805.06610v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.06610</link>
<description rdf:parseType="Literal">&lt;p&gt;Recursive self-improving (RSI) systems have been dreamed of since the early
days of computer science and artificial intelligence. However, many existing
studies on RSI systems remain philosophical, and lacks clear formulation and
results. In this paper, we provide a formal definition for one class of RSI
systems, and then demonstrate the existence of computable and efficient RSI
systems on a restricted version. We use simulation to empirically show that we
achieve logarithmic runtime complexity with respect to the size of the search
space, and these results suggest it is possible to achieve an efficient
recursive self-improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenyi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06621">
<title>Generative networks as inverse problems with Scattering transforms. (arXiv:1805.06621v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06621</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs)
provide impressive image generations from Gaussian white noise, but the
underlying mathematics are not well understood. We compute deep convolutional
network generators by inverting a fixed embedding operator. Therefore, they do
not require to be optimized with a discriminator or an encoder. The embedding
is Lipschitz continuous to deformations so that generators transform linear
interpolations between input white noise vectors into deformations between
output images. This embedding is computed with a wavelet Scattering transform.
Numerical experiments demonstrate that the resulting Scattering generators have
similar properties as GANs or VAEs, without learning a discriminative network
or an encoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angles_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Angles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallat_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Mallat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06822">
<title>DNN or $k$-NN: That is the Generalize vs. Memorize Question. (arXiv:1805.06822v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06822</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the relationship between the classification performed by
deep neural networks and the $k$-NN decision at the embedding space of these
networks. This simple important connection shown here provides a better
understanding of the relationship between the ability of neural networks to
generalize and their tendency to memorize the training data, which are
traditionally considered to be contradicting to each other and here shown to be
compatible and complementary. Our results support the conjecture that deep
neural networks approach Bayes optimal error rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1&quot;&gt;Gilad Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapiro_G/0/1/0/all/0/1&quot;&gt;Guillermo Sapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06824">
<title>Learning Time-Sensitive Strategies in Space Fortress. (arXiv:1805.06824v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.06824</link>
<description rdf:parseType="Literal">&lt;p&gt;Although there has been remarkable progress and impressive performance on
reinforcement learning (RL) on Atari games, there are many problems with
challenging characteristics that have not yet been explored in Deep Learning
for RL. These include reward sparsity, abrupt context-dependent reversals of
strategy and time-sensitive game play. In this paper, we present Space
Fortress, a game that incorporates all these characteristics and experimentally
show that the presence of any of these renders state of the art Deep RL
algorithms incapable of learning. Then, we present our enhancements to an
existing algorithm and show big performance increases through each enhancement
through an ablation study. We discuss how each of these enhancements was able
to help and also argue that appropriate transfer learning boosts performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Akshat Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06864">
<title>Resource allocation under uncertainty: an algebraic and qualitative treatment. (arXiv:1805.06864v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.06864</link>
<description rdf:parseType="Literal">&lt;p&gt;We use an algebraic viewpoint, namely a matrix framework to deal with the
problem of resource allocation under uncertainty in the context of a
qualitative approach. Our basic qualitative data are a plausibility relation
over the resources, a hierarchical relation over the agents and of course the
preference that the agents have over the resources. With this data we propose a
qualitative binary relation $\unrhd$ between allocations such that
$\mathcal{F}\unrhd \mathcal{G}$ has the following intended meaning: the
allocation $\mathcal{F}$ produces more or equal social welfare than the
allocation $\mathcal{G}$. We prove that there is a family of allocations which
are maximal with respect to $\unrhd$. We prove also that there is a notion of
simple deal such that optimal allocations can be reached by sequences of simple
deals. Finally, we introduce some mechanism for discriminating {optimal}
allocations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camacho_F/0/1/0/all/0/1&quot;&gt;Franklin Camacho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chacon_G/0/1/0/all/0/1&quot;&gt;Gerardo Chac&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_R/0/1/0/all/0/1&quot;&gt;Ram&amp;#xf3;n Pino Per&amp;#xe9;z&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06879">
<title>Neural language representations predict outcomes of scientific research. (arXiv:1805.06879v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.06879</link>
<description rdf:parseType="Literal">&lt;p&gt;Many research fields codify their findings in standard formats, often by
reporting correlations between quantities of interest. But the space of all
testable correlates is far larger than scientific resources can currently
address, so the ability to accurately predict correlations would be useful to
plan research and allocate resources. Using a dataset of approximately 170,000
correlational findings extracted from leading social science journals, we show
that a trained neural network can accurately predict the reported correlations
using only the text descriptions of the correlates. Accurate predictive models
such as these can guide scientists towards promising untested correlates,
better quantify the information gained from new findings, and has implications
for moving artificial intelligence systems from predicting structures to
predicting relationships in the real world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bagrow_J/0/1/0/all/0/1&quot;&gt;James P. Bagrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berenberg_D/0/1/0/all/0/1&quot;&gt;Daniel Berenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Joshua Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01365">
<title>New Results on Multi-Step Traffic Flow Prediction. (arXiv:1803.01365v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01365</link>
<description rdf:parseType="Literal">&lt;p&gt;In its simplest form, the traffic flow prediction problem is restricted to
predicting a single time-step into the future. Multi-step traffic flow
prediction extends this set-up to the case where predicting multiple time-steps
into the future based on some finite history is of interest. This problem is
significantly more difficult than its single-step variant and is known to
suffer from degradation in predictions as the time step increases. In this
paper, two approaches to improve multi-step traffic flow prediction performance
in recursive and multi-output settings are introduced. In particular, a model
that allows recursive prediction approaches to take into account the temporal
context in term of time-step index when making predictions is introduced. In
addition, a conditional generative adversarial network-based data augmentation
method is proposed to improve prediction performance in the multi-output
setting. The experiments on a real-world traffic flow dataset show that the two
methods improve on multi-step traffic flow prediction in recursive and
multi-output settings, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koesdwiady_A/0/1/0/all/0/1&quot;&gt;Arief Koesdwiady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karray_F/0/1/0/all/0/1&quot;&gt;Fakhri Karray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06297">
<title>A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. (arXiv:1805.06297v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06297</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has managed to learn cross-lingual word embeddings without
parallel data by mapping monolingual embeddings to a shared space through
adversarial training. However, their evaluation has focused on favorable
conditions, using comparable corpora or closely-related languages, and we show
that they often fail in more realistic scenarios. This work proposes an
alternative approach based on a fully unsupervised initialization that
explicitly exploits the structural similarity of the embeddings, and a robust
self-learning algorithm that iteratively improves this solution. Our method
succeeds in all tested scenarios and obtains the best published results in
standard datasets, even surpassing previous supervised systems. Our
implementation is released as an open source project at
https://github.com/artetxem/vecmap
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1&quot;&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1&quot;&gt;Gorka Labaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1&quot;&gt;Eneko Agirre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06523">
<title>End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition. (arXiv:1805.06523v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06523</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we study the problem of learning the weights of a deep
convolutional neural network. We consider a network where convolutions are
carried out over non-overlapping patches with a single kernel in each layer. We
develop an algorithm for simultaneously learning all the kernels from the
training data. Our approach dubbed Deep Tensor Decomposition (DeepTD) is based
on a rank-1 tensor decomposition. We theoretically investigate DeepTD under a
realizable model for the training data where the inputs are chosen i.i.d. from
a Gaussian distribution and the labels are generated according to planted
convolutional kernels. We show that DeepTD is data-efficient and provably works
as soon as the sample size exceeds the total number of convolutional weights in
the network. We carry out a variety of numerical experiments to investigate the
effectiveness of DeepTD and verify our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1&quot;&gt;Mahdi Soltanolkotabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06546">
<title>Joint Classification and Prediction CNN Framework for Automatic Sleep Stage Classification. (arXiv:1805.06546v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06546</link>
<description rdf:parseType="Literal">&lt;p&gt;Sleep staging plays an important role in assessment and treatment of sleep
issues. This work proposes a joint classification-and-prediction framework
based on convolutional neural networks (CNNs) for automatic sleep staging, and,
subsequently, introduces a simple yet efficient CNN architecture to power the
framework. Given a single input epoch, the framework jointly determines its
label (classification) and those of its neighboring epochs in the contextual
output (prediction). While the proposed framework is orthogonal to the widely
adopted classification schemes, which takes one or multiple epochs as a
contextual input and produces a single classification decision on the target
epoch, we demonstrate its advantages in different ways. First, it leverages the
dependency among consecutive sleep epochs while surpassing the problems
experienced with the common classification schemes. Second, even with a single
model, the framework effortlessly produces multiple decisions as in
ensemble-of-models methods which are essential in obtaining a good performance.
Probabilistic aggregation techniques are then proposed to leverage the
availability of multiple decisions. We demonstrate good performance on the
Montreal Archive of Sleep Studies (MASS) dataset consisting 200 subjects with
an average accuracy of 83.6%. We also show that the proposed framework not only
is superior to the baselines based on the common classification schemes but
also outperforms existing deep-learning approaches. To our knowledge, this is
the first work going beyond the standard single-output classification to
consider neural networks with multiple outputs for automatic sleep staging.
This framework could provide avenues for further studies of different
neural-network architectures for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1&quot;&gt;Huy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreotti_F/0/1/0/all/0/1&quot;&gt;Fernando Andreotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooray_N/0/1/0/all/0/1&quot;&gt;Navin Cooray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1&quot;&gt;Oliver Y. Ch&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1&quot;&gt;Maarten De Vos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06604">
<title>Accelerating Nonnegative Matrix Factorization Algorithms using Extrapolation. (arXiv:1805.06604v1 [cs.NA])</title>
<link>http://arxiv.org/abs/1805.06604</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a general framework to accelerate significantly the
algorithms for nonnegative matrix factorization (NMF). This framework is
inspired from the extrapolation scheme used to accelerate gradient methods in
convex optimization and from the method of parallel tangents. However, the use
of extrapolation in the context of the two-block coordinate descent algorithms
tackling the non-convex NMF problems is novel. We illustrate the performance of
this approach on two state-of-the-art NMF algorithms, namely, accelerated
hierarchical alternating least squares (A-HALS) and alternating nonnegative
least squares (ANLS), using synthetic, image and document data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ang_A/0/1/0/all/0/1&quot;&gt;Andersen Man Shun Ang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillis_N/0/1/0/all/0/1&quot;&gt;Nicolas Gillis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06640">
<title>Testing for Conditional Mean Independence with Covariates through Martingale Difference Divergence. (arXiv:1805.06640v1 [math.ST])</title>
<link>http://arxiv.org/abs/1805.06640</link>
<description rdf:parseType="Literal">&lt;p&gt;As a crucial problem in statistics is to decide whether additional variables
are needed in a regression model. We propose a new multivariate test to
investigate the conditional mean independence of Y given X conditioning on some
known effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are
linearly related, we reformulate an equivalent notion of conditional mean
independence through transformation, which is approximated in practice. We
apply the martingale difference divergence (Shao and Zhang, 2014) to measure
conditional mean dependence, and show that the estimation error from
approximation is negligible, as it has no impact on the asymptotic distribution
of the test statistic under some regularity assumptions. The implementation of
our test is demonstrated by both simulations and a financial data example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Ze Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiaohan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Matteson_D/0/1/0/all/0/1&quot;&gt;David S. Matteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06657">
<title>Deep-learning Based Modeling of Fault Detachment Stability for Power Grid. (arXiv:1805.06657v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06657</link>
<description rdf:parseType="Literal">&lt;p&gt;The project intends to model the stability of power system with a deep
learning algorithm to the problem, aiming to delay the removal of the fault.
The so-called &quot;fail-delay cut-off&quot; refers to the occurrence of N-1 backup
protection action on the backbone network of the system, resulting in longer
time for the removal of the fault. In practice, through the analysis and
calculation of a large number of online data, we have found that the N-1
failure system of the main protection action will not be unstable, which is
also a guarantee of the operation mode arrangement. In the case of the N-1
backup protection action, there is an approximately 2.5% probability that the
system will be destabilized. Therefore, research is needed to improve the
operating arrangement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Haotian Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianggen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanhao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06660">
<title>Single Shot Active Learning using Pseudo Annotators. (arXiv:1805.06660v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.06660</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard myopic active learning assumes that human annotations are always
obtainable whenever new samples are selected. This, however, is unrealistic in
many real-world applications where human experts are not readily available at
all times. In this paper, we consider the single shot setting: all the required
samples should be chosen in a single shot and no human annotation can be
exploited during the selection process. We propose a new method, Active
Learning through Random Labeling (ALRL), which substitutes single human
annotator for multiple, what we will refer to as, pseudo annotators. These
pseudo annotators always provide uniform and random labels whenever new
unlabeled samples are queried. This random labeling enables standard active
learning algorithms to also exhibit the exploratory behavior needed for single
shot active learning. The exploratory behavior is further enhanced by selecting
the most representative sample via minimizing nearest neighbor distance between
unlabeled samples and queried samples. Experiments on real-world datasets
demonstrate that the proposed method outperforms several state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yazhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loog_M/0/1/0/all/0/1&quot;&gt;Marco Loog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06792">
<title>Faster Rates for Convex-Concave Games. (arXiv:1805.06792v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06792</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the use of no-regret algorithms to compute equilibria for
particular classes of convex-concave games. While standard regret bounds would
lead to convergence rates on the order of $O(T^{-1/2})$, recent work
\citep{RS13,SALS15} has established $O(1/T)$ rates by taking advantage of a
particular class of optimistic prediction algorithms. In this work we go
further, showing that for a particular class of games one achieves a $O(1/T^2)$
rate, and we show how this applies to the Frank-Wolfe method and recovers a
similar bound \citep{D15}. We also show that such no-regret techniques can even
achieve a linear rate, $O(\exp(-T))$, for equilibrium computation under
additional curvature assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1&quot;&gt;Jacob Abernethy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1&quot;&gt;Kevin A. Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1&quot;&gt;Kfir Y. Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun-Kun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06846">
<title>RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks. (arXiv:1805.06846v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.06846</link>
<description rdf:parseType="Literal">&lt;p&gt;Explicit encoding of group actions in deep features makes it possible for
convolutional neural networks (CNNs) to handle global deformations of images,
which is critical to success in many vision tasks. This paper proposes to
decompose the convolutional filters over joint steerable bases across the space
and the group geometry simultaneously, namely a rotation-equivariant CNN with
decomposed convolutional filters (RotDCF). This decomposition facilitates
computing the joint convolution, which is proved to be necessary for the group
equivariance. It significantly reduces the model size and computational
complexity while preserving performance, and truncation of the bases expansion
serves implicitly to regularize the filters. On datasets involving in-plane and
out-of-plane object rotations, RotDCF deep features demonstrate greater
robustness and interpretability than regular CNNs. The stability of the
equivariant representation to input variations is also proved theoretically
under generic assumptions on the filters in the decomposed form. The RotDCF
framework can be extended to groups other than rotations, providing a general
approach which achieves both group equivariance and representation stability at
a reduced model size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderbank_R/0/1/0/all/0/1&quot;&gt;Robert Calderbank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapiro_G/0/1/0/all/0/1&quot;&gt;Guillermo Sapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06862">
<title>Design Identification of Curve Patterns on Cultural Heritage Objects: Combining Template Matching and CNN-based Re-Ranking. (arXiv:1805.06862v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06862</link>
<description rdf:parseType="Literal">&lt;p&gt;The surfaces of many cultural heritage objects were embellished with various
patterns, especially curve patterns. In practice, most of the unearthed
cultural heritage objects are highly fragmented, e.g., sherds of potteries or
vessels, and each of them only shows a very small portion of the underlying
full design, with noise and deformations. The goal of this paper is to address
the challenging problem of automatically identifying the underlying full design
of curve patterns from such a sherd. Specifically, we formulate this problem as
template matching: curve structure segmented from the sherd is matched to each
location with each possible orientation of each known full design. In this
paper, we propose a new two-stage matching algorithm, with a different matching
cost in each stage. In Stage 1, we use a traditional template matching, which
is highly computationally efficient, over the whole search space and identify a
small set of candidate matchings. In Stage 2, we derive a new matching cost by
training a dual-source Convolutional Neural Network (CNN) and apply it to
re-rank the candidate matchings identified in Stage 1. We collect 600 pottery
sherds with 98 full designs from the Woodland Period in Southeastern North
America for experiments and the performance of the proposed algorithm is very
competitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuhang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Karen Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilder_C/0/1/0/all/0/1&quot;&gt;Colin Wilder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1508.04409">
<title>ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. (arXiv:1508.04409v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1508.04409</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the C++ application and R package ranger. The software is a fast
implementation of random forests for high dimensional data. Ensembles of
classification, regression and survival trees are supported. We describe the
implementation, provide examples, validate the package with a reference
implementation, and compare runtime and memory usage with other
implementations. The new software proves to scale best with the number of
features, samples, trees, and features tried for splitting. Finally, we show
that ranger is the fastest and most memory efficient implementation of random
forests to analyze data on the scale of a genome-wide association study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wright_M/0/1/0/all/0/1&quot;&gt;Marvin N. Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ziegler_A/0/1/0/all/0/1&quot;&gt;Andreas Ziegler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01012">
<title>On the convergence properties of a $K$-step averaging stochastic gradient descent algorithm for nonconvex optimization. (arXiv:1708.01012v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01012</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their popularity, the practical performance of asynchronous
stochastic gradient descent methods (ASGD) for solving large scale machine
learning problems are not as good as theoretical results indicate. We adopt and
analyze a synchronous K-step averaging stochastic gradient descent algorithm
which we call K-AVG. We establish the convergence results of K-AVG for
nonconvex objectives and explain why the K-step delay is necessary and leads to
better performance than traditional parallel stochastic gradient descent which
is a special case of K-AVG with $K=1$. We also show that K-AVG scales better
than ASGD. Another advantage of K-AVG over ASGD is that it allows larger
stepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD
implementations and achieves better accuracies and faster convergence for
\cifar dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1&quot;&gt;Guojing Cong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07359">
<title>Class-Splitting Generative Adversarial Networks. (arXiv:1709.07359v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07359</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) produce systematically better quality
samples when class label information is provided., i.e. in the conditional GAN
setup. This is still observed for the recently proposed Wasserstein GAN
formulation which stabilized adversarial training and allows considering high
capacity network architectures such as ResNet. In this work we show how to
boost conditional GAN by augmenting available class labels. The new classes
come from clustering in the representation space learned by the same GAN model.
The proposed strategy is also feasible when no class information is available,
i.e. in the unsupervised setup. Our generated samples reach state-of-the-art
Inception scores for CIFAR-10 and STL-10 datasets in both supervised and
unsupervised setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grinblat_G/0/1/0/all/0/1&quot;&gt;Guillermo L. Grinblat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Uzal_L/0/1/0/all/0/1&quot;&gt;Lucas C. Uzal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Granitto_P/0/1/0/all/0/1&quot;&gt;Pablo M. Granitto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10161">
<title>Development and analysis of a Bayesian water balance model for large lake systems. (arXiv:1710.10161v4 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10161</link>
<description rdf:parseType="Literal">&lt;p&gt;Water balance models (WBMs) are often employed to understand regional
hydrologic cycles over various time scales. Most WBMs, however, are
physically-based, and few employ state-of-the-art statistical methods to
reconcile independent input measurement uncertainty and bias. Further, few WBMs
exist for large lakes, and most large lake WBMs perform additive accounting,
with minimal consideration towards input data uncertainty. Here, we introduce a
framework for improving a previously developed large lake statistical water
balance model (L2SWBM). Focusing on the water balances of Lakes Superior and
Michigan-Huron, we demonstrate our new analytical framework, identifying
L2SWBMs from 26 alternatives that adequately close the water balance of the
lakes with satisfactory computation times compared with the prototype model. We
expect our new framework will be used to develop water balance models for other
lakes around the world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Joeseph P. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gronewold_A/0/1/0/all/0/1&quot;&gt;Andrew D. Gronewold&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06146">
<title>Universal Language Model Fine-tuning for Text Classification. (arXiv:1801.06146v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06146</link>
<description rdf:parseType="Literal">&lt;p&gt;Inductive transfer learning has greatly impacted computer vision, but
existing approaches in NLP still require task-specific modifications and
training from scratch. We propose Universal Language Model Fine-tuning
(ULMFiT), an effective transfer learning method that can be applied to any task
in NLP, and introduce techniques that are key for fine-tuning a language model.
Our method significantly outperforms the state-of-the-art on six text
classification tasks, reducing the error by 18-24% on the majority of datasets.
Furthermore, with only 100 labeled examples, it matches the performance of
training from scratch on 100x more data. We open-source our pretrained models
and code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_J/0/1/0/all/0/1&quot;&gt;Jeremy Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07887">
<title>Impact of Batch Size on Stopping Active Learning for Text Classification. (arXiv:1801.07887v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07887</link>
<description rdf:parseType="Literal">&lt;p&gt;When using active learning, smaller batch sizes are typically more efficient
from a learning efficiency perspective. However, in practice due to speed and
human annotator considerations, the use of larger batch sizes is necessary.
While past work has shown that larger batch sizes decrease learning efficiency
from a learning curve perspective, it remains an open question how batch size
impacts methods for stopping active learning. We find that large batch sizes
degrade the performance of a leading stopping method over and above the
degradation that results from reduced learning efficiency. We analyze this
degradation and find that it can be mitigated by changing the window size
parameter of how many past iterations of learning are taken into account when
making the stopping decision. We find that when using larger batch sizes,
stopping methods are more effective when smaller window sizes are used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beatty_G/0/1/0/all/0/1&quot;&gt;Garrett Beatty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochis_E/0/1/0/all/0/1&quot;&gt;Ethan Kochis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloodgood_M/0/1/0/all/0/1&quot;&gt;Michael Bloodgood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05010">
<title>Detecting Adversarial Samples for Deep Neural Networks through Mutation Testing. (arXiv:1805.05010v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05010</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, it has been shown that deep neural networks (DNN) are subject to
attacks through adversarial samples. Adversarial samples are often crafted
through adversarial perturbation, i.e., manipulating the original sample with
minor modifications so that the DNN model labels the sample incorrectly. Given
that it is almost impossible to train perfect DNN, adversarial samples are
shown to be easy to generate. As DNN are increasingly used in safety-critical
systems like autonomous cars, it is crucial to develop techniques for defending
such attacks. Existing defense mechanisms which aim to make adversarial
perturbation challenging have been shown to be ineffective. In this work, we
propose an alternative approach. We first observe that adversarial samples are
much more sensitive to perturbations than normal samples. That is, if we impose
random perturbations on a normal and an adversarial sample respectively, there
is a significant difference between the ratio of label change due to the
perturbations. Observing this, we design a statistical adversary detection
algorithm called nMutant (inspired by mutation testing from software
engineering community). Our experiments show that nMutant effectively detects
most of the adversarial samples generated by recently proposed attacking
methods. Furthermore, we provide an error bound with certain statistical
significance along with the detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;</dc:creator>
</item></rdf:RDF>