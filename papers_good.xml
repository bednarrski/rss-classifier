<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00815"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.00321"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00823"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00905"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.02511"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05583"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.01078">
<title>Recent Advances in Recurrent Neural Networks. (arXiv:1801.01078v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.01078</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are capable of learning features and long
term dependencies from sequential and time-series data. The RNNs have a stack
of non-linear units where at least one connection between units forms a
directed cycle. A well-trained RNN can model any dynamical system; however,
training RNNs is mostly plagued by issues in learning long-term dependencies.
In this paper, we present a survey on RNNs and several new advances for
newcomers and professionals in the field. The fundamentals and recent advances
are explained and the research challenges are introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehinejad_H/0/1/0/all/0/1&quot;&gt;Hojjat Salehinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baarbe_J/0/1/0/all/0/1&quot;&gt;Julianne Baarbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_S/0/1/0/all/0/1&quot;&gt;Sharan Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barfett_J/0/1/0/all/0/1&quot;&gt;Joseph Barfett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1&quot;&gt;Errol Colak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valaee_S/0/1/0/all/0/1&quot;&gt;Shahrokh Valaee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00815">
<title>Advice from the Oracle: Really Intelligent Information Retrieval. (arXiv:1801.00815v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00815</link>
<description rdf:parseType="Literal">&lt;p&gt;What is &quot;intelligent&quot; information retrieval? Essentially this is asking what
is intelligence, in this article I will attempt to show some of the aspects of
human intelligence, as related to information retrieval. I will do this by the
device of a semi-imaginary Oracle. Every Observatory has an oracle, someone who
is a distinguished scientist, has great administrative responsibilities, acts
as mentor to a number of less senior people, and as trusted advisor to even the
most accomplished scientists, and knows essentially everyone in the field. In
an appendix I will present a brief summary of the Statistical Factor Space
method for text indexing and retrieval, and indicate how it will be used in the
Astrophysics Data System Abstract Service. 2018 Keywords: Personal Digital
Assistant; Supervised Topic Models
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1&quot;&gt;Michael J. Kurtz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00984">
<title>Sentence Object Notation: Multilingual sentence notation based on Wordnet. (arXiv:1801.00984v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00984</link>
<description rdf:parseType="Literal">&lt;p&gt;The representation of sentences is a very important task. It can be used as a
way to exchange data inter-applications. One main characteristic, that a
notation must have, is a minimal size and a representative form. This can
reduce the transfer time, and hopefully the processing time as well.
&lt;/p&gt;
&lt;p&gt;Usually, sentence representation is associated to the processed language. The
grammar of this language affects how we represent the sentence. To avoid
language-dependent notations, we have to come up with a new representation
which don&apos;t use words, but their meanings. This can be done using a lexicon
like wordnet, instead of words we use their synsets. As for syntactic
relations, they have to be universal as much as possible.
&lt;/p&gt;
&lt;p&gt;Our new notation is called STON &quot;\textit{SenTences Object Notation}&quot;, which
somehow has similarities to JSON. It is meant to be minimal, representative and
language-independent syntactic representation. Also, we want it to be readable
and easy to be created. This simplifies developing simple automatic generators
and creating test banks manually. Its benefit is to be used as a medium between
different parts of applications like: text summarization, language translation,
etc. The notation is based on 4 languages: Arabic, English, Franch and
Japanese; and there are some cases where these languages don&apos;t agree on one
representation. Also, given the diversity of grammatical structure of different
world languages, this annotation may fail for some languages which allows more
future improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aries_A/0/1/0/all/0/1&quot;&gt;Abdelkrime Aries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zegour_D/0/1/0/all/0/1&quot;&gt;Djamal Eddine Zegour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hidouci_W/0/1/0/all/0/1&quot;&gt;Walid Khaled Hidouci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.00321">
<title>Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation. (arXiv:1705.00321v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1705.00321</link>
<description rdf:parseType="Literal">&lt;p&gt;Different from other sequential data, sentences in natural language are
structured by linguistic grammars. Previous generative conversational models
with chain-structured decoder ignore this structure in human language and might
generate plausible responses with less satisfactory relevance and fluency. In
this study, we aim to incorporate the results from linguistic analysis into the
process of sentence generation for high-quality conversation generation.
Specifically, we use a dependency parser to transform each response sentence
into a dependency tree and construct a training corpus of sentence-tree pairs.
A tree-structured decoder is developed to learn the mapping from a sentence to
its tree, where different types of hidden states are used to depict the local
dependencies from an internal tree node to its children. For training
acceleration, we propose a tree canonicalization method, which transforms trees
into equivalent ternary trees. Then, with a proposed tree-structured search
method, the model is able to generate the most probable responses in the form
of dependency trees, which are finally flattened into sequences as the system
output. Experimental results demonstrate that the proposed X2Tree framework
outperforms baseline methods over 11.15% increase of acceptance ratio.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Ganbin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1&quot;&gt;Rongyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yijun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qing He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04008">
<title>Investigating the Impact of Data Volume and Domain Similarity on Transfer Learning Applications. (arXiv:1712.04008v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04008</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer Learning helps to build a system to recognize and apply knowledge
and experience learned in previous tasks (source task) to new tasks or new
domains (target task), which share some commonality. The two important factors
that impact the performance of transfer learning models are: (a) the size of
the target dataset and (b) the similarity in distribution between source and
target domains. Thus far there has been little investigation into just how
important these factors are. In this paper, we investigated the impact of
target dataset size and source/target domain similarity on model performance
through a series of experiments. We found that more data is always beneficial,
and that model performance improved linearly with the log of data size, until
we were out of data. As source/target domains differ, more data is required and
fine tuning will render better performance than feature extraction. When
source/target domains are similar and data size is small, fine tuning and
feature extraction renders equivalent performance. We hope that our study
inspires further work in transfer learning, which continues to be a very
important technique for developing practical machine learning applications in
business domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernico_M/0/1/0/all/0/1&quot;&gt;Michael Bernico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuntao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dingchao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00823">
<title>MVG Mechanism: Differential Privacy under Matrix-Valued Query. (arXiv:1801.00823v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1801.00823</link>
<description rdf:parseType="Literal">&lt;p&gt;Differential privacy mechanism design has traditionally been tailored for a
scalar-valued query function. Although many mechanisms such as the Laplace and
Gaussian mechanisms can be extended to a matrix-valued query function by adding
i.i.d. noise to each element of the matrix, this method is often suboptimal as
it forfeits an opportunity to exploit the structural characteristics typically
associated with matrix analysis. To address this challenge, we propose a novel
differential privacy mechanism called the Matrix-Variate Gaussian (MVG)
mechanism, which adds a matrix-valued noise drawn from a matrix-variate
Gaussian distribution, and we rigorously prove that the MVG mechanism preserves
$(\epsilon,\delta)$-differential privacy. Furthermore, we introduce the concept
of directional noise made possible by the design of the MVG mechanism.
Directional noise allows the impact of the noise on the utility of the
matrix-valued query function to be moderated. Finally, we experimentally
demonstrate the performance of our mechanism using three matrix-valued queries
on three privacy-sensitive datasets. We find that the MVG mechanism notably
outperforms four previous state-of-the-art approaches, and provides comparable
utility to the non-private baseline. Our work thus presents a promising
prospect for both future research and implementation of differential privacy
for matrix-valued query functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanyaswad_T/0/1/0/all/0/1&quot;&gt;Thee Chanyaswad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dytso_A/0/1/0/all/0/1&quot;&gt;Alex Dytso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1&quot;&gt;H. Vincent Poor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1&quot;&gt;Prateek Mittal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00857">
<title>Optimal Bayesian Transfer Learning. (arXiv:1801.00857v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.00857</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning has recently attracted significant research attention, as
it simultaneously learns from different source domains, which have plenty of
labeled data, and transfers the relevant knowledge to the target domain with
limited labeled data to improve the prediction performance. We propose a
Bayesian transfer learning framework where the source and target domains are
related through the joint prior density of the model parameters. The modeling
of joint prior densities enables better understanding of the &quot;transferability&quot;
between domains. We define a joint Wishart density for the precision matrices
of the Gaussian feature-label distributions in the source and target domains to
act like a bridge that transfers the useful information of the source domain to
help classification in the target domain by improving the target posteriors.
Using several theorems in multivariate statistics, the posteriors and posterior
predictive densities are derived in closed forms with hypergeometric functions
of matrix argument, leading to our novel closed-form and fast Optimal Bayesian
Transfer Learning (OBTL) classifier. Experimental results on both synthetic and
real-world benchmark data confirm the superb performance of the OBTL compared
to the other state-of-the-art transfer learning and domain adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karbalayghareh_A/0/1/0/all/0/1&quot;&gt;Alireza Karbalayghareh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaoning Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dougherty_E/0/1/0/all/0/1&quot;&gt;Edward R. Dougherty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00905">
<title>Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space. (arXiv:1801.00905v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.00905</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Neural networks have seen a huge surge in its adoption due to their
ability to provide high accuracy on various tasks. On the other hand, the
existence of adversarial examples have raised suspicions regarding the
generalization capabilities of neural networks. In this work, we focus on the
weight matrix learnt by the neural networks and hypothesize that ill
conditioned weight matrix is one of the contributing factors in neural
network&apos;s susceptibility towards adversarial examples. For ensuring that the
learnt weight matrix&apos;s condition number remains sufficiently low, we suggest
using orthogonal regularizer. We show that this indeed helps in increasing the
adversarial accuracy on MNIST and F-MNIST datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mayank Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Abhishek Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.02511">
<title>Heterogeneous Transfer Learning: An Unsupervised Approach. (arXiv:1701.02511v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1701.02511</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning leverages the knowledge in one domain, the source domain,
to improve learning efficiency in another domain, the target domain. Existing
transfer learning research is relatively well-progressed, but only in
situations where the feature spaces of the domains are homogeneous and the
target domain contains at least a few labeled instances. However, transfer
learning has not been well-studied in heterogeneous settings with an unlabeled
target domain. To contribute to the research in this emerging field, this paper
presents: (1) an unsupervised knowledge transfer theorem that prevents negative
transfer; and (2) a principal angle-based metric to measure the distance
between two pairs of domains. The metric shows the extent to which homogeneous
representations have preserved the information in original source and target
domains. The unsupervised knowledge transfer theorem sets out the transfer
conditions necessary to prevent negative transfer. Linear monotonic maps meet
the transfer conditions of the theorem and, hence, are used to construct
homogeneous representations of the heterogeneous domains, which in principle
prevents negative transfer. The metric and the theorem have been implemented in
an innovative transfer model, called a Grassmann-LMM-geodesic flow kernel
(GLG), that is specifically designed for knowledge transfer across
heterogeneous domains. The GLG model learns homogeneous representations of
heterogeneous domains by minimizing the proposed metric. Knowledge is
transferred through these learned representations via a geodesic flow kernel.
Notably, the theorem presented in this paper provides the sufficient transfer
conditions needed to guarantee that knowledge is transferred from a source
domain to an unlabeled target domain with correctness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guanquan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jie Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07433">
<title>Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. (arXiv:1704.07433v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07433</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-paced learning and hard example mining re-weight training instances to
improve learning accuracy. This paper presents two improved alternatives based
on lightweight estimates of sample uncertainty in stochastic gradient descent
(SGD): the variance in predicted probability of the correct class across
iterations of mini-batch SGD, and the proximity of the correct class
probability to the decision threshold. Extensive experimental results on six
datasets show that our methods reliably improve accuracy in various network
architectures, including additional gains on top of other popular training
techniques, such as residual learning, momentum, ADAM, batch normalization,
dropout, and distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Haw-Shiuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Learned_Miller_E/0/1/0/all/0/1&quot;&gt;Erik Learned-Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCallum_A/0/1/0/all/0/1&quot;&gt;Andrew McCallum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05583">
<title>Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification. (arXiv:1709.05583v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05583</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have transformed several artificial intelligence
research areas including computer vision, speech recognition, and natural
language processing. However, recent studies demonstrated that DNNs are
vulnerable to adversarial manipulations at testing time. Specifically, suppose
we have a testing example, whose label can be correctly predicted by a DNN
classifier. An attacker can add a small carefully crafted noise to the testing
example such that the DNN classifier predicts an incorrect label, where the
crafted testing example is called adversarial example. Such attacks are called
evasion attacks. Evasion attacks are one of the biggest challenges for
deploying DNNs in safety and security critical applications such as
self-driving cars. In this work, we develop new methods to defend against
evasion attacks. Our key observation is that adversarial examples are close to
the classification boundary. Therefore, we propose region-based classification
to be robust to adversarial examples. For a benign/adversarial testing example,
we ensemble information in a hypercube centered at the example to predict its
label. In contrast, traditional classifiers are point-based classification,
i.e., given a testing example, the classifier predicts its label based on the
testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets
demonstrate that our region-based classification can significantly mitigate
evasion attacks without sacrificing classification accuracy on benign examples.
Specifically, our region-based classification achieves the same classification
accuracy on testing benign examples as point-based classification, but our
region-based classification is significantly more robust than point-based
classification to various evasion attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;</dc:creator>
</item></rdf:RDF>