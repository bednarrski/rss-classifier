<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05344"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07025"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04659"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.08340">
<title>Reducing Parameter Space for Neural Network Training. (arXiv:1805.08340v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08340</link>
<description rdf:parseType="Literal">&lt;p&gt;For neural networks (NNs) with rectified linear unit (ReLU) or binary
activation functions, we show that their training can be accomplished in a
reduced parameter space. Specifically, the weights in each neuron can be
trained on the unit sphere, as opposed to the entire space, and the threshold
can be trained in a bounded interval, as opposed to the real line. We show that
the NNs in the reduced parameter space are mathematically equivalent to the
standard NNs with parameters in the whole space. The reduced parameter space
shall facilitate the optimization procedure for the network training, as the
search space becomes (much) smaller. We demonstrate the improved training
performance using numerical examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Ling Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiu_D/0/1/0/all/0/1&quot;&gt;Dongbin Xiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11551">
<title>Deep Recurrent Neural Networks for ECG Signal Denoising. (arXiv:1807.11551v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11551</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to denoise electrocardiographic signals (ECG),
utilizing deep recurrent neural network built of Long-Short Term Memory (LSTM)
units. The network is pretrained using synthetic data, generated by dynamic
model ECG and fine-tuned with a real data from Physionet PDB database of ECG
signals. The results show that a 10-layer DRNN has a mean squared error as low
as 0.179 for denoising real signals with white noise of amplitude 0.2 mV,
making it a viable alternative for other commonly used methods. We also
investigate the impact of synthetic data on the network performance on real
signals. Our results show that networks pretrained with synthetic data have
better results than network trained with real data only, regardless of the
training set size. We propose to explain this by means of the transfer learning
framework and the analogy to human cognitive process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antczak_K/0/1/0/all/0/1&quot;&gt;Karol Antczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02016">
<title>MCRM: Mother Compact Recurrent Memory. (arXiv:1808.02016v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1808.02016</link>
<description rdf:parseType="Literal">&lt;p&gt;LSTMs and GRUs are the most common recurrent neural network architectures
used to solve temporal sequence problems. The two architectures have differing
data flows dealing with a common component called the cell state (also referred
to as the memory). We attempt to enhance the memory by presenting a
modification that we call the Mother Compact Recurrent Memory (MCRM). MCRMs are
a type of a nested LSTM-GRU architecture where the cell state is the GRU hidden
state. The concatenation of the forget gate and input gate interactions from
the LSTM are considered an input to the GRU cell. Because MCRMs has this type
of nesting, MCRMs have a compact memory pattern consisting of neurons that acts
explicitly in both long-term and short-term fashions. For some specific tasks,
empirical results show that MCRMs outperform previously used architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abduallah A. Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claudel_C/0/1/0/all/0/1&quot;&gt;Christian Claudel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05839">
<title>Neuromorphic Architecture for the Hierarchical Temporal Memory. (arXiv:1808.05839v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.05839</link>
<description rdf:parseType="Literal">&lt;p&gt;A biomimetic machine intelligence algorithm, that holds promise in creating
invariant representations of spatiotemporal input streams is the hierarchical
temporal memory (HTM). This unsupervised online algorithm has been demonstrated
on several machine-learning tasks, including anomaly detection. Significant
effort has been made in formalizing and applying the HTM algorithm to different
classes of problems. There are few early explorations of the HTM hardware
architecture, especially for the earlier version of the spatial pooler of HTM
algorithm. In this article, we present a full-scale HTM architecture for both
spatial pooler and temporal memory. Synthetic synapse design is proposed to
address the potential and dynamic interconnections occurring during learning.
The architecture is interweaved with parallel cells and columns that enable
high processing speed for the HTM. The proposed architecture is verified for
two different datasets: MNIST and the European number plate font (EUNF), with
and without the presence of noise. The spatial pooler architecture is
synthesized on Xilinx ZYNQ-7, with 91.16% classification accuracy for MNIST and
90\% accuracy for EUNF, with noise. For the temporal memory sequence
prediction, first and second order predictions are observed for a 5-number long
sequence generated from EUNF dataset and 95% accuracy is obtained. Moreover,
the proposed hardware architecture offers 1364X speedup over the software
realization. These results indicate that the proposed architecture can serve as
a digital core to build the HTM in hardware and eventually as a standalone
self-learning system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zyarah_A/0/1/0/all/0/1&quot;&gt;Abdullah M. Zyarah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kudithipudi_D/0/1/0/all/0/1&quot;&gt;Dhireesha Kudithipudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05622">
<title>Stories for Images-in-Sequence by using Visual and Narrative Components. (arXiv:1805.05622v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05622</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research in AI is focusing towards generating narrative stories about
visual scenes. It has the potential to achieve more human-like understanding
than just basic description generation of images- in-sequence. In this work, we
propose a solution for generating stories for images-in-sequence that is based
on the Sequence to Sequence model. As a novelty, our encoder model is composed
of two separate encoders, one that models the behaviour of the image sequence
and other that models the sentence-story generated for the previous image in
the sequence of images. By using the image sequence encoder we capture the
temporal dependencies between the image sequence and the sentence-story and by
using the previous sentence-story encoder we achieve a better story flow. Our
solution generates long human-like stories that not only describe the visual
context of the image sequence but also contains narrative and evaluative
language. The obtained results were confirmed by manual human evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smilevski_M/0/1/0/all/0/1&quot;&gt;Marko Smilevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalkovski_I/0/1/0/all/0/1&quot;&gt;Ilija Lalkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madjarov_G/0/1/0/all/0/1&quot;&gt;Gjorgi Madjarov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07107">
<title>Extending Dynamic Bayesian Networks for Anomaly Detection in Complex Logs. (arXiv:1805.07107v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07107</link>
<description rdf:parseType="Literal">&lt;p&gt;Checking various log files from different processes can be a tedious task as
these logs contain lots of events, each with a (possibly large) number of
attributes. We developed a way to automatically model log files and detect
outlier traces in the data. For that we extend Dynamic Bayesian Networks to
model the normal behavior found in log files. We introduce a new algorithm that
is able to learn a model of a log file starting from the data itself. The model
is capable of scoring traces even when new values or new combinations of values
appear in the log file.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauwels_S/0/1/0/all/0/1&quot;&gt;Stephen Pauwels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calders_T/0/1/0/all/0/1&quot;&gt;Toon Calders&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05344">
<title>Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model based on BLSTM. (arXiv:1808.05344v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05344</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, most of the objective speech quality assessment tools (e.g.,
perceptual evaluation of speech quality (PESQ)) are based on the comparison of
the degraded/processed speech with its clean counterpart. The need of a
&quot;golden&quot; reference considerably restricts the practicality of such assessment
tools in real-world scenarios since the clean reference usually cannot be
accessed. On the other hand, human beings can readily evaluate the speech
quality without any reference (e.g., mean opinion score (MOS) tests), implying
the existence of an objective and non-intrusive (no clean reference needed)
quality assessment mechanism. In this study, we propose a novel end-to-end,
non-intrusive speech quality evaluation model, termed Quality-Net, based on
bidirectional long short-term memory. The evaluation of utterance-level quality
in Quality-Net is based on the frame-level assessment. Frame constraints and
sensible initializations of forget gate biases are applied to learn meaningful
frame-level quality assessment from the utterance-level quality label.
Experimental results show that Quality-Net can yield high correlation to PESQ
(0.9 for the noisy speech and 0.84 for the speech processed by speech
enhancement). We believe that Quality-Net has potential to be used in a wide
variety of applications of speech signal processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hsin-Te Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hsin-Min Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05697">
<title>Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study. (arXiv:1808.05697v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.05697</link>
<description rdf:parseType="Literal">&lt;p&gt;Several recent papers investigate Active Learning (AL) for mitigating the
data dependence of deep learning for natural language processing. However, the
applicability of AL to real-world problems remains an open question. While in
supervised learning, practitioners can try many different methods, evaluating
each against a validation set before selecting a model, AL affords no such
luxury. Over the course of one AL run, an agent annotates its dataset
exhausting its labeling budget. Thus, given a new task, an active learner has
no opportunity to compare models and acquisition functions. This paper provides
a large scale empirical study of deep active learning, addressing multiple
tasks and, for each, multiple datasets, multiple models, and a full suite of
acquisition functions. We find that across all settings, Bayesian active
learning by disagreement, using uncertainty estimates provided either by
Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and
usually outperforms classic uncertainty sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1&quot;&gt;Aditya Siddhant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05760">
<title>Data Poisoning Attacks in Contextual Bandits. (arXiv:1808.05760v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05760</link>
<description rdf:parseType="Literal">&lt;p&gt;We study offline data poisoning attacks in contextual bandits, a class of
reinforcement learning problems with important applications in online
recommendation and adaptive medical treatment, among others. We provide a
general attack framework based on convex optimization and show that by slightly
manipulating rewards in the data, an attacker can force the bandit algorithm to
pull a target arm for a target contextual vector. The target arm and target
contextual vector are both chosen by the attacker. That is, the attacker can
hijack the behavior of a contextual bandit. We also investigate the feasibility
and the side effects of such attacks, and identify future directions for
defense. Experiments on both synthetic and real-world data demonstrate the
efficiency of the attack algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1&quot;&gt;Kwang-Sung Jun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lihong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaojin Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05784">
<title>Multiview Boosting by Controlling the Diversity and the Accuracy of View-specific Voters. (arXiv:1808.05784v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05784</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a boosting based multiview learning algorithm,
referred to as PB-MVBoost, which iteratively learns i) weights over
view-specific voters capturing view-specific information; and ii) weights over
views by optimizing a PAC-Bayes multiview C-Bound that takes into account the
accuracy of view-specific classifiers and the diversity between the views. We
derive a generalization bound for this strategy following the PAC-Bayes theory
which is a suitable tool to deal with models expressed as weighted combination
over a set of voters. Different experiments on three publicly available
datasets show the efficiency of the proposed approach with respect to
state-of-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anil Goyal&lt;/a&gt; (AMA, LHC), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morvant_E/0/1/0/all/0/1&quot;&gt;Emilie Morvant&lt;/a&gt; (LHC), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Germain_P/0/1/0/all/0/1&quot;&gt;Pascal Germain&lt;/a&gt; (MODAL), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amini_M/0/1/0/all/0/1&quot;&gt;Massih-Reza Amini&lt;/a&gt; (AMA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07025">
<title>Effective Representations of Clinical Notes. (arXiv:1705.07025v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07025</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical notes are a rich source of information about patient state. However,
using them to predict clinical events with machine learning models is
challenging. They are very high dimensional, sparse and have complex structure.
Furthermore, training data is often scarce because it is expensive to obtain
reliable labels for many clinical events. These difficulties have traditionally
been addressed by manual feature engineering encoding task specific domain
knowledge. We explored the use of neural networks and transfer learning to
learn representations of clinical notes that are useful for predicting future
clinical events of interest, such as all causes mortality, inpatient
admissions, and emergency room visits. Our data comprised 2.7 million notes and
115 thousand patients at Stanford Hospital. We used the learned
representations, along with commonly used bag of words and topic model
representations, as features for predictive models of clinical events. We
evaluated the effectiveness of these representations with respect to the
performance of the models trained on small datasets. Models using the neural
network derived representations performed significantly better than models
using the baseline representations with small ($N &amp;lt; 1000$) training datasets.
The learned representations offer significant performance gains over commonly
used baseline representations for a range of predictive modeling tasks and
cohort sizes, offering an effective alternative to task specific feature
engineering when plentiful labeled training data is not available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubois_S/0/1/0/all/0/1&quot;&gt;Sebastien Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Romano_N/0/1/0/all/0/1&quot;&gt;Nathanael Romano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kale_D/0/1/0/all/0/1&quot;&gt;David C. Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kenneth Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04659">
<title>Asynch-SGBDT: Asynchronous Parallel Stochastic Gradient Boosting Decision Tree based on Parameters Server. (arXiv:1804.04659v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04659</link>
<description rdf:parseType="Literal">&lt;p&gt;In AI research and industry, machine learning is the most widely used tool.
One of the most important machine learning algorithms is Gradient Boosting
Decision Tree, i.e. GBDT whose training process needs considerable
computational resources and time. To shorten GBDT training time, many works
tried to apply GBDT on Parameter Server. However, those GBDT algorithms are
synchronous parallel algorithms which fail to make full use of Parameter
Server. In this paper, we examine the possibility of using asynchronous
parallel methods to train GBDT model and name this algorithm as asynch-SGBDT
(asynchronous parallel stochastic gradient boosting decision tree). Our
theoretical and experimental results indicate that the scalability of
asynch-SGBDT is influenced by the sample diversity of datasets, sampling rate,
step length and the setting of GBDT tree. Experimental results also show
asynch-SGBDT training process reaches a linear speedup in asynchronous parallel
manner when datasets and GBDT trees meet high scalability requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daning_C/0/1/0/all/0/1&quot;&gt;Cheng Daning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fen_X/0/1/0/all/0/1&quot;&gt;Xia Fen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shigang_L/0/1/0/all/0/1&quot;&gt;Li Shigang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yunquan_Z/0/1/0/all/0/1&quot;&gt;Zhang Yunquan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>