<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02477"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02155"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02189"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02192"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02490"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01672"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01747"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1311.6876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02164"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02235"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02442"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02471"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08717"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09492"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07528"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.02477">
<title>Development of a sensory-neural network for medical diagnosing. (arXiv:1807.02477v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.02477</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance of a sensory-neural network developed for diagnosing of diseases
is described. Information about patient&apos;s condition is provided by answers to
the questionnaire. Questions correspond to sensors generating signals when
patients acknowledge symptoms. These signals excite neurons in which
characteristics of the diseases are represented by synaptic weights associated
with indicators of symptoms. The disease corresponding to the most excited
neuron is proposed as the result of diagnosing. Its reliability is estimated by
the likelihood defined by the ratio of excitation of the most excited neuron
and the complete neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabec_I/0/1/0/all/0/1&quot;&gt;Igor Grabec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svegl_E/0/1/0/all/0/1&quot;&gt;Eva &amp;#x160;vegl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sok_M/0/1/0/all/0/1&quot;&gt;Mihael Sok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09692">
<title>Been There, Done That: Meta-Learning with Episodic Recall. (arXiv:1805.09692v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09692</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-learning agents excel at rapidly learning new tasks from open-ended task
distributions; yet, they forget what they learn about each task as soon as the
next begins. When tasks reoccur - as they do in natural environments -
metalearning agents must explore again instead of immediately exploiting
previously discovered solutions. We propose a formalism for generating
open-ended yet repetitious environments, then develop a meta-learning
architecture for solving these environments. This architecture melds the
standard LSTM working memory with a differentiable neural episodic memory. We
explore the capabilities of agents with this episodic LSTM in five
meta-learning environments with reoccurring tasks, ranging from bandits to
navigation and stochastic sequential decision problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ritter_S/0/1/0/all/0/1&quot;&gt;Samuel Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jane X. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kurth_Nelson_Z/0/1/0/all/0/1&quot;&gt;Zeb Kurth-Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jayakumar_S/0/1/0/all/0/1&quot;&gt;Siddhant M. Jayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botvinick_M/0/1/0/all/0/1&quot;&gt;Matthew Botvinick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02155">
<title>Gridbot: An autonomous robot controlled by a Spiking Neural Network mimicking the brain&apos;s navigational system. (arXiv:1807.02155v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1807.02155</link>
<description rdf:parseType="Literal">&lt;p&gt;It is true that the &quot;best&quot; neural network is not necessarily the one with the
most &quot;brain-like&quot; behavior. Understanding biological intelligence, however, is
a fundamental goal for several distinct disciplines. Translating our
understanding of intelligence to machines is a fundamental problem in robotics.
Propelled by new advancements in Neuroscience, we developed a spiking neural
network (SNN) that draws from mounting experimental evidence that a number of
individual neurons is associated with spatial navigation. By following the
brain&apos;s structure, our model assumes no initial all-to-all connectivity, which
could inhibit its translation to a neuromorphic hardware, and learns an
uncharted territory by mapping its identified components into a limited number
of neural representations, through spike-timing dependent plasticity (STDP). In
our ongoing effort to employ a bioinspired SNN-controlled robot to real-world
spatial mapping applications, we demonstrate here how an SNN may robustly
control an autonomous robot in mapping and exploring an unknown environment,
while compensating for its own intrinsic hardware imperfections, such as
partial or total loss of visual input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_G/0/1/0/all/0/1&quot;&gt;Guangzhi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Michmizos_K/0/1/0/all/0/1&quot;&gt;Konstantinos P. Michmizos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02189">
<title>Functional Object-Oriented Network: Construction &amp; Expansion. (arXiv:1807.02189v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.02189</link>
<description rdf:parseType="Literal">&lt;p&gt;We build upon the functional object-oriented network (FOON), a structured
knowledge representation which is constructed from observations of human
activities and manipulations. A FOON can be used for representing object-motion
affordances. Knowledge retrieval through graph search allows us to obtain novel
manipulation sequences using knowledge spanning across many video sources,
hence the novelty in our approach. However, we are limited to the sources
collected. To further improve the performance of knowledge retrieval as a
follow up to our previous work, we discuss generalizing knowledge to be applied
to objects which are similar to what we have in FOON without manually
annotating new sources of knowledge. We discuss two means of generalization: 1)
expanding our network through the use of object similarity to create new
functional units from those we already have, and 2) compressing the functional
units by object categories rather than specific objects. We discuss experiments
which compare the performance of our knowledge retrieval algorithm with both
expansion and compression by categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulius_D/0/1/0/all/0/1&quot;&gt;David Paulius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jelodar_A/0/1/0/all/0/1&quot;&gt;Ahmad Babaeian Jelodar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02192">
<title>A Survey of Knowledge Representation and Retrieval for Learning in Service Robotics. (arXiv:1807.02192v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.02192</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the realm of service robotics, researchers have placed a great amount
of effort into learning motions and manipulations for task execution by robots.
The task of robot learning is very broad, as it involves many tasks such as
object detection, action recognition, motion planning, localization, knowledge
representation and retrieval, and the intertwining of computer vision and
machine learning techniques. In this paper, we focus on how knowledge can be
gathered, represented, and reproduced to solve problems as done by researchers
in the past decades. We discuss the problems which have existed in robot
learning and the solutions, technologies or developments (if any) which have
contributed to solving them. Specifically, we look at three broad categories
involved in task representation and retrieval for robotics: 1) activity
recognition from demonstrations, 2) scene understanding and interpretation, and
3) task representation in robotics - datasets and networks. Within each
section, we discuss major breakthroughs and how their methods address present
issues in robot learning and manipulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulius_D/0/1/0/all/0/1&quot;&gt;David Paulius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02254">
<title>Singing Style Transfer Using Cycle-Consistent Boundary Equilibrium Generative Adversarial Networks. (arXiv:1807.02254v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1807.02254</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we make a famous rap singer like Eminem sing whatever our favorite song?
Singing style transfer attempts to make this possible, by replacing the vocal
of a song from the source singer to the target singer. This paper presents a
method that learns from unpaired data for singing style transfer using
generative adversarial networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cheng-Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jen-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jyh-Shing R. Jang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02314">
<title>JUMPER: Learning When to Make Classification Decisions in Reading. (arXiv:1807.02314v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.02314</link>
<description rdf:parseType="Literal">&lt;p&gt;In early years, text classification is typically accomplished by
feature-based machine learning models; recently, deep neural networks, as a
powerful learning machine, make it possible to work with raw input as the text
stands. However, exiting end-to-end neural networks lack explicit
interpretation of the prediction. In this paper, we propose a novel framework,
JUMPER, inspired by the cognitive process of text reading, that models text
classification as a sequential decision process. Basically, JUMPER is a neural
system that scans a piece of text sequentially and makes classification
decisions at the time it wishes. Both the classification result and when to
make the classification are part of the decision process, which is controlled
by a policy network and trained with reinforcement learning. Experimental
results show that a properly trained JUMPER has the following properties: (1)
It can make decisions whenever the evidence is enough, therefore reducing total
text reading by 30-40% and often finding the key rationale of prediction. (2)
It achieves classification accuracy better than or comparable to
state-of-the-art models in several benchmark and industrial datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianggen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1&quot;&gt;Lili Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Haotian Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhengdong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sen Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02340">
<title>Oracle-free Detection of Translation Issue for Neural Machine Translation. (arXiv:1807.02340v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.02340</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Machine Translation (NMT) has been widely adopted over recent years
due to its advantages on various translation tasks. However, NMT systems can be
error-prone due to the intractability of natural languages and the design of
neural networks, bringing issues to their translations. These issues could
potentially lead to information loss, wrong semantics, and low readability in
translations, compromising the usefulness of NMT and leading to potential
non-trivial consequences. Although there are existing approaches, such as using
the BLEU score, on quality assessment and issue detection for NMT, such
approaches face two serious limitations. First, such solutions require oracle
translations, i.e., reference translations, which are often unavailable, e.g.,
in production environments. Second, such approaches cannot pinpoint the issue
types and locations within translations. To address such limitations, we
propose a new approach aiming to precisely detect issues in translations
without requiring oracle translations. Our approach focuses on two most
prominent issues in NMT translations by including two detection algorithms. Our
experimental results show that our new approach could achieve high
effectiveness on real-world datasets. Our successful experience on deploying
the proposed algorithms in both the development and production environments of
WeChat, a messenger app with over one billion of monthly active users, helps
eliminate numerous defects of our NMT model, monitor the effectiveness on
real-world translation tasks, and collect in-house test cases, producing high
industry impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wujie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changrong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Qinsong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yuetang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tao Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02383">
<title>Natural Language Processing for Information Extraction. (arXiv:1807.02383v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.02383</link>
<description rdf:parseType="Literal">&lt;p&gt;With rise of digital age, there is an explosion of information in the form of
news, articles, social media, and so on. Much of this data lies in unstructured
form and manually managing and effectively making use of it is tedious, boring
and labor intensive. This explosion of information and need for more
sophisticated and efficient information handling tools gives rise to
Information Extraction(IE) and Information Retrieval(IR) technology.
Information Extraction systems takes natural language text as input and
produces structured information specified by certain criteria, that is relevant
to a particular application. Various sub-tasks of IE such as Named Entity
Recognition, Coreference Resolution, Named Entity Linking, Relation Extraction,
Knowledge Base reasoning forms the building blocks of various high end Natural
Language Processing (NLP) tasks such as Machine Translation, Question-Answering
System, Natural Language Understanding, Text Summarization and Digital
Assistants like Siri, Cortana and Google Now. This paper introduces Information
Extraction technology, its various sub-tasks, highlights state-of-the-art
research in various IE subtasks, current challenges and future research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sonit Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02490">
<title>Deep Multiple Instance Feature Learning via Variational Autoencoder. (arXiv:1807.02490v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02490</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a novel weakly supervised deep learning framework that combines
both the discriminative and generative models to learn meaningful
representation in the multiple instance learning (MIL) setting. MIL is a weakly
supervised learning problem where labels are associated with groups of
instances (referred as bags) instead of individual instances. To address the
essential challenge in MIL problems raised from the uncertainty of positive
instances label, we use a discriminative model regularized by variational
autoencoders (VAEs) to maximize the differences between latent representations
of all instances and negative instances. As a result, the hidden layer of the
variational autoencoder learns meaningful representation. This representation
can effectively be used for MIL problems as illustrated by better performance
on the standard benchmark datasets comparing to the state-of-the-art
approaches. More importantly, unlike most related studies, the proposed
framework can be easily scaled to large dataset problems, as illustrated by the
audio event detection and segmentation task. Visualization also confirms the
effectiveness of the latent representation in discriminating positive and
negative classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaffarzadegan_S/0/1/0/all/0/1&quot;&gt;Shabnam Ghaffarzadegan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06064">
<title>A Meaning-based Statistical English Math Word Problem Solver. (arXiv:1803.06064v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06064</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MeSys, a meaning-based approach, for solving English math word
problems (MWPs) via understanding and reasoning in this paper. It first
analyzes the text, transforms both body and question parts into their
corresponding logic forms, and then performs inference on them. The associated
context of each quantity is represented with proposed role-tags (e.g., nsubj,
verb, etc.), which provides the flexibility for annotating an extracted math
quantity with its associated context information (i.e., the physical meaning of
this quantity). Statistical models are proposed to select the operator and
operands. A noisy dataset is designed to assess if a solver solves MWPs mainly
via understanding or mechanical pattern matching. Experimental results show
that our approach outperforms existing systems on both benchmark datasets and
the noisy dataset, which demonstrates that the proposed approach understands
the meaning of each quantity in the text more.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chao-Chun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1&quot;&gt;Yu-Shiang Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi-Chung Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Keh-Yih Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10188">
<title>Dialogue Modeling Via Hash Functions. (arXiv:1804.10188v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10188</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel machine-learning framework for dialogue modeling which
uses representations based on hash functions. More specifically, each person&apos;s
response is represented by a binary hashcode where each bit reflects presence
or absence of a certain text pattern in the response. Hashcodes serve as
compressed text representations, allowing for efficient similarity search.
Moreover, hashcode of one person&apos;s response can be used as a feature vector for
predicting the hashcode representing another person&apos;s response. The proposed
hashing model of dialogue is obtained by maximizing a novel lower bound on the
mutual information between the hashcodes of consecutive responses. We apply our
approach in psychotherapy domain evaluating its effectiveness on a real life
dataset consisting of therapy sessions with patients suffering from depression;
in addition, we also model transcripts of interview sessions between Larry King
(television host) and his guests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1&quot;&gt;Guillermo Cecchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shuyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1&quot;&gt;Sarik Ghazarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Palash Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1&quot;&gt;Aram Galstyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01672">
<title>Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization. (arXiv:1807.01672v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01672</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial self-play in two-player games has delivered impressive results
when used with reinforcement learning algorithms that combine deep neural
networks and tree search. Algorithms like AlphaZero and Expert Iteration learn
tabula-rasa, producing highly informative training data on the fly. However,
the self-play training strategy is not directly applicable to single-player
games. Recently, several practically important combinatorial optimization
problems, such as the traveling salesman problem and the bin packing problem,
have been reformulated as reinforcement learning problems, increasing the
importance of enabling the benefits of self-play beyond two-player games. We
present the Ranked Reward (R2) algorithm which accomplishes this by ranking the
rewards obtained by a single agent over multiple games to create a relative
performance metric. Results from applying the R2 algorithm to instances of a
two-dimensional bin packing problem show that it outperforms generic Monte
Carlo tree search, heuristic algorithms and reinforcement learning algorithms
not using ranked rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laterre_A/0/1/0/all/0/1&quot;&gt;Alexandre Laterre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yunguan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabri_M/0/1/0/all/0/1&quot;&gt;Mohamed Khalil Jabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Alain-Sam Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kas_D/0/1/0/all/0/1&quot;&gt;David Kas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajjar_K/0/1/0/all/0/1&quot;&gt;Karl Hajjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_T/0/1/0/all/0/1&quot;&gt;Torbjorn S. Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerkeni_A/0/1/0/all/0/1&quot;&gt;Amine Kerkeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beguir_K/0/1/0/all/0/1&quot;&gt;Karim Beguir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01747">
<title>Shannon entropy for intuitionistic fuzzy information. (arXiv:1807.01747v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01747</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents an extension of Shannon fuzzy entropy for intuitionistic
fuzzy one. Firstly, we presented a new formula for calculating the distance and
similarity of intuitionistic fuzzy information. Then, we constructed measures
for information features like score, certainty and uncertainty. Also, a new
concept was introduced, namely escort fuzzy information. Then, using the escort
fuzzy information, Shannon&apos;s formula for intuitionistic fuzzy information was
obtained. It should be underlined that Shannon&apos;s entropy for intuitionistic
fuzzy information verifies the four defining conditions of intuitionistic fuzzy
uncertainty. The measures of its two components were also identified: fuzziness
(ambiguity) and incompleteness (ignorance).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrascu_V/0/1/0/all/0/1&quot;&gt;Vasile Patrascu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1311.6876">
<title>Want a Good Answer? Ask a Good Question First!. (arXiv:1311.6876v1 [cs.DB] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1311.6876</link>
<description rdf:parseType="Literal">&lt;p&gt;Community Question Answering (CQA) websites have become valuable repositories
which host a massive volume of human knowledge. To maximize the utility of such
knowledge, it is essential to evaluate the quality of an existing question or
answer, especially soon after it is posted on the CQA website.
&lt;/p&gt;
&lt;p&gt;In this paper, we study the problem of inferring the quality of questions and
answers through a case study of a software CQA (Stack Overflow). Our key
finding is that the quality of an answer is strongly positively correlated with
that of its question. Armed with this observation, we propose a family of
algorithms to jointly predict the quality of questions and answers, for both
quantifying numerical quality scores and differentiating the high-quality
questions/answers from those of low quality. We conduct extensive experimental
evaluations to demonstrate the effectiveness and efficiency of our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1&quot;&gt;Hanghang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Feng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jian Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02164">
<title>V-CNN: When Convolutional Neural Network encounters Data Visualization. (arXiv:1807.02164v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1807.02164</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep learning poses a deep technical revolution in almost
every field and attracts great attentions from industry and academia.
Especially, the convolutional neural network (CNN), one representative model of
deep learning, achieves great successes in computer vision and natural language
processing. However, simply or blindly applying CNN to the other fields results
in lower training effects or makes it quite difficult to adjust the model
parameters. In this poster, we propose a general methodology named V-CNN by
introducing data visualizing for CNN. V-CNN introduces a data visualization
model prior to CNN modeling to make sure the data after processing is fit for
the features of images as well as CNN modeling. We apply V-CNN to the network
intrusion detection problem based on a famous practical dataset: AWID.
Simulation results confirm V-CNN significantly outperforms other studies and
the recall rate of each invasion category is more than 99.8%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1&quot;&gt;Guanxiong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhongjiang Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02234">
<title>Distributed Self-Paced Learning in Alternating Direction Method of Multipliers. (arXiv:1807.02234v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02234</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-paced learning (SPL) mimics the cognitive process of humans, who
generally learn from easy samples to hard ones. One key issue in SPL is the
training process required for each instance weight depends on the other samples
and thus cannot easily be run in a distributed manner in a large-scale dataset.
In this paper, we reformulate the self-paced learning problem into a
distributed setting and propose a novel Distributed Self-Paced Learning method
(DSPL) to handle large-scale datasets. Specifically, both the model and
instance weights can be optimized in parallel for each batch based on a
consensus alternating direction method of multipliers. We also prove the
convergence of our algorithm under mild conditions. Extensive experiments on
both synthetic and real datasets demonstrate that our approach is superior to
those of existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chang-Tien Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02235">
<title>Towards more Reliable Transfer Learning. (arXiv:1807.02235v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02235</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-source transfer learning has been proven effective when within-target
labeled data is scarce. Previous work focuses primarily on exploiting domain
similarities and assumes that source domains are richly or at least comparably
labeled. While this strong assumption is never true in practice, this paper
relaxes it and addresses challenges related to sources with diverse labeling
volume and diverse reliability. The first challenge is combining domain
similarity and source reliability by proposing a new transfer learning method
that utilizes both source-target similarities and inter-source relationships.
The second challenge involves pool-based active learning where the oracle is
only available in source domains, resulting in an integrated active transfer
learning framework that incorporates distribution matching and uncertainty
sampling. Extensive experiments on synthetic and two real-world datasets
clearly demonstrate the superiority of our proposed methods over several
baselines including state-of-the-art transfer learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonell_J/0/1/0/all/0/1&quot;&gt;Jaime Carbonell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02324">
<title>Sum-Product Networks for Sequence Labeling. (arXiv:1807.02324v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02324</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider higher-order linear-chain conditional random fields (HO-LC-CRFs)
for sequence modelling, and use sum-product networks (SPNs) for representing
higher-order input- and output-dependent factors. SPNs are a recently
introduced class of deep models for which exact and efficient inference can be
performed. By combining HO-LC-CRFs with SPNs, expressive models over both the
output labels and the hidden variables are instantiated while still enabling
efficient exact inference. Furthermore, the use of higher-order factors allows
us to capture relations of multiple input segments and multiple output labels
as often present in real-world data. These relations can not be modelled by the
commonly used first-order models and higher-order models with local factors
including only a single output label. We demonstrate the effectiveness of our
proposed models for sequence labeling. In extensive experiments, we outperform
other state-of-the-art methods in optical character recognition and achieve
competitive results in phone classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratajczak_M/0/1/0/all/0/1&quot;&gt;Martin Ratajczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschiatschek_S/0/1/0/all/0/1&quot;&gt;Sebastian Tschiatschek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pernkopf_F/0/1/0/all/0/1&quot;&gt;Franz Pernkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02373">
<title>Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes. (arXiv:1807.02373v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02373</link>
<description rdf:parseType="Literal">&lt;p&gt;While designing the state space of an MDP, it is common to include states
that are transient or not reachable by any policy (e.g., in mountain car, the
product space of speed and position contains configurations that are not
physically reachable). This leads to defining weakly-communicating or
multi-chain MDPs. In this paper, we introduce \tucrl, the first algorithm able
to perform efficient exploration-exploitation in any finite Markov Decision
Process (MDP) without requiring any form of prior knowledge. In particular, for
any MDP with $S^{\texttt{C}}$ communicating states, $A$ actions and
$\Gamma^{\texttt{C}} \leq S^{\texttt{C}}$ possible communicating next states,
we derive a $\widetilde{O}(D^{\texttt{C}} \sqrt{\Gamma^{\texttt{C}}
S^{\texttt{C}} AT})$ regret bound, where $D^{\texttt{C}}$ is the diameter
(i.e., the longest shortest path) of the communicating part of the MDP. This is
in contrast with optimistic algorithms (e.g., UCRL, Optimistic PSRL) that
suffer linear regret in weakly-communicating MDPs, as well as posterior
sampling or regularised algorithms (e.g., REGAL), which require prior knowledge
on the bias span of the optimal policy to bias the exploration to achieve
sub-linear regret. We also prove that in weakly-communicating MDPs, no
algorithm can ever achieve a logarithmic growth of the regret without first
suffering a linear regret for a number of steps that is exponential in the
parameters of the MDP. Finally, we report numerical simulations supporting our
theoretical findings and showing how TUCRL overcomes the limitations of the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fruit_R/0/1/0/all/0/1&quot;&gt;Ronan Fruit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1&quot;&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1&quot;&gt;Alessandro Lazaric&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02374">
<title>A Structured Prediction Approach for Label Ranking. (arXiv:1807.02374v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02374</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to solve a label ranking problem as a structured output regression
task. We adopt a least square surrogate loss approach that solves a supervised
learning problem in two steps: the regression step in a well-chosen feature
space and the pre-image step. We use specific feature maps/embeddings for
ranking data, which convert any ranking/permutation into a vector
representation. These embeddings are all well-tailored for our approach, either
by resulting in consistent estimators, or by solving trivially the pre-image
problem which is often the bottleneck in structured prediction. We also propose
their natural extension to the case of partial rankings and prove their
efficiency on real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Korba_A/0/1/0/all/0/1&quot;&gt;Anna Korba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Alexandre Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buc_F/0/1/0/all/0/1&quot;&gt;Florence d&amp;#x27;Alch&amp;#xe9; Buc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02391">
<title>Extracting Actionable Knowledge from Domestic Violence Discourses on Social Media. (arXiv:1807.02391v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.02391</link>
<description rdf:parseType="Literal">&lt;p&gt;Domestic Violence (DV) is considered as big social issue and there exists a
strong relationship between DV and health impacts of the public. Existing
research studies have focused on social media to track and analyse real world
events like emerging trends, natural disasters, user sentiment analysis,
political opinions, and health care. However there is less attention given on
social welfare issues like DV and its impact on public health. Recently, the
victims of DV turned to social media platforms to express their feelings in the
form of posts and seek the social and emotional support, for sympathetic
encouragement, to show compassion and empathy among public. But, it is
difficult to mine the actionable knowledge from large conversational datasets
from social media due to the characteristics of high dimensions, short, noisy,
huge volume, high velocity, and so on. Hence, this paper will propose a novel
framework to model and discover the various themes related to DV from the
public domain. The proposed framework would possibly provide unprecedentedly
valuable information to the public health researchers, national family health
organizations, government and public with data enrichment and consolidation to
improve the social welfare of the community. Thus provides actionable knowledge
by monitoring and analysing continuous and rich user generated content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramani_S/0/1/0/all/0/1&quot;&gt;Sudha Subramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_M/0/1/0/all/0/1&quot;&gt;Manjula O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02442">
<title>Multi-Task Learning with Incomplete Data for Healthcare. (arXiv:1807.02442v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02442</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning is a type of transfer learning that trains multiple tasks
simultaneously and leverages the shared information between related tasks to
improve the generalization performance. However, missing features in the input
matrix is a much more difficult problem which needs to be carefully addressed.
Removing records with missing values can significantly reduce the sample size,
which is impractical for datasets with large percentage of missing values.
Popular imputation methods often distort the covariance structure of the data,
which causes inaccurate inference. In this paper we propose using plug-in
covariance matrix estimators to tackle the challenge of missing features.
Specifically, we analyze the plug-in estimators under the framework of robust
multi-task learning with LASSO and graph regularization, which captures the
relatedness between tasks via graph regularization. We use the Alzheimer&apos;s
disease progression dataset as an example to show how the proposed framework is
effective for prediction and model estimation when missing data is present.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hunt_X/0/1/0/all/0/1&quot;&gt;Xin J. Hunt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Emrani_S/0/1/0/all/0/1&quot;&gt;Saba Emrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kabul_I/0/1/0/all/0/1&quot;&gt;Ilknur Kaynar Kabul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Jorge Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02471">
<title>A Review of Different Word Embeddings for Sentiment Classification using Deep Learning. (arXiv:1807.02471v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.02471</link>
<description rdf:parseType="Literal">&lt;p&gt;The web is loaded with textual content, and Natural Language Processing is a
standout amongst the most vital fields in Machine Learning. But when data is
huge simple Machine Learning algorithms are not able to handle it and that is
when Deep Learning comes into play which based on Neural Networks. However
since neural networks cannot process raw text, we have to change over them
through some diverse strategies of word embedding. This paper demonstrates
those distinctive word embedding strategies implemented on an Amazon Review
Dataset, which has two sentiments to be classified: Happy and Unhappy based on
numerous customer reviews. Moreover we demonstrate the distinction in accuracy
with a discourse about which word embedding to apply when.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_D/0/1/0/all/0/1&quot;&gt;Debadri Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08717">
<title>Auto-Differentiating Linear Algebra. (arXiv:1710.08717v3 [cs.MS] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08717</link>
<description rdf:parseType="Literal">&lt;p&gt;Development systems for deep learning (DL), such as Theano, Torch,
TensorFlow, or MXNet, are easy-to-use tools for creating complex neural network
models. Since gradient computations are automatically baked in, and execution
is mapped to high performance hardware, these models can be trained end-to-end
on large amounts of data. However, it is currently not easy to implement many
basic machine learning primitives in these systems (such as Gaussian processes,
least squares estimation, principal components analysis, Kalman smoothing),
mainly because they lack efficient support of linear algebra primitives as
differentiable operators. We detail how a number of matrix decompositions
(Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators.
We have implemented these primitives in MXNet, running on CPU and GPU in single
and double precision. We sketch use cases of these new operators, learning
Gaussian process and Bayesian linear regression models, where we demonstrate
very substantial reductions in implementation complexity and running time
compared to previous codes. Our MXNet extension allows end-to-end learning of
hybrid models, which combine deep neural networks (DNNs) with Bayesian
concepts, with applications in advanced Gaussian process models, scalable
Bayesian optimization, and Bayesian active learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seeger_M/0/1/0/all/0/1&quot;&gt;Matthias Seeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hetzel_A/0/1/0/all/0/1&quot;&gt;Asmus Hetzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zhenwen Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meissner_E/0/1/0/all/0/1&quot;&gt;Eric Meissner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawrence_N/0/1/0/all/0/1&quot;&gt;Neil D. Lawrence&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09492">
<title>Robust Subspace Learning: Robust PCA, Robust Subspace Tracking, and Robust Subspace Recovery. (arXiv:1711.09492v4 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09492</link>
<description rdf:parseType="Literal">&lt;p&gt;PCA is one of the most widely used dimension reduction techniques. A related
easier problem is &quot;subspace learning&quot; or &quot;subspace estimation&quot;. Given
relatively clean data, both are easily solved via singular value decomposition
(SVD). The problem of subspace learning or PCA in the presence of outliers is
called robust subspace learning or robust PCA (RPCA). For long data sequences,
if one tries to use a single lower dimensional subspace to represent the data,
the required subspace dimension may end up being quite large. For such data, a
better model is to assume that it lies in a low-dimensional subspace that can
change over time, albeit gradually. The problem of tracking such data (and the
subspaces) while being robust to outliers is called robust subspace tracking
(RST). This article provides a magazine-style overview of the entire field of
robust subspace learning and tracking. In particular solutions for three
problems are discussed in detail: RPCA via sparse+low-rank matrix decomposition
(S+LR), RST via S+LR, and &quot;robust subspace recovery (RSR)&quot;. RSR assumes that an
entire data vector is either an outlier or an inlier. The S+LR formulation
instead assumes that outliers occur on only a few data vector indices and hence
are well modeled as sparse corruptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1&quot;&gt;Namrata Vaswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouwmans_T/0/1/0/all/0/1&quot;&gt;Thierry Bouwmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1&quot;&gt;Sajid Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1&quot;&gt;Praneeth Narayanamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01013">
<title>An Overview of Robust Subspace Recovery. (arXiv:1803.01013v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01013</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper will serve as an introduction to the body of work on robust
subspace recovery. Robust subspace recovery involves finding an underlying
low-dimensional subspace in a dataset that is possibly corrupted with outliers.
While this problem is easy to state, it has been difficult to develop optimal
algorithms due to its underlying nonconvexity. This work emphasizes advantages
and disadvantages of proposed approaches and unsolved problems in the area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerman_G/0/1/0/all/0/1&quot;&gt;Gilad Lerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maunu_T/0/1/0/all/0/1&quot;&gt;Tyler Maunu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07528">
<title>Uncertainty in Multitask Transfer Learning. (arXiv:1806.07528v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;Using variational Bayes neural networks, we develop an algorithm capable of
accumulating knowledge into a prior from multiple different tasks. The result
is a rich and meaningful prior capable of few-shot learning on new tasks. The
posterior can go beyond the mean field approximation and yields good
uncertainty on the performed experiments. Analysis on toy tasks shows that it
can learn from significantly different tasks while finding similarities among
them. Experiments of Mini-Imagenet yields the new state of the art with 74.5%
accuracy on 5 shot learning. Finally, we provide experiments showing that other
existing methods can fail to perform well in different benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oreshkin_B/0/1/0/all/0/1&quot;&gt;Boris Oreshkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chung_W/0/1/0/all/0/1&quot;&gt;Wonchang Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boquet_T/0/1/0/all/0/1&quot;&gt;Thomas Boquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rostamzadeh_N/0/1/0/all/0/1&quot;&gt;Negar Rostamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;</dc:creator>
</item></rdf:RDF>