<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02674"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04517"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02643"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02798"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.03423"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10130"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.01096">
<title>Software Engineers vs. Machine Learning Algorithms: An Empirical Study Assessing Performance and Reuse Tasks. (arXiv:1802.01096v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01096</link>
<description rdf:parseType="Literal">&lt;p&gt;Several papers have recently contained reports on applying machine learning
(ML) to the automation of software engineering (SE) tasks, such as project
management, modeling and development. However, there appear to be no approaches
comparing how software engineers fare against machine-learning algorithms as
applied to specific software development tasks. Such a comparison is essential
to gain insight into which tasks are better performed by humans and which by
machine learning and how cooperative work or human-in-the-loop processes can be
implemented more effectively. In this paper, we present an empirical study that
compares how software engineers and machine-learning algorithms perform and
reuse tasks. The empirical study involves the synthesis of the control
structure of an autonomous streetlight application. Our approach consists of
four steps. First, we solved the problem using machine learning to determine
specific performance and reuse tasks. Second, we asked software engineers with
different domain knowledge levels to provide a solution to the same tasks.
Third, we compared how software engineers fare against machine-learning
algorithms when accomplishing the performance and reuse tasks based on criteria
such as energy consumption and safety. Finally, we analyzed the results to
understand which tasks are better performed by either humans or algorithms so
that they can work together more effectively. Such an understanding and the
resulting human-in-the-loop approaches, which take into account the strengths
and weaknesses of humans and machine-learning algorithms, are fundamental not
only to provide a basis for cooperative work in support of software
engineering, but also, in other areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_N/0/1/0/all/0/1&quot;&gt;Nathalia Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucena_C/0/1/0/all/0/1&quot;&gt;Carlos Lucena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alencar_P/0/1/0/all/0/1&quot;&gt;Paulo Alencar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cowan_D/0/1/0/all/0/1&quot;&gt;Donald Cowan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02674">
<title>Efficient collective swimming by harnessing vortices through deep reinforcement learning. (arXiv:1802.02674v1 [physics.flu-dyn])</title>
<link>http://arxiv.org/abs/1802.02674</link>
<description rdf:parseType="Literal">&lt;p&gt;Fish in schooling formations navigate complex flow-fields replete with
mechanical energy in the vortex wakes of their companions. Their schooling
behaviour has been associated with evolutionary advantages including collective
energy savings. How fish harvest energy from their complex fluid environment
and the underlying physical mechanisms governing energy-extraction during
collective swimming, is still unknown. Here we show that fish can improve their
sustained propulsive efficiency by actively following, and judiciously
intercepting, vortices in the wake of other swimmers. This swimming strategy
leads to collective energy-savings and is revealed through the first ever
combination of deep reinforcement learning with high-fidelity flow simulations.
We find that a `smart-swimmer&apos; can adapt its position and body deformation to
synchronise with the momentum of the oncoming vortices, improving its average
swimming-efficiency at no cost to the leader. The results show that fish may
harvest energy deposited in vortices produced by their peers, and support the
conjecture that swimming in formation is energetically advantageous. Moreover,
this study demonstrates that deep reinforcement learning can produce navigation
algorithms for complex flow-fields, with promising implications for energy
savings in autonomous robotic swarms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Siddhartha Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Novati_G/0/1/0/all/0/1&quot;&gt;Guido Novati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Koumoutsakos_P/0/1/0/all/0/1&quot;&gt;Petros Koumoutsakos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02732">
<title>The Higher-Order Prover Leo-III. (arXiv:1802.02732v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.02732</link>
<description rdf:parseType="Literal">&lt;p&gt;The automated theorem prover Leo-III for classical higher-order logic with
Henkin semantics and choice is presented. Leo-III is based on extensional
higher-order paramodulation and accepts every common TPTP dialect (FOF, TFF,
THF), including their recent extensions to rank-1 polymorphism (TF1, TH1). In
addition, the prover natively supports almost every normal higher-order modal
logic. Leo-III cooperates with first-order reasoning tools using translations
to (polymorphic) many-sorted first-order logic and produces verifiable proof
certificates. The prover is evaluated on heterogeneous benchmark sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steen_A/0/1/0/all/0/1&quot;&gt;Alexander Steen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1&quot;&gt;Christoph Benzm&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02896">
<title>Learning Role-based Graph Embeddings. (arXiv:1802.02896v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02896</link>
<description rdf:parseType="Literal">&lt;p&gt;Random walks are at the heart of many existing network embedding methods.
However, such algorithms have many limitations that arise from the use of
random walks, e.g., the features resulting from these methods are unable to
transfer to new nodes and graphs as they are tied to vertex identity. In this
work, we introduce the Role2Vec framework which uses the flexible notion of
attributed random walks, and serves as a basis for generalizing existing
methods such as DeepWalk, node2vec, and many others that leverage random walks.
Our proposed framework enables these methods to be more widely applicable for
both transductive and inductive learning as well as for use on graphs with
attributes (if available). This is achieved by learning functions that
generalize to new nodes and graphs. We show that our proposed framework is
effective with an average AUC improvement of 16:55% while requiring on average
853x less space than existing methods on a variety of graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen K. Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;John Boaz Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangnan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Willke_T/0/1/0/all/0/1&quot;&gt;Theodore L. Willke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Rong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eldardiry_H/0/1/0/all/0/1&quot;&gt;Hoda Eldardiry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00868">
<title>Active learning machine learns to create new quantum experiments. (arXiv:1706.00868v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00868</link>
<description rdf:parseType="Literal">&lt;p&gt;How useful can machine learning be in a quantum laboratory? Here we raise the
question of the potential of intelligent machines in the context of scientific
research. A major motivation for the present work is the unknown reachability
of various entanglement classes in quantum experiments. We investigate this
question by using the projective simulation model, a physics-oriented approach
to artificial intelligence. In our approach, the projective simulation system
is challenged to design complex photonic quantum experiments that produce
high-dimensional entangled multiphoton states, which are of high interest in
modern quantum experiments. The artificial intelligence system learns to create
a variety of entangled states, and improves the efficiency of their
realization. In the process, the system autonomously (re)discovers experimental
techniques which are only now becoming standard in modern quantum optical
experiments - a trait which was not explicitly demanded from the system but
emerged through the process of learning. Such features highlight the
possibility that machines could have a significantly more creative role in
future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Melnikov_A/0/1/0/all/0/1&quot;&gt;Alexey A. Melnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Nautrup_H/0/1/0/all/0/1&quot;&gt;Hendrik Poulsen Nautrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Krenn_M/0/1/0/all/0/1&quot;&gt;Mario Krenn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1&quot;&gt;Vedran Dunjko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Tiersch_M/0/1/0/all/0/1&quot;&gt;Markus Tiersch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zeilinger_A/0/1/0/all/0/1&quot;&gt;Anton Zeilinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Briegel_H/0/1/0/all/0/1&quot;&gt;Hans J. Briegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04517">
<title>Visualizations for an Explainable Planning Agent. (arXiv:1709.04517v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04517</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we report on the visualization capabilities of an Explainable
AI Planning (XAIP) agent that can support human in the loop decision making.
Imposing transparency and explainability requirements on such agents is
especially important in order to establish trust and common ground with the
end-to-end automated planning system. Visualizing the agent&apos;s internal
decision-making processes is a crucial step towards achieving this. This may
include externalizing the &quot;brain&quot; of the agent -- starting from its sensory
inputs, to progressively higher order decisions made by it in order to drive
its planning components. We also show how the planner can bootstrap on the
latest techniques in explainable planning to cast plan visualization as a plan
explanation problem, and thus provide concise model-based visualization of its
plans. We demonstrate these functionalities in the context of the automated
planning components of a smart assistant in an instrumented meeting space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1&quot;&gt;Tathagata Chakraborti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadnis_K/0/1/0/all/0/1&quot;&gt;Kshitij P. Fadnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talamadupula_K/0/1/0/all/0/1&quot;&gt;Kartik Talamadupula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dholakia_M/0/1/0/all/0/1&quot;&gt;Mishal Dholakia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Biplav Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kephart_J/0/1/0/all/0/1&quot;&gt;Jeffrey O. Kephart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellamy_R/0/1/0/all/0/1&quot;&gt;Rachel K. E. Bellamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11277">
<title>Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning. (arXiv:1710.11277v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11277</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new method --- adversarial advantage actor-critic
(Adversarial A2C), which significantly improves the efficiency of dialogue
policy learning in task-completion dialogue systems. Inspired by generative
adversarial networks (GAN), we train a discriminator to differentiate
responses/actions generated by dialogue agents from responses/actions by
experts. Then, we incorporate the discriminator as another critic into the
advantage actor-critic (A2C) framework, to encourage the dialogue agent to
explore state-action within the regions where the agent takes actions similar
to those of the experts. Experimental results in a movie-ticket booking domain
show that the proposed Adversarial A2C can accelerate policy exploration
efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Baolin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun-Nung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kam-Fai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02617">
<title>Recognition of Acoustic Events Using Masked Conditional Neural Networks. (arXiv:1802.02617v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.02617</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic feature extraction using neural networks has accomplished
remarkable success for images, but for sound recognition, these models are
usually modified to fit the nature of the multi-dimensional temporal
representation of the audio signal in spectrograms. This may not efficiently
harness the time-frequency representation of the signal. The ConditionaL Neural
Network (CLNN) takes into consideration the interrelation between the temporal
frames, and the Masked ConditionaL Neural Network (MCLNN) extends upon the CLNN
by forcing a systematic sparseness over the network&apos;s weights using a binary
mask. The masking allows the network to learn about frequency bands rather than
bins, mimicking a filterbank used in signal transformations such as MFCC.
Additionally, the Mask is designed to consider various combinations of
features, which automates the feature hand-crafting process. We applied the
MCLNN for the Environmental Sound Recognition problem using the Urbansound8k,
YorNoise, ESC-10 and ESC-50 datasets. The MCLNN have achieved competitive
performance compared to state-of-the-art Convolutional Neural Networks and
hand-crafted attempts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medhat_F/0/1/0/all/0/1&quot;&gt;Fady Medhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chesmore_D/0/1/0/all/0/1&quot;&gt;David Chesmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1&quot;&gt;John Robinson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02643">
<title>Gradient conjugate priors and deep neural networks. (arXiv:1802.02643v1 [math.ST])</title>
<link>http://arxiv.org/abs/1802.02643</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper deals with learning the probability distribution of the observed
data by artificial neural networks. We suggest a so-called gradient conjugate
prior (GCP) update appropriate for neural networks, which is a modification of
the classical Bayesian update for conjugate priors. We establish a connection
between the gradient conjugate prior update and the maximization of the
log-likelihood of the predictive distribution. Unlike for the Bayesian neural
networks, we do not impose a prior on the weights of the neural networks, but
rather assume that the ground truth distribution is normal with unknown mean
and variance and learn by neural networks the parameters of a prior
(normal-gamma distribution) for these unknown mean and variance. The update of
the parameters is done, using the gradient that, at each step, directs towards
minimizing the Kullback--Leibler divergence from the prior to the posterior
distribution (both being normal-gamma). We obtain a corresponding dynamical
system for the prior&apos;s parameters and analyze its properties. In particular, we
study the limiting behavior of all the prior&apos;s parameters and show how it
differs from the case of the classical full Bayesian update. The results are
validated on synthetic and real world data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gurevich_P/0/1/0/all/0/1&quot;&gt;Pavel Gurevich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stuke_H/0/1/0/all/0/1&quot;&gt;Hannes Stuke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02664">
<title>Geometry Score: A Method For Comparing Generative Adversarial Networks. (arXiv:1802.02664v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.02664</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the biggest challenges in the research of generative adversarial
networks (GANs) is assessing the quality of generated samples and detecting
various levels of mode collapse. In this work, we construct a novel measure of
performance of a GAN by comparing geometrical properties of the underlying data
manifold and the generated one, which provides both qualitative and
quantitative means for evaluation. Our algorithm can be applied to datasets of
an arbitrary nature and is not limited to visual data. We test the obtained
metric on various real-life models and datasets and demonstrate that our method
provides new insights into properties of GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1&quot;&gt;Valentin Khrulkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02798">
<title>Transductive Adversarial Networks (TAN). (arXiv:1802.02798v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02798</link>
<description rdf:parseType="Literal">&lt;p&gt;Transductive Adversarial Networks (TAN) is a novel domain-adaptation machine
learning framework that is designed for learning a conditional probability
distribution on unlabelled input data in a target domain, while also only
having access to: (1) easily obtained labelled data from a related source
domain, which may have a different conditional probability distribution than
the target domain, and (2) a marginalised prior distribution on the labels for
the target domain. TAN leverages a fully adversarial training procedure and a
unique generator/encoder architecture which approximates the transductive
combination of the available source- and target-domain data. A benefit of TAN
is that it allows the distance between the source- and target-domain
label-vector marginal probability distributions to be greater than 0 (i.e.
different tasks across the source and target domains) whereas other
domain-adaptation algorithms require this distance to equal 0 (i.e. a single
task across the source and target domains). TAN can, however, still handle the
latter case and is a more generalised approach to this case. Another benefit of
TAN is that due to being a fully adversarial algorithm, it has the potential to
accurately approximate highly complex distributions. Theoretical analysis
demonstrates the viability of the TAN framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowan_S/0/1/0/all/0/1&quot;&gt;Sean Rowan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02961">
<title>Learning Sparse Wavelet Representations. (arXiv:1802.02961v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.02961</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose a method for learning wavelet filters directly from
data. We accomplish this by framing the discrete wavelet transform as a
modified convolutional neural network. We introduce an autoencoder wavelet
transform network that is trained using gradient descent. We show that the
model is capable of learning structured wavelet filters from synthetic and real
data. The learned wavelets are shown to be similar to traditional wavelets that
are derived using Fourier methods. Our method is simple to implement and easily
incorporated into neural network architectures. A major advantage to our model
is that we can learn from raw audio data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Recoskie_D/0/1/0/all/0/1&quot;&gt;Daniel Recoskie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_R/0/1/0/all/0/1&quot;&gt;Richard Mann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.03423">
<title>Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks. (arXiv:1709.03423v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.03423</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has become the state of the art approach in many machine
learning problems such as classification. It has recently been shown that deep
learning is highly vulnerable to adversarial perturbations. Taking the camera
systems of self-driving cars as an example, small adversarial perturbations can
cause the system to make errors in important tasks, such as classifying traffic
signs or detecting pedestrians. Hence, in order to use deep learning without
safety concerns a proper defense strategy is required. We propose to use
ensemble methods as a defense strategy against adversarial perturbations. We
find that an attack leading one model to misclassify does not imply the same
for other networks performing the same task. This makes ensemble methods an
attractive defense strategy against adversarial attacks. We empirically show
for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve
the accuracy of neural networks on test data but also increase their robustness
against adversarial perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strauss_T/0/1/0/all/0/1&quot;&gt;Thilo Strauss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanselmann_M/0/1/0/all/0/1&quot;&gt;Markus Hanselmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Junginger_A/0/1/0/all/0/1&quot;&gt;Andrej Junginger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ulmer_H/0/1/0/all/0/1&quot;&gt;Holger Ulmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10130">
<title>Spherical CNNs. (arXiv:1801.10130v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10130</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have become the method of choice for
learning problems involving 2D planar images. However, a number of problems of
recent interest have created a demand for models that can analyze spherical
images. Examples include omnidirectional vision for drones, robots, and
autonomous cars, molecular regression problems, and global weather and climate
modelling. A naive application of convolutional networks to a planar projection
of the spherical signal is destined to fail, because the space-varying
distortions introduced by such a projection will make translational weight
sharing ineffective.
&lt;/p&gt;
&lt;p&gt;In this paper we introduce the building blocks for constructing spherical
CNNs. We propose a definition for the spherical cross-correlation that is both
expressive and rotation-equivariant. The spherical correlation satisfies a
generalized Fourier theorem, which allows us to compute it efficiently using a
generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We
demonstrate the computational efficiency, numerical accuracy, and effectiveness
of spherical CNNs applied to 3D model recognition and atomization energy
regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco S. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1&quot;&gt;Mario Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_J/0/1/0/all/0/1&quot;&gt;Jonas Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item></rdf:RDF>