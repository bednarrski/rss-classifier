<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09859"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.02320"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.04058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09817"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05116"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07139"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09812"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10023"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10080"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.01234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07172"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.09859">
<title>Competitive Learning Enriches Learning Representation and Accelerates the Fine-tuning of CNNs. (arXiv:1804.09859v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09859</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose the integration of competitive learning into
convolutional neural networks (CNNs) to improve the representation learning and
efficiency of fine-tuning. Conventional CNNs use back propagation learning, and
it enables powerful representation learning by a discrimination task. However,
it requires huge amount of labeled data, and acquisition of labeled data is
much harder than that of unlabeled data. Thus, efficient use of unlabeled data
is getting crucial for DNNs. To address the problem, we introduce unsupervised
competitive learning into the convolutional layer, and utilize unlabeled data
for effective representation learning. The results of validation experiments
using a toy model demonstrated that strong representation learning effectively
extracted bases of images into convolutional filters using unlabeled data, and
accelerated the speed of the fine-tuning of subsequent supervised back
propagation learning. The leverage was more apparent when the number of filters
was sufficiently large, and, in such a case, the error rate steeply decreased
in the initial phase of fine-tuning. Thus, the proposed method enlarged the
number of filters in CNNs, and enabled a more detailed and generalized
representation. It could provide a possibility of not only deep but broad
neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1&quot;&gt;Takashi Shinozaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09862">
<title>Accelerator-Aware Pruning for Convolutional Neural Networks. (arXiv:1804.09862v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.09862</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks have shown tremendous performance in computer
vision tasks,but their excessive amount of weights and operations prevent them
from being adopted in embedded environments. One of the solutions involves
pruning, where some unimportant weights are forced to be zero. Many pruning
schemes have been proposed, but have focused mainly on the number of pruned
weights. The previous pruning schemes hardly considered ASIC or FPGA
accelerator architectures. When the pruned networks are run on the
accelerators, the lack of architecture consideration casues some inefficiency
problems including internal buffer mis-alignment and load imbalance. This paper
proposes a new pruning scheme that reflects accelerator architectures. In the
proposed scheme, pruning is performed so that the same number of weights remain
for each weight group corresponding to activations fetched simultaneously. In
this way, the pruning scheme resolves the inefficiency problems. Even with the
constraint, the proposed pruning scheme reached a pruning ratio similar to that
of the previous unconstrained pruning schemes not only in AlexNet and VGG16 but
also in the state-of-the-art very-deep networks like ResNet. Furthermore, the
proposed scheme demonstrated a comparable pruning ratio in slimmed networks
that were already pruned channel-wisely. In addition to improving the
efficiency of previous sparse accelerators, it will be also shown that the
proposed pruning scheme can be used to reduce the logic complexity of sparse
accelerators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hyeong-Ju Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10200">
<title>The loss landscape of overparameterized neural networks. (arXiv:1804.10200v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10200</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore some mathematical features of the loss landscape of
overparameterized neural networks. A priori one might imagine that the loss
function looks like a typical function from $\mathbb{R}^n$ to $\mathbb{R}$ - in
particular, nonconvex, with discrete global minima. In this paper, we prove
that in at least one important way, the loss function of an overparameterized
neural network does not look like a typical function. If a neural net has $n$
parameters and is trained on $d$ data points, with $n&amp;gt;d$, we show that the
locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$
dimensional submanifold of $\mathbb{R}^n$. In practice, neural nets commonly
have orders of magnitude more parameters than data points, so this observation
implies that $M$ is typically a very high-dimensional subset of $\mathbb{R}^n$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_Y/0/1/0/all/0/1&quot;&gt;Y Cooper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.02320">
<title>Adversarial Ladder Networks. (arXiv:1611.02320v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1611.02320</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of unsupervised data in addition to supervised data in training
discriminative neural networks has improved the performance of this clas-
sification scheme. However, the best results were achieved with a training
process that is divided in two parts: first an unsupervised pre-training step
is done for initializing the weights of the network and after these weights are
refined with the use of supervised data. On the other hand adversarial noise
has improved the results of clas- sical supervised learning. Recently, a new
neural network topology called Ladder Network, where the key idea is based in
some properties of hierar- chichal latent variable models, has been proposed as
a technique to train a neural network using supervised and unsupervised data at
the same time with what is called semi-supervised learning. This technique has
reached state of the art classification. In this work we add adversarial noise
to the ladder network and get state of the art classification, with several
important conclusions on how adversarial noise can help in addition with new
possible lines of investi- gation. We also propose an alternative to add
adversarial noise to unsu- pervised data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molano_J/0/1/0/all/0/1&quot;&gt;Juan Maro&amp;#xf1;as Molano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colomer_A/0/1/0/all/0/1&quot;&gt;Alberto Albiol Colomer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacios_R/0/1/0/all/0/1&quot;&gt;Roberto Paredes Palacios&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.04058">
<title>Neural Style Transfer: A Review. (arXiv:1705.04058v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1705.04058</link>
<description rdf:parseType="Literal">&lt;p&gt;The seminal work of Gatys et al. demonstrated the power of Convolutional
Neural Networks (CNN) in creating artistic imagery by separating and
recombining image content and style. This process of using CNN to render a
content image in different styles is referred to as Neural Style Transfer
(NST). Since then, NST has become a trending topic both in academic literature
and industrial applications. It is receiving increasing attention and a variety
of approaches are proposed to either improve or extend the original NST
algorithm. This review aims to provide an overview of the current progress
towards NST, as well as discussing its various applications and open problems
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1&quot;&gt;Yongcheng Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yezhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zunlei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jingwen Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09817">
<title>Multiagent Soft Q-Learning. (arXiv:1804.09817v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.09817</link>
<description rdf:parseType="Literal">&lt;p&gt;Policy gradient methods are often applied to reinforcement learning in
continuous multiagent games. These methods perform local search in the
joint-action space, and as we show, they are susceptable to a game-theoretic
pathology known as relative overgeneralization. To resolve this issue, we
propose Multiagent Soft Q-learning, which can be seen as the analogue of
applying Q-learning to continuous controls. We compare our method to MADDPG, a
state-of-the-art approach, and show that our method achieves better
coordination in multiagent cooperative tasks, converging to better local optima
in the joint action space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_E/0/1/0/all/0/1&quot;&gt;Ermo Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicke_D/0/1/0/all/0/1&quot;&gt;Drew Wicke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freelan_D/0/1/0/all/0/1&quot;&gt;David Freelan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luke_S/0/1/0/all/0/1&quot;&gt;Sean Luke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09849">
<title>The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation. (arXiv:1804.09849v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09849</link>
<description rdf:parseType="Literal">&lt;p&gt;The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)
modeling for Machine Translation (MT). The classic RNN-based approaches to MT
were first out-performed by the convolutional seq2seq model, which was then
out-performed by the more recent Transformer model. Each of these new
approaches consists of a fundamental architecture accompanied by a set of
modeling and training techniques that are in principle applicable to other
seq2seq architectures. In this paper, we tease apart the new architectures and
their accompanying techniques in two ways. First, we identify several key
modeling and training techniques, and apply them to the RNN architecture,
yielding a new RNMT+ model that outperforms all of the three fundamental
architectures on the benchmark WMT&apos;14 English to French and English to German
tasks. Second, we analyze the properties of each fundamental seq2seq
architecture and devise new hybrid architectures intended to combine their
strengths. Our hybrid models obtain further improvements, outperforming the
RNMT+ model on both benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mia Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1&quot;&gt;Ankur Bapna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Melvin Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1&quot;&gt;Wolfgang Macherey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1&quot;&gt;George Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1&quot;&gt;Llion Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1&quot;&gt;Niki Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_M/0/1/0/all/0/1&quot;&gt;Mike Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Macduff Hughes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09856">
<title>Action Categorization for Computationally Improved Task Learning and Planning. (arXiv:1804.09856v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.09856</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the problem of task learning and planning, contributing
the Action-Category Representation (ACR) to improve computational performance
of both Planning and Reinforcement Learning (RL). ACR is an algorithm-agnostic,
abstract data representation that maps objects to action categories (groups of
actions), inspired by the psychological concept of action codes. We validate
our approach in StarCraft and Lightworld domains; our results demonstrate
several benefits of ACR relating to improved computational performance of
planning and RL, by reducing the action space for the agent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_L/0/1/0/all/0/1&quot;&gt;Lakshmi Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1&quot;&gt;Sonia Chernova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09997">
<title>PANDA: Facilitating Usable AI Development. (arXiv:1804.09997v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in artificial intelligence (AI) and machine learning have
created a general perception that AI could be used to solve complex problems,
and in some situations over-hyped as a tool that can be so easily used.
Unfortunately, the barrier to realization of mass adoption of AI on various
business domains is too high because most domain experts have no background in
AI. Developing AI applications involves multiple phases, namely data
preparation, application modeling, and product deployment. The effort of AI
research has been spent mostly on new AI models (in the model training stage)
to improve the performance of benchmark tasks such as image recognition. Many
other factors such as usability, efficiency and security of AI have not been
well addressed, and therefore form a barrier to democratizing AI. Further, for
many real world applications such as healthcare and autonomous driving,
learning via huge amounts of possibility exploration is not feasible since
humans are involved. In many complex applications such as healthcare, subject
matter experts (e.g. Clinicians) are the ones who appreciate the importance of
features that affect health, and their knowledge together with existing
knowledge bases are critical to the end results. In this paper, we take a new
perspective on developing AI solutions, and present a solution for making AI
usable. We hope that this resolution will enable all subject matter experts
(eg. Clinicians) to exploit AI like data scientists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jinyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meihui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1&quot;&gt;H.V. Jagadish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1&quot;&gt;Teck Khim Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1&quot;&gt;Beng Chin Ooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10028">
<title>Decentralized learning with budgeted network load using Gaussian copulas and classifier ensembles. (arXiv:1804.10028v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10028</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine a network of learners which address the same classification task
but must learn from different data sets. The learners can share a limited
portion of their data sets so as to preserve the network load. We introduce
DELCO (standing for Decentralized Ensemble Learning with COpulas), a new
approach in which the shared data and the trained models are sent to a central
machine that allows to build an ensemble of classifiers. The proposed method
aggregates the base classifiers using a probabilistic model relying on Gaussian
copulas. Experiments on logistic regressor ensembles demonstrate competing
accuracy and increased robustness as compared to gold standard approaches. A
companion python implementation can be downloaded at
https://github.com/john-klein/DELCO
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klein_J/0/1/0/all/0/1&quot;&gt;John Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Albardan_M/0/1/0/all/0/1&quot;&gt;Mahmoud Albardan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guedj_B/0/1/0/all/0/1&quot;&gt;Benjamin Guedj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Colot_O/0/1/0/all/0/1&quot;&gt;Olivier Colot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10188">
<title>Dialogue Modeling Via Hash Functions: Applications to Psychotherapy. (arXiv:1804.10188v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10188</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel machine-learning framework for dialogue modeling which
uses representations based on hash functions. More specifically, each person&apos;s
response is represented by a binary hashcode where each bit reflects presence
or absence of a certain text pattern in the response. Hashcodes serve as
compressed text representations, allowing for efficient similarity search.
Moreover, hashcode of one person&apos;s response can be used as a feature vector for
predicting the hashcode representing another person&apos;s response. The proposed
hashing model of dialogue is obtained by maximizing a novel lower bound on the
mutual information between the hashcodes of consecutive responses. We apply our
approach in psychotherapy domain, evaluating its effectiveness on a real-life
dataset consisting of therapy sessions with patients suffering from depression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1&quot;&gt;Guillermo Cecchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shuyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhaskar_B/0/1/0/all/0/1&quot;&gt;Bhavana Bhaskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Palash Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1&quot;&gt;Aram Galstyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05116">
<title>Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering. (arXiv:1711.05116v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05116</link>
<description rdf:parseType="Literal">&lt;p&gt;A popular recent approach to answering open-domain questions is to first
search for question-related passages and then apply reading comprehension
models to extract answers. Existing methods usually extract answers from single
passages independently. But some questions require a combination of evidence
from across different sources to answer correctly. In this paper, we propose
two models which make use of multiple passages to generate their answers. Both
use an answer-reranking approach which reorders the answer candidates generated
by an existing state-of-the-art QA model. We propose two methods, namely,
strength-based re-ranking and coverage-based re-ranking, to make use of the
aggregated evidence from different passages to better determine the answer. Our
models have achieved state-of-the-art results on three public open-domain QA
datasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with
about 8 percentage points of improvement over the former two datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuohang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shiyu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1&quot;&gt;Tim Klinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1&quot;&gt;Gerald Tesauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1&quot;&gt;Murray Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07139">
<title>English-Catalan Neural Machine Translation in the Biomedical Domain through the cascade approach. (arXiv:1803.07139v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07139</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the methodology followed to build a neural machine
translation system in the biomedical domain for the English-Catalan language
pair. This task can be considered a low-resourced task from the point of view
of the domain and the language pair. To face this task, this paper reports
experiments on a cascade pivot strategy through Spanish for the neural machine
translation using the English-Spanish SCIELO and Spanish-Catalan El Peri\&apos;odico
database. To test the final performance of the system, we have created a new
test data set for English-Catalan in the biomedical domain which is freely
available on request.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1&quot;&gt;Marta R. Costa-juss&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_N/0/1/0/all/0/1&quot;&gt;Noe Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melero_M/0/1/0/all/0/1&quot;&gt;Maite Melero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09758">
<title>On the Performance of a Canonical Labeling for Matching Correlated Erd\H{o}s-R\&apos;enyi Graphs. (arXiv:1804.09758v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1804.09758</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph matching in two correlated random graphs refers to the task of
identifying the correspondence between vertex sets of the graphs. Recent
results have characterized the exact information-theoretic threshold for graph
matching in correlated Erd\H{o}s-R\&apos;enyi graphs. However, very little is known
about the existence of efficient algorithms to achieve graph matching without
seeds. In this work we identify a region in which a straightforward $O(n^2\log
n)$-time canonical labeling algorithm, initially introduced in the context of
graph isomorphism, succeeds in matching correlated Erd\H{o}s-R\&apos;enyi graphs.
The algorithm has two steps. In the first step, all vertices are labeled by
their degrees and a trivial minimum distance matching (i.e., simply sorting
vertices according to their degrees) matches a fixed number of highest degree
vertices in the two graphs. Having identified this subset of vertices, the
remaining vertices are matched using a matching algorithm for bipartite graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_O/0/1/0/all/0/1&quot;&gt;Osman Emre Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cullina_D/0/1/0/all/0/1&quot;&gt;Daniel Cullina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grossglauser_M/0/1/0/all/0/1&quot;&gt;Matthias Grossglauser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09812">
<title>Improved Classification Based on Deep Belief Networks. (arXiv:1804.09812v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09812</link>
<description rdf:parseType="Literal">&lt;p&gt;For better classification generative models are used to initialize the model
and model features before training a classifier. Typically it is needed to
solve separate unsupervised and supervised learning problems. Generative
restricted Boltzmann machines and deep belief networks are widely used for
unsupervised learning. We developed several supervised models based on DBN in
order to improve this two-phase strategy. Modifying the loss function to
account for expectation with respect to the underlying generative model,
introducing weight bounds, and multi-level programming are applied in model
development. The proposed models capture both unsupervised and supervised
objectives effectively. The computational study verifies that our models
perform better than the two-phase training approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koo_J/0/1/0/all/0/1&quot;&gt;Jaehoon Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10023">
<title>Weak Labeling for Crowd Learning. (arXiv:1804.10023v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10023</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowdsourcing has become very popular among the machine learning community as
a way to obtain labels that allow a ground truth to be estimated for a given
dataset. In most of the approaches that use crowdsourced labels, annotators are
asked to provide, for each presented instance, a single class label. Such a
request could be inefficient, that is, considering that the labelers may not be
experts, that way to proceed could fail to take real advantage of the knowledge
of the labelers. In this paper, the use of weak labeling for crowd learning is
proposed, where the annotators may provide more than a single label per
instance to try not to miss the real label. The main hypothesis is that, by
allowing weak labeling, knowledge can be extracted from the labelers more
efficiently by than in the standard crowd learning scenario. Empirical evidence
which supports that hypothesis is presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benaran_Munoz_I/0/1/0/all/0/1&quot;&gt;Iker Be&amp;#xf1;aran-Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hernandez_Gonzalez_J/0/1/0/all/0/1&quot;&gt;Jer&amp;#xf3;nimo Hern&amp;#xe1;ndez-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Aritz P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10080">
<title>On deep speaker embeddings for text-independent speaker recognition. (arXiv:1804.10080v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1804.10080</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate deep neural network performance in the textindependent speaker
recognition task. We demonstrate that using angular softmax activation at the
last classification layer of a classification neural network instead of a
simple softmax activation allows to train a more generalized discriminative
speaker embedding extractor. Cosine similarity is an effective metric for
speaker verification in this embedding space. We also address the problem of
choosing an architecture for the extractor. We found that deep networks with
residual frame level connections outperform wide but relatively shallow
architectures. This paper also proposes several improvements for previous
DNN-based extractor systems to increase the speaker recognition accuracy. We
show that the discriminatively trained similarity metric learning approach
outperforms the standard LDA-PLDA method as an embedding backend. The results
obtained on Speakers in the Wild and NIST SRE 2016 evaluation sets demonstrate
robustness of the proposed systems when dealing with close to real-life
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novoselov_S/0/1/0/all/0/1&quot;&gt;Sergey Novoselov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shulipa_A/0/1/0/all/0/1&quot;&gt;Andrey Shulipa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kremnev_I/0/1/0/all/0/1&quot;&gt;Ivan Kremnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozlov_A/0/1/0/all/0/1&quot;&gt;Alexandr Kozlov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shchemelinin_V/0/1/0/all/0/1&quot;&gt;Vadim Shchemelinin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10168">
<title>Handling Missing Values using Decision Trees with Branch-Exclusive Splits. (arXiv:1804.10168v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10168</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we propose a new decision tree construction algorithm. The
proposed approach allows the algorithm to interact with some predictors that
are only defined in subspaces of the feature space. One way to utilize this new
tool is to create or use one of the predictors to keep track of missing values.
This predictor can later be used to define the subspace where predictors with
missing values are available for the data partitioning process. By doing so,
this new classification tree can handle missing values for both modelling and
prediction. The algorithm is tested against simulated and real data. The result
is a classification procedure that efficiently handles missing values and
produces results that are more accurate and more interpretable than most common
procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beaulac_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Beaulac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosenthal_J/0/1/0/all/0/1&quot;&gt;Jeffrey S. Rosenthal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.01234">
<title>Ensemble Validation: Selectivity has a Price, but Variety is Free. (arXiv:1610.01234v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.01234</link>
<description rdf:parseType="Literal">&lt;p&gt;Suppose some classifiers are selected from a set of hypothesis classifiers to
form an equally-weighted ensemble that selects a member classifier at random
for each input example. Then the ensemble has an error bound consisting of the
average error bound for the member classifiers, a term for selectivity that
varies from zero (if all hypothesis classifiers are selected) to a standard
uniform error bound (if only a single classifier is selected), and small
constants. There is no penalty for using a richer hypothesis set if the same
fraction of the hypothesis classifiers are selected for the ensemble.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bax_E/0/1/0/all/0/1&quot;&gt;Eric Bax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kooti_F/0/1/0/all/0/1&quot;&gt;Farshad Kooti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07172">
<title>SpectralLeader: Online Spectral Learning for Single Topic Models. (arXiv:1709.07172v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07172</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning a latent variable model from a stream of
data. Latent variable models are popular in practice because they can explain
observed data in terms of unobserved concepts. These models have been
traditionally studied in the offline setting. In the online setting, on the
other hand, the online EM is arguably the most popular algorithm for learning
latent variable models. Although the online EM is computationally efficient, it
typically converges to a local optimum. In this work, we develop a new online
learning algorithm for latent variable models, which we call SpectralLeader.
SpectralLeader always converges to the global optimum, and we derive a
sublinear upper bound on its $n$-step regret in the bag-of-words model. In both
synthetic and real-world experiments, we show that SpectralLeader performs
similarly to or better than the online EM with tuned hyper-parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Zheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1&quot;&gt;Hung Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mengshoel_O/0/1/0/all/0/1&quot;&gt;Ole J. Mengshoel&lt;/a&gt;</dc:creator>
</item></rdf:RDF>