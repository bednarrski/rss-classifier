<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.06160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.05884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00682"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00662"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00680"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09090"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.00554">
<title>Generating Redundant Features with Unsupervised Multi-Tree Genetic Programming. (arXiv:1802.00554v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.00554</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, feature selection has become an increasingly important area of
research due to the surge in high-dimensional datasets in all areas of modern
life. A plethora of feature selection algorithms have been proposed, but it is
difficult to truly analyse the quality of a given algorithm. Ideally, an
algorithm would be evaluated by measuring how well it removes known bad
features. Acquiring datasets with such features is inherently difficult, and so
a common technique is to add synthetic bad features to an existing dataset.
While adding noisy features is an easy task, it is very difficult to
automatically add complex, redundant features. This work proposes one of the
first approaches to generating redundant features, using a novel genetic
programming approach. Initial experiments show that our proposed method can
automatically create difficult, redundant features which have the potential to
be used for creating high-quality feature selection benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lensen_A/0/1/0/all/0/1&quot;&gt;Andrew Lensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.06160">
<title>DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients. (arXiv:1606.06160v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1606.06160</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose DoReFa-Net, a method to train convolutional neural networks that
have low bitwidth weights and activations using low bitwidth parameter
gradients. In particular, during backward pass, parameter gradients are
stochastically quantized to low bitwidth numbers before being propagated to
convolutional layers. As convolutions during forward/backward passes can now
operate on low bitwidth weights and activations/gradients respectively,
DoReFa-Net can use bit convolution kernels to accelerate both training and
inference. Moreover, as bit convolutions can be efficiently implemented on CPU,
FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low
bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet
datasets prove that DoReFa-Net can achieve comparable prediction accuracy as
32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has
1-bit weights, 2-bit activations, can be trained from scratch using 6-bit
gradients to get 46.1\% top-1 accuracy on ImageNet validation set. The
DoReFa-Net AlexNet model is released publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuchang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1&quot;&gt;Zekun Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;He Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuheng Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.05884">
<title>A Quantum Implementation Model for Artificial Neural Networks. (arXiv:1609.05884v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1609.05884</link>
<description rdf:parseType="Literal">&lt;p&gt;The learning process for multi layered neural networks with many nodes makes
heavy demands on computational resources. In some neural network models, the
learning formulas, such as the Widrow-Hoff formula, do not change the
eigenvectors of the weight matrix while flatting the eigenvalues. In infinity,
this iterative formulas result in terms formed by the principal components of
the weight matrix: i.e., the eigenvectors corresponding to the non-zero
eigenvalues. In quantum computing, the phase estimation algorithm is known to
provide speed-ups over the conventional algorithms for the eigenvalue-related
problems. Combining the quantum amplitude amplification with the phase
estimation algorithm, a quantum implementation model for artificial neural
networks using the Widrow-Hoff learning rule is presented. The complexity of
the model is found to be linear in the size of the weight matrix. This provides
a quadratic improvement over the classical algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Daskin_A/0/1/0/all/0/1&quot;&gt;Ammar Daskin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05136">
<title>Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05136</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently on sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1&quot;&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kappel_D/0/1/0/all/0/1&quot;&gt;David Kappel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1&quot;&gt;Wolfgang Maass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legenstein_R/0/1/0/all/0/1&quot;&gt;Robert Legenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00510">
<title>Adaptive Memory Networks. (arXiv:1802.00510v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00510</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Adaptive Memory Networks (AMN) that processes input-question pairs
to dynamically construct a network architecture optimized for lower inference
times for Question Answering (QA) tasks. AMN processes the input story to
extract entities and stores them in memory banks. Starting from a single bank,
as the number of input entities increases, AMN learns to create new banks as
the entropy in a single bank becomes too high. Hence, after processing an
input-question(s) pair, the resulting network represents a hierarchical
structure where entities are stored in different banks, distanced by question
relevance. At inference, one or few banks are used, creating a tradeoff between
accuracy and performance. AMN is enabled by dynamic networks that allow input
dependent network creation and efficiency in dynamic mini-batching as well as
our novel bank controller that allows learning discrete decision making with
high accuracy. In our results, we demonstrate that AMN learns to create
variable depth networks depending on task complexity and reduces inference
times for QA tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daniel Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1&quot;&gt;Asim Kadav&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00541">
<title>Causal Learning and Explanation of Deep Neural Networks via Autoencoded Activations. (arXiv:1802.00541v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00541</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are complex and opaque. As they enter application in a
variety of important and safety critical domains, users seek methods to explain
their output predictions. We develop an approach to explaining deep neural
networks by constructing causal models on salient concepts contained in a CNN.
We develop methods to extract salient concepts throughout a target network by
using autoencoders trained to extract human-understandable representations of
network activations. We then build a bayesian causal model using these
extracted concepts as variables in order to explain image classification.
Finally, we use this causal model to identify and visualize features with
significant causal influence on final classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harradon_M/0/1/0/all/0/1&quot;&gt;Michael Harradon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Druce_J/0/1/0/all/0/1&quot;&gt;Jeff Druce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruttenberg_B/0/1/0/all/0/1&quot;&gt;Brian Ruttenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00560">
<title>Interpretable Deep Convolutional Neural Networks via Meta-learning. (arXiv:1802.00560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00560</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model&apos;s outputs. The recent movement
for &quot;algorithmic fairness&quot; also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00682">
<title>How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation. (arXiv:1802.00682v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00682</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen a boom in interest in machine learning systems that
can provide a human-understandable rationale for their predictions or
decisions. However, exactly what kinds of explanation are truly
human-interpretable remains poorly understood. This work advances our
understanding of what makes explanations interpretable in the specific context
of verification. Suppose we have a machine learning system that predicts X, and
we provide rationale for this prediction X. Given an input, an explanation, and
an output, is the output consistent with the input and the supposed rationale?
Via a series of user-studies, we identify what kinds of increases in complexity
have the greatest effect on the time it takes for humans to verify the
rationale, and which seem relatively insensitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_M/0/1/0/all/0/1&quot;&gt;Menaka Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Emily Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jeffrey He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1&quot;&gt;Sam Gershman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00690">
<title>Modelling contextuality by probabilistic programs with hypergraph semantics. (arXiv:1802.00690v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00690</link>
<description rdf:parseType="Literal">&lt;p&gt;Models of a phenomenon are often developed by examining it under different
experimental conditions, or measurement contexts. The resultant probabilistic
models assume that the underlying random variables, which define a measurable
set of outcomes, can be defined independent of the measurement context. The
phenomenon is deemed contextual when this assumption fails. Contextuality is an
important issue in quantum physics. However, there has been growing speculation
that it manifests outside the quantum realm with human cognition being a
particularly prominent area of investigation. This article contributes the
foundations of a probabilistic programming language that allows convenient
exploration of contextuality in wide range of applications relevant to
cognitive science and artificial intelligence. Specific syntax is proposed to
allow the specification of &quot;measurement contexts&quot;. Each such context delivers a
partial model of the phenomenon based on the associated experimental condition
described by the measurement context. The probabilistic program is translated
into a hypergraph in a modular way. Recent theoretical results from the field
of quantum physics show that contextuality can be equated with the possibility
of constructing a probabilistic model on the resulting hypergraph. The use of
hypergraphs opens the door for a theoretically succinct and efficient
computational semantics sensitive to modelling both contextual and
non-contextual phenomena. Finally, this article raises awareness of
contextuality beyond quantum physics and to contribute formal methods to detect
its presence by means of hypergraph semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruza_P/0/1/0/all/0/1&quot;&gt;Peter D. Bruza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09547">
<title>An Improved Tabu Search Heuristics for Static Dial-A-Ride Problem. (arXiv:1801.09547v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09547</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-vehicle routing has become increasingly important with the rapid
development of autonomous vehicle technology. Dial-a-ride problem (DARP), a
variant of vehicle routing problem (VRP), deals with the allocation of customer
requests to vehicles, scheduling the pick-up and drop-off times and the
sequence of serving those requests by ensuring high customer satisfaction with
minimized travel cost. In this paper, we propose an improved tabu search (ITS)
heuristic for static DARP with the objective of obtaining high-quality
solutions in short time. Two new techniques, initialization heuristic, and time
window adjustment are proposed to achieve faster convergence to the global
optimum. Various numerical experiments are conducted for the proposed solution
methodology using DARP test instances from the literature and the convergence
speed up is validated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1&quot;&gt;Songguang Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagavarapu_S/0/1/0/all/0/1&quot;&gt;Sarat Chandra Nagavarapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandi_R/0/1/0/all/0/1&quot;&gt;Ramesh Ramasamy Pandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauwels_J/0/1/0/all/0/1&quot;&gt;Justin Dauwels&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00295">
<title>A Semantic Model for Historical Manuscripts. (arXiv:1802.00295v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00295</link>
<description rdf:parseType="Literal">&lt;p&gt;The study and publication of historical scientific manuscripts are com- plex
tasks that involve, among others, the explicit representation of the text mean-
ings and reasoning on temporal entities. In this paper we present the first
results of an interdisciplinary project dedicated to the study of Saussure&apos;s
manuscripts. These results aim to fulfill requirements elaborated with
Saussurean humanists. They comprise a model for the representation of
time-varying statements and time-varying domain knowledge (in particular
terminologies) as well as imple- mentation techniques for the semantic indexing
of manuscripts and for temporal reasoning on knowledge extracted from the
manuscripts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljalbout_S/0/1/0/all/0/1&quot;&gt;Sahar Aljalbout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falquet_G/0/1/0/all/0/1&quot;&gt;Gilles Falquet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00662">
<title>Dual Memory Neural Computer for Asynchronous Two-view Sequential Learning. (arXiv:1802.00662v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00662</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the core task in multi-view learning is to capture all relations among
views. For sequential data, the relations not only span across views, but also
extend throughout the view length to form long-term intra-view and cross-view
interactions. In this paper, we present a new memory augmented neural network
model that aims to model these complex interactions between two sequential
views that are asynchronous. Our model uses two neural encoders for reading
from and writing to two external memories for encoding input views. The
intra-view interactions and the long-term dependencies are captured by the use
of memories during this encoding process. There are two modes of memory
accessing in our system: late-fusion and early-fusion, corresponding to late
and early cross-view interactions. In late-fusion mode, the two memories are
separated, containing only view-specific contents. In the early-fusion mode,
the two memories share the same addressing space, allowing cross-memory
accessing. In both cases, the knowledge from the memories finally will be
synthesized by a decoder to make predictions over the output space. The
resulting dual memory neural computer is demonstrated on various of
experiments, from the synthetic sum of two sequences task to the tasks of drug
prescription and disease progressions in healthcare. The results show improved
performance over both traditional algorithms and deep learning methods designed
for multi-view problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hung Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Truyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00680">
<title>A Generative Model for Natural Sounds Based on Latent Force Modelling. (arXiv:1802.00680v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00680</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in analysis of subband amplitude envelopes of natural sounds
have resulted in convincing synthesis, showing subband amplitudes to be a
crucial component of perception. Probabilistic latent variable analysis is
particularly revealing, but existing approaches don&apos;t incorporate prior
knowledge about the physical behaviour of amplitude envelopes, such as
exponential decay and feedback. We use latent force modelling, a probabilistic
learning paradigm that incorporates physical knowledge into Gaussian process
regression, to model correlation across spectral subband envelopes. We augment
the standard latent force model approach by explicitly modelling correlations
over multiple time steps. Incorporating this prior knowledge strengthens the
interpretation of the latent functions as the source that generated the signal.
We examine this interpretation via an experiment which shows that sounds
generated by sampling from our probabilistic model are perceived to be more
realistic than those generated by similar models based on nonnegative matrix
factorisation, even in cases where our model is outperformed from a
reconstruction error perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilkinson_W/0/1/0/all/0/1&quot;&gt;William J. Wilkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiss_J/0/1/0/all/0/1&quot;&gt;Joshua D. Reiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04044">
<title>Efficient Representation for Natural Language Processing via Kernelized Hashcodes. (arXiv:1711.04044v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04044</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel methods have been used widely in a number of tasks, but have had
limited success in Natural Language Processing (NLP) due to high cost of
computing kernel similarities between discrete natural language structures. A
recently proposed technique, Kernelized Locality Sensitive Hashing (KLSH), can
significantly reduce the computational cost, but is only applicable to
classifiers operating on kNN graphs. Here we propose to use random subspaces of
KLSH codes for efficiently constructing explicit representation of natural
language structures suitable for general classification methods. Further, we
propose an approach for optimizing a KLSH model for classification problems, by
maximizing a variational lower bound on the mutual information between the KLSH
codes (feature vectors) and the class labels. We apply the proposed approach to
a biomedical information extraction task, and observe robust improvements in
accuracy, along with significant speedup compared to conventional kernel
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1&quot;&gt;Aram Galstyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1&quot;&gt;Guillermo Cecchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shuyang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09090">
<title>Invariance of Weight Distributions in Rectified MLPs. (arXiv:1711.09090v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09090</link>
<description rdf:parseType="Literal">&lt;p&gt;An interesting approach to analyzing and developing tools for neural networks
that has received renewed attention is to examine the equivalent kernel of the
neural network. This is based on the fact that a fully connected feedforward
network with one hidden layer, a certain weight distribution, an activation
function, and an infinite number of neurons is a mapping that can be viewed as
a projection into a Hilbert space. We show that the equivalent kernel of an MLP
with ReLU or Leaky ReLU activations for all rotationally-invariant weight
distributions is the same, generalizing a previous result that required
Gaussian weight distributions. We derive the equivalent kernel for these cases.
In deep networks, the equivalent kernel approaches a pathological fixed point,
which can be used to argue why training randomly initialized networks can be
difficult. Our results also have implications for weight initialization and the
level sets in neural network cost functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuchida_R/0/1/0/all/0/1&quot;&gt;Russell Tsuchida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roosta_Khorasani_F/0/1/0/all/0/1&quot;&gt;Farbod Roosta-Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1&quot;&gt;Marcus Gallagher&lt;/a&gt;</dc:creator>
</item></rdf:RDF>