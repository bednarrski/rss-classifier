<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08091"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1401.8008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05207"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1705.08927">
<title>Compiling quantum circuits to realistic hardware architectures using temporal planners. (arXiv:1705.08927v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08927</link>
<description rdf:parseType="Literal">&lt;p&gt;To run quantum algorithms on emerging gate-model quantum hardware, quantum
circuits must be compiled to take into account constraints on the hardware. For
near-term hardware, with only limited means to mitigate decoherence, it is
critical to minimize the duration of the circuit. We investigate the
application of temporal planners to the problem of compiling quantum circuits
to newly emerging quantum hardware. While our approach is general, we focus on
compiling to superconducting hardware architectures with nearest neighbor
constraints. Our initial experiments focus on compiling Quantum Alternating
Operator Ansatz (QAOA) circuits whose high number of commuting gates allow
great flexibility in the order in which the gates can be applied. That freedom
makes it more challenging to find optimal compilations but also means there is
a greater potential win from more optimized compilation than for less flexible
circuits. We map this quantum circuit compilation problem to a temporal
planning problem, and generated a test suite of compilation problems for QAOA
circuits of various sizes to a realistic hardware architecture. We report
compilation results from several state-of-the-art temporal planners on this
test set. This early empirical evaluation demonstrates that temporal planning
is a viable approach to quantum circuit compilation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Venturelli_D/0/1/0/all/0/1&quot;&gt;Davide Venturelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Do_M/0/1/0/all/0/1&quot;&gt;Minh Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rieffel_E/0/1/0/all/0/1&quot;&gt;Eleanor Rieffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Frank_J/0/1/0/all/0/1&quot;&gt;Jeremy Frank&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05881">
<title>Morphology dictates a robot&apos;s ability to ground crowd-proposed language. (arXiv:1712.05881v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.05881</link>
<description rdf:parseType="Literal">&lt;p&gt;As more robots act in physical proximity to people, it is essential to ensure
they make decisions and execute actions that align with human values. To do so,
robots need to understand the true intentions behind human-issued commands. In
this paper, we define a safe robot as one that receives a natural-language
command from humans, considers an action in response to that command, and
accurately predicts how humans will judge that action if is executed in
reality. Our contribution is two-fold: First, we introduce a web platform for
human users to propose commands to simulated robots. The robots receive
commands and act based on those proposed commands, and then the users provide
positive and/or negative reinforcement. Next, we train a critic for each robot
to predict the crowd&apos;s responses to one of the crowd-proposed commands. Second,
we show that the morphology of a robot plays a role in the way it grounds
language: The critics show that two of the robots used in the experiment
achieve a lower prediction error than the others. Thus, those two robots are
safer, according to our definition, since they ground the proposed command more
accurately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoor_Z/0/1/0/all/0/1&quot;&gt;Zahra Mahoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felag_J/0/1/0/all/0/1&quot;&gt;Jack Felag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08091">
<title>Multiview Deep Learning for Predicting Twitter Users&apos; Location. (arXiv:1712.08091v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08091</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of predicting the location of users on large social networks like
Twitter has emerged from real-life applications such as social unrest detection
and online marketing. Twitter user geolocation is a difficult and active
research topic with a vast literature. Most of the proposed methods follow
either a content-based or a network-based approach. The former exploits
user-generated content while the latter utilizes the connection or interaction
between Twitter users. In this paper, we introduce a novel method combining the
strength of both approaches. Concretely, we propose a multi-entry neural
network architecture named MENET leveraging the advances in deep learning and
multiview learning. The generalizability of MENET enables the integration of
multiple data representations. In the context of Twitter user geolocation, we
realize MENET with textual, network, and metadata features. Considering the
natural distribution of Twitter users across the concerned geographical area,
we subdivide the surface of the earth into multi-scale cells and train MENET
with the labels of the cells. We show that our method outperforms the state of
the art by a large margin on three benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1&quot;&gt;Tien Huu Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsiligianni_E/0/1/0/all/0/1&quot;&gt;Evaggelia Tsiligianni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelis_B/0/1/0/all/0/1&quot;&gt;Bruno Cornelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1&quot;&gt;Nikos Deligiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08104">
<title>Truncated Variational Sampling for &quot;Black Box&quot; Optimization of Generative Models. (arXiv:1712.08104v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.08104</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the optimization of two generative models with binary hidden
variables using a novel variational EM approach. The novel approach
distinguishes itself from previous variational approaches by using hidden
states as variational parameters. Here we use efficient and general purpose
sampling procedures to vary the hidden states, and investigate the &quot;black box&quot;
applicability of the resulting optimization procedure. For general purpose
applicability, samples are drawn from approximate marginal distributions of the
considered generative model and from the prior distribution of a given
generative model. As such, sampling is defined in a generic form with no
additional derivations required. As a proof of concept, we then apply the novel
procedure (A) to Binary Sparse Coding (a model with continuous observables),
and (B) to basic Sigmoid Belief Networks (which are models with binary
observables). The approach is applicable without any further analytical steps
and efficiently as well as effectively increases the variational free-energy
objective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucke_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg L&amp;#xfc;cke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zhenwen Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Exarchakis_G/0/1/0/all/0/1&quot;&gt;Georgios Exarchakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1401.8008">
<title>Support vector comparison machines. (arXiv:1401.8008v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1401.8008</link>
<description rdf:parseType="Literal">&lt;p&gt;In ranking problems, the goal is to learn a ranking function from labeled
pairs of input points. In this paper, we consider the related comparison
problem, where the label indicates which element of the pair is better, or if
there is no significant difference. We cast the learning problem as a margin
maximization, and show that it can be solved by converting it to a standard
SVM. We use simulated nonlinear patterns, a real learning to rank sushi data
set, and a chess data set to show that our proposed SVMcompare algorithm
outperforms SVMrank when there are equality pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venuto_D/0/1/0/all/0/1&quot;&gt;David Venuto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hocking_T/0/1/0/all/0/1&quot;&gt;Toby Dylan Hocking&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sphanurattana_L/0/1/0/all/0/1&quot;&gt;Lakjaree Sphanurattana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05207">
<title>Learning Universal Adversarial Perturbations with Generative Models. (arXiv:1708.05207v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05207</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are known to be vulnerable to adversarial examples, inputs
that have been intentionally perturbed to remain visually similar to the source
input, but cause a misclassification. It was recently shown that given a
dataset and classifier, there exists so called universal adversarial
perturbations, a single perturbation that causes a misclassification when
applied to any input. In this work, we introduce universal adversarial
networks, a generative network that is capable of fooling a target classifier
when it&apos;s generated output is added to a clean sample from a dataset. We show
that this technique improves on known universal adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_J/0/1/0/all/0/1&quot;&gt;Jamie Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danezis_G/0/1/0/all/0/1&quot;&gt;George Danezis&lt;/a&gt;</dc:creator>
</item></rdf:RDF>