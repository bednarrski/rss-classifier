<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11410"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03453"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10772"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.01394"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07164"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10743"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.11389">
<title>Single Stream Parallelization of Recurrent Neural Networks for Low Power and Fast Inference. (arXiv:1803.11389v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1803.11389</link>
<description rdf:parseType="Literal">&lt;p&gt;As neural network algorithms show high performance in many applications,
their efficient inference on mobile and embedded systems are of great
interests. When a single stream recurrent neural network (RNN) is executed for
a personal user in embedded systems, it demands a large amount of DRAM accesses
because the network size is usually much bigger than the cache size and the
weights of an RNN are used only once at each time step. We overcome this
problem by parallelizing the algorithm and executing it multiple time steps at
a time. This approach also reduces the power consumption by lowering the number
of DRAM accesses. QRNN (Quasi Recurrent Neural Networks) and SRU (Simple
Recurrent Unit) based recurrent neural networks are used for implementation.
The experiments for SRU showed about 300% and 930% of speed-up when the numbers
of multi time steps are 4 and 16, respectively, in an ARM CPU based system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1&quot;&gt;Wonyong Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jinhwan Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11410">
<title>On the Resistance of Neural Nets to Label Noise. (arXiv:1803.11410v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.11410</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the behavior of convolutional neural networks (CNN) in the
presence of label noise. We show empirically that CNN prediction for a given
test sample depends on the labels of the training samples in its local
neighborhood. This is similar to the way that the K-nearest neighbors (K-NN)
classifier works. With this understanding, we derive an analytical expression
for the expected accuracy of a K-NN, and hence a CNN, classifier for any level
of noise. In particular, we show that K-NN, and CNN, are resistant to label
noise that is randomly spread across the training set, but are very sensitive
to label noise that is concentrated. Experiments on real datasets validate our
analytical expression by showing that they match the empirical results for
varying degrees of label noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drory_A/0/1/0/all/0/1&quot;&gt;Amnon Drory&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1&quot;&gt;Shai Avidan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03453">
<title>The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities. (arXiv:1803.03453v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03453</link>
<description rdf:parseType="Literal">&lt;p&gt;Biological evolution provides a creative fount of complex and subtle
adaptations, often surprising the scientists who discover them. However,
because evolution is an algorithmic process that transcends the substrate in
which it occurs, evolution&apos;s creativity is not limited to nature. Indeed, many
researchers in the field of digital evolution have observed their evolving
algorithms and organisms subverting their intentions, exposing unrecognized
bugs in their code, producing unexpected adaptations, or exhibiting outcomes
uncannily convergent with ones in nature. Such stories routinely reveal
creativity by evolution in these digital worlds, but they rarely fit into the
standard scientific narrative. Instead they are often treated as mere obstacles
to be overcome, rather than results that warrant study in their own right. The
stories themselves are traded among researchers through oral tradition, but
that mode of information transmission is inefficient and prone to error and
outright loss. Moreover, the fact that these stories tend to be shared only
among practitioners means that many natural scientists do not realize how
interesting and lifelike digital organisms are and how natural their evolution
can be. To our knowledge, no collection of such anecdotes has been published
before. This paper is the crowd-sourced product of researchers in the fields of
artificial life and evolutionary computation who have provided first-hand
accounts of such cases. It thus serves as a written, fact-checked collection of
scientifically important and even entertaining stories. In doing so we also
present here substantial evidence that the existence and importance of
evolutionary surprises extends beyond the natural world, and may indeed be a
universal property of all complex evolving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misevic_D/0/1/0/all/0/1&quot;&gt;Dusan Misevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adami_C/0/1/0/all/0/1&quot;&gt;Christoph Adami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaulieu_J/0/1/0/all/0/1&quot;&gt;Julie Beaulieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bentley_P/0/1/0/all/0/1&quot;&gt;Peter J. Bentley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_S/0/1/0/all/0/1&quot;&gt;Samuel Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beslon_G/0/1/0/all/0/1&quot;&gt;Guillaume Beslon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bryson_D/0/1/0/all/0/1&quot;&gt;David M. Bryson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrabaszcz_P/0/1/0/all/0/1&quot;&gt;Patryk Chrabaszcz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doncieux_S/0/1/0/all/0/1&quot;&gt;Stephane Doncieux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_F/0/1/0/all/0/1&quot;&gt;Fred C. Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellefsen_K/0/1/0/all/0/1&quot;&gt;Kai Olav Ellefsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldt_R/0/1/0/all/0/1&quot;&gt;Robert Feldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_S/0/1/0/all/0/1&quot;&gt;Stephan Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forrest_S/0/1/0/all/0/1&quot;&gt;Stephanie Forrest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frenoy_A/0/1/0/all/0/1&quot;&gt;Antoine Fr&amp;#xe9;noy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goff_L/0/1/0/all/0/1&quot;&gt;Leni Le Goff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabowski_L/0/1/0/all/0/1&quot;&gt;Laura M. Grabowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodjat_B/0/1/0/all/0/1&quot;&gt;Babak Hodjat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_L/0/1/0/all/0/1&quot;&gt;Laurent Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knibbe_C/0/1/0/all/0/1&quot;&gt;Carole Knibbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krcah_P/0/1/0/all/0/1&quot;&gt;Peter Krcah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenski_R/0/1/0/all/0/1&quot;&gt;Richard E. Lenski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacCurdy_R/0/1/0/all/0/1&quot;&gt;Robert MacCurdy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maestre_C/0/1/0/all/0/1&quot;&gt;Carlos Maestre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitri_S/0/1/0/all/0/1&quot;&gt;Sara Mitri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moriarty_D/0/1/0/all/0/1&quot;&gt;David E. Moriarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ofria_C/0/1/0/all/0/1&quot;&gt;Charles Ofria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parizeau_M/0/1/0/all/0/1&quot;&gt;Marc Parizeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsons_D/0/1/0/all/0/1&quot;&gt;David Parsons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennock_R/0/1/0/all/0/1&quot;&gt;Robert T. Pennock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Punch_W/0/1/0/all/0/1&quot;&gt;William F. Punch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_T/0/1/0/all/0/1&quot;&gt;Thomas S. Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenauer_M/0/1/0/all/0/1&quot;&gt;Marc Schoenauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shulte_E/0/1/0/all/0/1&quot;&gt;Eric Shulte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sims_K/0/1/0/all/0/1&quot;&gt;Karl Sims&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taddei_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Taddei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarapore_D/0/1/0/all/0/1&quot;&gt;Danesh Tarapore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thibault_S/0/1/0/all/0/1&quot;&gt;Simon Thibault&lt;/a&gt;, et al. (3 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11256">
<title>Challenges and Characteristics of Intelligent Autonomy for Internet of Battle Things in Highly Adversarial Environments. (arXiv:1803.11256v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1803.11256</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous, artificially intelligent, networked things will populate the
battlefield of the future, operating in close collaboration with human
warfighters, and fighting as teams in highly adversarial environments. This
paper explores the characteristics, capabilities and intelligence required of
such a network of intelligent things and humans - Internet of Battle Things
(IOBT). It will experience unique challenges that are not yet well addressed by
the current generation of AI and machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kott_A/0/1/0/all/0/1&quot;&gt;Alexander Kott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11373">
<title>Learning to generate classifiers. (arXiv:1803.11373v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.11373</link>
<description rdf:parseType="Literal">&lt;p&gt;We train a network to generate mappings between training sets and
classification policies (a &apos;classifier generator&apos;) by conditioning on the
entire training set via an attentional mechanism. The network is directly
optimized for test set performance on an training set of related tasks, which
is then transferred to unseen &apos;test&apos; tasks. We use this to optimize for
performance in the low-data and unsupervised learning regimes, and obtain
significantly better performance in the 10-50 datapoint regime than support
vector classifiers, random forests, XGBoost, and k-nearest neighbors on a range
of small datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttenberg_N/0/1/0/all/0/1&quot;&gt;Nicholas Guttenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanai_R/0/1/0/all/0/1&quot;&gt;Ryota Kanai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11439">
<title>Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present. (arXiv:1803.11439v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.11439</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, caption generation with an encoder-decoder framework has been
extensively studied and applied in different domains, such as image captioning,
code captioning, and so on. In this paper, we propose a novel architecture,
namely Auto-Reconstructor Network (ARNet), which, coupling with the
conventional encoder-decoder framework, works in an end-to-end fashion to
generate captions. ARNet aims at reconstructing the previous hidden state with
the present one, besides behaving as the input-dependent transition operator.
Therefore, ARNet encourages the current hidden state to embed more information
from the previous one, which can help regularize the transition dynamics of
recurrent neural networks (RNNs). Extensive experimental results show that our
proposed ARNet boosts the performance over the existing encoder-decoder models
on both image captioning and source code captioning tasks. Additionally, ARNet
remarkably reduces the discrepancy between training and inference processes for
caption generation. Furthermore, the performance on permuted sequential MNIST
demonstrates that ARNet can effectively regularize RNN, especially on modeling
long-term dependencies. Our code is available at:
https://github.com/chenxinpeng/ARNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinpeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jian Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10772">
<title>Tensorizing Generative Adversarial Nets. (arXiv:1710.10772v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10772</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Network (GAN) and its variants exhibit
state-of-the-art performance in the class of generative models. To capture
higher-dimensional distributions, the common learning procedure requires high
computational complexity and a large number of parameters. The problem of
employing such massive framework arises when deploying it on a platform with
limited computational power such as mobile phones. In this paper, we present a
new generative adversarial framework by representing each layer as a tensor
structure connected by multilinear operations, aiming to reduce the number of
model parameters by a large factor while preserving the generative performance
and sample quality. To learn the model, we employ an efficient algorithm which
alternatively optimizes both discriminator and generator. Experimental outcomes
demonstrate that our model can achieve high compression rate for model
parameters up to $35$ times when compared to the original GAN for MNIST
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xingwei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xuyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qibin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11347">
<title>Learning to Adapt: Meta-Learning for Model-Based Control. (arXiv:1803.11347v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.11347</link>
<description rdf:parseType="Literal">&lt;p&gt;Although reinforcement learning methods can achieve impressive results in
simulation, the real world presents two major challenges: generating samples is
exceedingly expensive, and unexpected perturbations can cause proficient but
narrowly-learned policies to fail at test time. In this work, we propose to
learn how to quickly and effectively adapt online to new situations as well as
to perturbations. To enable sample-efficient meta-learning, we consider
learning online adaptation in the context of model-based reinforcement
learning. Our approach trains a global model such that, when combined with
recent data, the model can be be rapidly adapted to the local context. Our
experiments demonstrate that our approach can enable simulated agents to adapt
their behavior online to novel terrains, to a crippled leg, and in
highly-dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clavera_I/0/1/0/all/0/1&quot;&gt;Ignasi Clavera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1&quot;&gt;Anusha Nagabandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fearing_R/0/1/0/all/0/1&quot;&gt;Ronald S. Fearing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11521">
<title>Online Regression with Model Selection. (arXiv:1803.11521v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.11521</link>
<description rdf:parseType="Literal">&lt;p&gt;Online learning algorithms have a wide variety of applications in large scale
machine learning problems due to their low computational and memory
requirements. However, standard online learning methods still suffer some
issues such as lower convergence rates and limited capability to select
features or to recover the true features. In this paper, we present a novel
framework for online learning based on running averages and introduce a series
of online versions of some popular existing offline algorithms such as Adaptive
Lasso, Elastic Net and Feature Selection with Annealing. We prove the
equivalence between our online methods and their offline counterparts and give
theoretical feature selection and convergence guarantees for some of them. In
contrast to the existing online methods, the proposed methods can extract model
with any desired sparsity level at any time. Numerical experiments indicate
that our new methods enjoy high feature selection accuracy and a fast
convergence rate, compared with standard stochastic algorithms and offline
learning algorithms. We also present some applications to large datasets where
again the proposed framework shows competitive results compared to popular
online and offline algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lizhe Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barbu_A/0/1/0/all/0/1&quot;&gt;Adrian Barbu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11551">
<title>The eigenvalues of stochastic blockmodel graphs. (arXiv:1803.11551v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.11551</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive the limiting distribution for the largest eigenvalues of the
adjacency matrix for a stochastic blockmodel graph when the number of vertices
tends to infinity. We show that, in the limit, these eigenvalues are jointly
multivariate normal with bounded covariances. Our result extends the classic
result of F\&quot;{u}redi and Koml\&apos;{o}s on the fluctuation of the largest
eigenvalue for Erd\H{o}s-R\&apos;{e}nyi graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Minh Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.01394">
<title>On spectral partitioning of signed graphs. (arXiv:1701.01394v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1701.01394</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that the standard graph Laplacian is preferable for spectral
partitioning of signed graphs compared to the signed Laplacian. Simple examples
demonstrate that partitioning based on signs of components of the leading
eigenvectors of the signed Laplacian may be meaningless, in contrast to
partitioning based on the Fiedler vector of the standard graph Laplacian for
signed graphs. We observe that negative eigenvalues are beneficial for spectral
partitioning of signed graphs, making the Fiedler vector easier to compute.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knyazev_A/0/1/0/all/0/1&quot;&gt;Andrew V. Knyazev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07164">
<title>Relaxed Wasserstein with Applications to GANs. (arXiv:1705.07164v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07164</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel class of statistical divergences called \textit{Relaxed
Wasserstein} (RW) divergence. RW divergence generalizes Wasserstein divergence
and is parametrized by a class of strictly convex and differentiable functions.
We establish for RW divergence several probabilistic properties, which are
critical for the success of Wasserstein divergence. In particular, we show that
RW divergence is dominated by Total Variation (TV) and Wasserstein-$L^2$
divergence, and that RW divergence has continuity, differentiability and
duality representation. Finally, we provide a nonasymptotic moment estimate and
a concentration inequality for RW divergence.
&lt;/p&gt;
&lt;p&gt;Our experiments on the image generation task demonstrate that RW divergence
is a suitable choice for GANs. Indeed, the performance of RWGANs with
Kullback-Leibler (KL) divergence is very competitive with other
state-of-the-art GANs approaches. Furthermore, RWGANs possess better
convergence properties than the existing WGANs with competitive inception
scores. To the best of our knowledge, our new conceptual framework is the first
to not only provide the flexibility in designing effective GANs scheme, but
also the possibility in studying different losses under a unified mathematical
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Johnny Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianyi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Nan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08502">
<title>Convolutional Neural Knowledge Graph Learning. (arXiv:1710.08502v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08502</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous models for learning entity and relationship embeddings of knowledge
graphs such as TransE, TransH, and TransR aim to explore new links based on
learned representations. However, these models interpret relationships as
simple translations on entity embeddings. In this paper, we try to learn more
complex connections between entities and relationships. In particular, we use a
Convolutional Neural Network (CNN) to learn entity and relationship
representations in knowledge graphs. In our model, we treat entities and
relationships as one-dimensional numerical sequences with the same length.
After that, we combine each triplet of head, relationship, and tail together as
a matrix with height 3. CNN is applied to the triplets to get confidence
scores. Positive and manually corrupted negative triplets are used to train the
embeddings and the CNN model simultaneously. Experimental results on public
benchmark datasets show that the proposed model outperforms state-of-the-art
models on exploring unseen relationships, which proves that CNN is effective to
learn complex interactive patterns between entities and relationships.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feipeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1&quot;&gt;Martin Renqiang Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Amit Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10743">
<title>Intertwiners between Induced Representations (with Applications to the Theory of Equivariant Neural Networks). (arXiv:1803.10743v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10743</link>
<description rdf:parseType="Literal">&lt;p&gt;Group equivariant and steerable convolutional neural networks (regular and
steerable G-CNNs) have recently emerged as a very effective model class for
learning from signal data such as 2D and 3D images, video, and other data where
symmetries are present. In geometrical terms, regular G-CNNs represent data in
terms of scalar fields (&quot;feature channels&quot;), whereas the steerable G-CNN can
also use vector or tensor fields (&quot;capsules&quot;) to represent data. In algebraic
terms, the feature spaces in regular G-CNNs transform according to a regular
representation of the group G, whereas the feature spaces in Steerable G-CNNs
transform according to the more general induced representations of G. In order
to make the network equivariant, each layer in a G-CNN is required to
intertwine between the induced representations associated with its input and
output space.
&lt;/p&gt;
&lt;p&gt;In this paper we present a general mathematical framework for G-CNNs on
homogeneous spaces like Euclidean space or the sphere. We show, using
elementary methods, that the layers of an equivariant network are convolutional
if and only if the input and output feature spaces transform according to an
induced representation. This result, which follows from G.W. Mackey&apos;s abstract
theory on induced representations, establishes G-CNNs as a universal class of
equivariant network architectures, and generalizes the important recent work of
Kondor &amp;amp; Trivedi on the intertwiners between regular representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco S. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1&quot;&gt;Mario Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1&quot;&gt;Maurice Weiler&lt;/a&gt;</dc:creator>
</item></rdf:RDF>