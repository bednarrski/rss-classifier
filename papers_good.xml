<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01418"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01430"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01628"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01659"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01672"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.04582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01349"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01367"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01619"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1508.01993"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01202"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.01418">
<title>Selective Deep Convolutional Neural Network for Low Cost Distorted Image Classification. (arXiv:1807.01418v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.01418</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks have proven to be well suited for image
classification applications. However, if there is distortion in the image, the
classification accuracy can be significantly degraded, even with
state-of-the-art neural networks. The accuracy cannot be significantly improved
by simply training with distorted images. Instead, this paper proposes a
multiple neural network topology referred to as a selective deep convolutional
neural network. By modifying existing state-of-the-art neural networks in the
proposed manner, it is shown that a similar level of classification accuracy
can be achieved, but at a significantly lower cost. The cost reduction is
obtained primarily through the use of fewer weight parameters. Using fewer
weights reduces the number of multiply-accumulate operations and also reduces
the energy required for data accesses. Finally, it is shown that the
effectiveness of the proposed selective deep convolutional neural network can
be further improved by combining it with previously proposed network cost
reduction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1&quot;&gt;Minho Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byeon_Y/0/1/0/all/0/1&quot;&gt;Younghoon Byeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Youngjoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sunggu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01430">
<title>SGAD: Soft-Guided Adaptively-Dropped Neural Network. (arXiv:1807.01430v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01430</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been proven to have many redundancies.
Hence, many efforts have been made to compress DNNs. However, the existing
model compression methods treat all the input samples equally while ignoring
the fact that the difficulties of various input samples being correctly
classified are different. To address this problem, DNNs with adaptive dropping
mechanism are well explored in this work. To inform the DNNs how difficult the
input samples can be classified, a guideline that contains the information of
input samples is introduced to improve the performance. Based on the developed
guideline and adaptive dropping mechanism, an innovative soft-guided
adaptively-dropped (SGAD) neural network is proposed in this paper. Compared
with the 32 layers residual neural networks, the presented SGAD can reduce the
FLOPs by 77% with less than 1% drop in accuracy on CIFAR-10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fangxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08727">
<title>Improving the Neural GPU Architecture for Algorithm Learning. (arXiv:1702.08727v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08727</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithm learning is a core problem in artificial intelligence with
significant implications on automation level that can be achieved by machines.
Recently deep learning methods are emerging for synthesizing an algorithm from
its input-output examples, the most successful being the Neural GPU, capable of
learning multiplication. We present several improvements to the Neural GPU that
substantially reduces training time and improves generalization. We introduce a
new technique - hard nonlinearities with saturation costs- that has general
applicability. We also introduce a technique of diagonal gates that can be
applied to active-memory models. The proposed architecture is the first capable
of learning decimal multiplication end-to-end.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freivalds_K/0/1/0/all/0/1&quot;&gt;Karlis Freivalds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liepins_R/0/1/0/all/0/1&quot;&gt;Renars Liepins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09926">
<title>Rapid Adaptation with Conditionally Shifted Neurons. (arXiv:1712.09926v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09926</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a mechanism by which artificial neural networks can learn rapid
adaptation - the ability to adapt on the fly, with little data, to new tasks -
that we call conditionally shifted neurons. We apply this mechanism in the
framework of metalearning, where the aim is to replicate some of the
flexibility of human learning in machines. Conditionally shifted neurons modify
their activation values with task-specific shifts retrieved from a memory
module, which is populated rapidly based on limited task experience. On
metalearning benchmarks from the vision and language domains, models augmented
with conditionally shifted neurons achieve state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munkhdalai_T/0/1/0/all/0/1&quot;&gt;Tsendsuren Munkhdalai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xingdi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1&quot;&gt;Soroush Mehri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1&quot;&gt;Adam Trischler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01554">
<title>Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding. (arXiv:1807.01554v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.01554</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of data augmentation for language
understanding in task-oriented dialogue system. In contrast to previous work
which augments an utterance without considering its relation with other
utterances, we propose a sequence-to-sequence generation based data
augmentation framework that leverages one utterance&apos;s same semantic
alternatives in the training data. A novel diversity rank is incorporated into
the utterance representation to make the model produce diverse utterances and
these diversely augmented utterances help to improve the language understanding
module. Experimental results on the Airline Travel Information System dataset
and a newly created semantic frame annotation on Stanford Multi-turn,
Multidomain Dialogue Dataset show that our framework achieves significant
improvements of 6.38 and 10.04 F-scores respectively when only a training set
of hundreds utterances is represented. Case studies also confirm that our
method generates diverse utterances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yutai Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1&quot;&gt;Wanxiang Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01586">
<title>Answering Hindsight Queries with Lifted Dynamic Junction Trees. (arXiv:1807.01586v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.01586</link>
<description rdf:parseType="Literal">&lt;p&gt;The lifted dynamic junction tree algorithm (LDJT) efficiently answers
filtering and prediction queries for probabilistic relational temporal models
by building and then reusing a first-order cluster representation of a
knowledge base for multiple queries and time steps. We extend LDJT to (i) solve
the smoothing inference problem to answer hindsight queries by introducing an
efficient backward pass and (ii) discuss different options to instantiate a
first-order cluster representation during a backward pass. Further, our
relational forward backward algorithm makes hindsight queries to the very
beginning feasible. LDJT answers multiple temporal queries faster than the
static lifted junction tree algorithm on an unrolled model, which performs
smoothing during message passing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrke_M/0/1/0/all/0/1&quot;&gt;Marcel Gehrke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_T/0/1/0/all/0/1&quot;&gt;Tanya Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1&quot;&gt;Ralf M&amp;#xf6;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01628">
<title>Partially Observable Reinforcement Learning for Intelligent Transportation Systems. (arXiv:1807.01628v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.01628</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent Transportation Systems (ITS) have attracted the attention of
researchers and the general public alike as a means to alleviate traffic
congestion. Recently, the maturity of wireless technology has enabled a
cost-efficient way to achieve ITS by detecting vehicles using Vehicle to
Infrastructure (V2I) communications. Traditional ITS algorithms, in most cases,
assume that every vehicle is observed, such as by a camera or a loop detector,
but a V2I implementation would detect only those vehicles with wireless
communications capability. We examine a family of transportation systems, which
we will refer to as `Partially Detected Intelligent Transportation Systems&apos;. An
algorithm that can act well under a small detection rate is highly desirable
due to gradual penetration rates of the underlying wireless technologies such
as Dedicated Short Range Communications (DSRC) technology. Artificial
Intelligence (AI) techniques for Reinforcement Learning (RL) are suitable tools
for finding such an algorithm due to utilizing varied inputs and not requiring
explicit analytic understanding or modeling of the underlying system dynamics.
In this paper, we report a RL algorithm for partially observable ITS based on
DSRC. The performance of this system is studied under different car flows,
detection rates, and topologies of the road network. Our system is able to
efficiently reduce the average waiting time of vehicles at an intersection,
even with a low detection rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rusheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_A/0/1/0/all/0/1&quot;&gt;Akihiro Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Striner_B/0/1/0/all/0/1&quot;&gt;Benjamin Striner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonguz_O/0/1/0/all/0/1&quot;&gt;Ozan Tonguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01659">
<title>MIXGAN: Learning Concepts from Different Domains for Mixture Generation. (arXiv:1807.01659v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01659</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present an interesting attempt on mixture generation:
absorbing different image concepts (e.g., content and style) from different
domains and thus generating a new domain with learned concepts. In particular,
we propose a mixture generative adversarial network (MIXGAN). MIXGAN learns
concepts of content and style from two domains respectively, and thus can join
them for mixture generation in a new domain, i.e., generating images with
content from one domain and style from another. MIXGAN overcomes the limitation
of current GAN-based models which either generate new images in the same domain
as they observed in training stage, or require off-the-shelf content templates
for transferring or translation. Extensive experimental results demonstrate the
effectiveness of MIXGAN as compared to related state-of-the-art GAN-based
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1&quot;&gt;Guang-Yuan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-Shi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01672">
<title>Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization. (arXiv:1807.01672v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01672</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial self-play in two-player games has delivered impressive results
when used with reinforcement learning algorithms that combine deep neural
networks and tree search. Algorithms like AlphaZero and Expert Iteration learn
tabula-rasa, producing highly informative training data on the fly. However,
the self-play training strategy is not directly applicable to single-player
games. Recently, several practically important combinatorial optimization
problems, such as the traveling salesman problem and the bin packing problem,
have been reformulated as reinforcement learning problems, increasing the
importance of enabling the benefits of self-play beyond two-player games. We
present the Ranked Reward (R2) algorithm which accomplishes this by ranking the
rewards obtained by a single agent over multiple games to create a relative
performance metric. Results from applying the R2 algorithm to instances of a
two-dimensional bin packing problem show that it outperforms generic Monte
Carlo tree search, heuristic algorithms and reinforcement learning algorithms
not using ranked rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laterre_A/0/1/0/all/0/1&quot;&gt;Alexandre Laterre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yunguan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabri_M/0/1/0/all/0/1&quot;&gt;Mohamed Khalil Jabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Alain-Sam Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kas_D/0/1/0/all/0/1&quot;&gt;David Kas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajjar_K/0/1/0/all/0/1&quot;&gt;Karl Hajjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_T/0/1/0/all/0/1&quot;&gt;Torbjorn S. Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerkeni_A/0/1/0/all/0/1&quot;&gt;Amine Kerkeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beguir_K/0/1/0/all/0/1&quot;&gt;Karim Beguir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01675">
<title>Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion. (arXiv:1807.01675v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01675</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating model-free and model-based approaches in reinforcement learning
has the potential to achieve the high performance of model-free algorithms with
low sample complexity. However, this is difficult because an imperfect dynamics
model can degrade the performance of the learning algorithm, and in
sufficiently complex environments, the dynamics model will almost always be
imperfect. As a result, a key challenge is to combine model-based approaches
with model-free learning in such a way that errors in the model do not degrade
performance. We propose stochastic ensemble value expansion (STEVE), a novel
model-based technique that addresses this issue. By dynamically interpolating
between model rollouts of various horizon lengths for each individual example,
STEVE ensures that the model is only utilized when doing so does not introduce
significant errors. Our approach outperforms model-free baselines on
challenging continuous control benchmarks with an order-of-magnitude increase
in sample efficiency, and in contrast to previous model-based approaches,
performance does not degrade in complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckman_J/0/1/0/all/0/1&quot;&gt;Jacob Buckman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafner_D/0/1/0/all/0/1&quot;&gt;Danijar Hafner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1&quot;&gt;George Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1&quot;&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.04582">
<title>Search versus Decision: The Opacity of Backbones and Backdoors Under a Weak Assumption. (arXiv:1706.04582v7 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.04582</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoors and backbones of Boolean formulas are hidden structural properties.
A natural goal, already in part realized, is that solver algorithms seek to
obtain substantially better performance by exploiting these structures.
&lt;/p&gt;
&lt;p&gt;However, the present paper is not intended to improve the performance of SAT
solvers, but rather is a cautionary paper. In particular, the theme of this
paper is that there is a potential chasm between the existence of such
structures in the Boolean formula and being able to effectively exploit them.
This does not mean that these structures are not useful to solvers. It does
mean that one must be very careful not to assume that it is computationally
easy to go from the existence of a structure to being able to get one&apos;s hands
on it and/or being able to exploit the structure.
&lt;/p&gt;
&lt;p&gt;For example, in this paper we show that, under the assumption that P $\neq$
NP, there are easily recognizable families of Boolean formulas with strong
backdoors that are easy to find, yet for which it is hard (in fact,
NP-complete) to determine whether the formulas are satisfiable. We also show
that, also under the assumption P $\neq$ NP, there are easily recognizable sets
of Boolean formulas for which it is hard (in fact, NP-complete) to determine
whether they have a large backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemaspaandra_L/0/1/0/all/0/1&quot;&gt;Lane A. Hemaspaandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narvaez_D/0/1/0/all/0/1&quot;&gt;David E. Narv&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06188">
<title>VC-Dimension Based Generalization Bounds for Relational Learning. (arXiv:1804.06188v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06188</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications of relational learning, the available data can be seen
as a sample from a larger relational structure (e.g. we may be given a small
fragment from some social network). In this paper we are particularly concerned
with scenarios in which we can assume that (i) the domain elements appearing in
the given sample have been uniformly sampled without replacement from the
(unknown) full domain and (ii) the sample is complete for these domain elements
(i.e. it is the full substructure induced by these elements). Within this
setting, we study bounds on the error of sufficient statistics of relational
models that are estimated on the available data. As our main result, we prove a
bound based on a variant of the Vapnik-Chervonenkis dimension which is suitable
for relational data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzelka_O/0/1/0/all/0/1&quot;&gt;Ondrej Kuzelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01270">
<title>Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study. (arXiv:1807.01270v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01270</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural sequence-to-sequence (seq2seq) approaches have proven to be successful
in grammatical error correction (GEC). Based on the seq2seq framework, we
propose a novel fluency boost learning and inference mechanism. Fluency
boosting learning generates diverse error-corrected sentence pairs during
training, enabling the error correction model to learn how to improve a
sentence&apos;s fluency from more instances, while fluency boosting inference allows
the model to correct a sentence incrementally with multiple inference steps.
Combining fluency boost learning and inference with convolutional seq2seq
models, our approach achieves the state-of-the-art performance: 75.72 (F_{0.5})
on CoNLL-2014 10 annotation dataset and 62.42 (GLEU) on JFLEG test set
respectively, becoming the first GEC system that reaches human-level
performance (72.58 for CoNLL and 62.37 for JFLEG) on both of the benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01349">
<title>Anomaly Detection for Skin Disease Images Using Variational Autoencoder. (arXiv:1807.01349v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01349</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we demonstrate the potential of applying Variational
Autoencoder (VAE) [10] for anomaly detection in skin disease images. VAE is a
class of deep generative models which is trained by maximizing the evidence
lower bound of data distribution [10]. When trained on only normal data, the
resulting model is able to perform efficient inference and to determine if a
test image is normal or not. We perform experiments on ISIC2018 Challenge
Disease Classification dataset (Task 3) and compare different methods to use
VAE to detect anomaly. The model is able to detect all diseases with 0.779
AUCROC. If we focus on specific diseases, the model is able to detect melanoma
with 0.864 AUCROC and detect actinic keratosis with 0.872 AUCROC, even if it
only sees the images of nevus. To the best of our knowledge, this is the first
applied work of deep generative models for anomaly detection in dermatology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuchen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01367">
<title>EmbNum: Semantic labeling for numerical values with deep metric learning. (arXiv:1807.01367v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1807.01367</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic labeling is a task of matching unknown data source to labeled data
sources. The semantic labels could be properties, classes in knowledge bases or
labeled data are manually annotated by domain experts. In this paper, we
presentEmbNum, a novel approach to match numerical columns from different table
data sources. We use a representation network architecture consisting of
triplet network and convolutional neural network to learn a mapping function
from numerical columns toa transformed space. In this space, the Euclidean
distance can be used to measure &quot;semantic similarity&quot; of two columns. Our
experiments onCity-Data and Open-Data demonstrate thatEmbNumachieves
considerable improvements in comparison with the state-of-the-art methods in
effectiveness and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khai Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichise_R/0/1/0/all/0/1&quot;&gt;Ryutaro Ichise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_H/0/1/0/all/0/1&quot;&gt;Hideaki Takeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01406">
<title>Connecting Weighted Automata and Recurrent Neural Networks through Spectral Learning. (arXiv:1807.01406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01406</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we unravel a fundamental connection between weighted finite
automata~(WFAs) and second-order recurrent neural networks~(2-RNNs): in the
case of sequences of discrete symbols, WFAs and 2-RNNs with linear activation
functions are expressively equivalent. Motivated by this result, we build upon
a recent extension of the spectral learning algorithm to vector-valued WFAs and
propose the first provable learning algorithm for linear 2-RNNs defined over
sequences of continuous input vectors. This algorithm relies on estimating low
rank sub-blocks of the so-called Hankel tensor, from which the parameters of a
linear 2-RNN can be provably recovered. The performances of the proposed method
are assessed in a simulation study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1&quot;&gt;Guillaume Rabusseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01619">
<title>Ensemble learning with Conformal Predictors: Targeting credible predictions of conversion from Mild Cognitive Impairment to Alzheimer&apos;s Disease. (arXiv:1807.01619v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01619</link>
<description rdf:parseType="Literal">&lt;p&gt;Most machine learning classifiers give predictions for new examples
accurately, yet without indicating how trustworthy predictions are. In the
medical domain, this hampers their integration in decision support systems,
which could be useful in the clinical practice. We use a supervised learning
approach that combines Ensemble learning with Conformal Predictors to predict
conversion from Mild Cognitive Impairment to Alzheimer&apos;s Disease. Our goal is
to enhance the classification performance (Ensemble learning) and complement
each prediction with a measure of credibility (Conformal Predictors). Our
results showed the superiority of the proposed approach over a similar ensemble
framework with standard classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_T/0/1/0/all/0/1&quot;&gt;Telma Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_S/0/1/0/all/0/1&quot;&gt;Sandra Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1&quot;&gt;Dina Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerreiro_M/0/1/0/all/0/1&quot;&gt;Manuela Guerreiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendonca_A/0/1/0/all/0/1&quot;&gt;Alexandre de Mendon&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madeira_S/0/1/0/all/0/1&quot;&gt;Sara C. Madeira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1508.01993">
<title>Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures. (arXiv:1508.01993v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1508.01993</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision analytics commonly focuses on the text mining of financial news
sources in order to provide managerial decision support and to predict stock
market movements. Existing predictive frameworks almost exclusively apply
traditional machine learning methods, whereas recent research indicates that
traditional machine learning methods are not sufficiently capable of extracting
suitable features and capturing the non-linear nature of complex tasks. As a
remedy, novel deep learning models aim to overcome this issue by extending
traditional neural network models with additional hidden layers. Indeed, deep
learning has been shown to outperform traditional methods in terms of
predictive performance. In this paper, we adapt the novel deep learning
technique to financial decision support. In this instance, we aim to predict
the direction of stock movements following financial disclosures. As a result,
we show how deep learning can outperform the accuracy of random forests as a
benchmark for machine learning by 5.66%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feuerriegel_S/0/1/0/all/0/1&quot;&gt;Stefan Feuerriegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fehrer_R/0/1/0/all/0/1&quot;&gt;Ralph Fehrer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09385">
<title>An Unsupervised Learning Classifier with Competitive Error Performance. (arXiv:1806.09385v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09385</link>
<description rdf:parseType="Literal">&lt;p&gt;An unsupervised learning classification model is described. It achieves
classification error probability competitive with that of popular supervised
learning classifiers such as SVM or kNN. The model is based on the incremental
execution of small step shift and rotation operations upon selected
discriminative hyperplanes at the arrival of input samples. When applied, in
conjunction with a selected feature extractor, to a subset of the ImageNet
dataset benchmark, it yields 6.2 % Top 3 probability of error; this exceeds by
merely about 2 % the result achieved by (supervised) k-Nearest Neighbor, both
using same feature extractor. This result may also be contrasted with popular
unsupervised learning schemes such as k-Means which is shown to be practically
useless on same dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nissani_D/0/1/0/all/0/1&quot;&gt;Daniel N. Nissani&lt;/a&gt; (Nissensohn)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10909">
<title>ResNet with one-neuron hidden layers is a Universal Approximator. (arXiv:1806.10909v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10909</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate that a very deep ResNet with stacked modules with one neuron
per hidden layer and ReLU activation functions can uniformly approximate any
Lebesgue integrable function in $d$ dimensions, i.e. $\ell_1(\mathbb{R}^d)$.
Because of the identity mapping inherent to ResNets, our network has
alternating layers of dimension one and $d$. This stands in sharp contrast to
fully connected networks, which are not universal approximators if their width
is the input dimension $d$ [Lu et al, 2017; Hanin and Sellke, 2017]. Hence, our
result implies an increase in representational power for narrow deep networks
by the ResNet architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongzhou Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00560">
<title>Weight-importance sparse training in keyword spotting. (arXiv:1807.00560v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00560</link>
<description rdf:parseType="Literal">&lt;p&gt;Large size models are implemented in recently ASR system to deal with complex
speech recognition problems. The num- ber of parameters in these models makes
them hard to deploy, especially on some resource-short devices such as car
tablet. Besides this, at most of time, ASR system is used to deal with
real-time problem such as keyword spotting (KWS). It is contradictory to the
fact that large model requires long com- putation time. To deal with this
problem, we apply some sparse algo- rithms to reduces number of parameters in
some widely used models, Deep Neural Network (DNN) KWS, which requires real
short computation time. We can prune more than 90 % even 95% of parameters in
the model with tiny effect decline. And the sparse model performs better than
baseline models which has same order number of parameters. Besides this, sparse
algorithm can lead us to find rational model size au- tomatically for certain
problem without concerning choosing an original model size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1&quot;&gt;Sihao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1&quot;&gt;Fan Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Min Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jue Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01082">
<title>Domain Aware Markov Logic Networks. (arXiv:1807.01082v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01082</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining logic and probability has been a long standing goal of AI. Markov
Logic Networks (MLNs) achieve this by attaching weights to formulae in
first-order logic, and can be seen as templates for constructing features for
ground Markov networks. Most techniques for learning weights of MLNs are
domain-size agnostic, i.e., the size of the domain is not explicitly taken into
account while learning the parameters of the model. This results in incorrect
(often extreme) probabilities when testing on domain sizes different from those
seen during training times. In this paper, we propose Domain Aware Markov logic
Networks (DA-MLNs) which present a principled solution to this problem, by
dividing the ground feature weight by a function of the number of connections
each ground atom (in the feature) is involved in, when defining the ground
Markov network distribution. We show that standard MLNs fall out as a special
case of our formalism when this function is a constant (and is equal to 1).
Experiments on a benchmark domain show that our approach results in
significantly higher accuracies (compared to baselines) when testing on domain
sizes different than those seen during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_H/0/1/0/all/0/1&quot;&gt;Happy Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1&quot;&gt;Ayush Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gogate_V/0/1/0/all/0/1&quot;&gt;Vibhav Gogate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1&quot;&gt;Parag Singla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01202">
<title>Generating Multi-Categorical Samples with Generative Adversarial Networks. (arXiv:1807.01202v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01202</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to train generative adversarial networks on mutivariate
feature vectors representing multiple categorical values. In contrast to the
continuous domain, where GAN-based methods have delivered considerable results,
GANs struggle to perform equally well on discrete data. We propose and compare
several architectures based on multiple (Gumbel) softmax output layers taking
into account the structure of the data. We evaluate the performance of our
architecture on datasets with different sparsity, number of features, ranges of
categorical values, and dependencies among the features. Our proposed
architecture and method outperforms existing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Camino_R/0/1/0/all/0/1&quot;&gt;Ramiro Camino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hammerschmidt_C/0/1/0/all/0/1&quot;&gt;Christian Hammerschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+State_R/0/1/0/all/0/1&quot;&gt;Radu State&lt;/a&gt;</dc:creator>
</item></rdf:RDF>