<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05377"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01498"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01628"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07980"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01129"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01477"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01478"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01496"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01506"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05052"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07594"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09540"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00403"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.05978">
<title>Bayesian Convolutional Neural Networks. (arXiv:1806.05978v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05978</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Bayesian Convolutional Neural Networks (BayesCNNs), a variant of
Convolutional Neural Networks (CNNs) which is built upon Bayes by Backprop. We
demonstrate how this novel reliable variational inference method can serve as a
fundamental construct for various network architectures. On multiple datasets
in supervised learning settings (MNIST, CIFAR-10, CIFAR-100, and STL-10), our
proposed variational inference method achieves performances equivalent to
frequentist inference in identical architectures, while a measurement for
uncertainties and a regularisation are incorporated naturally. In the past,
Bayes by Backprop has been successfully implemented in feedforward and
recurrent neural networks, but not in convolutional ones. This work symbolises
the extension of Bayesian neural networks which encompasses all three
aforementioned types of network architectures now.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1&quot;&gt;Kumar Shridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laumann_F/0/1/0/all/0/1&quot;&gt;Felix Laumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maurin_A/0/1/0/all/0/1&quot;&gt;Adrian Llopart Maurin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1&quot;&gt;Marcus Liwicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05377">
<title>Neural Architecture Search: A Survey. (arXiv:1808.05377v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05377</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning has enabled remarkable progress over the last years on a
variety of tasks, such as image recognition, speech recognition, and machine
translation. One crucial aspect for this progress are novel neural
architectures. Currently employed architectures have mostly been developed
manually by human experts, which is a time-consuming and error-prone process.
Because of this, there is growing interest in automated neural architecture
search methods. We provide an overview of existing work in this field of
research and categorize them according to three dimensions: search space,
search strategy, and performance estimation strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elsken_T/0/1/0/all/0/1&quot;&gt;Thomas Elsken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Metzen_J/0/1/0/all/0/1&quot;&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01272">
<title>Unsupervised Statistical Machine Translation. (arXiv:1809.01272v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01272</link>
<description rdf:parseType="Literal">&lt;p&gt;While modern machine translation has relied on large parallel corpora, a
recent line of work has managed to train Neural Machine Translation (NMT)
systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al.,
2018). Despite the potential of this approach for low-resource settings,
existing systems are far behind their supervised counterparts, limiting their
practical interest. In this paper, we propose an alternative approach based on
phrase-based Statistical Machine Translation (SMT) that significantly closes
the gap with supervised systems. Our method profits from the modular
architecture of SMT: we first induce a phrase table from monolingual corpora
through cross-lingual embedding mappings, combine it with an n-gram language
model, and fine-tune hyperparameters through an unsupervised MERT variant. In
addition, iterative backtranslation improves results further, yielding, for
instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and
English-French, respectively, an improvement of more than 7-10 BLEU points over
previous unsupervised systems, and closing the gap with supervised SMT (Moses
trained on Europarl) down to 2-5 BLEU points. Our implementation is available
at https://github.com/artetxem/monoses
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1&quot;&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1&quot;&gt;Gorka Labaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1&quot;&gt;Eneko Agirre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01341">
<title>Embedding Multimodal Relational Data for Knowledge Base Completion. (arXiv:1809.01341v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.01341</link>
<description rdf:parseType="Literal">&lt;p&gt;Representing entities and relations in an embedding space is a well-studied
approach for machine learning on relational data. Existing approaches, however,
primarily focus on simple link structure between a finite set of entities,
ignoring the variety of data types that are often used in knowledge bases, such
as text, images, and numerical values. In this paper, we propose multimodal
knowledge base embeddings (MKBE) that use different neural encoders for this
variety of observed data, and combine them with existing relational models to
learn embeddings of the entities and multimodal data. Further, using these
learned embedings and different neural decoders, we introduce a novel
multimodal imputation model to generate missing multimodal values, like text
and images, from information in the knowledge base. We enrich existing
relational datasets to create two novel benchmarks that contain additional
information such as textual descriptions and images of the original entities.
We demonstrate that our models utilize this additional information effectively
to provide more accurate link prediction, achieving state-of-the-art results
with a considerable gap of 5-7% over existing methods. Further, we evaluate the
quality of our generated multimodal values via a user study. We have release
the datasets and the open-source implementation of our models at
https://github.com/pouyapez/mkbe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1&quot;&gt;Pouya Pezeshkpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sameer Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01497">
<title>Chinese Discourse Segmentation Using Bilingual Discourse Commonality. (arXiv:1809.01497v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01497</link>
<description rdf:parseType="Literal">&lt;p&gt;Discourse segmentation aims to segment Elementary Discourse Units (EDUs) and
is a fundamental task in discourse analysis. For Chinese, previous researches
identify EDUs just through discriminating the functions of punctuations. In
this paper, we argue that Chinese EDUs may not end at the punctuation positions
and should follow the definition of EDU in RST-DT. With this definition, we
conduct Chinese discourse segmentation with the help of English labeled
data.Using discourse commonality between English and Chinese, we design an
adversarial neural network framework to extract common language-independent
features and language-specific features which are useful for discourse
segmentation, when there is no or only a small scale of Chinese labeled data
available. Experiments on discourse segmentation demonstrate that our models
can leverage common features from bilingual data, and learn efficient
Chinese-specific features from a small amount of Chinese labeled data,
outperforming the baseline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingfeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sujian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01498">
<title>Skip-gram word embeddings in hyperbolic space. (arXiv:1809.01498v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01498</link>
<description rdf:parseType="Literal">&lt;p&gt;Embeddings of tree-like graphs in hyperbolic space were recently shown to
surpass their Euclidean counterparts in performance by a large margin. Inspired
by these results, we present an algorithm for learning word embeddings in
hyperbolic space from free text. An objective function based on the hyperbolic
distance is derived and included in the skip-gram architecture from word2vec.
The hyperbolic word embeddings are then evaluated on word similarity and
analogy benchmarks. The results demonstrate the potential of hyperbolic word
embeddings, particularly in low dimensions, though without clear superiority
over their Euclidean counterparts. We further discuss problems in the
formulation of the analogy task resulting from the curvature of hyperbolic
space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leimeister_M/0/1/0/all/0/1&quot;&gt;Matthias Leimeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1&quot;&gt;Benjamin J. Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01564">
<title>Traffic Density Estimation using a Convolutional Neural Network. (arXiv:1809.01564v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01564</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this project is to introduce and present a machine learning
application that aims to improve the quality of life of people in Singapore. In
particular, we investigate the use of machine learning solutions to tackle the
problem of traffic congestion in Singapore. In layman&apos;s terms, we seek to make
Singapore (or any other city) a smoother place. To accomplish this aim, we
present an end-to-end system comprising of 1. A traffic density estimation
algorithm at traffic lights/junctions and 2. a suitable traffic signal control
algorithms that make use of the density information for better traffic control.
Traffic density estimation can be obtained from traffic junction images using
various machine learning techniques (combined with CV tools). After research
into various advanced machine learning methods, we decided on convolutional
neural networks (CNNs). We conducted experiments on our algorithms, using the
publicly available traffic camera dataset published by the Land Transport
Authority (LTA) to demonstrate the feasibility of this approach. With these
traffic density estimates, different traffic algorithms can be applied to
minimize congestion at traffic junctions in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nubert_J/0/1/0/all/0/1&quot;&gt;Julian Nubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_N/0/1/0/all/0/1&quot;&gt;Nicholas Giai Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1&quot;&gt;Abel Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanujaya_H/0/1/0/all/0/1&quot;&gt;Herbert Ilhan Tanujaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_L/0/1/0/all/0/1&quot;&gt;Leah Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1&quot;&gt;Mai Anh Vu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01587">
<title>GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation. (arXiv:1809.01587v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1809.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent success in deep learning has generated immense interest among
practitioners and students, inspiring many to learn about this new technology.
While visual and interactive approaches have been successfully developed to
help people more easily learn deep learning, most existing tools focus on
simpler models. In this work, we present GAN Lab, the first interactive
visualization tool designed for non-experts to learn and experiment with
Generative Adversarial Networks (GANs), a popular class of complex deep
learning models. With GAN Lab, users can interactively train generative models
and visualize the dynamic training process&apos;s intermediate results. GAN Lab
tightly integrates an model overview graph that summarizes GAN&apos;s structure, and
a layered distributions view that helps users interpret the interplay between
submodels. GAN Lab introduces new interactive experimentation features for
learning complex deep learning models, such as step-by-step training at
multiple levels of abstraction for understanding intricate training dynamics.
Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web
browsers, without the need for installation or specialized hardware, overcoming
a major practical challenge in deploying interactive tools for deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1&quot;&gt;Minsuk Kahng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorat_N/0/1/0/all/0/1&quot;&gt;Nikhil Thorat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1&quot;&gt;Fernanda Vi&amp;#xe9;gas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1&quot;&gt;Martin Wattenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01628">
<title>Online local pool generation for dynamic classifier selection: an extended version. (arXiv:1809.01628v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01628</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic Classifier Selection (DCS) techniques have difficulty in selecting
the most competent classifier in a pool, even when its presence is assured.
Since the DCS techniques rely only on local data to estimate a classifier&apos;s
competence, the manner in which the pool is generated could affect the choice
of the best classifier for a given sample. That is, the global perspective in
which pools are generated may not help the DCS techniques in selecting a
competent classifier for samples that are likely to be mislabelled. Thus, we
propose in this work an online pool generation method that produces a locally
accurate pool for test samples in difficult regions of the feature space. The
difficulty of a given area is determined by the classification difficulty of
the samples in it. That way, by using classifiers that were generated in a
local scope, it could be easier for the DCS techniques to select the best one
for the difficult samples. For the query samples in easy regions, a simple
nearest neighbors rule is used. In the extended version of this work, a deep
analysis on the correlation between instance hardness and the performance of
DCS techniques is presented. An instance hardness measure that conveys the
degree of local class overlap is then used to decide when the local pool is
used in the proposed scheme. The proposed method yielded significantly greater
recognition rates in comparison to a Bagging-generated pool and two other
global pool generation schemes for all DCS techniques evaluated. The proposed
scheme&apos;s performance was also significantly superior to three state-of-the-art
classification models and statistically equivalent to five of them. Moreover,
an extended analysis on the computational complexity of the proposed method and
of several DS techniques is presented in this version. We also provide the
implementation of the proposed technique using the DESLib library on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_M/0/1/0/all/0/1&quot;&gt;Mariana A. Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalcanti_G/0/1/0/all/0/1&quot;&gt;George D. C. Cavalcanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08329">
<title>Guided Feature Transformation (GFT): A Neural Language Grounding Module for Embodied Agents. (arXiv:1805.08329v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08329</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently there has been a rising interest in training agents, embodied in
virtual environments, to perform language-directed tasks by deep reinforcement
learning. In this paper, we propose a simple but effective neural language
grounding module for embodied agents that can be trained end to end from
scratch taking raw pixels, unstructured linguistic commands, and sparse rewards
as the inputs. We model the language grounding process as a language-guided
transformation of visual features, where latent sentence embeddings are used as
the transformation matrices. In several language-directed navigation tasks that
feature challenging partial observability and require simple reasoning, our
module significantly outperforms the state of the art. We also release
XWorld3D, an easy-to-customize 3D environment that can potentially be modified
to evaluate a variety of embodied agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haonan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1&quot;&gt;Xiaochen Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haichao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07561">
<title>Training Deeper Neural Machine Translation Models with Transparent Attention. (arXiv:1808.07561v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07561</link>
<description rdf:parseType="Literal">&lt;p&gt;While current state-of-the-art NMT models, such as RNN seq2seq and
Transformers, possess a large number of parameters, they are still shallow in
comparison to convolutional models used for both text and vision applications.
In this work we attempt to train significantly (2-3x) deeper Transformer and
Bi-RNN encoders for machine translation. We propose a simple modification to
the attention mechanism that eases the optimization of deeper models, and
results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT&apos;14
English-German and WMT&apos;15 Czech-English tasks for both architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1&quot;&gt;Ankur Bapna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mia Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07980">
<title>Ontology Reasoning with Deep Neural Networks. (arXiv:1808.07980v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07980</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to conduct logical reasoning is a fundamental aspect of
intelligent behavior, and thus an important problem along the way to
human-level artificial intelligence. Traditionally, symbolic methods from the
field of knowledge representation and reasoning have been used to equip agents
with capabilities that resemble human logical reasoning qualities. More
recently, however, there has been an increasing interest in using machine
learning rather than logic-based formalisms to tackle these tasks. In this
paper, we employ state-of-the-art methods for training deep neural networks to
devise a novel model that is able to learn how to effectively perform basic
ontology reasoning. This is an important and at the same time very natural
reasoning problem, which is why the presented approach is applicable to a
plethora of important real-world problems. We present the outcomes of several
experiments, which show that our model learned to perform precise reasoning on
diverse and challenging tasks. Furthermore, it turned out that the suggested
approach suffers much less from different obstacles that prohibit symbolic
reasoning, and, at the same time, is surprisingly plausible from a biological
point of view.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohenecker_P/0/1/0/all/0/1&quot;&gt;Patrick Hohenecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1&quot;&gt;Thomas Lukasiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09888">
<title>KDSL: a Knowledge-Driven Supervised Learning Framework for Word Sense Disambiguation. (arXiv:1808.09888v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09888</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose KDSL, a new word sense disambiguation (WSD) framework that
utilizes knowledge to automatically generate sense-labeled data for supervised
learning. First, from WordNet, we automatically construct a semantic knowledge
base called DisDict, which provides refined feature words that highlight the
differences among word senses, i.e., synsets. Second, we automatically generate
new sense-labeled data by DisDict from unlabeled corpora. Third, these
generated data, together with manually labeled data and unlabeled data, are fed
to a neural framework conducting supervised and unsupervised learning jointly
to model the semantic relations among synsets, feature words and their
contexts. The experimental results show that KDSL outperforms several
representative state-of-the-art methods on various major benchmarks.
Interestingly, it performs relatively well even when manually labeled data is
unavailable, thus provides a potential solution for similar tasks in a lack of
manual annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1&quot;&gt;Shi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianmin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruili Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01129">
<title>Lipschitz Networks and Distributional Robustness. (arXiv:1809.01129v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.01129</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust risk minimisation has several advantages: it has been studied with
regards to improving the generalisation properties of models and robustness to
adversarial perturbation. We bound the distributionally robust risk for a model
class rich enough to include deep neural networks by a regularised empirical
risk involving the Lipschitz constant of the model. This allows us to
interpretand quantify the robustness properties of a deep neural network. As an
application we show the distributionally robust risk upperbounds the
adversarial training risk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranko_Z/0/1/0/all/0/1&quot;&gt;Zac Cranko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kornblith_S/0/1/0/all/0/1&quot;&gt;Simon Kornblith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01185">
<title>DeepPINK: reproducible feature selection in deep neural networks. (arXiv:1809.01185v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01185</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has become increasingly popular in both supervised and
unsupervised machine learning thanks to its outstanding empirical performance.
However, because of their intrinsic complexity, most deep learning methods are
largely treated as black box tools with little interpretability. Even though
recent attempts have been made to facilitate the interpretability of deep
neural networks (DNNs), existing methods are susceptible to noise and lack of
robustness.
&lt;/p&gt;
&lt;p&gt;Therefore, scientists are justifiably cautious about the reproducibility of
the discoveries, which is often related to the interpretability of the
underlying statistical models. In this paper, we describe a method to increase
the interpretability and reproducibility of DNNs by incorporating the idea of
feature selection with controlled error rate. By designing a new DNN
architecture and integrating it with the recently proposed knockoffs framework,
we perform feature selection with a controlled error rate, while maintaining
high power. This new method, DeepPINK (Deep feature selection using
Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data
sets to demonstrate its empirical utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Young Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jinchi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yingying Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noble_W/0/1/0/all/0/1&quot;&gt;William Stafford Noble&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01316">
<title>Learning User Preferences and Understanding Calendar Contexts for Event Scheduling. (arXiv:1809.01316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01316</link>
<description rdf:parseType="Literal">&lt;p&gt;With online calendar services gaining popularity worldwide, calendar data has
become one of the richest context sources for understanding human behavior.
However, event scheduling is still time-consuming even with the development of
online calendars. Although machine learning based event scheduling models have
automated scheduling processes to some extent, they often fail to understand
subtle user preferences and complex calendar contexts with event titles written
in natural language. In this paper, we propose Neural Event Scheduling
Assistant (NESA) which learns user preferences and understands calendar
contexts, directly from raw online calendars for fully automated and highly
effective event scheduling. We leverage over 593K calendar events for NESA to
learn scheduling personal events, and we further utilize NESA for
multi-attendee event scheduling. NESA successfully incorporates deep neural
networks such as Bidirectional Long Short-Term Memory, Convolutional Neural
Network, and Highway Network for learning the preferences of each user and
understanding calendar context based on natural languages. The experimental
results show that NESA significantly outperforms previous baseline models in
terms of various evaluation metrics on both personal and multi-attendee event
scheduling tasks. Our qualitative analysis demonstrates the effectiveness of
each layer in NESA and learned user preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jinhyuk Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Donghee Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaehoon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01319">
<title>Cross validation residuals for generalised least squares and other correlated data models. (arXiv:1809.01319v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1809.01319</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross validation residuals are well known for the ordinary least squares
model. Here leave-M-out cross validation is extended to generalised least
squares. The relationship between cross validation residuals and Cook&apos;s
distance is demonstrated, in terms of an approximation to the difference in the
generalised residual sum of squares for a model fit to all the data (training
and test) and a model fit to a reduced dataset (training data only). For
generalised least squares, as for ordinary least squares, there is no need to
refit the model to reduced size datasets as all the values for K fold cross
validation are available after fitting the model to all the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baade_I/0/1/0/all/0/1&quot;&gt;Ingrid Annette Baade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01369">
<title>Towards quantitative methods to assess network generative models. (arXiv:1809.01369v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01369</link>
<description rdf:parseType="Literal">&lt;p&gt;Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mostofi_V/0/1/0/all/0/1&quot;&gt;Vahid Mostofi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliakbary_S/0/1/0/all/0/1&quot;&gt;Sadegh Aliakbary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01382">
<title>Anytime Hedge achieves optimal regret in the stochastic regime. (arXiv:1809.01382v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.01382</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is about a surprising fact: we prove that the anytime Hedge
algorithm with decreasing learning rate, which is one of the simplest algorithm
for the problem of prediction with expert advice, is actually both worst-case
optimal and adaptive to the easier stochastic and adversarial with a gap
problems. This runs counter to the common belief in the literature that this
algorithm is overly conservative, and that only new adaptive algorithms can
simultaneously achieve minimax regret and adapt to the difficulty of the
problem. Moreover, our analysis exhibits qualitative differences with other
variants of the Hedge algorithm, based on the so-called &quot;doubling trick&quot;, and
the fixed-horizon version (with constant learning rate).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mourtada_J/0/1/0/all/0/1&quot;&gt;Jaouad Mourtada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01477">
<title>A Supervised Learning Approach For Heading Detection. (arXiv:1809.01477v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.01477</link>
<description rdf:parseType="Literal">&lt;p&gt;As the Portable Document Format (PDF) file format increases in popularity,
research in analysing its structure for text extraction and analysis is
necessary. Detecting headings can be a crucial component of classifying and
extracting meaningful data. This research involves training a supervised
learning model to detect headings with features carefully selected through
recursive feature elimination. The best performing classifier had an accuracy
of 96.95%, sensitivity of 0.986 and a specificity of 0.953. This research into
heading detection contributes to the field of PDF based text extraction and can
be applied to the automation of large scale PDF text analysis in a variety of
professional and policy based contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budhiraja_S/0/1/0/all/0/1&quot;&gt;Sahib Singh Budhiraja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1&quot;&gt;Vijay Mago&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01478">
<title>Weakly-Supervised Neural Text Classification. (arXiv:1809.01478v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.01478</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are gaining increasing popularity for the classic text
classification task, due to their strong expressive power and less requirement
for feature engineering. Despite such attractiveness, neural text
classification models suffer from the lack of training data in many real-world
applications. Although many semi-supervised and weakly-supervised text
classification models exist, they cannot be easily applied to deep neural
models and meanwhile support limited supervision types. In this paper, we
propose a weakly-supervised method that addresses the lack of training data in
neural text classification. Our method consists of two modules: (1) a
pseudo-document generator that leverages seed information to generate
pseudo-labeled documents for model pre-training, and (2) a self-training module
that bootstraps on real unlabeled data for model refinement. Our method has the
flexibility to handle different types of weak supervision and can be easily
integrated into existing deep neural models for text classification. We have
performed extensive experiments on three real-world datasets from different
domains. The results demonstrate that our proposed method achieves inspiring
performance without requiring excessive training data and outperforms baseline
methods significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiaming Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01494">
<title>Interpretation of Natural Language Rules in Conversational Machine Reading. (arXiv:1809.01494v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01494</link>
<description rdf:parseType="Literal">&lt;p&gt;Most work in machine reading focuses on question answering problems where the
answer is directly expressed in the text to read. However, many real-world
question answering problems require the reading of text not because it contains
the literal answer, but because it contains a recipe to derive an answer
together with the reader&apos;s background knowledge. One example is the task of
interpreting regulations to answer &quot;Can I...?&quot; or &quot;Do I have to...?&quot; questions
such as &quot;I am working in Canada. Do I have to carry on paying UK National
Insurance?&quot; after reading a UK government website about this topic. This task
requires both the interpretation of rules and the application of background
knowledge. It is further complicated due to the fact that, in practice, most
questions are underspecified, and a human assistant will regularly have to ask
clarification questions such as &quot;How long have you been working abroad?&quot; when
the answer cannot be directly derived from the question and text. In this
paper, we formalise this task and develop a crowd-sourcing strategy to collect
32k task instances based on real-world rules and crowd-generated questions and
scenarios. We analyse the challenges of this task and assess its difficulty by
evaluating the performance of rule-based and machine-learning baselines. We
observe promising results when no background knowledge is necessary, and
substantial room for improvement whenever background knowledge is needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1&quot;&gt;Marzieh Saeidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1&quot;&gt;Max Bartolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1&quot;&gt;Patrick Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sameer Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheldon_M/0/1/0/all/0/1&quot;&gt;Mike Sheldon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_G/0/1/0/all/0/1&quot;&gt;Guillaume Bouchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01495">
<title>A Reinforcement Learning-driven Translation Model for Search-Oriented Conversational Systems. (arXiv:1809.01495v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01495</link>
<description rdf:parseType="Literal">&lt;p&gt;Search-oriented conversational systems rely on information needs expressed in
natural language (NL). We focus here on the understanding of NL expressions for
building keyword-based queries. We propose a reinforcement-learning-driven
translation model framework able to 1) learn the translation from NL
expressions to queries in a supervised way, and, 2) to overcome the lack of
large-scale dataset by framing the translation model as a word selection
approach and injecting relevance feedback in the learning process. Experiments
are carried out on two TREC datasets and outline the effectiveness of our
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aissa_W/0/1/0/all/0/1&quot;&gt;Wafa Aissa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1&quot;&gt;Laure Soulier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1&quot;&gt;Ludovic Denoyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01496">
<title>Learning Gender-Neutral Word Embeddings. (arXiv:1809.01496v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01496</link>
<description rdf:parseType="Literal">&lt;p&gt;Word embedding models have become a fundamental component in a wide range of
Natural Language Processing (NLP) applications. However, embeddings trained on
human-generated corpora have been demonstrated to inherit strong gender
stereotypes that reflect social constructs. To address this concern, in this
paper, we propose a novel training procedure for learning gender-neutral word
embeddings. Our approach aims to preserve gender information in certain
dimensions of word vectors while compelling other dimensions to be free of
gender influence. Based on the proposed method, we generate a Gender-Neutral
variant of GloVe (GN-GloVe). Quantitative and qualitative experiments
demonstrate that GN-GloVe successfully isolates gender information without
sacrificing the functionality of the embedding model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yichao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01499">
<title>Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts. (arXiv:1809.01499v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01499</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an adversarial method for producing high-recall explanations of
neural text classifier decisions. Building on an existing architecture for
extractive explanations via hard attention, we add an adversarial layer which
scans the residual of the attention for remaining predictive signal. Motivated
by the important domain of detecting personal attacks in social media comments,
we additionally demonstrate the importance of manually setting a semantically
appropriate `default&apos; behavior for the model by explicitly manipulating its
bias term. We develop a validation set of human-annotated personal attacks to
evaluate the impact of these changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carton_S/0/1/0/all/0/1&quot;&gt;Samuel Carton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1&quot;&gt;Qiaozhu Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resnick_P/0/1/0/all/0/1&quot;&gt;Paul Resnick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01506">
<title>Deep Reinforcement Learning in High Frequency Trading. (arXiv:1809.01506v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01506</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to give a precise and fast prediction for the price movement of
stocks is the key to profitability in High Frequency Trading. The main
objective of this paper is to propose a novel way of modeling the high
frequency trading problem using Deep Reinforcement Learning and to argue why
Deep RL can have a lot of potential in the field of High Frequency Trading. We
have analyzed the model&apos;s performance based on it&apos;s prediction accuracy as well
as prediction speed across full-day trading simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1&quot;&gt;Prakhar Ganesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakheja_P/0/1/0/all/0/1&quot;&gt;Puneet Rakheja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01534">
<title>Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models. (arXiv:1809.01534v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.01534</link>
<description rdf:parseType="Literal">&lt;p&gt;Text normalization is an important enabling technology for several NLP tasks.
Recently, neural-network-based approaches have outperformed well-established
models in this task. However, in languages other than English, there has been
little exploration in this direction. Both the scarcity of annotated data and
the complexity of the language increase the difficulty of the problem. To
address these challenges, we use a sequence-to-sequence model with
character-based attention, which in addition to its self-learned character
embeddings, uses word embeddings pre-trained with an approach that also models
subword information. This provides the neural model with access to more
linguistic information especially suitable for text normalization, without
large parallel corpora. We show that providing the model with word-level
features bridges the gap for the neural network approach to achieve a
state-of-the-art F1 score on a standard Arabic language correction shared task
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1&quot;&gt;Daniel Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1&quot;&gt;Nasser Zalmout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1&quot;&gt;Nizar Habash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01571">
<title>Knowledge Integrated Classifier Design Based on Utility Optimization. (arXiv:1809.01571v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.01571</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a systematic framework to design a classification model
that yields a classifier which optimizes a utility function based on prior
knowledge. Specifically, as the data size grows, we prove that the produced
classifier asymptotically converges to the optimal classifier, an extended
version of the Bayes rule, which maximizes the utility function. Therefore, we
provide a meaningful theoretical interpretation for modeling with the knowledge
incorporated. Our knowledge incorporation method allows domain experts to guide
the classifier towards correctly classifying data that they think to be more
significant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shaohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chuanhou Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00232">
<title>Optimal Algorithms for Distributed Optimization. (arXiv:1712.00232v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00232</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the optimal convergence rate for distributed convex
optimization problems in networks. We model the communication restrictions
imposed by the network as a set of affine constraints and provide optimal
complexity bounds for four different setups, namely: the function $F(\xb)
\triangleq \sum_{i=1}^{m}f_i(\xb)$ is strongly convex and smooth, either
strongly convex or smooth or just convex. Our results show that Nesterov&apos;s
accelerated gradient descent on the dual problem can be executed in a
distributed manner and obtains the same optimal rates as in the centralized
version of the problem (up to constant or logarithmic factors) with an
additional cost related to the spectral gap of the interaction matrix. Finally,
we discuss some extensions to the proposed setup such as proximal friendly
functions, time-varying graphs, improvement of the condition numbers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Uribe_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar A. Uribe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soomin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1&quot;&gt;Alexander Gasnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nedic_A/0/1/0/all/0/1&quot;&gt;Angelia Nedi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03487">
<title>Spurious local minima in neural networks: a critical view. (arXiv:1802.03487v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03487</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the loss surface of nonlinear neural networks. We prove that
even for networks with one hidden layer and &quot;slightest&quot; nonlinearity, there can
be spurious local minima. Our results thus indicate that in general &quot;no
spurious local minima&quot; is a property limited to deep linear networks.
Specifically, for ReLU(-like) networks we prove that for almost all (in
contrast to previous results) practical datasets there exist infinitely many
local minima. We also present a counterexample for more general activation
functions (such as sigmoid, tanh, arctan, ReLU, etc.), for which there exists a
local minimum strictly inferior to the global minimum. Our results make the
least restrictive assumptions relative to the existing results on local
optimality in neural networks. We complete our discussion by presenting a
comprehensive characterization of global optimality for deep linear networks.
Our results unify and subsume other results on this topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1&quot;&gt;Chulhee Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1&quot;&gt;Suvrit Sra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05052">
<title>Machine Learning: Basic Principles. (arXiv:1805.05052v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05052</link>
<description rdf:parseType="Literal">&lt;p&gt;This tutorial is based on the lecture notes for the courses &quot;Machine
Learning: Basic Principles&quot; and &quot;Artificial Intelligence&quot;, which I have
(co-)taught since 2015 at Aalto University. The aim is to provide an accessible
introduction to some of the main concepts and methods within machine learning.
Many of the current systems which are considered (artificial) intelligent are
based on combinations of few basic machine learning methods. After formalizing
the main building blocks of a machine learning problem, some popular
algorithmic design patterns for machine learning methods are discussed in some
detail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07594">
<title>Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions. (arXiv:1805.07594v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07594</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding complex objects as vectors in low dimensional spaces is a
longstanding problem in machine learning. We propose in this work an extension
of that approach, which consists in embedding objects as elliptical probability
distributions, namely distributions whose densities have elliptical level sets.
We endow these measures with the 2-Wasserstein metric, with two important
benefits: (i) For such measures, the squared 2-Wasserstein metric has a closed
form, equal to the sum of the squared Euclidean distance between means and the
squared Bures metric between covariance matrices. The latter is a Riemannian
metric between positive semi-definite matrices, which turns out to be Euclidean
on a suitable factor representation of such matrices, which is valid on the
entire geodesic between these matrices. (ii) The 2-Wasserstein distance boils
down to the usual Euclidean metric when comparing Diracs, and therefore
provides the natural framework to extend point embeddings. We show that for
these reasons Wasserstein elliptical embeddings are more intuitive and yield
tools that are better behaved numerically than the alternative choice of
Gaussian embeddings with the Kullback-Leibler divergence. In particular, and
unlike previous work based on the KL geometry, we learn elliptical
distributions that are not necessarily diagonal. We demonstrate the advantages
of elliptical embeddings by using them for visualization, to compute embeddings
of words, and to reflect entailment or hypernymy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Muzellec_B/0/1/0/all/0/1&quot;&gt;Boris Muzellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cuturi_M/0/1/0/all/0/1&quot;&gt;Marco Cuturi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07233">
<title>Neural Architecture Optimization. (arXiv:1808.07233v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07233</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic neural architecture design has shown its potential in discovering
powerful neural network architectures. Existing methods, no matter based on
reinforcement learning or evolutionary algorithms (EA), conduct architecture
search in a discrete space, which is highly inefficient. In this paper, we
propose a simple and efficient method to automatic neural architecture design
based on continuous optimization. We call this new approach neural architecture
optimization (NAO). There are three key components in our proposed approach:
(1) An encoder embeds/maps neural network architectures into a continuous
space. (2) A predictor takes the continuous representation of a network as
input and predicts its accuracy. (3) A decoder maps a continuous representation
of a network back to its architecture. The performance predictor and the
encoder enable us to perform gradient based optimization in the continuous
space to find the embedding of a new architecture with potentially better
accuracy. Such a better embedding is then decoded to a network by the decoder.
Experiments show that the architecture discovered by our method is very
competitive for image classification task on CIFAR-10 and language modeling
task on PTB, outperforming or on par with the best results of previous
architecture search methods with a significantly reduction of computational
resources. Specifically we obtain $2.07\%$ test set error rate for CIFAR-10
image classification task and $55.9$ test set perplexity of PTB language
modeling task. The best discovered architectures on both tasks are successfully
transferred to other tasks such as CIFAR-100 and WikiText-2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Renqian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1&quot;&gt;Fei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09540">
<title>Lipschitz regularized Deep Neural Networks converge and generalize. (arXiv:1808.09540v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09540</link>
<description rdf:parseType="Literal">&lt;p&gt;Lipschitz regularized neural networks augment the usual fidelity term used in
training with a regularization term corresponding the excess Lipschitz constant
of the network compared to the Lipschitz constant of the data. We prove that
Lipschitz regularized neural networks converge, and provide a rate, in the
limit as the number of data points $n\to\infty$. We consider the regime where
perfect fitting of data is possible, which means the size of the network grows
with $n$. There are two regimes: in the case of perfect labels, we prove
convergence to the label function which corresponds to zero loss. In the case
of corrupted labels which occurs when the Lipschitz constant of the data blows
up, we prove convergence to a regularized label function which is the solution
of a limiting variational problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1&quot;&gt;Adam M Oberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1&quot;&gt;Jeff Calder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00403">
<title>Effective Exploration for Deep Reinforcement Learning via Bootstrapped Q-Ensembles under Tsallis Entropy Regularization. (arXiv:1809.00403v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.00403</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently deep reinforcement learning (DRL) has achieved outstanding success
on solving many difficult and large-scale RL problems. However the high sample
cost required for effective learning often makes DRL unaffordable in
resource-limited applications. With the aim of improving sample efficiency and
learning performance, we will develop a new DRL algorithm in this paper that
seamless integrates entropy-induced and bootstrap-induced techniques for
efficient and deep exploration of the learning environment. Specifically, a
general form of Tsallis entropy regularizer will be utilized to drive
entropy-induced exploration based on efficient approximation of optimal
action-selection policies. Different from many existing works that rely on
action dithering strategies for exploration, our algorithm is efficient in
exploring actions with clear exploration value. Meanwhile, by employing an
ensemble of Q-networks under varied Tsallis entropy regularization, the
diversity of the ensemble can be further enhanced to enable effective
bootstrap-induced exploration. Experiments on Atari game playing tasks clearly
demonstrate that our new algorithm can achieve more efficient and effective
exploration for DRL, in comparison to recently proposed exploration methods
including Bootstrapped Deep Q-Network and UCB Q-Ensemble.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yiming Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhang&lt;/a&gt;</dc:creator>
</item></rdf:RDF>