<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00557"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00948"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00964"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00994"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01076"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01208"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.11040"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00600"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00646"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00709"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00725"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00929"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00991"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01106"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01235"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01252"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.00154"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.02596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06040"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04763"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07783"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03817"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09681"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00465"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00504"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00520"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00556"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00559"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00699"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00731"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00828"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00867"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01038"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01137"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01141"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01169"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1508.01717"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.01171"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.06764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.10277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.06084"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.03971"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.06176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02633"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05338"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02213"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04992"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06424"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11511"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00171"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00351"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.00519">
<title>An Elementary Analysis of the Probability that a Binomial Random Variable Exceeds its Expectation. (arXiv:1712.00519v1 [math.PR])</title>
<link>http://arxiv.org/abs/1712.00519</link>
<description rdf:parseType="Literal">&lt;p&gt;We give an elementary proof of the fact that a binomial random variable $X$
with parameters $n$ and $0.29/n \le p &amp;lt; 1$ with probability at least $1/4$
strictly exceeds its expectation. We also show that for $1/n \le p &amp;lt; 1 - 1/n$,
$X$ exceeds its expectation by more than one with probability at least
$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to
infinity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00557">
<title>Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection. (arXiv:1712.00557v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.00557</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated analysis methods are crucial aids for monitoring and defending a
network to protect the sensitive or confidential data it hosts. This work
introduces a flexible, powerful, and unsupervised approach to detecting
anomalous behavior in computer and network logs, one that largely eliminates
domain-dependent feature engineering employed by existing methods. By treating
system logs as threads of interleaved &quot;sentences&quot; (event log lines) to train
online unsupervised neural network language models, our approach provides an
adaptive model of normal network behavior. We compare the effectiveness of both
standard and bidirectional recurrent neural network language models at
detecting malicious activity within network log data. Extending these models,
we introduce a tiered recurrent architecture, which provides context by
modeling sequences of users&apos; actions over time. Compared to Isolation Forest
and Principal Components Analysis, two popular anomaly detection algorithms, we
observe superior performance on the Los Alamos National Laboratory Cyber
Security dataset. For log-line-level red team detection, our best performing
character-based model provides test set area under the receiver operator
characteristic curve of 0.98, demonstrating the strong fine-grained anomaly
detection performance of this approach on open vocabulary logging sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuor_A/0/1/0/all/0/1&quot;&gt;Aaron Tuor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baerwolf_R/0/1/0/all/0/1&quot;&gt;Ryan Baerwolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knowles_N/0/1/0/all/0/1&quot;&gt;Nicolas Knowles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1&quot;&gt;Brian Hutchinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichols_N/0/1/0/all/0/1&quot;&gt;Nicole Nichols&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jasper_R/0/1/0/all/0/1&quot;&gt;Rob Jasper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00712">
<title>Evaluation of Alzheimer&apos;s Disease by Analysis of MR Images using Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the ADC Maps. (arXiv:1712.00712v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1712.00712</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s disease is the most common cause of dementia, yet hard to
diagnose precisely without invasive techniques, particularly at the onset of
the disease. This work approaches image analysis and classification of
synthetic multispectral images composed by diffusion-weighted magnetic
resonance (MR) cerebral images for the evaluation of cerebrospinal fluid area
and measuring the advance of Alzheimer&apos;s disease. A clinical 1.5 T MR imaging
system was used to acquire all images presented. The classification methods are
based on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We
assume the classes of interest can be separated by hyperquadrics. Therefore, a
2-degree polynomial network is used to classify the original image, generating
the ground truth image. The classification results are used to improve the
usual analysis of the apparent diffusion coefficient map.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Santos_W/0/1/0/all/0/1&quot;&gt;Wellington Pinheiro dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Souza_R/0/1/0/all/0/1&quot;&gt;Ricardo Emmanuel de Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Filho_P/0/1/0/all/0/1&quot;&gt;Pl&amp;#xed;nio B. dos Santos Filho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00754">
<title>Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. (arXiv:1712.00754v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.00754</link>
<description rdf:parseType="Literal">&lt;p&gt;A new class of non-homogeneous state-affine systems is introduced. Sufficient
conditions are identified that guarantee first, that the associated reservoir
computers with linear readouts are causal, time-invariant, and satisfy the
fading memory property and second, that a subset of this class is universal in
the category of fading memory filters with stochastic almost surely bounded
inputs. This means that any discrete-time filter that satisfies the fading
memory property with random inputs of that type can be uniformly approximated
by elements in the non-homogeneous state-affine family.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grigoryeva_L/0/1/0/all/0/1&quot;&gt;Lyudmila Grigoryeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1&quot;&gt;Juan-Pablo Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00789">
<title>Reconstruction of Electrical Impedance Tomography Using Fish School Search, Non-Blind Search, and Genetic Algorithm. (arXiv:1712.00789v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/1712.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrical Impedance Tomography (EIT) is a noninvasive imaging technique that
does not use ionizing radiation, with application both in environmental
sciences and in health. Image reconstruction is performed by solving an inverse
problem and ill-posed. Evolutionary Computation and Swarm Intelligence have
become a source of methods for solving inverse problems. Fish School Search
(FSS) is a promising search and optimization method, based on the dynamics of
schools of fish. In this article the authors present a method for
reconstruction of EIT images based on FSS and Non-Blind Search (NBS). The
method was evaluated using numerical phantoms consisting of electrical
conductivity images with subjects in the center, between the center and the
edge and on the edge of a circular section, with meshes of 415 finite elements.
The authors performed 20 simulations for each configuration. Results showed
that both FSS and FSS-NBS were able to converge faster than genetic algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Barbosa_V/0/1/0/all/0/1&quot;&gt;Valter Augusto de Freitas Barbosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ribeiro_R/0/1/0/all/0/1&quot;&gt;Reiga Ramalho Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Feitosa_A/0/1/0/all/0/1&quot;&gt;Allan Rivalles Souza Feitosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Silva_V/0/1/0/all/0/1&quot;&gt;Victor Luiz Bezerra Ara&amp;#xfa;jo da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rocha_A/0/1/0/all/0/1&quot;&gt;Arthur Diego Dias Rocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Freitas_R/0/1/0/all/0/1&quot;&gt;Rafaela Covello de Freitas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Souza_R/0/1/0/all/0/1&quot;&gt;Ricardo Emmanuel de Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Santos_W/0/1/0/all/0/1&quot;&gt;Wellington Pinheiro dos Santos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00948">
<title>Hierarchical Actor-Critic. (arXiv:1712.00948v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00948</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to hierarchical reinforcement learning called
Hierarchical Actor-Critic (HAC). HAC aims to make learning tasks with sparse
binary rewards more efficient by enabling agents to learn how to break down
tasks from scratch. The technique uses of a set of actor-critic networks that
learn to decompose tasks into a hierarchy of subgoals. We demonstrate that HAC
significantly improves sample efficiency in a series of tasks that involve
sparse binary rewards and require behavior over a long time horizon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1&quot;&gt;Andrew Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platt_R/0/1/0/all/0/1&quot;&gt;Robert Platt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00964">
<title>Drift Analysis. (arXiv:1712.00964v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.00964</link>
<description rdf:parseType="Literal">&lt;p&gt;Drift analysis is one of the major tools for analysing evolutionary
algorithms and nature-inspired search heuristics. In this chapter we give an
introduction to drift analysis and give some examples of how to use it for the
analysis of evolutionary algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengler_J/0/1/0/all/0/1&quot;&gt;Johannes Lengler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00994">
<title>NEURAghe: Exploiting CPU-FPGA Synergies for Efficient and Flexible CNN Inference Acceleration on Zynq SoCs. (arXiv:1712.00994v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.00994</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (CNNs) obtain outstanding results in tasks
that require human-level understanding of data, like image or speech
recognition. However, their computational load is significant, motivating the
development of CNN-specialized accelerators. This work presents NEURAghe, a
flexible and efficient hardware/software solution for the acceleration of CNNs
on Zynq SoCs. NEURAghe leverages the synergistic usage of Zynq ARM cores and of
a powerful and flexible Convolution-Specific Processor deployed on the
reconfigurable logic. The Convolution-Specific Processor embeds both a
convolution engine and a programmable soft core, releasing the ARM processors
from most of the supervision duties and allowing the accelerator to be
controlled by software at an ultra-fine granularity. This methodology opens the
way for cooperative heterogeneous computing: while the accelerator takes care
of the bulk of the CNN workload, the ARM cores can seamlessly execute
hard-to-accelerate parts of the computational graph, taking advantage of the
NEON vector engines to further speed up computation. Through the companion
NeuDNN SW stack, NEURAghe supports end-to-end CNN-based classification with a
peak performance of 169 Gops/s, and an energy efficiency of 17 Gops/W. Thanks
to our heterogeneous computing model, our platform improves upon the
state-of-the-art, achieving a frame rate of 5.5 fps on the end-to-end execution
of VGG-16, and 6.6 fps on ResNet-18.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meloni_P/0/1/0/all/0/1&quot;&gt;Paolo Meloni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capotondi_A/0/1/0/all/0/1&quot;&gt;Alessandro Capotondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deriu_G/0/1/0/all/0/1&quot;&gt;Gianfranco Deriu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brian_M/0/1/0/all/0/1&quot;&gt;Michele Brian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1&quot;&gt;Francesco Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1&quot;&gt;Davide Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffo_L/0/1/0/all/0/1&quot;&gt;Luigi Raffo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01076">
<title>Natural Langevin Dynamics for Neural Networks. (arXiv:1712.01076v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.01076</link>
<description rdf:parseType="Literal">&lt;p&gt;One way to avoid overfitting in machine learning is to use model parameters
distributed according to a Bayesian posterior given the data, rather than the
maximum likelihood estimator. Stochastic gradient Langevin dynamics (SGLD) is
one algorithm to approximate such Bayesian posteriors for large models and
datasets. SGLD is a standard stochastic gradient descent to which is added a
controlled amount of noise, specifically scaled so that the parameter converges
in law to the posterior distribution [WT11, TTV16]. The posterior predictive
distribution can be approximated by an ensemble of samples from the trajectory.
&lt;/p&gt;
&lt;p&gt;Choice of the variance of the noise is known to impact the practical behavior
of SGLD: for instance, noise should be smaller for sensitive parameter
directions. Theoretically, it has been suggested to use the inverse Fisher
information matrix of the model as the variance of the noise, since it is also
the variance of the Bayesian posterior [PT13, AKW12, GC11]. But the Fisher
matrix is costly to compute for large- dimensional models.
&lt;/p&gt;
&lt;p&gt;Here we use the easily computed Fisher matrix approximations for deep neural
networks from [MO16, Oll15]. The resulting natural Langevin dynamics combines
the advantages of Amari&apos;s natural gradient descent and Fisher-preconditioned
Langevin dynamics for large neural networks.
&lt;/p&gt;
&lt;p&gt;Small-scale experiments on MNIST show that Fisher matrix preconditioning
brings SGLD close to dropout as a regularizing technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marceau_Caron_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xe9;tan Marceau-Caron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ollivier_Y/0/1/0/all/0/1&quot;&gt;Yann Ollivier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01208">
<title>The Case for Learned Index Structures. (arXiv:1712.01208v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1712.01208</link>
<description rdf:parseType="Literal">&lt;p&gt;Indexes are models: a B-Tree-Index can be seen as a model to map a key to the
position of a record within a sorted array, a Hash-Index as a model to map a
key to a position of a record within an unsorted array, and a BitMap-Index as a
model to indicate if a data record exists or not. In this exploratory research
paper, we start from this premise and posit that all existing index structures
can be replaced with other types of models, including deep-learning models,
which we term learned indexes. The key idea is that a model can learn the sort
order or structure of lookup keys and use this signal to effectively predict
the position or existence of records. We theoretically analyze under which
conditions learned indexes outperform traditional index structures and describe
the main challenges in designing learned index structures. Our initial results
show, that by using neural nets we are able to outperform cache-optimized
B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over
several real-world data sets. More importantly though, we believe that the idea
of replacing core components of a data management system through learned models
has far reaching implications for future systems designs and that this work
just provides a glimpse of what might be possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1&quot;&gt;Tim Kraska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1&quot;&gt;Alex Beutel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1&quot;&gt;Ed H. Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1&quot;&gt;Jeffrey Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyzotis_N/0/1/0/all/0/1&quot;&gt;Neoklis Polyzotis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.11040">
<title>End-to-End Differentiable Proving. (arXiv:1705.11040v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1705.11040</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce neural networks for end-to-end differentiable proving of queries
to knowledge bases by operating on dense vector representations of symbols.
These neural networks are constructed recursively by taking inspiration from
the backward chaining algorithm as used in Prolog. Specifically, we replace
symbolic unification with a differentiable computation on vector
representations of symbols using a radial basis function kernel, thereby
combining symbolic reasoning with learning subsymbolic vector representations.
By using gradient descent, the resulting neural network can be trained to infer
facts from a given incomplete knowledge base. It learns to (i) place
representations of similar symbols in close proximity in a vector space, (ii)
make use of such similarities to prove queries, (iii) induce logical rules, and
(iv) use provided and induced logical rules for multi-hop reasoning. We
demonstrate that this architecture outperforms ComplEx, a state-of-the-art
neural link prediction model, on three out of four benchmark knowledge bases
while at the same time inducing interpretable function-free first-order logic
rules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00489">
<title>Visual Features for Context-Aware Speech Recognition. (arXiv:1712.00489v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.00489</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic transcriptions of consumer-generated multi-media content such as
&quot;Youtube&quot; videos still exhibit high word error rates. Such data typically
occupies a very broad domain, has been recorded in challenging conditions, with
cheap hardware and a focus on the visual modality, and may have been
post-processed or edited. In this paper, we extend our earlier work on adapting
the acoustic model of a DNN-based speech recognition system to an RNN language
model and show how both can be adapted to the objects and scenes that can be
automatically detected in the video. We are working on a corpus of &quot;how-to&quot;
videos from the web, and the idea is that an object that can be seen (&quot;car&quot;),
or a scene that is being detected (&quot;kitchen&quot;) can be used to condition both
models on the &quot;context&quot; of the recording, thereby reducing perplexity and
improving transcription. We achieve good improvements in both cases and compare
and analyze the respective reductions in word error rate. We expect that our
results can be used for any type of speech processing in which &quot;context&quot;
information is available, for example in robotics, man-machine interaction, or
when indexing large audio-visual archives, and should ultimately help to bring
together the &quot;video-to-text&quot; and &quot;speech-to-text&quot; communities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1&quot;&gt;Yajie Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1&quot;&gt;Leonardo Neves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1&quot;&gt;Florian Metze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00547">
<title>Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences. (arXiv:1712.00547v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00547</link>
<description rdf:parseType="Literal">&lt;p&gt;In his seminal book `The Inmates are Running the Asylum: Why High-Tech
Products Drive Us Crazy And How To Restore The Sanity&apos; [2004, Sams
Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is
often poorly designed (from a user perspective) is that programmers are in
charge of design decisions, rather than interaction designers. As a result,
programmers design software for themselves, rather than for their target
audience; a phenomenon he refers to as the `inmates running the asylum&apos;. This
paper argues that explainable AI risks a similar fate. While the re-emergence
of explainable AI is positive, this paper argues most of us as AI researchers
are building explanatory agents for ourselves, rather than for the intended
users. But explainable AI is more likely to succeed if researchers and
practitioners understand, adopt, implement, and improve models from the vast
and valuable bodies of research in philosophy, psychology, and cognitive
science; and if evaluation of these models is focused more on people than on
technology. From a light scan of literature, we demonstrate that there is
considerable scope to infuse more results from the social and behavioural
sciences into explainable AI, and present some key results from these fields
that are relevant to explainable AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1&quot;&gt;Tim Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howe_P/0/1/0/all/0/1&quot;&gt;Piers Howe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonenberg_L/0/1/0/all/0/1&quot;&gt;Liz Sonenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00576">
<title>Interactive Reinforcement Learning for Object Grounding via Self-Talking. (arXiv:1712.00576v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00576</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans are able to identify a referred visual object in a complex scene via a
few rounds of natural language communications. Success communication requires
both parties to engage and learn to adapt for each other. In this paper, we
introduce an interactive training method to improve the natural language
conversation system for a visual grounding task. During interactive training,
both agents are reinforced by the guidance from a common reward function. The
parametrized reward function also cooperatively updates itself via
interactions, and contribute to accomplishing the task. We evaluate the method
on GuessWhat?! visual grounding task, and significantly improve the task
success rate. However, we observe language drifting problem during training and
propose to use reward engineering to improve the interpretability for the
generated conversations. Our result also indicates evaluating goal-ended visual
conversation tasks require semantic relevant metrics beyond task success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00600">
<title>MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence. (arXiv:1712.00600v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00600</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MAgent, a platform to support research and development of
many-agent reinforcement learning. Unlike previous research platforms on single
or multi-agent reinforcement learning, MAgent focuses on supporting the tasks
and the applications that require hundreds to millions of agents. Within the
interactions among a population of agents, it enables not only the study of
learning algorithms for agents&apos; optimal polices, but more importantly, the
observation and understanding of individual agent&apos;s behaviors and social
phenomena emerging from the AI society, including communication languages,
leaderships, altruism. MAgent is highly scalable and can host up to one million
agents on a single GPU server. MAgent also provides flexible configurations for
AI researchers to design their customized environments and agents. In this
demo, we present three environments designed on MAgent and show emerged
collective intelligence by learning from scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lianmin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Han Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00634">
<title>PFAx: Predictable Feature Analysis to Perform Control. (arXiv:1712.00634v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00634</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictable Feature Analysis (PFA) (Richthofer, Wiskott, ICMLA 2015) is an
algorithm that performs dimensionality reduction on high dimensional input
signal. It extracts those subsignals that are most predictable according to a
certain prediction model. We refer to these extracted signals as predictable
features.
&lt;/p&gt;
&lt;p&gt;In this work we extend the notion of PFA to take supplementary information
into account for improving its predictions. Such information can be a
multidimensional signal like the main input to PFA, but is regarded external.
That means it won&apos;t participate in the feature extraction - no features get
extracted or composed of it. Features will be exclusively extracted from the
main input such that they are most predictable based on themselves and the
supplementary information. We refer to this enhanced PFA as PFAx (PFA
extended).
&lt;/p&gt;
&lt;p&gt;Even more important than improving prediction quality is to observe the
effect of supplementary information on feature selection. PFAx transparently
provides insight how the supplementary information adds to prediction quality
and whether it is valuable at all. Finally we show how to invert that relation
and can generate the supplementary information such that it would yield a
certain desired outcome of the main signal.
&lt;/p&gt;
&lt;p&gt;We apply this to a setting inspired by reinforcement learning and let the
algorithm learn how to control an agent in an environment. With this method it
is feasible to locally optimize the agent&apos;s state, i.e. reach a certain goal
that is near enough. We are preparing a follow-up paper that extends this
method such that also global optimization is feasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richthofer_S/0/1/0/all/0/1&quot;&gt;Stefan Richthofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiskott_L/0/1/0/all/0/1&quot;&gt;Laurenz Wiskott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00646">
<title>From knowledge-based to data-driven modeling of fuzzy rule-based systems: A critical reflection. (arXiv:1712.00646v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00646</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper briefly elaborates on a development in (applied) fuzzy logic that
has taken place in the last couple of decades, namely, the complementation or
even replacement of the traditional knowledge-based approach to fuzzy
rule-based systems design by a data-driven one. It is argued that the classical
rule-based modeling paradigm is actually more amenable to the knowledge-based
approach, for which it has originally been conceived, while being less apt to
data-driven model design. An important reason that prevents fuzzy (rule-based)
systems from being leveraged in large-scale applications is the flat structure
of rule bases, along with the local nature of fuzzy rules and their limited
ability to express complex dependencies between variables. This motivates
alternative approaches to fuzzy systems modeling, in which functional
dependencies can be represented more flexibly and more compactly in terms of
hierarchical structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00709">
<title>Simulated Annealing Algorithm for Graph Coloring. (arXiv:1712.00709v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00709</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this Random Walks project is to code and experiment the Markov
Chain Monte Carlo (MCMC) method for the problem of graph coloring. In this
report, we present the plots of cost function \(\mathbf{H}\) by varying the
parameters like \(\mathbf{q}\) (Number of colors that can be used in coloring)
and \(\mathbf{c}\) (Average node degree). The results are obtained by using
simulated annealing scheme, where the temperature (inverse of
\(\mathbf{\beta}\)) parameter in the MCMC is lowered progressively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kose_A/0/1/0/all/0/1&quot;&gt;Alper Kose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonmez_B/0/1/0/all/0/1&quot;&gt;Berke Aral Sonmez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaban_M/0/1/0/all/0/1&quot;&gt;Metin Balaban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00725">
<title>Sentiment Classification using Images and Label Embeddings. (arXiv:1712.00725v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.00725</link>
<description rdf:parseType="Literal">&lt;p&gt;In this project we analysed how much semantic information images carry, and
how much value image data can add to sentiment analysis of the text associated
with the images. To better understand the contribution from images, we compared
models which only made use of image data, models which only made use of text
data, and models which combined both data types. We also analysed if this
approach could help sentiment classifiers generalize to unknown sentiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graesser_L/0/1/0/all/0/1&quot;&gt;Laura Graesser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_L/0/1/0/all/0/1&quot;&gt;Lakshay Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1&quot;&gt;Evelina Bakhturina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00779">
<title>Gradient Descent Learns One-hidden-layer CNN: Don&apos;t be Afraid of Spurious Local Minima. (arXiv:1712.00779v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00779</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a one-hidden-layer neural network with
non-overlapping convolutional layer and ReLU activation function, i.e.,
$f(\mathbf{Z}; \mathbf{w}, \mathbf{a}) = \sum_j
a_j\sigma(\mathbf{w}^\top\mathbf{Z}_j)$, in which both the convolutional
weights $\mathbf{w}$ and the output weights $\mathbf{a}$ are parameters to be
learned. We prove that with Gaussian input $\mathbf{Z}$, there is a spurious
local minimum that is not a global mininum. Surprisingly, in the presence of
local minimum, starting from randomly initialized weights, gradient descent
with weight normalization can still be proven to recover the true parameters
with constant probability (which can be boosted to arbitrarily high accuracy
with multiple restarts). We also show that with constant probability, the same
procedure could also converge to the spurious local minimum, showing that the
local minimum plays a non-trivial role in the dynamics of gradient descent.
Furthermore, a quantitative analysis shows that the gradient descent dynamics
has two phases: it starts off slow, but converges much faster after several
iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnabas Poczos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00840">
<title>Visual Explanation by High-Level Abduction: On Answer-Set Programming Driven Reasoning about Moving Objects. (arXiv:1712.00840v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00840</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a hybrid architecture for systematically computing robust visual
explanation(s) encompassing hypothesis formation, belief revision, and default
reasoning with video data. The architecture consists of two tightly integrated
synergistic components: (1) (functional) answer set programming based abductive
reasoning with space-time tracklets as native entities; and (2) a visual
processing pipeline for detection based object tracking and motion analysis.
&lt;/p&gt;
&lt;p&gt;We present the formal framework, its general implementation as a
(declarative) method in answer set programming, and an example application and
evaluation based on two diverse video datasets: the MOTChallenge benchmark
developed by the vision community, and a recently developed Movie Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchan_J/0/1/0/all/0/1&quot;&gt;Jakob Suchan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_M/0/1/0/all/0/1&quot;&gt;Mehul Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walega_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Wa&amp;#x142;&amp;#x119;ga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schultz_C/0/1/0/all/0/1&quot;&gt;Carl Schultz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00846">
<title>Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection. (arXiv:1712.00846v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00846</link>
<description rdf:parseType="Literal">&lt;p&gt;Web-based human trafficking activity has increased in recent years but it
remains sparsely dispersed among escort advertisements and difficult to
identify due to its often-latent nature. The use of intelligent systems to
detect trafficking can thus have a direct impact on investigative resource
allocation and decision-making, and, more broadly, help curb a widespread
social problem. Trafficking detection involves assigning a normalized score to
a set of escort advertisements crawled from the Web -- a higher score indicates
a greater risk of trafficking-related (involuntary) activities. In this paper,
we define and study the problem of trafficking detection and present a
trafficking detection pipeline architecture developed over three years of
research within the DARPA Memex program. Drawing on multi-institutional data,
systems, and experiences collected during this time, we also conduct post hoc
bias analyses and present a bias mitigation plan. Our findings show that, while
automatic trafficking detection is an important application of AI for social
good, it also provides cautionary lessons for deploying predictive machine
learning algorithms without appropriate de-biasing. This ultimately led to
integration of an interpretable solution into a search system that contains
over 100 million advertisements and is used by over 200 law enforcement
agencies to investigate leads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hundman_K/0/1/0/all/0/1&quot;&gt;Kyle Hundman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1&quot;&gt;Thamme Gowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1&quot;&gt;Mayank Kejriwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1&quot;&gt;Benedikt Boecking&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00898">
<title>Proceedings of the Fifth Workshop on Proof eXchange for Theorem Proving. (arXiv:1712.00898v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1712.00898</link>
<description rdf:parseType="Literal">&lt;p&gt;This volume of EPTCS contains the proceedings of the Fifth Workshop on Proof
Exchange for Theorem Proving (PxTP 2017), held on September 23-24, 2017 as part
of the Tableaux, FroCoS and ITP conferences in Brasilia, Brazil. The PxTP
workshop series brings together researchers working on various aspects of
communication, integration, and cooperation between reasoning systems and
formalisms, with a special focus on proofs. The progress in computer-aided
reasoning, both automated and interactive, during the past decades, made it
possible to build deduction tools that are increasingly more applicable to a
wider range of problems and are able to tackle larger problems progressively
faster. In recent years, cooperation between such tools in larger systems has
demonstrated the potential to reduce the amount of manual intervention.
Cooperation between reasoning systems relies on availability of theoretical
formalisms and practical tools to exchange problems, proofs, and models. The
PxTP workshop series strives to encourage such cooperation by inviting
contributions on all aspects of cooperation between reasoning tools, whether
automatic or interactive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubois_C/0/1/0/all/0/1&quot;&gt;Catherine Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paleo_B/0/1/0/all/0/1&quot;&gt;Bruno Woltzenlogel Paleo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00912">
<title>Deep Learning Can Reverse Photon Migration for Diffuse Optical Tomography. (arXiv:1712.00912v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.00912</link>
<description rdf:parseType="Literal">&lt;p&gt;Can artificial intelligence (AI) learn complicated non-linear physics? Here
we propose a novel deep learning approach that learns non-linear photon
scattering physics and obtains accurate 3D distribution of optical anomalies.
In contrast to the traditional black-box deep learning approaches to inverse
problems, our deep network learns to invert the Lippmann-Schwinger integral
equation which describes the essential physics of photon migration of diffuse
near-infrared (NIR) photons in turbid media. As an example for clinical
relevance, we applied the method to our prototype diffuse optical tomography
(DOT). We show that our deep neural network, trained with only simulation data,
can accurately recover the location of anomalies within biomimetic phantoms and
live animals without the use of an exogenous contrast agent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_S/0/1/0/all/0/1&quot;&gt;Sohail Sabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1&quot;&gt;Duchang Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kee Hyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1&quot;&gt;Abdul Wahab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yoonseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seul-I Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_E/0/1/0/all/0/1&quot;&gt;Eun Young Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hak Hee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_Y/0/1/0/all/0/1&quot;&gt;Young Min Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Young-wook Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Seungryong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00929">
<title>SERKET: An Architecture For Connecting Stochastic Models to Realize a Large-Scale Cognitive Model. (arXiv:1712.00929v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00929</link>
<description rdf:parseType="Literal">&lt;p&gt;To realize human-like robot intelligence, a large-scale cognitive
architecture is required for robots to understand the environment through a
variety of sensors with which they are equipped. In this paper, we propose a
novel framework named Serket that enables the construction of a large-scale
generative model and its inference easily by connecting sub-modules to allow
the robots to acquire various capabilities through interaction with their
environments and others. We consider that large-scale cognitive models can be
constructed by connecting smaller fundamental models hierarchically while
maintaining their programmatic independence. Moreover, connected modules are
dependent on each other, and parameters are required to be optimized as a
whole. Conventionally, the equations for parameter estimation have to be
derived and implemented depending on the models. However, it becomes harder to
derive and implement those of a larger scale model. To solve these problems, in
this paper, we propose a method for parameter estimation by communicating the
minimal parameters between various modules while maintaining their programmatic
independence. Therefore, Serket makes it easy to construct large-scale models
and estimate their parameters via the connection of modules. Experimental
results demonstrated that the model can be constructed by connecting modules,
the parameters can be optimized as a whole, and they are comparable with the
original models that we have proposed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamura_T/0/1/0/all/0/1&quot;&gt;Tomoaki Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagai_T/0/1/0/all/0/1&quot;&gt;Takayuki Nagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1&quot;&gt;Tadahiro Taniguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00988">
<title>End-to-End Relation Extraction using Markov Logic Networks. (arXiv:1712.00988v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.00988</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of end-to-end relation extraction consists of two sub-tasks: i)
identifying entity mentions along with their types and ii) recognizing semantic
relations among the entity mention pairs. %Identifying entity mentions along
with their types and recognizing semantic relations among the entity mentions,
are two very important problems in Information Extraction. It has been shown
that for better performance, it is necessary to address these two sub-tasks
jointly. We propose an approach for simultaneous extraction of entity mentions
and relations in a sentence, by using inference in Markov Logic Networks (MLN).
We learn three different classifiers : i) local entity classifier, ii) local
relation classifier and iii) &quot;pipeline&quot; relation classifier which uses
predictions of the local entity classifier. Predictions of these classifiers
may be inconsistent with each other. We represent these predictions along with
some domain knowledge using weighted first-order logic rules in an MLN and
perform joint inference over the MLN to obtain a global output with minimum
inconsistencies. Experiments on the ACE (Automatic Content Extraction) 2004
dataset demonstrate that our approach of joint extraction using MLNs
outperforms the baselines of individual classifiers. Our end-to-end relation
extraction performance is better than 2 out of 3 previous results reported on
the ACE 2004 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawar_S/0/1/0/all/0/1&quot;&gt;Sachin Pawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1&quot;&gt;Pushpak Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palshikar_G/0/1/0/all/0/1&quot;&gt;Girish K. Palshikar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00991">
<title>Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals. (arXiv:1712.00991v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.00991</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance appraisal (PA) is an important HR process to periodically measure
and evaluate every employee&apos;s performance vis-a-vis the goals established by
the organization. A PA process involves purposeful multi-step multi-modal
communication between employees, their supervisors and their peers, such as
self-appraisal, supervisor assessment and peer feedback. Analysis of the
structured data and text produced in PA is crucial for measuring the quality of
appraisals and tracking actual improvements. In this paper, we apply text
mining techniques to produce insights from PA text. First, we perform sentence
classification to identify strengths, weaknesses and suggestions of
improvements found in the supervisor assessments and then use clustering to
discover broad categories among them. Next we use multi-class multi-label
classification techniques to match supervisor assessments to predefined broad
perspectives on performance. Finally, we propose a short-text summarization
technique to produce a summary of peer feedback comments for a given employee
and compare it with manual summaries. All techniques are illustrated using a
real-life dataset of supervisor assessment and peer feedback text produced
during the PA of 4528 employees in a large multi-national IT company.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palshikar_G/0/1/0/all/0/1&quot;&gt;Girish Keshav Palshikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawar_S/0/1/0/all/0/1&quot;&gt;Sachin Pawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chourasia_S/0/1/0/all/0/1&quot;&gt;Saheb Chourasia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramrakhiyani_N/0/1/0/all/0/1&quot;&gt;Nitin Ramrakhiyani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01001">
<title>Characterizing and Computing Causes for Query Answers in Databases from Database Repairs and Repair Programs. (arXiv:1712.01001v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1712.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;A correspondence between database tuples as causes for query answers in
databases and tuple-based repairs of inconsistent databases with respect to
denial constraints has already been established. In this work, answer-set
programs that specify repairs of databases are used as a basis for solving
computational and reasoning problems about causes. Here, causes are also
introduced at the attribute level by appealing to a both null-based and
attribute-based repair semantics. The corresponding repair programs are
presented, and they are used as a basis for computation and reasoning about
attribute-level causes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01093">
<title>The mind as a computational system. (arXiv:1712.01093v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.01093</link>
<description rdf:parseType="Literal">&lt;p&gt;The present document is an excerpt of an essay that I wrote as part of my
application material to graduate school in Computer Science (with a focus on
Artificial Intelligence), in 1986. I was not invited by any of the schools that
received it, so I became a theoretical physicist instead. The essay&apos;s full
title was &quot;Some Topics in Philosophy and Computer Science&quot;. I am making this
text (unchanged from 1985, preserving the typesetting as much as possible)
available now in memory of Jerry Fodor, whose writings had influenced me
significantly at the time (even though I did not always agree).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adami_C/0/1/0/all/0/1&quot;&gt;Christoph Adami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01106">
<title>Transferring Autonomous Driving Knowledge on Simulated and Real Intersections. (arXiv:1712.01106v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.01106</link>
<description rdf:parseType="Literal">&lt;p&gt;We view intersection handling on autonomous vehicles as a reinforcement
learning problem, and study its behavior in a transfer learning setting. We
show that a network trained on one type of intersection generally is not able
to generalize to other intersections. However, a network that is pre-trained on
one intersection and fine-tuned on another performs better on the new task
compared to training in isolation. This network also retains knowledge of the
prior task, even though some forgetting occurs. Finally, we show that the
benefits of fine-tuning hold when transferring simulated intersection handling
knowledge to a real autonomous vehicle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isele_D/0/1/0/all/0/1&quot;&gt;David Isele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosgun_A/0/1/0/all/0/1&quot;&gt;Akansel Cosgun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01235">
<title>On the Real-time Vehicle Placement Problem. (arXiv:1712.01235v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.01235</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by ride-sharing platforms&apos; efforts to reduce their riders&apos; wait
times for a vehicle, this paper introduces a novel problem of placing vehicles
to fulfill real-time pickup requests in a spatially and temporally changing
environment. The real-time nature of this problem makes it fundamentally
different from other placement and scheduling problems, as it requires not only
real-time placement decisions but also handling real-time request dynamics,
which are influenced by human mobility patterns. We use a dataset of ten
million ride requests from four major U.S. cities to show that the requests
exhibit significant self-similarity. We then propose distributed online
learning algorithms for the real-time vehicle placement problem and bound their
expected performance under this observed self-similarity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jauhri_A/0/1/0/all/0/1&quot;&gt;Abhinav Jauhri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1&quot;&gt;Carlee Joe-Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;John Paul Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01252">
<title>An Equivalence of Fully Connected Layer and Convolutional Layer. (arXiv:1712.01252v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.01252</link>
<description rdf:parseType="Literal">&lt;p&gt;This article demonstrates that convolutional operation can be converted to
matrix multiplication, which has the same calculation way with fully connected
layer. The article is helpful for the beginners of the neural network to
understand how fully connected layer and the convolutional layer work in the
backend. To be concise and to make the article more readable, we only consider
the linear case. It can be extended to the non-linear case easily through
plugging in a non-linear encapsulation to the values like this $\sigma(x)$
denoted as $x^{\prime}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jun Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.00154">
<title>Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary. (arXiv:1705.00154v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1705.00154</link>
<description rdf:parseType="Literal">&lt;p&gt;Current domain-independent, classical planners require symbolic models of the
problem domain and instance as input, resulting in a knowledge acquisition
bottleneck. Meanwhile, although deep learning has achieved significant success
in many fields, the knowledge is encoded in a subsymbolic representation which
is incompatible with symbolic systems such as planners. We propose LatPlan, an
unsupervised architecture combining deep learning and classical planning. Given
only an unlabeled set of image pairs showing a subset of transitions allowed in
the environment (training inputs), and a pair of images representing the
initial and the goal states (planning inputs), LatPlan finds a plan to the goal
state in a symbolic latent space and returns a visualized plan execution. The
contribution of this paper is twofold: (1) State Autoencoder, which finds a
propositional state representation of the environment using a Variational
Autoencoder. It generates a discrete latent vector from the images, based on
which a PDDL model can be constructed and then solved by an off-the-shelf
planner. (2) Action Autoencoder / Discriminator, a neural architecture which
jointly finds the action symbols and the implicit action models
(preconditions/effects), and provides a successor function for the implicit
graph search. We evaluate LatPlan using image-based versions of 3 planning
domains: 8-puzzle, Towers of Hanoi and LightsOut.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asai_M/0/1/0/all/0/1&quot;&gt;Masataro Asai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukunaga_A/0/1/0/all/0/1&quot;&gt;Alex Fukunaga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08439">
<title>Thinking Fast and Slow with Deep Learning and Tree Search. (arXiv:1705.08439v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08439</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential decision making problems, such as structured prediction, robotic
control, and game playing, require a combination of planning policies and
generalisation of those plans. In this paper, we present Expert Iteration
(ExIt), a novel reinforcement learning algorithm which decomposes the problem
into separate planning and generalisation tasks. Planning new policies is
performed by tree search, while a deep neural network generalises those plans.
Subsequently, tree search is improved by using the neural network policy to
guide search, increasing the strength of new plans. In contrast, standard deep
Reinforcement Learning algorithms rely on a neural network not only to
generalise plans, but to discover them too. We show that ExIt outperforms
REINFORCE for training a neural network to play the board game Hex, and our
final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most
recent Olympiad Champion player to be publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthony_T/0/1/0/all/0/1&quot;&gt;Thomas Anthony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zheng Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1&quot;&gt;David Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.02596">
<title>Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. (arXiv:1708.02596v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.02596</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-free deep reinforcement learning algorithms have been shown to be
capable of learning a wide range of robotic skills, but typically require a
very large number of samples to achieve good performance. Model-based
algorithms, in principle, can provide for much more efficient learning, but
have proven difficult to extend to expressive, high-capacity models such as
deep neural networks. In this work, we demonstrate that medium-sized neural
network models can in fact be combined with model predictive control (MPC) to
achieve excellent sample complexity in a model-based reinforcement learning
algorithm, producing stable and plausible gaits to accomplish various complex
locomotion tasks. We also propose using deep neural network dynamics models to
initialize a model-free learner, in order to combine the sample efficiency of
model-based approaches with the high task-specific performance of model-free
methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure
model-based approach trained on just random action data can follow arbitrary
trajectories with excellent sample efficiency, and that our hybrid algorithm
can accelerate model-free learning on high-speed benchmark tasks, achieving
sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents.
Videos can be found at https://sites.google.com/view/mbmf
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1&quot;&gt;Anusha Nagabandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_G/0/1/0/all/0/1&quot;&gt;Gregory Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fearing_R/0/1/0/all/0/1&quot;&gt;Ronald S. Fearing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06040">
<title>Neural Block Sampling. (arXiv:1708.06040v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06040</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient Monte Carlo inference often requires manual construction of
model-specific proposals. We propose an approach to automated proposal
construction by training neural networks to provide fast approximations to
block Gibbs conditionals. The learned proposals generalize to occurrences of
common structural motifs both within a given model and across models, allowing
for the construction of a library of learned inference primitives that can
accelerate inference on unseen models with no model-specific training required.
We explore several applications including open-universe Gaussian mixture
models, in which our learned proposals outperform a hand-tuned sampler, and a
real-world named entity recognition task, in which our sampler&apos;s ability to
escape local modes yields higher final F1 scores than single-site Gibbs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tongzhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_D/0/1/0/all/0/1&quot;&gt;David A. Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1&quot;&gt;Stuart J. Russell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07303">
<title>Learning 6-DOF Grasping Interaction with Deep Geometry-aware 3D Representations. (arXiv:1708.07303v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07303</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on the problem of learning 6-DOF grasping with a parallel
jaw gripper in simulation. We propose the notion of a geometry-aware
representation in grasping based on the assumption that knowledge of 3D
geometry is at the heart of interaction. Our key idea is constraining and
regularizing grasping interaction learning through 3D geometry prediction.
Specifically, we formulate the learning of deep geometry-aware grasping model
in two steps: First, we learn to build mental geometry-aware representation by
reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via
generative 3D shape modeling. Second, we learn to predict grasping outcome with
its internal geometry-aware representation. The learned outcome prediction
model is used to sequentially propose grasping solutions via
analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best
of our knowledge, we are presenting for the first time a method to learn a
6-DOF grasping net from RGBD input; (2) We build a grasping dataset from
demonstrations in virtual reality with rich sensory and interaction
annotations. This dataset includes 101 everyday objects spread across 7
categories, additionally, we propose a data augmentation strategy for effective
learning; (3) We demonstrate that the learned geometry-aware representation
leads to about 10 percent relative performance improvement over the baseline
CNN on grasping objects from our dataset. (4) We further demonstrate that the
model generalizes to novel viewpoints and object instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xinchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1&quot;&gt;Jasmine Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khansari_M/0/1/0/all/0/1&quot;&gt;Mohi Khansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunfei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1&quot;&gt;Arkanath Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_J/0/1/0/all/0/1&quot;&gt;James Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04555">
<title>Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. (arXiv:1709.04555v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04555</link>
<description rdf:parseType="Literal">&lt;p&gt;The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wengong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1&quot;&gt;Connor W. Coley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1&quot;&gt;Regina Barzilay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04763">
<title>Motif-based Rule Discovery for Predicting Real-valued Time Series. (arXiv:1709.04763v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04763</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series prediction is of great significance in many applications and has
attracted extensive attention from the data mining community. Existing work
suggests that for many problems, the shape in the current time series may
correlate an upcoming shape in the same or another series. Therefore, it is a
promising strategy to associate two recurring patterns as a rule&apos;s antecedent
and consequent: the occurrence of the antecedent can foretell the occurrence of
the consequent, and the learned shape of consequent will give accurate
predictions. Earlier work employs symbolization methods, but the symbolized
representation maintains too little information of the original series to mine
valid rules. The state-of-the-art work, though directly manipulating the
series, fails to segment the series precisely for seeking
antecedents/consequents, resulting in inaccurate rules in common scenarios. In
this paper, we propose a novel motif-based rule discovery method, which
utilizes motif discovery to accurately extract frequently occurring consecutive
subsequences, i.e. motifs, as antecedents/consequents. It then investigates the
underlying relationships between motifs by matching motifs as rule candidates
and ranking them based on the similarities. Experimental results on real open
datasets show that the proposed approach outperforms the baseline method by
23.9%. Furthermore, it extends the applicability from single time series to
multiple ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuanduo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xu Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Juguang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jingyue Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04822">
<title>Fast Top-k Area Topics Extraction with Knowledge Base. (arXiv:1710.04822v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04822</link>
<description rdf:parseType="Literal">&lt;p&gt;What are the most popular research topics in Artificial Intelligence (AI)? We
formulate the problem as extracting top-$k$ topics that can best represent a
given area with the help of knowledge base. We theoretically prove that the
problem is NP-hard and propose an optimization model, FastKATE, to address this
problem by combining both explicit and latent representations for each topic.
We leverage a large-scale knowledge base (Wikipedia) to generate topic
embeddings using neural networks and use this kind of representations to help
capture the representativeness of topics for given areas. We develop a fast
heuristic algorithm to efficiently solve the problem with a provable error
bound. We evaluate the proposed model on three real-world datasets.
Experimental results demonstrate our model&apos;s effectiveness, robustness,
real-timeness (return results in $&amp;lt;1$s), and its superiority over several
alternative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaochen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jingfei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1&quot;&gt;Marie-Francine Moens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07783">
<title>A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07783</link>
<description rdf:parseType="Literal">&lt;p&gt;SGD (Stochastic Gradient Descent) is a popular algorithm for large scale
optimization problems due to its low iterative cost. However, SGD can not
achieve linear convergence rate as FGD (Full Gradient Descent) because of the
inherent gradient variance. To attack the problem, mini-batch SGD was proposed
to get a trade-off in terms of convergence rate and iteration cost. In this
paper, a general CVI (Convergence-Variance Inequality) equation is presented to
state formally the interaction of convergence rate and gradient variance. Then
a novel algorithm named SSAG (Stochastic Stratified Average Gradient) is
introduced to reduce gradient variance based on two techniques, stratified
sampling and averaging over iterations that is a key idea in SAG (Stochastic
Average Gradient). Furthermore, SSAG can achieve linear convergence rate of
$\mathcal {O}((1-\frac{\mu}{8CL})^k)$ at smaller storage and iterative costs,
where $C\geq 2$ is the category number of training data. This convergence rate
depends mainly on the variance between classes, but not on the variance within
the classes. In the case of $C\ll N$ ($N$ is the training data size), SSAG&apos;s
convergence rate is much better than SAG&apos;s convergence rate of $\mathcal
{O}((1-\frac{\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG
and many other algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Aixiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bingchuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_X/0/1/0/all/0/1&quot;&gt;Xiaolong Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_R/0/1/0/all/0/1&quot;&gt;Rui Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hengguang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07983">
<title>Safety-Aware Apprenticeship Learning. (arXiv:1710.07983v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07983</link>
<description rdf:parseType="Literal">&lt;p&gt;Apprenticeship learning (AL) is a class of &quot;learning from demonstrations&quot;
techniques where the reward function of a Markov Decision Process (MDP) is
unknown to the learning agent and the agent has to derive a good policy by
observing an expert&apos;s demonstrations. In this paper, we study the problem of
how to make AL algorithms inherently safe while still meeting its learning
objective. We consider a setting where the unknown reward function is assumed
to be a linear combination of a set of state features, and the safety property
is specified in Probabilistic Computation Tree Logic (PCTL). By embedding
probabilistic model checking inside AL, we propose a novel
counterexample-guided approach that can ensure both safety and performance of
the learned policy. We demonstrate the effectiveness of our approach on several
challenging AL scenarios where safety is essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Weichao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenchao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09549">
<title>Context-Aware Generative Adversarial Privacy. (arXiv:1710.09549v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09549</link>
<description rdf:parseType="Literal">&lt;p&gt;Preserving the utility of published datasets while simultaneously providing
provable privacy guarantees is a well-known challenge. On the one hand,
context-free privacy solutions, such as differential privacy, provide strong
privacy guarantees, but often lead to a significant reduction in utility. On
the other hand, context-aware privacy solutions, such as information theoretic
privacy, achieve an improved privacy-utility tradeoff, but assume that the data
holder has access to dataset statistics. We circumvent these limitations by
introducing a novel context-aware privacy framework called generative
adversarial privacy (GAP). GAP leverages recent advancements in generative
adversarial networks (GANs) to allow the data holder to learn privatization
schemes from the dataset itself. Under GAP, learning the privacy mechanism is
formulated as a constrained minimax game between two players: a privatizer that
sanitizes the dataset in a way that limits the risk of inference attacks on the
individuals&apos; private variables, and an adversary that tries to infer the
private variables from the sanitized dataset. To evaluate GAP&apos;s performance, we
investigate two simple (yet canonical) statistical dataset models: (a) the
binary data model, and (b) the binary Gaussian mixture model. For both models,
we derive game-theoretically optimal minimax privacy mechanisms, and show that
the privacy mechanisms learned from data (in a generative adversarial fashion)
match the theoretically optimal ones. This demonstrates that our framework can
be easily applied in practice, even in the absence of dataset statistics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1&quot;&gt;Peter Kairouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1&quot;&gt;Lalitha Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1&quot;&gt;Ram Rajagopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03817">
<title>Learning with Options that Terminate Off-Policy. (arXiv:1711.03817v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03817</link>
<description rdf:parseType="Literal">&lt;p&gt;A temporally abstract action, or an option, is specified by a policy and a
termination condition: the policy guides option behavior, and the termination
condition roughly determines its length. Generally, learning with longer
options (like learning with multi-step returns) is known to be more efficient.
However, if the option set for the task is not ideal, and cannot express the
primitive optimal policy exactly, shorter options offer more flexibility and
can yield a better solution. Thus, the termination condition puts learning
efficiency at odds with solution quality. We propose to resolve this dilemma by
decoupling the behavior and target terminations, just like it is done with
policies in off-policy learning. To this end, we give a new algorithm,
Q(\beta), that learns the solution with respect to any termination condition,
regardless of how the options actually terminate. We derive Q(\beta) by casting
learning with options into a common framework with well-studied multi-step
off-policy learning. We validate our algorithm empirically, and show that it
holds up to its motivating claims.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harutyunyan_A/0/1/0/all/0/1&quot;&gt;Anna Harutyunyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vrancx_P/0/1/0/all/0/1&quot;&gt;Peter Vrancx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1&quot;&gt;Pierre-Luc Bacon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1&quot;&gt;Ann Nowe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08819">
<title>Improvised Comedy as a Turing Test. (arXiv:1711.08819v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08819</link>
<description rdf:parseType="Literal">&lt;p&gt;The best improvisational theatre actors can make any scene partner, of any
skill level or ability, appear talented and proficient in the art form, and
thus &quot;make them shine&quot;. To challenge this improvisational paradigm, we built an
artificial intelligence (AI) trained to perform live shows alongside human
actors for human audiences. Over the course of 30 performances to a combined
audience of almost 3000 people, we have refined theatrical games which involve
combinations of human and (at times, adversarial) AI actors. We have developed
specific scene structures to include audience participants in interesting ways.
Finally, we developed a complete show structure that submitted the audience to
a Turing test and observed their suspension of disbelief, which we believe is
key for human/non-human theatre co-creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1&quot;&gt;Kory Wallace Mathewson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirowski_P/0/1/0/all/0/1&quot;&gt;Piotr Mirowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09681">
<title>Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation. (arXiv:1711.09681v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09681</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new algorithm for controlling classification results by
generating a small additive perturbation without changing the classifier
network. Our work is inspired by existing works generating adversarial
perturbation that worsens classification performance. In contrast to the
existing methods, our work aims to generate perturbations that can enhance
overall classification performance. To solve this performance enhancement
problem, we newly propose a perturbation generation network (PGN) influenced by
the adversarial learning strategy. In our problem, the information in a large
external dataset is summarized by a small additive perturbation, which helps to
improve the performance of the classifier trained with the target dataset. In
addition to this performance enhancement problem, we show that the proposed PGN
can be adopted to solve the classical adversarial problem without utilizing the
information on the target classifier. The mentioned characteristics of our
method are verified through extensive experiments on publicly available visual
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1&quot;&gt;YoungJoon Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seonguk Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Junyoung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1&quot;&gt;Nojun Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09744">
<title>How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence. (arXiv:1711.09744v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09744</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence is a central topic in the computer science
curriculum. From the year 2011 a project-based learning methodology based on
computer games has been designed and implemented into the intelligence
artificial course at the University of the Bio-Bio. The project aims to develop
software-controlled agents (bots) which are programmed by using heuristic
algorithms seen during the course. This methodology allows us to obtain good
learning results, however several challenges have been founded during its
implementation.
&lt;/p&gt;
&lt;p&gt;In this paper we show how linguistic descriptions of data can help to provide
students and teachers with technical and personalized feedback about the
learned algorithms. Algorithm behavior profile and a new Turing test for
computer games bots based on linguistic modelling of complex phenomena are also
proposed in order to deal with such challenges.
&lt;/p&gt;
&lt;p&gt;In order to show and explore the possibilities of this new technology, a web
platform has been designed and implemented by one of authors and its
incorporation in the process of assessment allows us to improve the teaching
learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubio_Manzano_C/0/1/0/all/0/1&quot;&gt;Clemente Rubio-Manzano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senoceain_T/0/1/0/all/0/1&quot;&gt;Tomas Lermanda Senoceain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00180">
<title>New Techniques for Inferring L-Systems Using Genetic Algorithm. (arXiv:1712.00180v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00180</link>
<description rdf:parseType="Literal">&lt;p&gt;Lindenmayer systems (L-systems) are a formal grammar system that iteratively
rewrites all symbols of a string, in parallel. When visualized with a graphical
interpretation, the images have self-similar shapes that appear frequently in
nature, and they have been particularly successful as a concise, reusable
technique for simulating plants. The L-system inference problem is to find an
L-system to simulate a given plant. This is currently done mainly by experts,
but this process is limited by the availability of experts, the complexity that
may be solved by humans, and time. This paper introduces the Plant Model
Inference Tool (PMIT) that infers deterministic context-free L-systems from an
initial sequence of strings generated by the system using a genetic algorithm.
PMIT is able to infer more complex systems than existing approaches. Indeed,
while existing approaches are limited to L-systems with a total sum of 20
combined symbols in the productions, PMIT can infer almost all L-systems tested
where the total sum is 140 symbols. This was validated using a test bed of 28
previously developed L-system models, in addition to models created
artificially by bootstrapping larger models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_J/0/1/0/all/0/1&quot;&gt;Jason Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McQuillan_I/0/1/0/all/0/1&quot;&gt;Ian McQuillan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10207">
<title>Learning to Rank based on Analogical Reasoning. (arXiv:1711.10207v1 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1711.10207</link>
<description rdf:parseType="Literal">&lt;p&gt;Object ranking or &quot;learning to rank&quot; is an important problem in the realm of
preference learning. On the basis of training data in the form of a set of
rankings of objects represented as feature vectors, the goal is to learn a
ranking function that predicts a linear order of any new set of objects. In
this paper, we propose a new approach to object ranking based on principles of
analogical reasoning. More specifically, our inference pattern is formalized in
terms of so-called analogical proportions and can be summarized as follows:
Given objects $A,B,C,D$, if object $A$ is known to be preferred to $B$, and $C$
relates to $D$ as $A$ relates to $B$, then $C$ is (supposedly) preferred to
$D$. Our method applies this pattern as a main building block and combines it
with ideas and techniques from instance-based learning and rank aggregation.
Based on first experimental results for data sets from various domains (sports,
education, tourism, etc.), we conclude that our approach is highly competitive.
It appears to be specifically interesting in situations in which the objects
are coming from different subdomains, and which hence require a kind of
knowledge transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fahandar_M/0/1/0/all/0/1&quot;&gt;Mohsen Ahmadi Fahandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00465">
<title>Subject Selection on a Riemannian Manifold for Unsupervised Cross-subject Seizure Detection. (arXiv:1712.00465v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00465</link>
<description rdf:parseType="Literal">&lt;p&gt;Inter-subject variability between individuals poses a challenge in
inter-subject brain signal analysis problems. A new algorithm for
subject-selection based on clustering covariance matrices on a Riemannian
manifold is proposed. After unsupervised selection of the subsets of relevant
subjects, data in a cluster is mapped to a tangent space at the mean point of
covariance matrices in that cluster and an SVM classifier on labeled data from
relevant subjects is trained. Experiment on an EEG seizure database shows that
the proposed method increases the accuracy over state-of-the-art from 86.83% to
89.84% and specificity from 87.38% to 89.64% while reducing the false positive
rate/hour from 0.8/hour to 0.77/hour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolagh_S/0/1/0/all/0/1&quot;&gt;Samaneh Nasiri Ghosheh Bolagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1&quot;&gt;Gari. D. Clifford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00481">
<title>Intelligent EHRs: Predicting Procedure Codes From Diagnosis Codes. (arXiv:1712.00481v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00481</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to submit a claim to insurance companies, a doctor needs to code a
patient encounter with both the diagnosis (ICDs) and procedures performed
(CPTs) in an Electronic Health Record (EHR). Identifying and applying relevant
procedures code is a cumbersome and time-consuming task as a doctor has to
choose from around 13,000 procedure codes with no predefined one-to-one
mapping. In this paper, we propose a state-of-the-art deep learning method for
automatic and intelligent coding of procedures (CPTs) from the diagnosis codes
(ICDs) entered by the doctor. Precisely, we cast the learning problem as a
multi-label classification problem and use distributed representation to learn
the input mapping of high-dimensional sparse ICDs codes. Our final model
trained on 2.3 million claims is able to outperform existing rule-based
probabilistic and association-rule mining based methods and has a recall of
90@3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haq_H/0/1/0/all/0/1&quot;&gt;Hasham Ul Haq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahmad_R/0/1/0/all/0/1&quot;&gt;Rameel Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hussain_S/0/1/0/all/0/1&quot;&gt;Sibt Ul Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00499">
<title>Prediction-Constrained Topic Models for Antidepressant Recommendation. (arXiv:1712.00499v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00499</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervisory signals can help topic models discover low-dimensional data
representations that are more interpretable for clinical tasks. We propose a
framework for training supervised latent Dirichlet allocation that balances two
goals: faithful generative explanations of high-dimensional data and accurate
prediction of associated class labels. Existing approaches fail to balance
these goals by not properly handling a fundamental asymmetry: the intended task
is always predicting labels from data, not data from labels. Our new
prediction-constrained objective trains models that predict labels from heldout
data well while also producing good generative likelihoods and interpretable
topic-word parameters. In a case study on predicting depression medications
from electronic health records, we demonstrate improved recommendations
compared to previous supervised topic models and high- dimensional logistic
regression from words alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Michael C. Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hope_G/0/1/0/all/0/1&quot;&gt;Gabriel Hope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiner_L/0/1/0/all/0/1&quot;&gt;Leah Weiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCoy_T/0/1/0/all/0/1&quot;&gt;Thomas H. McCoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perlis_R/0/1/0/all/0/1&quot;&gt;Roy H. Perlis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudderth_E/0/1/0/all/0/1&quot;&gt;Erik B. Sudderth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00504">
<title>A Neural Stochastic Volatility Model. (arXiv:1712.00504v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00504</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we show that the recent integration of statistical models with
deep recurrent neural networks provides a new way of formulating volatility
(the degree of variation of time series) models that have been widely used in
time series analysis and prediction in finance. The model comprises a pair of
complementary stochastic recurrent neural networks: the generative network
models the joint distribution of the stochastic volatility process; the
inference network approximates the conditional distribution of the latent
variables given the observables. Our focus here is on the formulation of
temporal dynamics of volatility over time under a stochastic recurrent neural
network framework. Experiments on real-world stock price datasets demonstrate
that the proposed model generates a better volatility estimation and prediction
that outperforms stronge baseline methods, including the deterministic models,
such as GARCH and its variants, and the stochastic MCMC-based models, and the
Gaussian-process-based, on the average negative log-likelihood measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Rui Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaojun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00520">
<title>Bayesian Semi-nonnegative Tri-matrix Factorization to Identify Pathways Associated with Cancer Types. (arXiv:1712.00520v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00520</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying altered pathways that are associated with specific cancer types
can potentially bring a significant impact on cancer patient treatment.
Accurate identification of such key altered pathways information can be used to
develop novel therapeutic agents as well as to understand the molecular
mechanisms of various types of cancers better. Tri-matrix factorization is an
efficient tool to learn associations between two different entities (e.g.,
cancer types and pathways in our case) from data. To successfully apply
tri-matrix factorization methods to biomedical problems, biological prior
knowledge such as pathway databases or protein-protein interaction (PPI)
networks, should be taken into account in the factorization model. However, it
is not straightforward in the Bayesian setting even though Bayesian methods are
more appealing than point estimate methods, such as a maximum likelihood or a
maximum posterior method, in the sense that they calculate distributions over
variables and are robust against overfitting. We propose a Bayesian
(semi-)nonnegative matrix factorization model for human cancer genomic data,
where the biological prior knowledge represented by a pathway database and a
PPI network is taken into account in the factorization model through a finite
dependent Beta-Bernoulli prior. We tested our method on The Cancer Genome Atlas
(TCGA) dataset and found that the pathways identified by our method can be used
as a prognostic biomarkers for patient subgroup identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sunho Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hwang_T/0/1/0/all/0/1&quot;&gt;Tae Hyun Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00535">
<title>Survival-Supervised Topic Modeling with Anchor Words: Characterizing Pancreatitis Outcomes. (arXiv:1712.00535v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00535</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new approach for topic modeling that is supervised by survival
analysis. Specifically, we build on recent work on unsupervised topic modeling
with so-called anchor words by providing supervision through an elastic-net
regularized Cox proportional hazards model. In short, an anchor word being
present in a document provides strong indication that the document is partially
about a specific topic. For example, by seeing &quot;gallstones&quot; in a document, we
are fairly certain that the document is partially about medicine. Our proposed
method alternates between learning a topic model and learning a survival model
to find a local minimum of a block convex optimization problem. We apply our
proposed approach to predicting how long patients with pancreatitis admitted to
an intensive care unit (ICU) will stay in the ICU. Our approach is as accurate
as the best of a variety of baselines while being more interpretable than any
of the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;George H. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weiss_J/0/1/0/all/0/1&quot;&gt;Jeremy C. Weiss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00556">
<title>A global feature extraction model for the effective computer aided diagnosis of mild cognitive impairment using structural MRI images. (arXiv:1712.00556v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00556</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple modalities of biomarkers have been proved to be very sensitive in
assessing the progression of Alzheimer&apos;s disease (AD), and using these
modalities and machine learning algorithms, several approaches have been
proposed to assist in the early diagnosis of AD. Among the recent investigated
state-of-the-art approaches, Gaussian discriminant analysis (GDA)-based
approaches have been demonstrated to be more effective and accurate in the
classification of AD, especially for delineating its prodromal stage of mild
cognitive impairment (MCI). Moreover, among those binary classification
investigations, the local feature extraction methods were mostly used, which
made them hardly be applied to a practical computer aided diagnosis system.
Therefore, this study presents a novel global feature extraction model taking
advantage of the recent proposed GDA-based dual high-dimensional decision
spaces, which can significantly improve the early diagnosis performance
comparing to those local feature extraction methods. In the true test using 20%
held-out data, for discriminating the most challenging MCI group from the
cognitively normal control (CN) group, an F1 score of 91.06%, an accuracy of
88.78%, a sensitivity of 91.80%, and a specificity of 83.78% were achieved that
can be considered as the best performance obtained so far.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Janwattanapong_P/0/1/0/all/0/1&quot;&gt;Panuwat Janwattanapong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adjouadi_M/0/1/0/all/0/1&quot;&gt;Malek Adjouadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00558">
<title>Where Classification Fails, Interpretation Rises. (arXiv:1712.00558v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00558</link>
<description rdf:parseType="Literal">&lt;p&gt;An intriguing property of deep neural networks is their inherent
vulnerability to adversarial inputs, which significantly hinders their
application in security-critical domains. Most existing detection methods
attempt to use carefully engineered patterns to distinguish adversarial inputs
from their genuine counterparts, which however can often be circumvented by
adaptive adversaries. In this work, we take a completely different route by
leveraging the definition of adversarial inputs: while deceiving for deep
neural networks, they are barely discernible for human visions. Building upon
recent advances in interpretable models, we construct a new detection framework
that contrasts an input&apos;s interpretation against its classification. We
validate the efficacy of this framework through extensive experiments using
benchmark datasets and attacks. We believe that this work opens a new direction
for designing adversarial input detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Chanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1&quot;&gt;Georgi Georgiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yujie Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00559">
<title>Progressive Neural Architecture Search. (arXiv:1712.00559v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.00559</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for learning CNN structures that is more efficient than
previous approaches: instead of using reinforcement learning (RL) or genetic
algorithms (GA), we use a sequential model-based optimization (SMBO) strategy,
in which we search for architectures in order of increasing complexity, while
simultaneously learning a surrogate function to guide the search, similar to A*
search. On the CIFAR-10 dataset, our method finds a CNN structure with the same
classification accuracy (3.41% error rate) as the RL method of Zoph et al.
(2017), but 2 times faster (in terms of number of models evaluated). It also
outperforms the GA method of Liu et al. (2017), which finds a model with worse
performance (3.63% error rate), and takes 5 times longer. Finally we show that
the model we learned on CIFAR also works well at the task of ImageNet
classification. In particular, we match the state-of-the-art performance of
82.9% top-1 and 96.1% top-5 accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1&quot;&gt;Barret Zoph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1&quot;&gt;Jonathon Shlens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wei Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jonathan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kevin Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00563">
<title>Anesthesiologist-level forecasting of hypoxemia with only SpO2 data using deep learning. (arXiv:1712.00563v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00563</link>
<description rdf:parseType="Literal">&lt;p&gt;We use a deep learning model trained only on a patient&apos;s blood oxygenation
data (measurable with an inexpensive fingertip sensor) to predict impending
hypoxemia (low blood oxygen) more accurately than trained anesthesiologists
with access to all the data recorded in a modern operating room. We also
provide a simple way to visualize the reason why a patient&apos;s risk is low or
high by assigning weight to the patient&apos;s past blood oxygen values. This work
has the potential to provide cutting-edge clinical decision support in
low-resource settings, where rates of surgical complication and death are
substantially greater than in high-resource areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erion_G/0/1/0/all/0/1&quot;&gt;Gabriel Erion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hugh Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1&quot;&gt;Scott M. Lundberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Su-In Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00573">
<title>Supervised Hashing based on Energy Minimization. (arXiv:1712.00573v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00573</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, supervised hashing methods have attracted much attention since they
can optimize retrieval speed and storage cost while preserving semantic
information. Because hashing codes learning is NP-hard, many methods resort to
some form of relaxation technique. But the performance of these methods can
easily deteriorate due to the relaxation. Luckily, many supervised hashing
formulations can be viewed as energy functions, hence solving hashing codes is
equivalent to learning marginals in the corresponding conditional random field
(CRF). By minimizing the KL divergence between a fully factorized distribution
and the Gibbs distribution of this CRF, a set of consistency equations can be
obtained, but updating them in parallel may not yield a local optimum since the
variational lower bound is not guaranteed to increase. In this paper, we use a
linear approximation of the sigmoid function to convert these consistency
equations to linear systems, which have a closed-form solution. By applying
this novel technique to two classical hashing formulations KSH and SPLH, we
obtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH.
Experimental results on three datasets show the superiority of our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zihao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiyi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongtao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00644">
<title>Short-term Mortality Prediction for Elderly Patients Using Medicare Claims Data. (arXiv:1712.00644v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00644</link>
<description rdf:parseType="Literal">&lt;p&gt;Risk prediction is central to both clinical medicine and public health. While
many machine learning models have been developed to predict mortality, they are
rarely applied in the clinical literature, where classification tasks typically
rely on logistic regression. One reason for this is that existing machine
learning models often seek to optimize predictions by incorporating features
that are not present in the databases readily available to providers and policy
makers, limiting generalizability and implementation. Here we tested a number
of machine learning classifiers for prediction of six-month mortality in a
population of elderly Medicare beneficiaries, using an administrative claims
database of the kind available to the majority of health care payers and
providers. We show that machine learning classifiers substantially outperform
current widely-used methods of risk prediction but only when used with an
improved feature set incorporating insights from clinical medicine, developed
for this study. Our work has applications to supporting patient and provider
decision making at the end of life, as well as population health-oriented
efforts to identify patients at high risk of poor outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Makar_M/0/1/0/all/0/1&quot;&gt;Maggie Makar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cutler_D/0/1/0/all/0/1&quot;&gt;David Cutler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Obermeyer_Z/0/1/0/all/0/1&quot;&gt;Ziad Obermeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00673">
<title>Towards Robust Neural Networks via Random Self-ensemble. (arXiv:1712.00673v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00673</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have revealed the vulnerability of deep neural networks - A
small adversarial perturbation that is imperceptible to human can easily make a
well-trained deep neural network mis-classify. This makes it unsafe to apply
neural networks in security-critical applications. In this paper, we propose a
new defensive algorithm called Random Self-Ensemble (RSE) by combining two
important concepts: ${\bf randomness}$ and ${\bf ensemble}$. To protect a
targeted model, RSE adds random noise layers to the neural network to prevent
from state-of-the-art gradient-based attacks, and ensembles the prediction over
random noises to stabilize the performance. We show that our algorithm is
equivalent to ensemble an infinite number of noisy models $f_\epsilon$ without
any additional memory overhead, and the proposed training procedure based on
noisy stochastic gradient descent can ensure the ensemble model has good
predictive capability. Our algorithm significantly outperforms previous defense
techniques on real datasets. For instance, on CIFAR-10 with VGG network (which
has $92\%$ accuracy without any attack), under the state-of-the-art C&amp;amp;W attack
within a certain distortion tolerance, the accuracy of unprotected model drops
to less than $10\%$, the best previous defense technique has $48\%$ accuracy,
while our method still has $86\%$ prediction accuracy under the same level of
attack. Finally, our method is simple and easy to integrate into any neural
network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuanqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Minhao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00679">
<title>GANGs: Generative Adversarial Network Games. (arXiv:1712.00679v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00679</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GAN) have become one of the most successful
frameworks for unsupervised generative modeling. As GANs are difficult to train
much research has focused on this. However, very little of this research has
directly exploited game-theoretic techniques. We introduce Generative
Adversarial Network Games (GANGs), which explicitly model a finite zero-sum
game between a generator ($G$) and classifier ($C$) that use mixed strategies.
The size of these games precludes exact solution methods, therefore we define
resource-bounded best responses (RBBRs), and a resource-bounded Nash
Equilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$
can find a better RBBR. The RB-NE solution concept is richer than the notion of
`local Nash equilibria&apos; in that it captures not only failures of escaping local
optima of gradient descent, but applies to any approximate best response
computations, including methods with random restarts. To validate our approach,
we solve GANGs with the Parallel Nash Memory algorithm, which provably
monotonically converges to an RB-NE. We compare our results to standard GAN
setups, and demonstrate that our method deals well with typical GAN problems
such as mode collapse, partial mode coverage and forgetting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliehoek_F/0/1/0/all/0/1&quot;&gt;Frans A. Oliehoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Savani_R/0/1/0/all/0/1&quot;&gt;Rahul Savani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gallego_Posada_J/0/1/0/all/0/1&quot;&gt;Jose Gallego-Posada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pol_E/0/1/0/all/0/1&quot;&gt;Elise van der Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jong_E/0/1/0/all/0/1&quot;&gt;Edwin D. de Jong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gross_R/0/1/0/all/0/1&quot;&gt;Roderich Gross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00699">
<title>Improving Network Robustness against Adversarial Attacks with Compact Convolution. (arXiv:1712.00699v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00699</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Convolutional Neural Networks (CNNs) have surpassed human-level
performance on tasks such as object classification and face verification, they
can easily be fooled by adversarial attacks. These attacks add a small
perturbation to the input image that causes the network to mis-classify the
sample. In this paper, we focus on neutralizing adversarial attacks by
exploring the effect of different loss functions such as CenterLoss and
L2-Softmax Loss for enhanced robustness to adversarial perturbations.
Additionally, we propose power convolution, a novel method of convolution that
when incorporated in conventional CNNs improve their robustness. Power
convolution ensures that features at every layer are bounded and close to each
other. Extensive experiments show that Power Convolutional Networks (PCNs)
neutralize multiple types of attacks, and perform better than existing methods
for defending adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1&quot;&gt;Rajeev Ranjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayanan_S/0/1/0/all/0/1&quot;&gt;Swami Sankaranarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1&quot;&gt;Carlos D. Castillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00716">
<title>Convolutional Phase Retrieval via Gradient Descent. (arXiv:1712.00716v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1712.00716</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the convolutional phase retrieval problem, which considers
recovering an unknown signal $\mathbf x \in \mathbb C^n $ from $m$ measurements
consisting of the magnitude of its cyclic convolution with a known kernel
$\mathbf a \in \mathbb C^m $. This model is motivated by applications such as
channel estimation, optics, and underwater acoustic communication, where the
signal of interest is acted on by a given channel/filter, and phase information
is difficult or impossible to acquire. We show that when $\mathbf a$ is random
and the sample number $m$ is sufficiently large, with high probability $\mathbf
x$ can be efficiently recovered up to a global phase using a combination of
spectral initialization and generalized gradient descent. The main challenge is
coping with dependencies in the measurement operator. We overcome this
challenge by using ideas from decoupling theory, suprema of chaos processes and
the restricted isometry property of random circulant matrices, and recent
analysis for alternating minimization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qu_Q/0/1/0/all/0/1&quot;&gt;Qing Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuqian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wright_J/0/1/0/all/0/1&quot;&gt;John Wright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00731">
<title>Joint Topic-Semantic-aware Social Recommendation for Online Voting. (arXiv:1712.00731v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00731</link>
<description rdf:parseType="Literal">&lt;p&gt;Online voting is an emerging feature in social networks, in which users can
express their attitudes toward various issues and show their unique interest.
Online voting imposes new challenges on recommendation, because the propagation
of votings heavily depends on the structure of social networks as well as the
content of votings. In this paper, we investigate how to utilize these two
factors in a comprehensive manner when doing voting recommendation. First, due
to the fact that existing text mining methods such as topic model and semantic
model cannot well process the content of votings that is typically short and
ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to
learn word and document representation by jointly considering their topics and
semantics. Then we propose our Joint Topic-Semantic-aware social Matrix
Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates
similarity among users and votings by combining their TEWE representation and
structural information of social networks, and preserves this
topic-semantic-social similarity during matrix factorization. To evaluate the
performance of TEWE representation and JTS-MF model, we conduct extensive
experiments on real online voting dataset. The results prove the efficacy of
our approach against several state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Miao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiannong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00732">
<title>SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction. (arXiv:1712.00732v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00732</link>
<description rdf:parseType="Literal">&lt;p&gt;In online social networks people often express attitudes towards others,
which forms massive sentiment links among users. Predicting the sign of
sentiment links is a fundamental task in many areas such as personal
advertising and public opinion analysis. Previous works mainly focus on textual
sentiment classification, however, text information can only disclose the &quot;tip
of the iceberg&quot; about users&apos; true opinions, of which the most are unobserved
but implied by other sources of information such as social relation and users&apos;
profile. To address this problem, in this paper we investigate how to predict
possibly existing sentiment links in the presence of heterogeneous information.
First, due to the lack of explicit sentiment links in mainstream social
networks, we establish a labeled heterogeneous sentiment dataset which consists
of users&apos; sentiment relation, social relation and profile knowledge by
entity-level sentiment extraction method. Then we propose a novel and flexible
end-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework
to extract users&apos; latent representations from heterogeneous networks and
predict the sign of unobserved sentiment links. SHINE utilizes multiple deep
autoencoders to map each user into a low-dimension feature space while
preserving the network structure. We demonstrate the superiority of SHINE over
state-of-the-art baselines on link prediction and node recommendation in two
real-world datasets. The experimental results also prove the efficacy of SHINE
in cold start scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_M/0/1/0/all/0/1&quot;&gt;Min Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00828">
<title>Tensor Train Neighborhood Preserving Embedding. (arXiv:1712.00828v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00828</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a Tensor Train Neighborhood Preserving Embedding
(TTNPE) to embed multi-dimensional tensor data into low dimensional tensor
subspace. Novel approaches to solve the optimization problem in TTNPE are
proposed. For this embedding, we evaluate novel trade-off gain among
classification, computation, and dimensionality reduction (storage) for
supervised learning. It is shown that compared to the state-of-the-arts tensor
embedding methods, TTNPE achieves superior trade-off in classification,
computation, and dimensionality reduction in MNIST handwritten digits and
Weizmann face datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1&quot;&gt;Shuchin Aeron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00867">
<title>Gaussian Process Regression for Arctic Coastal Erosion Forecasting. (arXiv:1712.00867v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/1712.00867</link>
<description rdf:parseType="Literal">&lt;p&gt;Arctic coastal morphology is governed by multiple factors, many of which are
affected by climatological changes. As the season length for shorefast ice
decreases and temperatures warm permafrost soils, coastlines are more
susceptible to erosion from storm waves. Such coastal erosion is a concern,
since the majority of the population centers and infrastructure in the Arctic
are located near the coasts. Stakeholders and decision makers increasingly need
models capable of scenario-based predictions to assess and mitigate the effects
of coastal morphology on infrastructure and land use. Our research uses
Gaussian process models to forecast Arctic coastal erosion along the Beaufort
Sea near Drew Point, AK. Gaussian process regression is a data-driven modeling
methodology capable of extracting patterns and trends from data-sparse
environments such as remote Arctic coastlines. To train our model, we use
annual coastline positions and near-shore summer temperature averages from
existing datasets and extend these data by extracting additional coastlines
from satellite imagery. We combine our calibrated models with future climate
models to generate a range of plausible future erosion scenarios. Our results
show that the Gaussian process methodology substantially improves yearly
predictions compared to linear and nonlinear least squares methods, and is
capable of generating detailed forecasts suitable for use by decision makers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kupilik_M/0/1/0/all/0/1&quot;&gt;Matthew Kupilik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Witmer_F/0/1/0/all/0/1&quot;&gt;Frank Witmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+MacLeod_E/0/1/0/all/0/1&quot;&gt;Euan-Angus MacLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Caixia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ravens_T/0/1/0/all/0/1&quot;&gt;Tom Ravens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00891">
<title>Data Dropout in Arbitrary Basis for Deep Network Regularization. (arXiv:1712.00891v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.00891</link>
<description rdf:parseType="Literal">&lt;p&gt;An important problem in training deep networks with high capacity is to
ensure that the trained network works well with new inputs. Dropout is an
effective regularization technique to boost the network generalization in which
a random subset of the elements of given data and the extracted features are
set to zero during the training process. In this paper, a new randomized
regularization technique is proposed in which we withhold a random part of data
without necessarily turning off the neurons/data-elements. In the proposed
method, of which the conventional dropout is shown to be a special case, random
data dropout is performed in an arbitrary basis. In addition, we present a
framework to efficiently apply the proposed technique to the convolutional
neural networks. The presented numerical experiments show that the proposed
technique yields notable performance gain. The proposed approach, dubbed
Generalized Dropout, provides a deep insight into the idea of dropout, shows
that we can achieve different performance gains using different basis matrices,
and opens up a new research question as of how to choose optimal basis matrices
that achieve maximal performance gain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_M/0/1/0/all/0/1&quot;&gt;Mostafa Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1&quot;&gt;George Atia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00961">
<title>Learning Independent Causal Mechanisms. (arXiv:1712.00961v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Independent causal mechanisms are a central concept in the study of causality
with implications for machine learning tasks. In this work we develop an
algorithm to recover a set of (inverse) independent mechanisms relating a
distribution transformed by the mechanisms to a reference distribution. The
approach is fully unsupervised and based on a set of experts that compete for
data to specialize and extract the mechanisms. We test and analyze the proposed
method on a series of experiments based on image transformations. Each expert
successfully maps a subset of the transformed data to the original domain, and
the learned mechanisms generalize to other domains. We discuss implications for
domain transfer and links to recent trends in generative modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_Carulla_M/0/1/0/all/0/1&quot;&gt;Mateo Rojas-Carulla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1&quot;&gt;Niki Kilbertus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00996">
<title>Learning to detect chest radiographs containing lung nodules using visual attention networks. (arXiv:1712.00996v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.00996</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning approaches hold great potential for the automated detection
of lung nodules in chest radiographs, but training the algorithms requires vary
large amounts of manually annotated images, which are difficult to obtain. Weak
labels indicating whether a radiograph is likely to contain pulmonary nodules
are typically easier to obtain at scale by parsing historical free-text
radiological reports associated to the radiographs. Using a repositotory of
over 700,000 chest radiographs, in this study we demonstrate that promising
nodule detection performance can be achieved using weak labels through
convolutional neural networks for radiograph classification. We propose two
network architectures for the classification of images likely to contain
pulmonary nodules using both weak labels and manually-delineated bounding
boxes, when these are available. Annotated nodules are used at training time to
deliver a visual attention mechanism informing the model about its localisation
performance. The first architecture extracts saliency maps from high-level
convolutional layers and compares the estimated position of a nodule against
the ground truth, when this is available. A corresponding localisation error is
then back-propagated along with the softmax classification error. The second
approach consists of a recurrent attention model that learns to observe a short
sequence of smaller image portions through reinforcement learning. When a
nodule annotation is available at training time, the reward function is
modified accordingly so that exploring portions of the radiographs away from a
nodule incurs a larger penalty. Our empirical results demonstrate the potential
advantages of these architectures in comparison to competing methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pesce_E/0/1/0/all/0/1&quot;&gt;Emanuele Pesce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ypsilantis_P/0/1/0/all/0/1&quot;&gt;Petros-Pavlos Ypsilantis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Withey_S/0/1/0/all/0/1&quot;&gt;Samuel Withey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bakewell_R/0/1/0/all/0/1&quot;&gt;Robert Bakewell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_V/0/1/0/all/0/1&quot;&gt;Vicky Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montana_G/0/1/0/all/0/1&quot;&gt;Giovanni Montana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01033">
<title>NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization. (arXiv:1712.01033v1 [math.OC])</title>
<link>http://arxiv.org/abs/1712.01033</link>
<description rdf:parseType="Literal">&lt;p&gt;Accelerated gradient (AG) methods are breakthroughs in convex optimization,
improving the convergence rate of the gradient descent method for optimization
with smooth functions. However, the analysis of AG methods for non-convex
optimization is still limited. It remains an open question whether AG methods
from convex optimization can accelerate the convergence of the gradient descent
method for finding local minimum of non-convex optimization problems. This
paper provides an affirmative answer to this question. In particular, we
analyze two renowned variants of AG methods (namely Polyak&apos;s Heavy Ball method
and Nesterov&apos;s Accelerated Gradient method) for extracting the negative
curvature from random noise, which is central to escaping from saddle points.
By leveraging the proposed AG methods for extracting the negative curvature, we
present a new AG algorithm with double loops for non-convex
optimization~\footnote{this is in contrast to a single-loop AG algorithm
proposed in a recent manuscript~\citep{AGNON}, which directly analyzed the
Nesterov&apos;s AG method for non-convex optimization and appeared online on
November 29, 2017. However, we emphasize that our work is an independent work,
which is inspired by our earlier work~\citep{NEON17} and is based on a
different novel analysis.}, which converges to second-order stationary point
$\x$ such that $\|\nabla f(\x)\|\leq \epsilon$ and $\nabla^2 f(\x)\geq
-\sqrt{\epsilon} I$ with $\widetilde O(1/\epsilon^{1.75})$ iteration
complexity, improving that of gradient descent method by a factor of
$\epsilon^{-0.25}$ and matching the best iteration complexity of second-order
Hessian-free methods for non-convex optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Rong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01038">
<title>Vprop: Variational Inference using RMSprop. (arXiv:1712.01038v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.01038</link>
<description rdf:parseType="Literal">&lt;p&gt;Many computationally-efficient methods for Bayesian deep learning rely on
continuous optimization algorithms, but the implementation of these methods
requires significant changes to existing code-bases. In this paper, we propose
Vprop, a method for Gaussian variational inference that can be implemented with
two minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces
the memory requirements of Black-Box Variational Inference by half. We derive
Vprop using the conjugate-computation variational inference method, and
establish its connections to Newton&apos;s method, natural-gradient methods, and
extended Kalman filters. Overall, this paper presents Vprop as a principled,
computationally-efficient, and easy-to-implement method for Bayesian deep
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tangkaratt_V/0/1/0/all/0/1&quot;&gt;Voot Tangkaratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yarin Gal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01048">
<title>Adaptive Quantization for Deep Neural Network. (arXiv:1712.01048v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.01048</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years Deep Neural Networks (DNNs) have been rapidly developed in
various applications, together with increasingly complex architectures. The
performance gain of these DNNs generally comes with high computational costs
and large memory consumption, which may not be affordable for mobile platforms.
Deep model quantization can be used for reducing the computation and memory
costs of DNNs, and deploying complex DNNs on mobile equipment. In this work, we
propose an optimization framework for deep model quantization. First, we
propose a measurement to estimate the effect of parameter quantization errors
in individual layers on the overall model prediction accuracy. Then, we propose
an optimization process based on this measurement for finding optimal
quantization bit-width for each layer. This is the first work that
theoretically analyse the relationship between parameter quantization errors of
individual layers and model accuracy. Our new quantization algorithm
outperforms previous quantization optimization methods, and achieves 20-40%
higher compression rate compared to equal bit-width quantization at the same
model prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1&quot;&gt;Seyed-Mohsen Moosavi-Dezfooli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1&quot;&gt;Pascal Frossard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01081">
<title>Determinants of Mobile Money Adoption in Pakistan. (arXiv:1712.01081v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.01081</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we analyze the problem of adoption of mobile money in Pakistan
by using the call detail records of a major telecom company as our input. Our
results highlight the fact that different sections of the society have
different patterns of adoption of digital financial services but user mobility
related features are the most important one when it comes to adopting and using
mobile money services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Raza Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blumenstock_J/0/1/0/all/0/1&quot;&gt;Joshua Blumenstock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01137">
<title>Inferring agent objectives at different scales of a complex adaptive system. (arXiv:1712.01137v1 [q-fin.TR])</title>
<link>http://arxiv.org/abs/1712.01137</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a framework to study the effective objectives at different time
scales of financial market microstructure. The financial market can be regarded
as a complex adaptive system, where purposeful agents collectively and
simultaneously create and perceive their environment as they interact with it.
It has been suggested that multiple agent classes operate in this system, with
a non-trivial hierarchy of top-down and bottom-up causation classes with
different effective models governing each level. We conjecture that agent
classes may in fact operate at different time scales and thus act differently
in response to the same perceived market state. Given scale-specific temporal
state trajectories and action sequences estimated from aggregate market
behaviour, we use Inverse Reinforcement Learning to compute the effective
reward function for the aggregate agent class at each scale, allowing us to
assess the relative attractiveness of feature vectors across different scales.
Differences in reward functions for feature vectors may indicate different
objectives of market participants, which could assist in finding the scale
boundary for agent classes. This has implications for learning algorithms
operating in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Hendricks_D/0/1/0/all/0/1&quot;&gt;Dieter Hendricks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Cobb_A/0/1/0/all/0/1&quot;&gt;Adam Cobb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Everett_R/0/1/0/all/0/1&quot;&gt;Richard Everett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Downing_J/0/1/0/all/0/1&quot;&gt;Jonathan Downing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen J. Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01141">
<title>Stochastic Maximum Likelihood Optimization via Hypernetworks. (arXiv:1712.01141v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.01141</link>
<description rdf:parseType="Literal">&lt;p&gt;This work explores maximum likelihood optimization of neural networks through
hypernetworks. A hypernetwork initializes the weights of another network, which
in turn can be employed for typical functional tasks such as regression and
classification. We optimize hypernetworks to directly maximize the conditional
likelihood of target variables given input. Using this approach we obtain
competitive empirical results on regression and classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheikh_A/0/1/0/all/0/1&quot;&gt;Abdul-Saboor Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rasul_K/0/1/0/all/0/1&quot;&gt;Kashif Rasul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Merentitis_A/0/1/0/all/0/1&quot;&gt;Andreas Merentitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bergmann_U/0/1/0/all/0/1&quot;&gt;Urs Bergmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01158">
<title>Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening. (arXiv:1712.01158v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.01158</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of statistical inference for ranking data,
specifically rank aggregation, under the assumption that samples are incomplete
in the sense of not comprising all choice alternatives. In contrast to most
existing methods, we explicitly model the process of turning a full ranking
into an incomplete one, which we call the coarsening process. To this end, we
propose the concept of rank-dependent coarsening, which assumes that incomplete
rankings are produced by projecting a full ranking to a random subset of ranks.
For a concrete instantiation of our model, in which full rankings are drawn
from a Plackett-Luce distribution and observations take the form of pairwise
preferences, we study the performance of various rank aggregation methods. In
addition to predictive accuracy in the finite sample setting, we address the
theoretical question of consistency, by which we mean the ability to recover a
target ranking when the sample size goes to infinity, despite a potential bias
in the observations caused by the (unknown) coarsening.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fahandar_M/0/1/0/all/0/1&quot;&gt;Mohsen Ahmadi Fahandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Couso_I/0/1/0/all/0/1&quot;&gt;In&amp;#xe9;s Couso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01169">
<title>Episodic memory for continual model learning. (arXiv:1712.01169v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.01169</link>
<description rdf:parseType="Literal">&lt;p&gt;Both the human brain and artificial learning agents operating in real-world
or comparably complex environments are faced with the challenge of online model
selection. In principle this challenge can be overcome: hierarchical Bayesian
inference provides a principled method for model selection and it converges on
the same posterior for both off-line (i.e. batch) and online learning. However,
maintaining a parameter posterior for each model in parallel has in general an
even higher memory cost than storing the entire data set and is consequently
clearly unfeasible. Alternatively, maintaining only a limited set of models in
memory could limit memory requirements. However, sufficient statistics for one
model will usually be insufficient for fitting a different kind of model,
meaning that the agent loses information with each model change. We propose
that episodic memory can circumvent the challenge of limited memory-capacity
online model selection by retaining a selected subset of data points. We design
a method to compute the quantities necessary for model selection even when the
data is discarded and only statistics of one (or few) learnt models are
available. We demonstrate on a simple model that a limited-sized episodic
memory buffer, when the content is optimised to retain data with statistics not
matching the current representation, can resolve the fundamental challenge of
online model selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagy_D/0/1/0/all/0/1&quot;&gt;David G. Nagy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orban_G/0/1/0/all/0/1&quot;&gt;Gerg&amp;#x151; Orb&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01193">
<title>A Dual Framework for Low-rank Tensor Completion. (arXiv:1712.01193v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.01193</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel formulation of the low-rank tensor completion problem that
is based on the duality theory and a particular choice of low-rank regularizer.
This low-rank regularizer along with the dual perspective provides a simple
characterization of the solution to the tensor completion problem. Motivated by
large-scale setting, we next derive a rank-constrained reformulation of the
proposed optimization problem, which is shown to lie on the Riemannian
spectrahedron manifold. We exploit the versatile Riemannian optimization
framework to develop computationally efficient conjugate gradient and
trust-region algorithms. The experiments confirm the benefits of our choice of
regularization and the proposed algorithms outperform state-of-the-art
algorithms on several real-world data sets in different applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nimishakavi_M/0/1/0/all/0/1&quot;&gt;Madhav Nimishakavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1&quot;&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1508.01717">
<title>Distributional Equivalence and Structure Learning for Bow-free Acyclic Path Diagrams. (arXiv:1508.01717v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1508.01717</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of structure learning for bow-free acyclic path
diagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG
models that allow for certain hidden variables. We present a first method for
this problem using a greedy score-based search algorithm. We also prove some
necessary and some sufficient conditions for distributional equivalence of BAPs
which are used in an algorithmic ap- proach to compute (nearly) equivalent
model structures. This allows us to infer lower bounds of causal effects. We
also present applications to real and simulated datasets using our publicly
available R-package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nowzohour_C/0/1/0/all/0/1&quot;&gt;Christopher Nowzohour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maathuis_M/0/1/0/all/0/1&quot;&gt;Marloes H. Maathuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Evans_R/0/1/0/all/0/1&quot;&gt;Robin J. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buhlmann_P/0/1/0/all/0/1&quot;&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.01171">
<title>Sparse Recovery Guarantees from Extreme Eigenvalues Small Deviations. (arXiv:1604.01171v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1604.01171</link>
<description rdf:parseType="Literal">&lt;p&gt;This article provides a new toolbox to derive sparse recovery guarantees from
small deviations on extreme singular values or extreme eigenvalues obtained in
Random Matrix Theory. This work is based on Restricted Isometry Constants
(RICs) which are a pivotal notion in Compressed Sensing and High-Dimensional
Statistics as these constants finely assess how a linear operator is
conditioned on the set of sparse vectors and hence how it performs in SRSR.
While it is an open problem to construct deterministic matrices with apposite
RICs, one can prove that such matrices exist using random matrices models. In
this paper, we show upper bounds on RICs for Gaussian and Rademacher matrices
using state-of-the-art small deviation estimates on their extreme eigenvalues.
This allows us to derive a lower bound on the probability of getting SRSR. One
benefit of this paper is a direct and explicit derivation of upper bounds on
RICs and lower bounds on SRSR from small deviations on the extreme eigenvalues
given by Random Matrix theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dallaporta_S/0/1/0/all/0/1&quot;&gt;Sandrine Dallaporta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Castro_Y/0/1/0/all/0/1&quot;&gt;Yohann De Castro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.06764">
<title>Saturating Splines and Feature Selection. (arXiv:1609.06764v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.06764</link>
<description rdf:parseType="Literal">&lt;p&gt;We extend the adaptive regression spline model by incorporating saturation,
the natural requirement that a function extend as a constant outside a certain
range. We fit saturating splines to data using a convex optimization problem
over a space of measures, which we solve using an efficient algorithm based on
the conditional gradient method. Unlike many existing approaches, our algorithm
solves the original infinite-dimensional (for splines of degree at least two)
optimization problem without pre-specified knot locations. We then adapt our
algorithm to fit generalized additive models with saturating splines as
coordinate functions and show that the saturation requirement allows our model
to simultaneously perform feature selection and nonlinear function fitting.
Finally, we briefly sketch how the method can be extended to higher order
splines and to different requirements on the extension outside the data range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boyd_N/0/1/0/all/0/1&quot;&gt;Nicholas Boyd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hastie_T/0/1/0/all/0/1&quot;&gt;Trevor Hastie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boyd_S/0/1/0/all/0/1&quot;&gt;Stephen Boyd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Recht_B/0/1/0/all/0/1&quot;&gt;Benjamin Recht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.10277">
<title>Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge. (arXiv:1611.10277v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1611.10277</link>
<description rdf:parseType="Literal">&lt;p&gt;While generative models such as Latent Dirichlet Allocation (LDA) have proven
fruitful in topic modeling, they often require detailed assumptions and careful
specification of hyperparameters. Such model complexity issues only compound
when trying to generalize generative models to incorporate human input. We
introduce Correlation Explanation (CorEx), an alternative approach to topic
modeling that does not assume an underlying generative model, and instead
learns maximally informative topics through an information-theoretic framework.
This framework naturally generalizes to hierarchical and semi-supervised
extensions with no additional modeling assumptions. In particular, word-level
domain knowledge can be flexibly incorporated within CorEx through anchor
words, allowing topic separability and representation to be promoted with
minimal human intervention. Across a variety of datasets, metrics, and
experiments, we demonstrate that CorEx produces topics that are comparable in
quality to those produced by unsupervised and semi-supervised variants of LDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallagher_R/0/1/0/all/0/1&quot;&gt;Ryan J. Gallagher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reing_K/0/1/0/all/0/1&quot;&gt;Kyle Reing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_D/0/1/0/all/0/1&quot;&gt;David Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.06084">
<title>Linear-Complexity Exponentially-Consistent Tests for Universal Outlying Sequence Detection. (arXiv:1701.06084v4 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1701.06084</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of universal outlying sequence detection is studied, where the
goal is to detect outlying sequences among $M$ sequences of samples. A sequence
is considered as outlying if the observations therein are generated by a
distribution different from those generating the observations in the majority
of the sequences. In the universal setting, we are interested in identifying
all the outlying sequences without knowing the underlying generating
distributions. In this paper, a class of tests based on distribution clustering
is proposed. These tests are shown to be exponentially consistent with linear
time complexity in $M$. Numerical results demonstrate that our clustering-based
tests achieve similar performance to existing tests, while being considerably
more computationally efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Y/0/1/0/all/0/1&quot;&gt;Yuheng Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1&quot;&gt;Shaofeng Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeravalli_V/0/1/0/all/0/1&quot;&gt;Venugopal V. Veeravalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.03971">
<title>On the Effects of Batch and Weight Normalization in Generative Adversarial Networks. (arXiv:1704.03971v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.03971</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) are highly effective unsupervised
learning frameworks that can generate very sharp data, even for data such as
images with complex, highly multimodal distributions. However GANs are known to
be very hard to train, suffering from problems such as mode collapse and
disturbing visual artifacts. Batch normalization (BN) techniques have been
introduced to address the training. Though BN accelerates the training in the
beginning, our experiments show that the use of BN can be unstable and
negatively impact the quality of the trained model. The evaluation of BN and
numerous other recent schemes for improving GAN training is hindered by the
lack of an effective objective quality measure for GAN models. To address these
issues, we first introduce a weight normalization (WN) approach for GAN
training that significantly improves the stability, efficiency and the quality
of the generated samples. To allow a methodical evaluation, we introduce
squared Euclidean reconstruction error on a test set as a new objective
measure, to assess training performance in terms of speed, stability, and
quality of generated samples. Our experiments with a standard DCGAN
architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10)
indicate that training using WN is generally superior to BN for GANs, achieving
10% lower mean squared loss for reconstruction and significantly better
qualitative results than BN. We further demonstrate the stability of WN on a
21-layer ResNet trained with the CelebA data set. The code for this paper is
available at https://github.com/stormraiser/gan-weightnorm-resnet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Sitao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.06176">
<title>Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks. (arXiv:1704.06176v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1704.06176</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) has been proposed as a complimentary method
to measure bone quality and assess fracture risk. However, manual segmentation
of MR images of bone is time-consuming, limiting the use of MRI measurements in
the clinical practice. The purpose of this paper is to present an automatic
proximal femur segmentation method that is based on deep convolutional neural
networks (CNNs). This study had institutional review board approval and written
informed consent was obtained from all subjects. A dataset of volumetric
structural MR images of the proximal femur from 86 subject were
manually-segmented by an expert. We performed experiments by training two
different CNN architectures with multiple number of initial feature maps and
layers, and tested their segmentation performance against the gold standard of
manual segmentations using four-fold cross-validation. Automatic segmentation
of the proximal femur achieved a high dice similarity score of 0.94$\pm$0.05
with precision = 0.95$\pm$0.02, and recall = 0.94$\pm$0.08 using a CNN
architecture based on 3D convolution exceeding the performance of 2D CNNs. The
high segmentation accuracy provided by CNNs has the potential to help bring the
use of structural MRI measurements of bone quality into clinical practice for
management of osteoporosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deniz_C/0/1/0/all/0/1&quot;&gt;Cem M. Deniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Siyuan Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallyburton_S/0/1/0/all/0/1&quot;&gt;Spencer Hallyburton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welbeck_A/0/1/0/all/0/1&quot;&gt;Arakua Welbeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honig_S/0/1/0/all/0/1&quot;&gt;Stephen Honig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1&quot;&gt;Gregory Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02633">
<title>Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs. (arXiv:1706.02633v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02633</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have shown remarkable success as a
framework for training models to produce realistic-looking data. In this work,
we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to
produce realistic real-valued multi-dimensional time series, with an emphasis
on their application to medical data. RGANs make use of recurrent neural
networks in the generator and the discriminator. In the case of RCGANs, both of
these RNNs are conditioned on auxiliary information. We demonstrate our models
in a set of toy datasets, where we show visually and quantitatively (using
sample likelihood and maximum mean discrepancy) that they can successfully
generate realistic time-series. We also describe novel evaluation methods for
GANs, where we generate a synthetic labelled training dataset, and evaluate on
a real test set the performance of a model trained on the synthetic data, and
vice-versa. We illustrate with these metrics that RCGANs can generate
time-series data useful for supervised training, with only minor degradation in
performance on real test data. This is demonstrated on digit classification
from &apos;serialised&apos; MNIST and by training an early warning system on a medical
dataset of 17,000 patients from an intensive care unit. We further discuss and
analyse the privacy concerns that may arise when using RCGANs to generate
realistic synthetic medical time series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Esteban_C/0/1/0/all/0/1&quot;&gt;Crist&amp;#xf3;bal Esteban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hyland_S/0/1/0/all/0/1&quot;&gt;Stephanie L. Hyland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01461">
<title>Labeled Memory Networks for Online Model Adaptation. (arXiv:1707.01461v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01461</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmenting a neural network with memory that can grow without growing the
number of trained parameters is a recent powerful concept with many exciting
applications. We propose a design of memory augmented neural networks (MANNs)
called Labeled Memory Networks (LMNs) suited for tasks requiring online
adaptation in classification models. LMNs organize the memory with classes as
the primary key.The memory acts as a second boosted stage following a regular
neural network thereby allowing the memory and the primary network to play
complementary roles. Unlike existing MANNs that write to memory for every
instance and use LRU based memory replacement, LMNs write only for instances
with non-zero loss and use label-based memory replacement. We demonstrate
significant accuracy gains on various tasks including word-modelling and
few-shot learning. In this paper, we establish their potential in online
adapting a batch trained neural network to domain-relevant labeled data at
deployment time. We show that LMNs are better than other MANNs designed for
meta-learning. We also found them to be more accurate and faster than
state-of-the-art methods of retuning model parameters for adapting to
domain-specific labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_S/0/1/0/all/0/1&quot;&gt;Shiv Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1&quot;&gt;Sunita Sarawagi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05917">
<title>Accelerating Kernel Classifiers Through Borders Mapping. (arXiv:1708.05917v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05917</link>
<description rdf:parseType="Literal">&lt;p&gt;Support vector machines (SVM) and other kernel techniques represent a family
of powerful statistical classification methods with high accuracy and broad
applicability. Because they use all or a significant portion of the training
data, however, they can be slow, especially for large problems. Piecewise
linear classifiers are similarly versatile, yet have the additional advantages
of simplicity, ease of interpretation and, if the number of component linear
classifiers is not too large, speed. Here we show how a simple, piecewise
linear classifier can be trained from a kernel-based classifier in order to
improve the classification speed. The method works by finding the root of the
difference in conditional probabilities between pairs of opposite classes to
build up a representation of the decision boundary. When tested on 17 different
datasets, it succeeded in improving the classification speed of a SVM for 9 of
them by factors as high as 88 times or more. The method is best suited to
problems with continuum features data and smooth probability functions. Because
the component linear classifiers are built up individually from an existing
classifier, rather than through a simultaneous optimization procedure, the
classifier is also fast to train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mills_P/0/1/0/all/0/1&quot;&gt;Peter Mills&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04212">
<title>Asymptotic Bayesian Generalization Error in a General Stochastic Matrix Factorization for Markov Chain and Bayesian Network. (arXiv:1709.04212v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic matrix factorization (SMF) can be regarded as a restriction of
non-negative matrix factorization (NMF). SMF is useful for inference of topic
models, NMF for binary matrices data, Markov chains, and Bayesian networks.
However, SMF needs strong assumptions to reach a unique factorization and its
theoretical prediction accuracy has not yet been clarified. In this paper, we
study the maximum the pole of zeta function (real log canonical threshold) of a
general SMF and derive an upper bound of the generalization error in Bayesian
inference. The results give a foundation for a widely applicable and rigorous
factorization method of SMF and mean that the generalization error in SMF
becomes smaller than regular statistical models by Bayesian inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hayashi_N/0/1/0/all/0/1&quot;&gt;Naoki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Sumio Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05338">
<title>Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics. (arXiv:1710.05338v7 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05338</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonconvex optimization problems arise in different research fields and arouse
lots of attention in signal processing, statistics and machine learning. In
this work, we explore the accelerated proximal gradient method and some of its
variants which have been shown to converge under nonconvex context recently. We
show that a novel variant proposed here, which exploits adaptive momentum and
block coordinate update with specific update rules, further improves the
performance of a broad class of nonconvex problems. In applications to sparse
linear regression with regularizations like Lasso, grouped Lasso, capped
$\ell_1$ and SCAP, the proposed scheme enjoys provable local linear
convergence, with experimental justification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lau_T/0/1/0/all/0/1&quot;&gt;Tsz Kit Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02213">
<title>Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. (arXiv:1711.02213v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02213</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are commonly developed and trained in 32-bit floating
point format. Significant gains in performance and energy efficiency could be
realized by training and inference in numerical formats optimized for deep
learning. Despite advances in limited precision inference in recent years,
training of neural networks in low bit-width remains a challenging problem.
Here we present the Flexpoint data format, aiming at a complete replacement of
32-bit floating point format training and inference, designed to support modern
deep network topologies without modifications. Flexpoint tensors have a shared
exponent that is dynamically adjusted to minimize overflows and maximize
available dynamic range. We validate Flexpoint by training AlexNet, a deep
residual network and a generative adversarial network, using a simulator
implemented with the neon deep learning framework. We demonstrate that 16-bit
Flexpoint closely matches 32-bit floating point in training all three models,
without any need for tuning of model hyperparameters. Our results suggest
Flexpoint as a promising numerical format for future hardware for training and
inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koster_U/0/1/0/all/0/1&quot;&gt;Urs K&amp;#xf6;ster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_T/0/1/0/all/0/1&quot;&gt;Tristan J. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1&quot;&gt;Marcel Nassar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1&quot;&gt;Arjun K. Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constable_W/0/1/0/all/0/1&quot;&gt;William H. Constable&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1&quot;&gt;O&amp;#x11f;uz H. Elibol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1&quot;&gt;Scott Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1&quot;&gt;Stewart Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hornof_L/0/1/0/all/0/1&quot;&gt;Luke Hornof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khosrowshahi_A/0/1/0/all/0/1&quot;&gt;Amir Khosrowshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloss_C/0/1/0/all/0/1&quot;&gt;Carey Kloss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_R/0/1/0/all/0/1&quot;&gt;Ruby J. Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1&quot;&gt;Naveen Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04992">
<title>Feature importance scores and lossless feature pruning using Banzhaf power indices. (arXiv:1711.04992v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04992</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the influence of features in machine learning is crucial to
interpreting models and selecting the best features for classification. In this
work we propose the use of principles from coalitional game theory to reason
about importance of features. In particular, we propose the use of the Banzhaf
power index as a measure of influence of features on the outcome of a
classifier. We show that features having Banzhaf power index of zero can be
losslessly pruned without damage to classifier accuracy. Computing the power
indices does not require having access to data samples. However, if samples are
available, the indices can be empirically estimated. We compute Banzhaf power
indices for a neural network classifier on real-life data, and compare the
results with gradient-based feature saliency, and coefficients of a logistic
regression model with $L_1$ regularization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kulynych_B/0/1/0/all/0/1&quot;&gt;Bogdan Kulynych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Troncoso_C/0/1/0/all/0/1&quot;&gt;Carmela Troncoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06424">
<title>A Resizable Mini-batch Gradient Descent based on a Randomized Weighted Majority. (arXiv:1711.06424v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06424</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining the appropriate batch size for mini-batch gradient descent is
always time consuming as it often relies on grid search. This paper considers a
resizable mini-batch gradient descent (RMGD) algorithm-inspired by the
randomized weighted majority algorithm-for achieving best performance in grid
search by selecting an appropriate batch size at each epoch with a probability
defined as a function of its previous success/failure and the validation error.
This probability encourages exploration of different batch size and then later
exploitation of batch size with history of success. At each epoch, the RMGD
samples a batch size from its probability distribution, then uses the selected
batch size for mini-batch gradient descent. After obtaining the validation
error at each epoch, the probability distribution is updated to incorporate the
effectiveness of the sampled batch size. The RMGD essentially assists the
learning process to explore the possible domain of the batch size and exploit
successful batch size. Experimental results show that the RMGD achieves
performance better than the best performing single batch size. Furthermore, it
attains this performance in a shorter amount of time than that of the best
performing. It is surprising that the RMGD achieves better performance than
grid search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Seong Jin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Sunghun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08576">
<title>Variational Encoding of Complex Dynamics. (arXiv:1711.08576v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08576</link>
<description rdf:parseType="Literal">&lt;p&gt;Often the analysis of time-dependent chemical and biophysical systems
produces high-dimensional time-series data for which it can be difficult to
interpret which individual features are most salient. While recent work from
our group and others has demonstrated the utility of time-lagged co-variate
models to study such systems, linearity assumptions can limit the compression
of inherently nonlinear dynamics into just a few characteristic components.
Recent work in the field of deep learning has led to the development of
variational autoencoders (VAE), which are able to compress complex datasets
into simpler manifolds. We present the use of a time-lagged VAE, or variational
dynamics encoder (VDE), to reduce complex, nonlinear processes to a single
embedding with high fidelity to the underlying dynamics. We demonstrate how the
VDE is able to capture nontrivial dynamics in a variety of examples, including
Brownian dynamics and atomistic protein folding. Additionally, we demonstrate a
method for analyzing the VDE model, inspired by saliency mapping, to determine
what features are selected by the VDE model to describe dynamics. The VDE
presents an important step in applying techniques from deep learning to more
accurately model and interpret complex biophysics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hernandez_C/0/1/0/all/0/1&quot;&gt;Carlos X. Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wayment_Steele_H/0/1/0/all/0/1&quot;&gt;Hannah K. Wayment-Steele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sultan_M/0/1/0/all/0/1&quot;&gt;Mohammad M. Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Husic_B/0/1/0/all/0/1&quot;&gt;Brooke E. Husic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay S. Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09535">
<title>Learning with Biased Complementary Labels. (arXiv:1711.09535v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09535</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we study the classification problem in which we have access to
easily obtainable surrogate for the true labels, namely complementary labels,
which specify classes that observations do \textbf{not} belong to. For example,
if one is familiar with monkeys but not meerkats, a meerkat is easily
identified as not a monkey, so &quot;monkey&quot; is annotated to the meerkat as a
complementary label. Specifically, let $Y$ and $\bar{Y}$ be the true and
complementary labels, respectively. We first model the annotation of
complementary labels via the transition probabilities $P(\bar{Y}=i|Y=j), i\neq
j\in\{1,\cdots,c\}$, where $c$ is the number of classes. All the previous
methods implicitly assume that the transition probabilities $P(\bar{Y}=i|Y=j)$
are identical, which is far from true in practice because humans are biased
toward their own experience. For example, if a person is more familiar with
monkey than prairie dog when providing complementary labels for meerkats,
he/she is more likely to employ &quot;monkey&quot; as a complementary label. We therefore
reason that the transition probabilities will be different. In this paper, we
address three fundamental problems raised by learning with biased complementary
labels. (1) How to estimate the transition probabilities? (2) How to modify the
traditional loss functions and extend standard deep neural network classifiers
to learn with biased complementary labels? (3) Does the classifier learned from
examples with complementary labels by our proposed method converge to the
optimal one learned from examples with true labels? Comprehensive experiments
on MNIST, CIFAR10, CIFAR100, and Tiny ImageNet empirically validate the
superiority of the proposed method to the current state-of-the-art methods with
accuracy gains of over 10\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiyu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Mingming Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11511">
<title>Thermostat-assisted Continuous-tempered Hamiltonian Monte Carlo for Multimodal Posterior Sampling. (arXiv:1711.11511v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11511</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new sampling method named as the
thermostat-assisted continuous-tempered Hamiltonian Monte Carlo for multimodal
posterior sampling on large datasets. It simulates a noisy system, which is
augmented by a coupling tempering variable as well as a set of Nos\&apos;e-Hoover
thermostats. This augmentation is devised to address two main issues of
concern: the first is to effectively generate i.i.d. samples from complex
multimodal posterior distributions; the second is to adaptively control the
system dynamics in the presence of unknown noise that arises from the use of
mini-batches. The experiment on synthetic distributions has been performed; the
result demonstrates the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Rui Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00126">
<title>An interpretable latent variable model for attribute applicability in the Amazon catalogue. (arXiv:1712.00126v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00126</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning attribute applicability of products in the Amazon catalog (e.g.,
predicting that a shoe should have a value for size, but not for battery-type
at scale is a challenge. The need for an interpretable model is contingent on
(1) the lack of ground truth training data, (2) the need to utilise prior
information about the underlying latent space and (3) the ability to understand
the quality of predictions on new, unseen data. To this end, we develop the
MaxMachine, a probabilistic latent variable model that learns distributed
binary representations, associated to sets of features that are likely to
co-occur in the data. Layers of MaxMachines can be stacked such that higher
layers encode more abstract information. Any set of variables can be clamped to
encode prior information. We develop fast sampling based posterior inference.
Preliminary results show that the model improves over the baseline in 17 out of
19 product groups and provides qualitatively reasonable predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rukat_T/0/1/0/all/0/1&quot;&gt;Tammo Rukat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lange_D/0/1/0/all/0/1&quot;&gt;Dustin Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Archambeau_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00171">
<title>Speaker identification from the sound of the human breath. (arXiv:1712.00171v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00171</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the speaker identification potential of breath sounds in
continuous speech. Speech is largely produced during exhalation. In order to
replenish air in the lungs, speakers must periodically inhale. When inhalation
occurs in the midst of continuous speech, it is generally through the mouth.
Intra-speech breathing behavior has been the subject of much study, including
the patterns, cadence, and variations in energy levels. However, an often
ignored characteristic is the {\em sound} produced during the inhalation phase
of this cycle. Intra-speech inhalation is rapid and energetic, performed with
open mouth and glottis, effectively exposing the entire vocal tract to enable
maximum intake of air. This results in vocal tract resonances evoked by
turbulence that are characteristic of the speaker&apos;s speech-producing apparatus.
Consequently, the sounds of inhalation are expected to carry information about
the speaker&apos;s identity. Moreover, unlike other spoken sounds which are subject
to active control, inhalation sounds are generally more natural and less
affected by voluntary influences. The goal of this paper is to demonstrate that
breath sounds are indeed bio-signatures that can be used to identify speakers.
We show that these sounds by themselves can yield remarkably accurate speaker
recognition with appropriate feature representations and classification
frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenbo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rita Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00351">
<title>Trans-allelic model for prediction of peptide:MHC-II interactions. (arXiv:1712.00351v1 [q-bio.QM] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.00351</link>
<description rdf:parseType="Literal">&lt;p&gt;Major histocompatibility complex class two (MHC-II) molecules are
trans-membrane proteins and key components of the cellular immune system. Upon
recognition of foreign peptides expressed on the MHC-II binding groove, helper
T cells mount an immune response against invading pathogens. Therefore,
mechanistic identification and knowledge of physico-chemical features that
govern interactions between peptides and MHC-II molecules is useful for the
design of effective epitope-based vaccines, as well as for understanding of
immune responses. In this paper, we present a comprehensive trans-allelic
prediction model, a generalized version of our previous biophysical model, that
can predict peptide interactions for all three human MHC-II loci (HLA-DR,
HLA-DP and HLA-DQ), using both peptide sequence data and structural information
of MHC-II molecules. The advantage of this approach over other machine learning
models is that it offers a simple and plausible physical explanation for
peptide-MHC-II interactions. We train the model using a benchmark experimental
dataset, and measure its predictive performance using novel data. Despite its
relative simplicity, we find that the model has comparable performance to the
state-of-the-art method. Focusing on the physical bases of peptide-MHC binding,
we find support for previous theoretical predictions about the contributions of
certain binding pockets to the binding energy. Additionally, we find that
binding pockets P 4 and P 5 of HLA-DP, which were not previously considered as
primary anchors, do make strong contributions to the binding energy. Together,
the results indicate that our model can serve as a useful complement to
alternative approaches to predicting peptide-MHC interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Degoot_A/0/1/0/all/0/1&quot;&gt;A. M. Degoot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chirove_F/0/1/0/all/0/1&quot;&gt;Faraimunashe Chirove&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ndifon_W/0/1/0/all/0/1&quot;&gt;Wilfred Ndifon&lt;/a&gt;</dc:creator>
</item></rdf:RDF>