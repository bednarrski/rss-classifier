<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.09724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10023"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05490"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.02668">
<title>An Occam&apos;s Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets. (arXiv:1808.02668v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.02668</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a light-weight and accurate deep neural model for
audiovisual emotion recognition. To design this model, the authors followed a
philosophy of simplicity, drastically limiting the number of parameters to
learn from the target datasets, always choosing the simplest earning methods:
i) transfer learning and low-dimensional space embedding allows to reduce the
dimensionality of the representations. ii) The isual temporal information is
handled by a simple score-per-frame selection process, averaged across time.
iii) A simple frame selection echanism is also proposed to weight the images of
a sequence. iv) The fusion of the different modalities is performed at
prediction level (late usion). We also highlight the inherent challenges of the
AFEW dataset and the difficulty of model selection with as few as 383
validation equences. The proposed real-time emotion classifier achieved a
state-of-the-art accuracy of 60.64 % on the test set of AFEW, and ranked 4th at
he Emotion in the Wild 2018 challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1&quot;&gt;Valentin Vielzeuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1&quot;&gt;Corentin Kervadec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pateux_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Pateux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1&quot;&gt;Alexis Lechervy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Jurie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05136">
<title>Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v5 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05136</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1&quot;&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kappel_D/0/1/0/all/0/1&quot;&gt;David Kappel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1&quot;&gt;Wolfgang Maass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legenstein_R/0/1/0/all/0/1&quot;&gt;Robert Legenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01280">
<title>Geared Rotationally Identical and Invariant Convolutional Neural Network Systems. (arXiv:1808.01280v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01280</link>
<description rdf:parseType="Literal">&lt;p&gt;Theorems and techniques to form different types of transformationally
invariant processing and to produce the same output quantitatively based on
either transformationally invariant operators or symmetric operations have
recently been introduced by the authors. In this study, we further propose to
compose a geared rotationally identical CNN system (GRI-CNN) with a small step
angle by connecting networks of participated processes at the first flatten
layer. Using an ordinary CNN structure as a base, requirements for constructing
a GRI-CNN include the use of either symmetric input vector or kernels with an
angle increment that can form a complete cycle as a &quot;gearwheel&quot;. Four basic
GRI-CNN structures were studied. Each of them can produce quantitatively
identical output results when a rotation angle of the input vector is evenly
divisible by the step angle of the gear. Our study showed when a rotated input
vector does not match to a step angle, the GRI-CNN can also produce a highly
consistent result. With a design of ultra-fine gear-tooth step angle (e.g., 1
degree or 0.1 degree), a virtually isotropic CNN system can be constructed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1&quot;&gt;ShihChung B. Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+D%2E_P/0/1/0/all/0/1&quot;&gt;Ph.D.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1&quot;&gt;Matthew T. Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+D%2E_M/0/1/0/all/0/1&quot;&gt;M.D.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_S/0/1/0/all/0/1&quot;&gt;Seong K. Mun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+D%2E_P/0/1/0/all/0/1&quot;&gt;Ph.D.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Heang-Ping Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+D_P/0/1/0/all/0/1&quot;&gt;Ph.D&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02550">
<title>Collaborative Planning for Mixed-Autonomy Lane Merging. (arXiv:1808.02550v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.02550</link>
<description rdf:parseType="Literal">&lt;p&gt;Driving is a social activity: drivers often indicate their intent to change
lanes via motion cues. We consider mixed-autonomy traffic where a Human-driven
Vehicle (HV) and an Autonomous Vehicle (AV) drive together. We propose a
planning framework where the degree to which the AV considers the other agent&apos;s
reward is controlled by a selfishness factor. We test our approach on a
simulated two-lane highway where the AV and HV merge into each other&apos;s lanes.
In a user study with 21 subjects and 6 different selfishness factors, we found
that our planning approach was sound and that both agents had less merging
times when a factor that balances the rewards for the two agents was chosen.
Our results on double lane merging suggest it to be a non-zero-sum game and
encourage further investigation on collaborative decision making algorithms for
mixed-autonomy traffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Shray Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosgun_A/0/1/0/all/0/1&quot;&gt;Akansel Cosgun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakhaei_A/0/1/0/all/0/1&quot;&gt;Alireza Nakhaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujimura_K/0/1/0/all/0/1&quot;&gt;Kikuo Fujimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02560">
<title>Belief likelihood function for generalised logistic regression. (arXiv:1808.02560v1 [math.ST])</title>
<link>http://arxiv.org/abs/1808.02560</link>
<description rdf:parseType="Literal">&lt;p&gt;The notion of belief likelihood function of repeated trials is introduced,
whenever the uncertainty for individual trials is encoded by a belief measure
(a finite random set). This generalises the traditional likelihood function,
and provides a natural setting for belief inference from statistical data.
Factorisation results are proven for the case in which conjunctive or
disjunctive combination are employed, leading to analytical expressions for the
lower and upper likelihoods of `sharp&apos; samples in the case of Bernoulli trials,
and to the formulation of a generalised logistic regression framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02632">
<title>Question-Guided Hybrid Convolution for Visual Question Answering. (arXiv:1808.02632v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.02632</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC)
network for Visual Question Answering (VQA). Most state-of-the-art VQA methods
fuse the high-level textual and visual features from the neural network and
abandon the visual spatial information when learning multi-modal features.To
address these problems, question-guided kernels generated from the input
question are designed to convolute with visual features for capturing the
textual and visual relationship in the early stage. The question-guided
convolution can tightly couple the textual and visual information but also
introduce more parameters when learning kernels. We apply the group
convolution, which consists of question-independent kernels and
question-dependent kernels, to reduce the parameter size and alleviate
over-fitting. The hybrid convolution can generate discriminative multi-modal
features with fewer parameters. The proposed approach is also complementary to
existing bilinear pooling fusion and attention based VQA methods. By
integrating with them, our method could further boost the performance.
Extensive experiments on public VQA datasets validate the effectiveness of
QGHC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1&quot;&gt;Steven Hoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02510">
<title>Message Passing Graph Kernels. (arXiv:1808.02510v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.02510</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph kernels have recently emerged as a promising approach for tackling the
graph similarity and learning tasks at the same time. In this paper, we propose
a general framework for designing graph kernels. The proposed framework
capitalizes on the well-known message passing scheme on graphs. The kernels
derived from the framework consist of two components. The first component is a
kernel between vertices, while the second component is a kernel between graphs.
The main idea behind the proposed framework is that the representations of the
vertices are implicitly updated using an iterative procedure. Then, these
representations serve as the building blocks of a kernel that compares pairs of
graphs. We derive four instances of the proposed framework, and show through
extensive experiments that these instances are competitive with
state-of-the-art methods in various tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nikolentzos_G/0/1/0/all/0/1&quot;&gt;Giannis Nikolentzos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vazirgiannis_M/0/1/0/all/0/1&quot;&gt;Michalis Vazirgiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02651">
<title>Adversarial Geometry and Lighting using a Differentiable Renderer. (arXiv:1808.02651v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.02651</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning classifiers are vulnerable to adversarial attacks,
inputs with perturbations designed to intentionally trigger misclassification.
Modern adversarial methods either directly alter pixel colors, or &quot;paint&quot;
colors onto a 3D shapes. We propose novel adversarial attacks that directly
alter the geometry of 3D objects and/or manipulate the lighting in a virtual
scene. We leverage a novel differentiable renderer that is efficient to
evaluate and analytically differentiate. Our renderer generates images
realistic enough for correct classification by common pre-trained models, and
we use it to design physical adversarial examples that consistently fool these
models. We conduct qualitative and quantitate experiments to validate our
adversarial geometry and adversarial lighting attack capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hsueh-Ti Derek Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Michael Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1&quot;&gt;Alec Jacobson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02822">
<title>Backprop Evolution. (arXiv:1808.02822v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.02822</link>
<description rdf:parseType="Literal">&lt;p&gt;The back-propagation algorithm is the cornerstone of deep learning. Despite
its importance, few variations of the algorithm have been attempted. This work
presents an approach to discover new variations of the back-propagation
equation. We use a domain specific lan- guage to describe update equations as a
list of primitive functions. An evolution-based method is used to discover new
propagation rules that maximize the generalization per- formance after a few
epochs of training. We find several update equations that can train faster with
short training times than standard back-propagation, and perform similar as
standard back-propagation at convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alber_M/0/1/0/all/0/1&quot;&gt;Maximilian Alber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_I/0/1/0/all/0/1&quot;&gt;Irwan Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1&quot;&gt;Barret Zoph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kindermans_P/0/1/0/all/0/1&quot;&gt;Pieter-Jan Kindermans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1&quot;&gt;Prajit Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.09724">
<title>Transfer Learning with Label Noise. (arXiv:1707.09724v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.09724</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning aims to improve learning in target domain by borrowing
knowledge from a related but different source domain. To reduce the
distribution shift between source and target domains, recent methods have
focused on exploring invariant representations that have similar distributions
across domains. However, when learning this invariant knowledge, existing
methods assume that the labels in source domain are uncontaminated, while in
reality, we often have access to source data with noisy labels. In this paper,
we first show how label noise adversely affect the learning of invariant
representations and the correcting of label shift in various transfer learning
scenarios. To reduce the adverse effects, we propose a novel Denoising
Conditional Invariant Component (DCIC) framework, which provably ensures (1)
extracting invariant representations given examples with noisy labels in source
domain and unlabeled examples in target domain; (2) estimating the label
distribution in target domain with no bias. Experimental results on both
synthetic and real-world data verify the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiyu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Mingming Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10023">
<title>Candidate Labeling for Crowd Learning. (arXiv:1804.10023v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10023</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowdsourcing has become very popular among the machine learning community as
a way to obtain labels that allow a ground truth to be estimated for a given
dataset. In most of the approaches that use crowdsourced labels, annotators are
asked to provide, for each presented instance, a single class label. Such a
request could be inefficient, that is, considering that the labelers may not be
experts, that way to proceed could fail to take real advantage of the knowledge
of the labelers. In this paper, the use of candidate labeling for crowd
learning is proposed, where the annotators may provide more than a single label
per instance to try not to miss the real label. The main hypothesis is that, by
allowing candidate labeling, knowledge can be extracted from the labelers more
efficiently by than in the standard crowd learning scenario. Empirical evidence
which supports that hypothesis is presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benaran_Munoz_I/0/1/0/all/0/1&quot;&gt;Iker Be&amp;#xf1;aran-Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hernandez_Gonzalez_J/0/1/0/all/0/1&quot;&gt;Jer&amp;#xf3;nimo Hern&amp;#xe1;ndez-Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Aritz P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03972">
<title>A Multi-task Deep Learning Architecture for Maritime Surveillance using AIS Data Streams. (arXiv:1806.03972v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03972</link>
<description rdf:parseType="Literal">&lt;p&gt;In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular timesampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadaine_R/0/1/0/all/0/1&quot;&gt;Rodolphe Vadaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajduch_G/0/1/0/all/0/1&quot;&gt;Guillaume Hajduch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garello_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Garello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05490">
<title>Semi-Supervised Feature Learning for Off-Line Writer Identifications. (arXiv:1807.05490v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05490</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional approaches used supervised learning to estimate off-line writer
identifications. In this study, we improved the off-line writer identifica-
tions by semi-supervised feature learning pipeline, which trained the extra
unla- beled data and the original labeled data simultaneously. In specific, we
proposed a weighted label smoothing regularization (WLSR) method, which
assigned the weighted uniform label distribution to the extra unlabeled data.
We regularized the convolutional neural network (CNN) baseline, which allows
learning more discriminative features to represent the properties of different
writing styles. Based on experiments on ICDAR2013, CVL and IAM benchmark
datasets, our results showed that semi-supervised feature learning improved the
baseline meas- urement and achieved better performance compared with existing
writer identifications approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zehong Cao&lt;/a&gt;</dc:creator>
</item></rdf:RDF>