<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00831"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05958"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05120"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.00831">
<title>AttnConvnet at SemEval-2018 Task 1: Attention-based Convolutional Neural Networks for Multi-label Emotion Classification. (arXiv:1804.00831v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00831</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an attention-based classifier that predicts
multiple emotions of a given sentence. Our model imitates human&apos;s two-step
procedure of sentence understanding and it can effectively represent and
classify sentences. With emoji-to-meaning preprocessing and extra lexicon
utilization, we further improve the model performance. We train and evaluate
our model with data provided by SemEval-2018 task 1-5, each sentence of which
has several labels among 11 given sentiments. Our model achieves 5-th/1-th rank
in English/Spanish respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yanghoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hwanhee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kyomin Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05834">
<title>CytonRL: an Efficient Reinforcement Learning Open-source Toolkit Implemented in C++. (arXiv:1804.05834v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05834</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an open-source enforcement learning toolkit named CytonRL
(https://github.com/arthurxlw/cytonRL). The toolkit implements four recent
advanced deep Q-learning algorithms from scratch using C++ and NVIDIA&apos;s
GPU-accelerated libraries. The code is simple and elegant, owing to an
open-source general-purpose neural network library named CytonLib. Benchmark
shows that the toolkit achieves competitive performances on the popular Atari
game of Breakout.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05839">
<title>BigDL: A Distributed Deep Learning Framework for Big Data. (arXiv:1804.05839v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1804.05839</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present BigDL, a distributed deep learning framework for
Big Data platforms and workflows. It is implemented on top of Apache Spark, and
allows users to write their deep learning applications as standard Spark
programs (running directly on large-scale big data clusters in a distributed
fashion). It provides an expressive, &quot;data-analytics integrated&quot; deep learning
programming model, so that users can easily build the end-to-end analytics + AI
pipelines under a unified programming paradigm; by implementing an AllReduce
like operation using existing primitives in Spark (e.g., shuffle, broadcast,
and in-memory data persistence), it also provides a highly efficient &quot;parameter
server&quot; style architecture, so as to achieve highly scalable, data-parallel
distributed training. Since its initial open source release, BigDL users have
built many analytics and deep learning applications (e.g., object detection,
sequence-to-sequence generation, neural recommendations, fraud detection, etc.)
on Spark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jason/0/1/0/all/0/1&quot;&gt;Jason&lt;/a&gt; (Jinquan)Dai, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1&quot;&gt;Ding Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xianyan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherry/0/1/0/all/0/1&quot;&gt;Cherry&lt;/a&gt; (Li) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1&quot;&gt;Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yan Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhichao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shengsheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_B/0/1/0/all/0/1&quot;&gt;Bowen She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1&quot;&gt;Dongjie Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guoqiong Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05997">
<title>A New Decidable Class of Tuple Generating Dependencies: The Triangularly-Guarded Class. (arXiv:1804.05997v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.05997</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a new class of tuple-generating dependencies
(TGDs) called triangularly-guarded TGDs, which are TGDs with certain
restrictions on the atomic derivation track embedded in the underlying rule
set. We show that conjunctive query answering under this new class of TGDs is
decidable. We further show that this new class strictly contains some other
decidable classes such as weak-acyclic, guarded, sticky and shy, which, to the
best of our knowledge, provides a unified representation of all these
aforementioned classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asuncion_V/0/1/0/all/0/1&quot;&gt;Vernon Asuncion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06020">
<title>Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource. (arXiv:1804.06020v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06020</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting temporal relations (before, after, overlapping, etc.) is a key
aspect of understanding events described in natural language. We argue that
this task would gain from the availability of a resource that provides prior
knowledge in the form of the temporal order that events usually follow. This
paper develops such a resource -- a probabilistic knowledge base acquired in
the news domain -- by extracting temporal relations between events from the New
York Times (NYT) articles over a 20-year span (1987--2007). We show that
existing temporal extraction systems can be improved via this resource. As a
byproduct, we also show that interesting statistics can be retrieved from this
resource, which can potentially benefit other time-aware tasks. The proposed
system and resource are both publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1&quot;&gt;Qiang Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Haoruo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06188">
<title>VC-Dimension Based Generalization Bounds for Relational Learning. (arXiv:1804.06188v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06188</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications of relational learning, the available data can be seen
as a sample from a larger relational structure (e.g. we may be given a small
fragment from some social network). In this paper we are particularly concerned
with scenarios in which we can assume that (i) the domain elements appearing in
the given sample have been uniformly sampled without replacement from the
(unknown) full domain and (ii) the sample is complete for these domain elements
(i.e. it is the full substructure induced by these elements). Within this
setting, we study bounds on the error of sufficient statistics of relational
models that are estimated on the available data. As our main result, we prove a
bound based on a variant of the Vapnik-Chervonenkis dimension which is suitable
for relational data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzelka_O/0/1/0/all/0/1&quot;&gt;Ondrej Kuzelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00513">
<title>Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks. (arXiv:1709.00513v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00513</link>
<description rdf:parseType="Literal">&lt;p&gt;There is an increasing interest on accelerating neural networks for real-time
applications. We study the student-teacher strategy, in which a small and fast
student network is trained with the auxiliary information learned from a large
and accurate teacher network. We propose to use conditional adversarial
networks to learn the loss function to transfer knowledge from teacher to
student. The proposed method is particularly effective for relatively small
student networks. Moreover, experimental results show the effect of network
size when the modern networks are used as student. We empirically study the
trade-off between inference time and classification accuracy, and provide
suggestions on choosing a proper student network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiawei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04942">
<title>Isolating Sources of Disentanglement in Variational Autoencoders. (arXiv:1802.04942v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04942</link>
<description rdf:parseType="Literal">&lt;p&gt;We decompose the evidence lower bound to show the existence of a term
measuring the total correlation between latent variables. We use this to
motivate our $\beta$-TCVAE (Total Correlation Variational Autoencoder), a
refinement of the state-of-the-art $\beta$-VAE objective for learning
disentangled representations, requiring no additional hyperparameters during
training. We further propose a principled classifier-free measure of
disentanglement called the mutual information gap (MIG). We perform extensive
quantitative and qualitative experiments, in both restricted and non-restricted
settings, and show a strong relation between total correlation and
disentanglement, when the latent variables model is trained using our
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tian Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02324">
<title>Annotation Artifacts in Natural Language Inference Data. (arXiv:1803.02324v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02324</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale datasets for natural language inference are created by presenting
crowd workers with a sentence (premise), and asking them to generate three new
sentences (hypotheses) that it entails, contradicts, or is logically neutral
with respect to. We show that, in a significant portion of such data, this
protocol leaves clues that make it possible to identify the label by looking
only at the hypothesis, without observing the premise. Specifically, we show
that a simple text categorization model can correctly classify the hypothesis
alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams
et. al, 2017). Our analysis reveals that specific linguistic phenomena such as
negation and vagueness are highly correlated with certain inference classes.
Our findings suggest that the success of natural language inference models to
date has been overestimated, and that the task remains a hard open problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1&quot;&gt;Suchin Gururangan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1&quot;&gt;Swabha Swayamdipta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1&quot;&gt;Omer Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1&quot;&gt;Roy Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09702">
<title>HAMLET: Interpretable Human And Machine co-LEarning Technique. (arXiv:1803.09702v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09702</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient label acquisition processes are key to obtaining robust
classifiers. However, data labeling is often challenging and subject to high
levels of label noise. This can arise even when classification targets are well
defined, if instances to be labeled are more difficult than the prototypes used
to define the class, leading to disagreements among the expert community. Here,
we enable efficient training of deep neural networks. From low-confidence
labels, we iteratively improve their quality by simultaneous learning of
machines and experts. We call it Human And Machine co-LEarning Technique
(HAMLET). Throughout the process, experts become more consistent, while the
algorithm provides them with explainable feedback for confirmation. HAMLET uses
a neural embedding function and a memory module filled with diverse reference
embeddings from different classes. Its output includes classification labels
and highly relevant reference embeddings as explanation. We took the study of
brain monitoring at intensive care unit (ICU) as an application of HAMLET on
continuous electroencephalography (cEEG) data. Although cEEG monitoring yields
large volumes of data, labeling costs and difficulty make it hard to build a
classifier. Additionally, while experts agree on the labels of clear-cut
examples of cEEG patterns, labeling many real-world cEEG data can be extremely
challenging. Thus, a large minority of sequences might be mislabeled. HAMLET
has shown significant performance gain against deep learning and other
baselines, increasing accuracy from 7.03% to 68.75% on challenging inputs.
Besides improved performance, clinical experts confirmed the interpretability
of those reference embeddings in helping explaining the classification results
by HAMLET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deiss_O/0/1/0/all/0/1&quot;&gt;Olivier Deiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswal_S/0/1/0/all/0/1&quot;&gt;Siddharth Biswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haoqi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westover_M/0/1/0/all/0/1&quot;&gt;M. Brandon Westover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00168">
<title>Learning to Navigate in Cities Without a Map. (arXiv:1804.00168v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00168</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating through unstructured environments is a basic capability of
intelligent creatures, and thus is of fundamental interest in the study and
development of artificial intelligence. Long-range navigation is a complex
cognitive task that relies on developing an internal representation of space,
grounded by recognisable landmarks and robust visual processing, that can
simultaneously support continuous self-localisation (&quot;I am here&quot;) and a
representation of the goal (&quot;I am going there&quot;). Building upon recent research
that applies deep reinforcement learning to maze navigation problems, we
present an end-to-end deep reinforcement learning approach that can be applied
on a city scale. Recognising that successful navigation relies on integration
of general policies with locale-specific knowledge, we propose a dual pathway
architecture that allows locale-specific features to be encapsulated, while
still enabling transfer to multiple cities. We present an interactive
navigation environment that uses Google StreetView for its photographic content
and worldwide coverage, and demonstrate that our learning method allows agents
to learn to navigate multiple cities and to traverse to target destinations
that may be kilometres away. A video summarizing our research and showing the
trained agent in diverse city environments as well as on the transfer task is
available at: https://sites.google.com/view/streetlearn.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirowski_P/0/1/0/all/0/1&quot;&gt;Piotr Mirowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimes_M/0/1/0/all/0/1&quot;&gt;Matthew Koichi Grimes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1&quot;&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermann_K/0/1/0/all/0/1&quot;&gt;Karl Moritz Hermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_K/0/1/0/all/0/1&quot;&gt;Keith Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teplyashin_D/0/1/0/all/0/1&quot;&gt;Denis Teplyashin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1&quot;&gt;Karen Simonyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kavukcuoglu_K/0/1/0/all/0/1&quot;&gt;Koray Kavukcuoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1&quot;&gt;Raia Hadsell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05958">
<title>Can Neural Machine Translation be Improved with User Feedback?. (arXiv:1804.05958v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.05958</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first real-world application of methods for improving neural
machine translation (NMT) with human reinforcement, based on explicit and
implicit user feedback collected on the eBay e-commerce platform. Previous work
has been confined to simulation experiments, whereas in this paper we work with
real logged feedback for offline bandit learning of NMT parameters. We conduct
a thorough analysis of the available explicit user judgments---five-star
ratings of translation quality---and show that they are not reliable enough to
yield significant improvements in bandit learning. In contrast, we successfully
utilize implicit task-based feedback collected in a cross-lingual search task
to improve task-specific and machine translation quality metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1&quot;&gt;Julia Kreutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1&quot;&gt;Shahram Khadivi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matusov_E/0/1/0/all/0/1&quot;&gt;Evgeny Matusov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1&quot;&gt;Stefan Riezler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05965">
<title>MaxGain: Regularisation of Neural Networks by Constraining Activation Magnitudes. (arXiv:1804.05965v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05965</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective regularisation of neural networks is essential to combat
overfitting due to the large number of parameters involved. We present an
empirical analogue to the Lipschitz constant of a feed-forward neural network,
which we refer to as the maximum gain. We hypothesise that constraining the
gain of a network will have a regularising effect, similar to how constraining
the Lipschitz constant of a network has been shown to improve generalisation. A
simple algorithm is provided that involves rescaling the weight matrix of each
layer after each parameter update. We conduct a series of studies on common
benchmark datasets, and also a novel dataset that we introduce to enable easier
significance testing for experiments using convolutional networks. Performance
on these datasets compares favourably with other common regularisation
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gouk_H/0/1/0/all/0/1&quot;&gt;Henry Gouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pfahringer_B/0/1/0/all/0/1&quot;&gt;Bernhard Pfahringer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frank_E/0/1/0/all/0/1&quot;&gt;Eibe Frank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cree_M/0/1/0/all/0/1&quot;&gt;Michael Cree&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06095">
<title>Parametric Models for Mutual Kernel Matrix Completion. (arXiv:1804.06095v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06095</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies utilize multiple kernel learning to deal with incomplete-data
problem. In this study, we introduce new methods that do not only complete
multiple incomplete kernel matrices simultaneously, but also allow control of
the flexibility of the model by parameterizing the model matrix. By imposing
restrictions on the model covariance, overfitting of the data is avoided. A
limitation of kernel matrix estimations done via optimization of an objective
function is that the positive definiteness of the result is not guaranteed. In
view of this limitation, our proposed methods employ the LogDet divergence,
which ensures the positive definiteness of the resulting inferred kernel
matrix. We empirically show that our proposed restricted covariance models,
employed with LogDet divergence, yield significant improvements in the
generalization performance of previous completion methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivero_R/0/1/0/all/0/1&quot;&gt;Rachelle Rivero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_T/0/1/0/all/0/1&quot;&gt;Tsuyoshi Kato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06114">
<title>A Support Tensor Train Machine. (arXiv:1804.06114v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06114</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been growing interest in extending traditional vector-based machine
learning techniques to their tensor forms. An example is the support tensor
machine (STM) that utilizes a rank-one tensor to capture the data structure,
thereby alleviating the overfitting and curse of dimensionality problems in the
conventional support vector machine (SVM). However, the expressive power of a
rank-one tensor is restrictive for many real-world data. To overcome this
limitation, we introduce a support tensor train machine (STTM) by replacing the
rank-one tensor in an STM with a tensor train. Experiments validate and confirm
the superiority of an STTM over the SVM and STM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batselier_K/0/1/0/all/0/1&quot;&gt;Kim Batselier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1&quot;&gt;Ching-Yun Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1&quot;&gt;Ngai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06207">
<title>MetaBags: Bagged Meta-Decision Trees for Regression. (arXiv:1804.06207v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06207</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensembles are popular methods for solving practical supervised learning
problems. They reduce the risk of having underperforming models in
production-grade software. Although critical, methods for learning
heterogeneous regression ensembles have not been proposed at large scale,
whereas in classical ML literature, stacking, cascading and voting are mostly
restricted to classification problems. Regression poses distinct learning
challenges that may result in poor performance, even when using well
established homogeneous ensemble schemas such as bagging or boosting.
&lt;/p&gt;
&lt;p&gt;In this paper, we introduce MetaBags, a novel, practically useful stacking
framework for regression. MetaBags is a meta-learning algorithm that learns a
set of meta-decision trees designed to select one base model (i.e. expert) for
each query, and focuses on inductive bias reduction. A set of meta-decision
trees are learned using different types of meta-features, specially created for
this purpose - to then be bagged at meta-level. This procedure is designed to
learn a model with a fair bias-variance trade-off, and its improvement over
base model performance is correlated with the prediction diversity of different
experts on specific input space subregions. The proposed method and
meta-features are designed in such a way that they enable good predictive
performance even in subregions of space which are not adequately represented in
the available training data.
&lt;/p&gt;
&lt;p&gt;An exhaustive empirical testing of the method was performed, evaluating both
generalization error and scalability of the approach on synthetic, open and
real-world application datasets. The obtained results show that our method
significantly outperforms existing state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khiari_J/0/1/0/all/0/1&quot;&gt;Jihed Khiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_Matias_L/0/1/0/all/0/1&quot;&gt;Luis Moreira-Matias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1&quot;&gt;Ammar Shaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenko_B/0/1/0/all/0/1&quot;&gt;Bernard Zenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1&quot;&gt;Saso Dzeroski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06216">
<title>Learning Sparse Latent Representations with the Deep Copula Information Bottleneck. (arXiv:1804.06216v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.06216</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep latent variable models are powerful tools for representation learning.
In this paper, we adopt the deep information bottleneck model, identify its
shortcomings and propose a model that circumvents them. To this end, we apply a
copula transformation which, by restoring the invariance properties of the
information bottleneck method, leads to disentanglement of the features in the
latent space. Building on that, we show how this transformation translates to
sparsity of the latent space in the new model. We evaluate our method on
artificial and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wieczorek_A/0/1/0/all/0/1&quot;&gt;Aleksander Wieczorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wieser_M/0/1/0/all/0/1&quot;&gt;Mario Wieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murezzan_D/0/1/0/all/0/1&quot;&gt;Damian Murezzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roth_V/0/1/0/all/0/1&quot;&gt;Volker Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06234">
<title>Clustering Analysis on Locally Asymptotically Self-similar Processes. (arXiv:1804.06234v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.06234</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we design algorithms for clustering locally asymptotically
self-similar stochastic processes. We show a sufficient condition on the
dissimilarity measure that leads to the consistency of the algorithms for
clustering offline and online data settings, respectively. As an example of
application, clustering synthetic data sampled from multifractional Brownian
motions is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peng_Q/0/1/0/all/0/1&quot;&gt;Qidi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_N/0/1/0/all/0/1&quot;&gt;Nan Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ran Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06309">
<title>On Improving Deep Reinforcement Learning for POMDPs. (arXiv:1804.06309v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06309</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Reinforcement Learning (RL) recently emerged as one of the most
competitive approaches for learning in sequential decision making problems with
fully observable environments, e.g., computer Go. However, very little work has
been done in deep RL to handle partially observable environments. We propose a
new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to
enhance learning performance in partially observable domains. Actions are
encoded by a fully connected layer and coupled with a convolutional observation
to form an action-observation pair. The time series of action-observation pairs
are then integrated by an LSTM layer that learns latent states based on which a
fully connected layer computes Q-values as in conventional Deep Q-Networks
(DQNs). We demonstrate the effectiveness of our new architecture in several
partially observable domains, including flickering Atari games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1&quot;&gt;Pascal Poupart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_G/0/1/0/all/0/1&quot;&gt;Guanghui Miao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06364">
<title>DGPose: Disentangled Semi-supervised Deep Generative Models for Human Body Analysis. (arXiv:1804.06364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.06364</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative modelling for robust human body analysis is an emerging
problem with many interesting applications, since it enables
analysis-by-synthesis and unsupervised learning. However, the latent space
learned by such models is typically not human-interpretable, resulting in less
flexible models. In this work, we adopt a structured semi-supervised
variational auto-encoder approach and present a deep generative model for human
body analysis where the pose and appearance are disentangled in the latent
space, allowing for pose estimation. Such a disentanglement allows independent
manipulation of pose and appearance and hence enables applications such as
pose-transfer without being explicitly trained for such a task. In addition,
the ability to train in a semi-supervised setting relaxes the need for labelled
data. We demonstrate the merits of our generative model on the Human3.6M and
ChictopiaPlus datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bem_R/0/1/0/all/0/1&quot;&gt;Rodrigo de Bem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arnab Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajanthan_T/0/1/0/all/0/1&quot;&gt;Thalaiyasingam Ajanthan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miksik_O/0/1/0/all/0/1&quot;&gt;Ondrej Miksik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1&quot;&gt;N. Siddharth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H.S. Torr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06378">
<title>Graph-based Selective Outlier Ensembles. (arXiv:1804.06378v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06378</link>
<description rdf:parseType="Literal">&lt;p&gt;An ensemble technique is characterized by the mechanism that generates the
components and by the mechanism that combines them. A common way to achieve the
consensus is to enable each component to equally participate in the aggregation
process. A problem with this approach is that poor components are likely to
negatively affect the quality of the consensus result. To address this issue,
alternatives have been explored in the literature to build selective classifier
and cluster ensembles, where only a subset of the components contributes to the
computation of the consensus. Of the family of ensemble methods, outlier
ensembles are the least studied. Only recently, the selection problem for
outlier ensembles has been discussed. In this work we define a new graph-based
class of ranking selection methods. A method in this class is characterized by
two main steps: (1) Mapping the rankings onto a graph structure; and (2) Mining
the resulting graph to identify a subset of rankings. We define a specific
instance of the graph-based ranking selection class. Specifically, we map the
problem of selecting ensemble components onto a mining problem in a graph. An
extensive evaluation was conducted on a variety of heterogeneous data and
methods. Our empirical results show that our approach outperforms
state-of-the-art selective outlier ensemble techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarvari_H/0/1/0/all/0/1&quot;&gt;Hamed Sarvari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domeniconi_C/0/1/0/all/0/1&quot;&gt;Carlotta Domeniconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stilo_G/0/1/0/all/0/1&quot;&gt;Giovanni Stilo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05120">
<title>Robust Dual View Deep Agent. (arXiv:1804.05120v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05120</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by recent advance of machine learning using Deep Reinforcement
Learning this paper proposes a modified architecture that produces more robust
agents and speeds up the training process. Our architecture is based on
Asynchronous Advantage Actor-Critic (A3C) algorithm where the total input
dimensionality is halved by dividing the input into two independent streams. We
use ViZDoom, 3D world software that is based on the classical first person
shooter video game, Doom, as a test case. The experiments show that in
comparison to single input agents, the proposed architecture succeeds to have
the same playing performance and shows more robust behavior, achieving
significant reduction in the number of training parameters of almost 30%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sobh_I/0/1/0/all/0/1&quot;&gt;Ibrahim M. Sobh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darwish_N/0/1/0/all/0/1&quot;&gt;Nevin M. Darwish&lt;/a&gt;</dc:creator>
</item></rdf:RDF>