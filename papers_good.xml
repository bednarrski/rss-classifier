<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00948"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10448"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10463"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10592"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.01491"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00137"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07736"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09728"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10116"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10542"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10282"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.10203">
<title>Behavioral Learning of Aircraft Landing Sequencing Using a Society of Probabilistic Finite State Machines. (arXiv:1802.10203v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.10203</link>
<description rdf:parseType="Literal">&lt;p&gt;Air Traffic Control (ATC) is a complex safety critical environment. A tower
controller would be making many decisions in real-time to sequence aircraft.
While some optimization tools exist to help the controller in some airports,
even in these situations, the real sequence of the aircraft adopted by the
controller is significantly different from the one proposed by the optimization
algorithm. This is due to the very dynamic nature of the environment. The
objective of this paper is to test the hypothesis that one can learn from the
sequence adopted by the controller some strategies that can act as heuristics
in decision support tools for aircraft sequencing. This aim is tested in this
paper by attempting to learn sequences generated from a well-known sequencing
method that is being used in the real world. The approach relies on a genetic
algorithm (GA) to learn these sequences using a society Probabilistic
Finite-state Machines (PFSMs). Each PFSM learns a different sub-space; thus,
decomposing the learning problem into a group of agents that need to work
together to learn the overall problem. Three sequence metrics (Levenshtein,
Hamming and Position distances) are compared as the fitness functions in GA. As
the results suggest, it is possible to learn the behavior of the
algorithm/heuristic that generated the original sequence from very limited
information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiangjun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbass_H/0/1/0/all/0/1&quot;&gt;Hussein A. Abbass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00948">
<title>Hierarchical Actor-Critic. (arXiv:1712.00948v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00948</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to learn at different resolutions in time may help overcome one
of the main challenges in deep reinforcement learning -- sample efficiency.
Hierarchical agents that operate at different levels of temporal abstraction
can learn tasks more quickly because they can divide the work of learning
behaviors among multiple policies and can also explore the environment at a
higher level. In this paper, we present a novel approach to hierarchical
reinforcement learning called Hierarchical Actor-Critic (HAC) that enables
agents to learn to break down problems involving continuous action spaces into
simpler subproblems belonging to different time scales. HAC has two key
advantages over most existing hierarchical learning methods: (i) the potential
for faster learning as agents learn short policies at each level of the
hierarchy and (ii) an end-to-end approach. We demonstrate that HAC
significantly accelerates learning in a series of tasks that require behavior
over a relatively long time horizon and involve sparse rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1&quot;&gt;Andrew Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platt_R/0/1/0/all/0/1&quot;&gt;Robert Platt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08567">
<title>Adversarial Training for Probabilistic Spiking Neural Networks. (arXiv:1802.08567v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08567</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifiers trained using conventional empirical risk minimization or maximum
likelihood methods are known to suffer dramatic performance degradations when
tested over examples adversarially selected based on knowledge of the
classifier&apos;s decision rule. Due to the prominence of Artificial Neural Networks
(ANNs) as classifiers, their sensitivity to adversarial examples, as well as
robust training schemes, have been recently the subject of intense
investigation. In this paper, for the first time, the sensitivity of spiking
neural networks (SNNs), or third-generation neural networks, to adversarial
examples is studied. The study considers rate and time encoding, as well as
rate and first-to-spike decoding. Furthermore, a robust training mechanism is
proposed that is demonstrated to enhance the performance of SNNs under
white-box attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bagheri_A/0/1/0/all/0/1&quot;&gt;Alireza Bagheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simeone_O/0/1/0/all/0/1&quot;&gt;Osvaldo Simeone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rajendran_B/0/1/0/all/0/1&quot;&gt;Bipin Rajendran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10363">
<title>General Video Game AI: a Multi-Track Framework for Evaluating Agents, Games and Content Generation Algorithms. (arXiv:1802.10363v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.10363</link>
<description rdf:parseType="Literal">&lt;p&gt;General Video Game Playing (GVGP) aims at designing an agent that is capable
of playing multiple video games with no human intervention. In 2014, The
General Video Game AI (GVGAI) competition framework was created and released
with the purpose of providing researchers a common open-source and easy to use
platform for testing their AI methods with potentially infinity of games
created using Video Game Description Language (VGDL). The framework has been
expanded into several tracks during the last few years to meet the demand of
different research directions. The agents are required to either play multiples
unknown games with or without access to game simulations, or to design new game
levels or rules. This survey paper presents the VGDL, the GVGAI framework,
existing tracks, and reviews the wide use of GVGAI framework in research,
education and competitions five years after its birth. A future plan of
framework improvements is also described.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1&quot;&gt;Diego Perez-Liebana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jialin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1&quot;&gt;Ahmed Khalifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaina_R/0/1/0/all/0/1&quot;&gt;Raluca D. Gaina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_S/0/1/0/all/0/1&quot;&gt;Simon M. Lucas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10448">
<title>Quantum cognition goes beyond-quantum: modeling the collective participant in psychological measurements. (arXiv:1802.10448v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1802.10448</link>
<description rdf:parseType="Literal">&lt;p&gt;In psychological measurements, two levels should be distinguished: the
&apos;individual level&apos;, relative to the different participants in a given cognitive
situation, and the &apos;collective level&apos;, relative to the overall statistics of
their outcomes, which we propose to associate with a notion of &apos;collective
participant&apos;. When the distinction between these two levels is properly
formalized, it reveals why the modeling of the collective participant generally
requires beyond-quantum - non-Bornian - probabilistic models, when sequential
measurements at the individual level are considered, and this though a pure
quantum description remains valid for single measurement situations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aerts_D/0/1/0/all/0/1&quot;&gt;Diederik Aerts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bianchi_M/0/1/0/all/0/1&quot;&gt;Massimiliano Sassoli de Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sozzo_S/0/1/0/all/0/1&quot;&gt;Sandro Sozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Veloz_T/0/1/0/all/0/1&quot;&gt;Tomas Veloz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10463">
<title>DiGrad: Multi-Task Reinforcement Learning with Shared Actions. (arXiv:1802.10463v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.10463</link>
<description rdf:parseType="Literal">&lt;p&gt;Most reinforcement learning algorithms are inefficient for learning multiple
tasks in complex robotic systems, where different tasks share a set of actions.
In such environments a compound policy may be learnt with shared neural network
parameters, which performs multiple tasks concurrently. However such compound
policy may get biased towards a task or the gradients from different tasks
negate each other, making the learning unstable and sometimes less data
efficient. In this paper, we propose a new approach for simultaneous training
of multiple tasks sharing a set of common actions in continuous action spaces,
which we call as DiGrad (Differential Policy Gradient). The proposed framework
is based on differential policy gradients and can accommodate multi-task
learning in a single actor-critic network. We also propose a simple heuristic
in the differential policy gradient update to further improve the learning. The
proposed architecture was tested on 8 link planar manipulator and 27 degrees of
freedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2
end effectors respectively. We show that our approach supports efficient
multi-task learning in complex robotic systems, outperforming related methods
in continuous action spaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dewangan_P/0/1/0/all/0/1&quot;&gt;Parijat Dewangan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phaniteja_S/0/1/0/all/0/1&quot;&gt;S Phaniteja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1&quot;&gt;K Madhava Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Abhishek Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1&quot;&gt;Balaraman Ravindran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10592">
<title>Model-Ensemble Trust-Region Policy Optimization. (arXiv:1802.10592v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.10592</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-free reinforcement learning (RL) methods are succeeding in a growing
number of tasks, aided by recent advances in deep learning. However, they tend
to suffer from high sample complexity, which hinders their use in real-world
domains. Alternatively, model-based reinforcement learning promises to reduce
sample complexity, but tends to require careful tuning and to date have
succeeded mainly in restrictive domains where simple models are sufficient for
learning. In this paper, we analyze the behavior of vanilla model-based
reinforcement learning methods when deep neural networks are used to learn both
the model and the policy, and show that the learned policy tends to exploit
regions where insufficient data is available for the model to be learned,
causing instability in training. To overcome this issue, we propose to use an
ensemble of models to maintain the model uncertainty and regularize the
learning process. We further show that the use of likelihood ratio derivatives
yields much more stable learning than backpropagation through time. Altogether,
our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO)
significantly reduces the sample complexity compared to model-free deep RL
methods on challenging continuous control benchmark tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurutach_T/0/1/0/all/0/1&quot;&gt;Thanard Kurutach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clavera_I/0/1/0/all/0/1&quot;&gt;Ignasi Clavera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1&quot;&gt;Aviv Tamar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.01491">
<title>Understanding Deep Neural Networks with Rectified Linear Units. (arXiv:1611.01491v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1611.01491</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we investigate the family of functions representable by deep
neural networks (DNN) with rectified linear units (ReLU). We give an algorithm
to train a ReLU DNN with one hidden layer to *global optimality* with runtime
polynomial in the data size albeit exponential in the input dimension. Further,
we improve on the known lower bounds on size (from exponential to super
exponential) for approximating a ReLU deep net function by a shallower ReLU
net. Our gap theorems hold for smoothly parametrized families of &quot;hard&quot;
functions, contrary to countable, discrete families known in the literature. An
example consequence of our gap theorems is the following: for every natural
number $k$ there exists a function representable by a ReLU DNN with $k^2$
hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$
hidden layers will require at least $\frac{1}{2}k^{k+1}-1$ total nodes.
Finally, for the family of $\mathbb{R}^n\to \mathbb{R}$ DNNs with ReLU
activations, we show a new lowerbound on the number of affine pieces, which is
larger than previous constructions in certain regimes of the network
architecture and most distinctively our lowerbound is demonstrated by an
explicit construction of a *smoothly parameterized* family of functions
attaining this scaling. Our construction utilizes the theory of zonotopes from
polyhedral theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Amitabh Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mianjy_P/0/1/0/all/0/1&quot;&gt;Poorya Mianjy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Anirbit Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00137">
<title>Pomegranate: fast and flexible probabilistic modeling in python. (arXiv:1711.00137v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00137</link>
<description rdf:parseType="Literal">&lt;p&gt;We present pomegranate, an open source machine learning package for
probabilistic modeling in Python. Probabilistic modeling encompasses a wide
range of methods that explicitly describe uncertainty using probability
distributions. Three widely used probabilistic models implemented in
pomegranate are general mixture models, hidden Markov models, and Bayesian
networks. A primary focus of pomegranate is to abstract away the complexities
of training models from their definition. This allows users to focus on
specifying the correct model for their application instead of being limited by
their understanding of the underlying algorithms. An aspect of this focus
involves the collection of additive sufficient statistics from data sets as a
strategy for training models. This approach trivially enables many useful
learning strategies, such as out-of-core learning, minibatch learning, and
semi-supervised learning, without requiring the user to consider how to
partition data or modify the algorithms to handle these tasks themselves.
pomegranate is written in Cython to speed up calculations and releases the
global interpreter lock to allow for built-in multithreaded parallelism, making
it competitive with---or outperform---other implementations of similar
algorithms. This paper presents an overview of the design choices in
pomegranate, and how they have enabled complex features to be supported by
simple code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schreiber_J/0/1/0/all/0/1&quot;&gt;Jacob Schreiber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07736">
<title>MaskGAN: Better Text Generation via Filling in the______. (arXiv:1801.07736v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07736</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural text generation models are often autoregressive language models or
seq2seq models. These models generate text by sampling words sequentially, with
each word conditioned on the previous word, and are state-of-the-art for
several machine translation and summarization benchmarks. These benchmarks are
often defined by validation perplexity even though this is not a direct measure
of the quality of the generated text. Additionally, these models are typically
trained via maxi- mum likelihood and teacher forcing. These methods are
well-suited to optimizing perplexity but can result in poor sample quality
since generating text requires conditioning on sequences of words that may have
never been observed at training time. We propose to improve sample quality
using Generative Adversarial Networks (GANs), which explicitly train the
generator to produce high quality samples and have shown a lot of success in
image generation. GANs were originally designed to output differentiable
values, so discrete language generation is challenging for them. We claim that
validation perplexity alone is not indicative of the quality of text generated
by a model. We introduce an actor-critic conditional GAN that fills in missing
text conditioned on the surrounding context. We show qualitatively and
quantitatively, evidence that this produces more realistic conditional and
unconditional text samples compared to a maximum likelihood trained model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedus_W/0/1/0/all/0/1&quot;&gt;William Fedus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09728">
<title>Modelling and Analysis of Temporal Preference Drifts Using A Component-Based Factorised Latent Approach. (arXiv:1802.09728v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09728</link>
<description rdf:parseType="Literal">&lt;p&gt;The changes in user preferences can originate from substantial reasons, like
personality shift, or transient and circumstantial ones, like seasonal changes
in item popularities. Disregarding these temporal drifts in modelling user
preferences can result in unhelpful recommendations. Moreover, different
temporal patterns can be associated with various preference domains, and
preference components and their combinations. These components comprise
preferences over features, preferences over feature values, conditional
dependencies between features, socially-influenced preferences, and bias. For
example, in the movies domain, the user can change his rating behaviour (bias
shift), her preference for genre over language (feature preference shift), or
start favouring drama over comedy (feature value preference shift). In this
paper, we first propose a novel latent factor model to capture the
domain-dependent component-specific temporal patterns in preferences. The
component-based approach followed in modelling the aspects of preferences and
their temporal effects enables us to arbitrarily switch components on and off.
We evaluate the proposed method on three popular recommendation datasets and
show that it significantly outperforms the most accurate state-of-the-art
static models. The experiments also demonstrate the greater robustness and
stability of the proposed dynamic model in comparison with the most successful
models to date. We also analyse the temporal behaviour of different preference
components and their combinations and show that the dynamic behaviour of
preference components is highly dependent on the preference dataset and domain.
Therefore, the results also highlight the importance of modelling temporal
effects but also underline the advantages of a component-based architecture
that is better suited to capture domain-specific balances in the contributions
of the aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafari_F/0/1/0/all/0/1&quot;&gt;F. Zafari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_I/0/1/0/all/0/1&quot;&gt;I. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baarslag_T/0/1/0/all/0/1&quot;&gt;T. Baarslag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07370">
<title>SufiSent - Universal Sentence Representations Using Suffix Encodings. (arXiv:1802.07370v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.07370</link>
<description rdf:parseType="Literal">&lt;p&gt;Computing universal distributed representations of sentences is a fundamental
task in natural language processing. We propose a method to learn such
representations by encoding the suffixes of word sequences in a sentence and
training on the Stanford Natural Language Inference (SNLI) dataset. We
demonstrate the effectiveness of our approach by evaluating it on the SentEval
benchmark, improving on existing approaches on several transfer tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1&quot;&gt;Siddhartha Brahma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07374">
<title>On the scaling of polynomial features for representation matching. (arXiv:1802.07374v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.07374</link>
<description rdf:parseType="Literal">&lt;p&gt;In many neural models, new features as polynomial functions of existing ones
are used to augment representations. Using the natural language inference task
as an example, we investigate the use of scaled polynomials of degree 2 and
above as matching features. We find that scaling degree 2 features has the
highest impact on performance, reducing classification error by 5% in the best
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1&quot;&gt;Siddhartha Brahma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10116">
<title>Generalized Byzantine-tolerant SGD. (arXiv:1802.10116v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1802.10116</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose three new robust aggregation rules for distributed synchronous
Stochastic Gradient Descent~(SGD) under a general Byzantine failure model. The
attackers can arbitrarily manipulate the data transferred between the servers
and the workers in the parameter server~(PS) architecture. We prove the
Byzantine resilience properties of these aggregation rules. Empirical analysis
shows that the proposed techniques outperform current approaches for realistic
use cases and Byzantine attack scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1&quot;&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_I/0/1/0/all/0/1&quot;&gt;Indranil Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10172">
<title>Semi-Supervised Learning Enabled by Multiscale Deep Neural Network Inversion. (arXiv:1802.10172v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.10172</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) provide state-of-the-art solutions in several
difficult machine perceptual tasks. However, their performance relies on the
availability of a large set of labeled training data, which limits the breadth
of their applicability. Hence, there is a need for new {\em semi-supervised
learning} methods for DNNs that can leverage both (a small amount of) labeled
and unlabeled training data. In this paper, we develop a general loss function
enabling DNNs of any topology to be trained in a semi-supervised manner without
extra hyper-parameters. As opposed to current semi-supervised techniques based
on topology-specific or unstable approaches, ours is both robust and general.
We demonstrate that our approach reaches state-of-the-art performance on the
SVHN ($9.82\%$ test error, with $500$ labels and wide Resnet) and CIFAR10
(16.38% test error, with 8000 labels and sigmoid convolutional neural network)
data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glotin_H/0/1/0/all/0/1&quot;&gt;Herve Glotin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10542">
<title>Memory-based Parameter Adaptation. (arXiv:1802.10542v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.10542</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have excelled on a wide range of problems, from vision
to language and game playing. Neural networks very gradually incorporate
information into weights as they process data, requiring very low learning
rates. If the training distribution shifts, the network is slow to adapt, and
when it does adapt, it typically performs badly on the training distribution
before the shift. Our method, Memory-based Parameter Adaptation, stores
examples in memory and then uses a context-based lookup to directly modify the
weights of a neural network. Much higher learning rates can be used for this
local adaptation, reneging the need for many iterations over similar data
before good predictions can be made. As our method is memory-based, it
alleviates several shortcomings of neural networks, such as catastrophic
forgetting, fast, stable acquisition of new knowledge, learning with an
imbalanced class labels, and fast learning during evaluation. We demonstrate
this on a range of supervised tasks: large-scale image classification and
language modelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sprechmann_P/0/1/0/all/0/1&quot;&gt;Pablo Sprechmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jayakumar_S/0/1/0/all/0/1&quot;&gt;Siddhant M. Jayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rae_J/0/1/0/all/0/1&quot;&gt;Jack W. Rae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pritzel_A/0/1/0/all/0/1&quot;&gt;Alexander Pritzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Badia_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe0; Puigdom&amp;#xe8;nech Badia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Uria_B/0/1/0/all/0/1&quot;&gt;Benigno Uria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hassabis_D/0/1/0/all/0/1&quot;&gt;Demis Hassabis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10567">
<title>Learning by Playing - Solving Sparse Reward Tasks from Scratch. (arXiv:1802.10567v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.10567</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in
the context of Reinforcement Learning (RL). SAC-X enables learning of complex
behaviors - from scratch - in the presence of multiple sparse reward signals.
To this end, the agent is equipped with a set of general auxiliary tasks, that
it attempts to learn simultaneously via off-policy RL. The key idea behind our
method is that active (learned) scheduling and execution of auxiliary policies
allows the agent to efficiently explore its environment - enabling it to excel
at sparse reward RL. Our experiments in several challenging robotic
manipulation settings demonstrate the power of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1&quot;&gt;Martin Riedmiller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafner_R/0/1/0/all/0/1&quot;&gt;Roland Hafner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampe_T/0/1/0/all/0/1&quot;&gt;Thomas Lampe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neunert_M/0/1/0/all/0/1&quot;&gt;Michael Neunert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Degrave_J/0/1/0/all/0/1&quot;&gt;Jonas Degrave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiele_T/0/1/0/all/0/1&quot;&gt;Tom Van de Wiele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1&quot;&gt;Volodymyr Mnih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1&quot;&gt;Nicolas Heess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1&quot;&gt;Jost Tobias Springenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10570">
<title>Statistical shape analysis in a Bayesian framework for shapes in two and three dimensions. (arXiv:1802.10570v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1802.10570</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we describe a novel shape classification method which is
embedded in the Bayesian paradigm. We discuss the modelling and the resulting
shape classification algorithm for two and three dimensional data shapes. We
conclude by evaluating the efficiency and efficacy of the proposed algorithm on
the Kimia shape database for the two dimensional case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsiftsi_T/0/1/0/all/0/1&quot;&gt;Thomai Tsiftsi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10282">
<title>Learning from Between-class Examples for Deep Sound Recognition. (arXiv:1711.10282v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10282</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning methods have achieved high performance in sound recognition
tasks. Deciding how to feed the training data is important for further
performance improvement. We propose a novel learning method for deep sound
recognition: Between-Class learning (BC learning). Our strategy is to learn a
discriminative feature space by recognizing the between-class sounds as
between-class sounds. We generate between-class sounds by mixing two sounds
belonging to different classes with a random ratio. We then input the mixed
sound to the model and train the model to output the mixing ratio. The
advantages of BC learning are not limited only to the increase in variation of
the training data; BC learning leads to an enlargement of Fisher&apos;s criterion in
the feature space and a regularization of the positional relationship among the
feature distributions of the classes. The experimental results show that BC
learning improves the performance on various sound recognition networks,
datasets, and data augmentation schemes, in which BC learning proves to be
always beneficial. Furthermore, we construct a new deep sound recognition
network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
performance surpasses the human level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokozume_Y/0/1/0/all/0/1&quot;&gt;Yuji Tokozume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item></rdf:RDF>