<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00368"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08194"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00130"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00224"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00263"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00339"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00509"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00594"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00832"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00837"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00858"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01036"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10938"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01509"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10071"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00052"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00175"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00338"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00770"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00836"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00977"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01018"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01079"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.10277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07169"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00804"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01630"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07233"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1809.00368">
<title>On overcoming the Curse of Dimensionality in Neural Networks. (arXiv:1809.00368v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1809.00368</link>
<description rdf:parseType="Literal">&lt;p&gt;Let $H$ be a reproducing Kernel Hilbert space. For $i=1,\cdots,N$, let
$x_i\in\mathbb{R}^{d}$ and $y_i\in\mathbb{R}^{m}$ comprise our dataset. Let
$f^*\in H$ be the unique global minimiser of the functional \begin{equation*}
J(f) = \frac{1}{2}\Vert f\Vert_{H}^{2} +
\frac{1}{N}\sum_{i=1}^{N}\frac{1}{2}\vert f(x_i)-y_i\vert^{2}. \end{equation*}
&lt;/p&gt;
&lt;p&gt;In this paper we show that for each $n\in\mathbb{N}$ there exists a two layer
network where the first layer has $nm$ number of basis functions
$\Phi_{x_{i_k},j}$ for $i_1,\cdots,i_n\in\{1,\cdots,N\}$, $j=1,\cdots,m$ and
the second layer takes a weighted summation of the first layer, such that the
functions $f_n$ realised by these networks satisfy \begin{equation*} \Vert
f_{n}-f^*\Vert_{H}\leq O(\frac{1}{\sqrt{n}})\enspace \text{for all}\enspace
n\in\mathbb{N}. \end{equation*}
&lt;/p&gt;
&lt;p&gt;Thus the error rate is independent of input dimension $d$, output dimension
$m$ and data size $N$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeressian_K/0/1/0/all/0/1&quot;&gt;Karen Yeressian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00596">
<title>Optimization Design of Decentralized Control for Complex Decentralized Systems. (arXiv:1809.00596v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1809.00596</link>
<description rdf:parseType="Literal">&lt;p&gt;A new method is developed to deal with the problem that a complex
decentralized control system needs to keep centralized control performance. The
systematic procedure emphasizes quickly finding the decentralized
subcontrollers that matching the closed-loop performance and robustness
characteristics of the centralized controller, which is featured by the fact
that GA is used to optimize the design of centralized H-infinity controller
K(s) and decentralized engine subcontroller KT(s), and that only one interface
variable needs to satisfy decentralized control system requirement according to
the proposed selection principle. The optimization design is motivated by the
implementation issues where it is desirable to reduce the time in trial and
error process and accurately find the best decentralized subcontrollers. The
method is applied to decentralized control system design for a short takeoff
and landing fighter. By comparing the simulation results of the decentralized
control system with those of the centralized control system, the target of the
decentralized control attains the performance and robustness of centralized
control is validated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Ying Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jiyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chen Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08481">
<title>Memory-Efficient Global Refinement of Decision-Tree Ensembles and its Application to Face Alignment. (arXiv:1702.08481v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08481</link>
<description rdf:parseType="Literal">&lt;p&gt;Ren et al. recently introduced a method for aggregating multiple decision
trees into a strong predictor by interpreting a path taken by a sample down
each tree as a binary vector and performing linear regression on top of these
vectors stacked together. They provided experimental evidence that the method
offers advantages over the usual approaches for combining decision trees
(random forests and boosting). The method truly shines when the regression
target is a large vector with correlated dimensions, such as a 2D face shape
represented with the positions of several facial landmarks. However, we argue
that their basic method is not applicable in many practical scenarios due to
large memory requirements. This paper shows how this issue can be solved
through the use of quantization and architectural changes of the predictor that
maps decision tree-derived encodings to the desired output.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markus_N/0/1/0/all/0/1&quot;&gt;Nenad Marku&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gogic_I/0/1/0/all/0/1&quot;&gt;Ivan Gogi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandzic_I/0/1/0/all/0/1&quot;&gt;Igor S. Pand&amp;#x17e;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahlberg_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rgen Ahlberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08150">
<title>Deep Learning in Spiking Neural Networks. (arXiv:1804.08150v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08150</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep learning has been a revolution in the field of machine
learning, for computer vision in particular. In this approach, a deep
(multilayer) artificial neural network (ANN) is trained in a supervised manner
using backpropagation. Huge amounts of labeled examples are required, but the
resulting classification accuracy is truly impressive, sometimes outperforming
humans. Neurons in an ANN are characterized by a single, static,
continuous-valued activation. Yet biological neurons use discrete spikes to
compute and transmit information, and the spike times, in addition to the spike
rates, matter. Spiking neural networks (SNNs) are thus more biologically
realistic than ANNs, and arguably the only viable option if one wants to
understand how the brain computes. SNNs are also more hardware friendly and
energy-efficient than ANNs, and are thus appealing for technology, especially
for portable devices. However, training deep SNNs remains a challenge. Spiking
neurons&apos; transfer function is usually non-differentiable, which prevents using
backpropagation. Here we review recent supervised and unsupervised methods to
train deep SNNs, and compare them in terms of accuracy, but also computational
cost and hardware friendliness. The emerging picture is that SNNs still lag
behind ANNs in terms of accuracy, but the gap is decreasing, and can even
vanish on some tasks, while the SNNs typically require much fewer operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavanaei_A/0/1/0/all/0/1&quot;&gt;Amirhossein Tavanaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghodrati_M/0/1/0/all/0/1&quot;&gt;Masoud Ghodrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1&quot;&gt;Saeed Reza Kheradpisheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timothee Masquelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony S. Maida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02942">
<title>SupportNet: solving catastrophic forgetting in class incremental learning with support data. (arXiv:1806.02942v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02942</link>
<description rdf:parseType="Literal">&lt;p&gt;A plain well-trained deep learning model often does not have the ability to
learn new knowledge without forgetting the previously learned knowledge, which
is known as the catastrophic forgetting. Here we propose a novel method,
SupportNet, to solve the catastrophic forgetting problem in class incremental
learning scenario efficiently and effectively. SupportNet combines the strength
of deep learning and support vector machine (SVM), where SVM is used to
identify the support data from the old data, which are fed to the deep learning
model together with the new data for further training so that the model can
review the essential information of the old data when learning the new
information. Two powerful consolidation regularizers are applied to ensure the
robustness of the learned model. Comprehensive experiments on various tasks,
including enzyme function prediction, subcellular structure classification and
breast tumor classification, show that SupportNet drastically outperforms the
state-of-the-art incremental learning methods and even reaches similar
performance as the deep learning model trained from scratch on both old and new
data. Our program is accessible at: https://github.com/lykaust15/SupportNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Lizhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yijie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuhui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08194">
<title>Towards Distributed Coevolutionary GANs. (arXiv:1807.08194v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08194</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have become one of the dominant
methods for deep generative modeling. Despite their demonstrated success on
multiple vision tasks, GANs are difficult to train and much research has been
dedicated towards understanding and improving their gradient-based learning
dynamics. Here, we investigate the use of coevolution, a class of black-box
(gradient-free) co-optimization techniques and a powerful tool in evolutionary
computing, as a supplement to gradient-based GAN training techniques.
Experiments on a simple model that exhibits several of the GAN gradient-based
dynamics (e.g., mode collapse, oscillatory behavior, and vanishing gradients)
show that coevolution is a promising framework for escaping degenerate GAN
training behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Dujaili_A/0/1/0/all/0/1&quot;&gt;Abdullah Al-Dujaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmiedlechner_T/0/1/0/all/0/1&quot;&gt;Tom Schmiedlechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemberg_a/0/1/0/all/0/1&quot;&gt;and Erik Hemberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1&quot;&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00130">
<title>Semi-supervised Learning on Graphs with Generative Adversarial Nets. (arXiv:1809.00130v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1809.00130</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate how generative adversarial nets (GANs) can help
semi-supervised learning on graphs. We first provide insights on working
principles of adversarial learning over graphs and then present GraphSGAN, a
novel approach to semi-supervised learning on graphs. In GraphSGAN, generator
and classifier networks play a novel competitive game. At equilibrium,
generator generates fake samples in low-density areas between subgraphs. In
order to discriminate fake samples from the real, classifier implicitly takes
the density property of subgraph into consideration. An efficient adversarial
learning algorithm has been developed to improve traditional normalized graph
Laplacian regularization with a theoretical guarantee. Experimental results on
several different genres of datasets show that the proposed GraphSGAN
significantly outperforms several state-of-the-art methods. GraphSGAN can be
also trained using mini-batch, thus enjoys the scalability advantage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00224">
<title>Finding the Answers with Definition Models. (arXiv:1809.00224v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00224</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by a previous attempt to answer crossword questions using neural
networks (Hill, Cho, Korhonen, &amp;amp; Bengio, 2015), this dissertation implements
extensions to improve the performance of this existing definition model on the
task of answering crossword questions. A discussion and evaluation of the
original implementation finds that there are some ways in which the recurrent
neural model could be extended. Insights from related fields neural language
modeling and neural machine translation provide the justification and means
required for these extensions. Two extensions are applied to the LSTM encoder,
first taking the average of LSTM states across the sequence and secondly using
a bidirectional LSTM, both implementations serve to improve model performance
on a definitions and crossword test set. In order to improve performance on
crossword questions, the training data is increased to include crossword
questions and answers, and this serves to improve results on definitions as
well as crossword questions. The final experiments are conducted using sub-word
unit segmentation, first on the source side and then later preliminary
experimentation is conducted to facilitate character-level output. Initially,
an exact reproduction of the baseline results proves unsuccessful. Despite
this, the extensions improve performance, allowing the definition model to
surpass the performance of the recurrent neural network variants of the
previous work (Hill, et al., 2015).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parry_J/0/1/0/all/0/1&quot;&gt;Jack Parry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00263">
<title>Stochastic Video Long-term Interpolation. (arXiv:1809.00263v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00263</link>
<description rdf:parseType="Literal">&lt;p&gt;Video interpolation is aiming to generate intermediate sequence between two
frames. While most existing studies require the two reference frames to be
consecutive, we propose a stochastic learning frame work that can infer a
possible intermediate sequence between a long interval. Therefore, our work
expands the usability of video interpolation in applications such as video
long-term temporal super-resolution, missing frames repair and motion dynamic
inference. Our model includes a deterministic estimation to guarantee the
spatial and temporal coherency among the generated frames and a stochastic
mechanism to sample sequences from possible realities. Like the studies of
stochastic video prediction, our generated sequences are both sharp and varied.
In addition, most of the motions are realistic and can smoothly transition from
the referred start frame to the end frame.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belhumeur_P/0/1/0/all/0/1&quot;&gt;Peter N. Belhumeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1&quot;&gt;Ulrich Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00329">
<title>Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet. (arXiv:1809.00329v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00329</link>
<description rdf:parseType="Literal">&lt;p&gt;Chinese pinyin input method engine (IME) converts pinyin into character so
that Chinese characters can be conveniently inputted into computer through
common keyboard. IMEs work relying on its core component, pinyin-to-character
conversion (P2C). Usually Chinese IMEs simply predict a list of character
sequences for user choice only according to user pinyin input at each turn.
However, Chinese inputting is a multi-turn online procedure, which can be
supposed to be exploited for further user experience promoting. This paper thus
for the first time introduces a sequence-to-sequence model with gated-attention
mechanism for the core task in IMEs. The proposed neural P2C model is learned
by encoding previous input utterance as extra context to enable our IME capable
of predicting character sequence with incomplete pinyin input. Our model is
evaluated in different benchmark datasets showing great user experience
improvement compared to traditional models, which demonstrates the first
engineering practice of building Chinese aided IME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yafang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hai Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00339">
<title>Chittron: An Automatic Bangla Image Captioning System. (arXiv:1809.00339v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00339</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic image caption generation aims to produce an accurate description of
an image in natural language automatically. However, Bangla, the fifth most
widely spoken language in the world, is lagging considerably in the research
and development of such domain. Besides, while there are many established data
sets to related to image annotation in English, no such resource exists for
Bangla yet. Hence, this paper outlines the development of &quot;Chittron&quot;, an
automatic image captioning system in Bangla. Moreover, to address the data set
availability issue, a collection of 16,000 Bangladeshi contextual images has
been accumulated and manually annotated in Bangla. This data set is then used
to train a model which integrates a pre-trained VGG16 image embedding model
with stacked LSTM layers. The model is trained to predict the caption when the
input is an image, one word at a time. The results show that the model has
successfully been able to learn a working language model and to generate
captions of images quite accurately in many cases. The results are evaluated
mainly qualitatively. However, BLEU scores are also reported. It is expected
that a better result can be obtained with a bigger and more varied data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Motiur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1&quot;&gt;Nabeel Mohammed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_N/0/1/0/all/0/1&quot;&gt;Nafees Mansoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Momen_S/0/1/0/all/0/1&quot;&gt;Sifat Momen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00385">
<title>Zero-shot User Intent Detection via Capsule Neural Networks. (arXiv:1809.00385v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00385</link>
<description rdf:parseType="Literal">&lt;p&gt;User intent detection plays a critical role in question-answering and dialog
systems. Most previous works treat intent detection as a classification problem
where utterances are labeled with predefined intents. However, it is
labor-intensive and time-consuming to label users&apos; utterances as intents are
diversely expressed and novel intents will continually be involved. Instead, we
study the zero-shot intent detection problem, which aims to detect emerging
user intents where no labeled utterances are currently available. We propose
two capsule-based architectures: INTENT-CAPSNET that extracts semantic features
from utterances and aggregates them to discriminate existing intents, and
INTENTCAPSNET-ZSL which gives INTENTCAPSNET the zero-shot learning ability to
discriminate emerging intents via knowledge transfer from existing intents.
Experiments on two real-world datasets show that our model not only can better
discriminate diversely expressed existing intents, but is also able to
discriminate emerging intents when no labeled utterances are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Congying Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiaohui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00509">
<title>DeFactoNLP: Fact Verification using Entity Recognition, TFIDF Vector Comparison and Decomposable Attention. (arXiv:1809.00509v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00509</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we describe DeFactoNLP, the system we designed for the FEVER
2018 Shared Task. The aim of this task was to conceive a system that can not
only automatically assess the veracity of a claim but also retrieve evidence
supporting this assessment from Wikipedia. In our approach, the Wikipedia
documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are
most similar to the vector of the claim and those documents whose names are
similar to those of the named entities (NEs) mentioned in the claim are
identified as the documents which might contain evidence. The sentences in
these documents are then supplied to a textual entailment recognition module.
This module calculates the probability of each sentence supporting the claim,
contradicting the claim or not providing any relevant information to assess the
veracity of the claim. Various features computed using these probabilities are
finally used by a Random Forest classifier to determine the overall
truthfulness of the claim. The sentences which support this classification are
returned as evidence. Our approach achieved a 0.4277 evidence F1-score, a
0.5136 label accuracy and a 0.3833 FEVER score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1&quot;&gt;Aniketh Janardhan Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocha_G/0/1/0/all/0/1&quot;&gt;Gil Rocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteves_D/0/1/0/all/0/1&quot;&gt;Diego Esteves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00549">
<title>Emergence of Communication in an Interactive World with Consistent Speakers. (arXiv:1809.00549v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00549</link>
<description rdf:parseType="Literal">&lt;p&gt;Training agents to communicate with one another given task-based supervision
only has attracted considerable attention recently, due to the growing interest
in developing models for human-agent interaction. Prior work on the topic
focused on simple environments, where training using policy gradient was
feasible despite the non-stationarity of the agents during training. In this
paper, we present a more challenging environment for testing the emergence of
communication from raw pixels, where training using policy gradient fails. We
propose a new model and training algorithm, that utilizes the structure of a
learned representation space to produce more consistent speakers at the initial
phases of training, which stabilizes learning. We empirically show that our
algorithm substantially improves performance compared to policy gradient. We
also propose a new alignment-based metric for measuring context-independence in
emerged communication and find our method increases context-independence
compared to policy gradient and other competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1&quot;&gt;Ben Bogin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1&quot;&gt;Mor Geva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00594">
<title>Adversarial Attack Type I: Generating False Positives. (arXiv:1809.00594v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00594</link>
<description rdf:parseType="Literal">&lt;p&gt;False positive and false negative rates are equally important for evaluating
the performance of a classifier. Adversarial examples by increasing false
negative rate have been studied in recent years. However, harming a classifier
by increasing false positive rate is almost blank, since it is much more
difficult to generate a new and meaningful positive than the negative. To
generate false positives, a supervised generative framework is proposed in this
paper. Experiment results show that our method is practical and effective to
generate those adversarial examples on large-scale image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Sanli Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingjian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00794">
<title>Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation. (arXiv:1809.00794v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00794</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Texar, an open-source toolkit aiming to support the broad set of
text generation tasks that transforms any inputs into natural language, such as
machine translation, summarization, dialog, content manipulation, and so forth.
With the design goals of modularity, versatility, and extensibility in mind,
Texar extracts common patterns underlying the diverse tasks and methodologies,
creates a library of highly reusable modules and functionalities, and allows
arbitrary model architectures and algorithmic paradigms. In Texar, model
architecture, losses, and learning processes are fully decomposed. Modules at
high concept level can be freely assembled or plugged in/swapped out. These
features make Texar particularly suitable for researchers and practitioners to
do fast prototyping and experimentation, as well as foster technique sharing
across different text generation tasks. We provide case studies to demonstrate
the use and advantage of the toolkit. Texar is released under Apache license
2.0 at https://github.com/asyml/texar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haoran Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Bowen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wentao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xingjiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Lianhui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xuezhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hector Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00832">
<title>Improving the Expressiveness of Deep Learning Frameworks with Recursion. (arXiv:1809.00832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00832</link>
<description rdf:parseType="Literal">&lt;p&gt;Recursive neural networks have widely been used by researchers to handle
applications with recursively or hierarchically structured data. However,
embedded control flow deep learning frameworks such as TensorFlow, Theano,
Caffe2, and MXNet fail to efficiently represent and execute such neural
networks, due to lack of support for recursion. In this paper, we add recursion
to the programming model of existing frameworks by complementing their design
with recursive execution of dataflow graphs as well as additional APIs for
recursive definitions. Unlike iterative implementations, which can only
understand the topological index of each node in recursive data structures, our
recursive implementation is able to exploit the recursive relationships between
nodes for efficient execution based on parallel computation. We present an
implementation on TensorFlow and evaluation results with various recursive
neural network models, showing that our recursive implementation not only
conveys the recursive nature of recursive neural networks better than other
implementations, but also uses given resources more effectively to reduce
training and inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1&quot;&gt;Eunji Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Joo Seong Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Soojeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gyeong-In Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_B/0/1/0/all/0/1&quot;&gt;Byung-Gon Chun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00837">
<title>Metabolize Neural Network. (arXiv:1809.00837v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00837</link>
<description rdf:parseType="Literal">&lt;p&gt;The metabolism of cells is the most basic and important part of human
function. Neural networks in deep learning stem from neuronal activity. It is
self-evident that the significance of metabolize neuronal network(MetaNet) in
model construction. In this study, we explore neuronal metabolism for shallow
network from proliferation and autophagy two aspects. First, we propose
different neuron proliferate methods that constructive the selfgrowing network
in metabolism cycle. Proliferate neurons alleviate resources wasting and
insufficient model learning problem when network initializes more or less
parameters. Then combined with autophagy mechanism in the process of model self
construction to ablate under-expressed neurons. The MetaNet can automatically
determine the number of neurons during training, further, save more resource
consumption. We verify the performance of the proposed methods on datasets:
MNIST, Fashion-MNIST and CIFAR-10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wenming Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Mingnan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00852">
<title>Multi-target Unsupervised Domain Adaptation without Exactly Shared Categories. (arXiv:1809.00852v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00852</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (UDA) aims to learn the unlabeled target
domain by transferring the knowledge from the labeled source domain. To date,
most of the existing works focus on the scenario of one source domain and one
target domain (1S1T), and just a few works concern UDA of multiple source
domains and one target domain (mS1T) for solving the insufficient knowledge
problem with single source domain. While, to the best of our knowledge, almost
no work concerns the scenario of one source domain and multiple target domains
(1SmT). In the 1SmT, these unlabeled target domains may not necessarily share
the same categories, therefore, in contrast to mS1T, 1SmT is more challenging.
In this paper, we study such a new UDA scenario, and accordingly propose a UDA
framework (PA-1SmT) through the model parameter adaptation among these target
domains and the source domain. A key ingredient of our framework is that we
firstly construct a model parameter dictionary which is shared not only between
the source domain and the individual target domains but also among the multiple
target domains. Then we use it to sparsely represent individual target
parameters, which attains knowledge transfer among the domains. Such a new
knowledge transfer is different from existing popular methods for UDA, such as
subspace alignment, distribution matching etc., and can also be directly used
for DA of privacy protection due to the fact that the knowledge is transferred
just via the model parameters rather than data itself. Finally, our
experimental results on three domain adaptation benchmark datasets demonstrate
the superiority of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Huanhuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Menglei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Songcan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00858">
<title>Non-monotonic Reasoning in Deductive Argumentation. (arXiv:1809.00858v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00858</link>
<description rdf:parseType="Literal">&lt;p&gt;Argumentation is a non-monotonic process. This reflects the fact that
argumentation involves uncertain information, and so new information can cause
a change in the conclusions drawn. However, the base logic does not need to be
non-monotonic. Indeed, most proposals for structured argumentation use a
monotonic base logic (e.g. some form of modus ponens with a rule-based
language, or classical logic). Nonetheless, there are issues in capturing
defeasible reasoning in argumentation including choice of base logic and
modelling of defeasible knowledge. And there are insights and tools to be
harnessed for research in non-monontonic logics. We consider some of these
issues in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_A/0/1/0/all/0/1&quot;&gt;Anthony Hunter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00946">
<title>Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing GANs. (arXiv:1809.00946v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00946</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for translating unlabeled images from one domain into
analog images in another domain. We employ a progressively growing
skip-connected encoder-generator structure and train it with a GAN loss for
realistic output, a cycle consistency loss for maintaining same-domain
translation identity, and a semantic consistency loss that encourages the
network to keep the input semantic features in the output. We apply our
framework on the task of translating face images, and show that it is capable
of learning semantic mappings for face images with no supervised one-to-one
image mapping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jerry Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00953">
<title>Deep Learning Based Vehicle Make-Model Classification. (arXiv:1809.00953v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00953</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problems of vehicle make &amp;amp; model classification. Some
of the main challenges are reaching high classification accuracy and reducing
the annotation time of the images. To address these problems, we have created a
fine-grained database using online vehicle marketplaces of Turkey. A pipeline
is proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN
(Convolutional Neural Network) model to train on the database. In the pipeline,
we first detect the vehicles by following an algorithm which reduces the time
for annotation. Then, we feed them into the CNN model. It is reached
approximately 4% better classification accuracy result than using a
conventional CNN model. Next, we propose to use the detected vehicles as ground
truth bounding box (GTBB) of the images and feed them into an SSD model in
another pipeline. At this stage, it is reached reasonable classification
accuracy result without using perfectly shaped GTBB. Lastly, an application is
implemented in a use case by using our proposed pipelines. It detects the
unauthorized vehicles by comparing their license plate numbers and make &amp;amp;
models. It is assumed that license plates are readable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satar_B/0/1/0/all/0/1&quot;&gt;Burak Satar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dirik_A/0/1/0/all/0/1&quot;&gt;Ahmet Emir Dirik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00979">
<title>Regularizing Matrix Factorization with User and Item Embeddings for Recommendation. (arXiv:1809.00979v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.00979</link>
<description rdf:parseType="Literal">&lt;p&gt;Following recent successes in exploiting both latent factor and word
embedding models in recommendation, we propose a novel Regularized
Multi-Embedding (RME) based recommendation model that simultaneously
encapsulates the following ideas via decomposition: (1) which items a user
likes, (2) which two users co-like the same items, (3) which two items users
often co-liked, and (4) which two items users often co-disliked. In
experimental validation, the RME outperforms competing state-of-the-art models
in both explicit and implicit feedback datasets, significantly improving
Recall@5 by 5.9~7.0%, NDCG@20 by 4.3~5.6%, and MAP@10 by 7.9~8.9%. In addition,
under the cold-start scenario for users with the lowest number of interactions,
against the competing models, the RME outperforms NDCG@5 by 20.2% and 29.4% in
MovieLens-10M and MovieLens-20M datasets, respectively. Our datasets and source
code are available at: https://github.com/thanhdtran/RME.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thanh Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyumin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yiming Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongwon Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01036">
<title>A Roadmap for the Value-Loading Problem. (arXiv:1809.01036v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.01036</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the value-loading problem. This is the problem of encoding moral
values into an AI agent interacting with a complex environment. Like many
before, we argue that this is both a major concern and an extremely challenging
problem. Solving it will likely require years, if not decades, of
multidisciplinary work by teams of top scientists and experts. Given how
uncertain the timeline of human-level AI research is, we thus argue that a
pragmatic partial solution should be designed as soon as possible. To this end,
we propose a preliminary research program. This roadmap identifies several key
steps. We hope that this will allow scholars, engineers and decision-makers to
better grasp the upcoming difficulties, and to foresee how they can best
contribute to the global effort.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1&quot;&gt;L&amp;#xea; Nguy&amp;#xea;n Hoang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01124">
<title>Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering. (arXiv:1809.01124v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.01124</link>
<description rdf:parseType="Literal">&lt;p&gt;Question answering is an important task for autonomous agents and virtual
assistants alike and was shown to support the disabled in efficiently
navigating an overwhelming environment. Many existing methods focus on
observation-based questions, ignoring our ability to seamlessly combine
observed content with general knowledge. To understand interactions with a
knowledge base, a dataset has been introduced recently and keyword matching
techniques were shown to yield compelling results despite being vulnerable to
misconceptions due to synonyms and homographs. To address this issue, we
develop a learning-based approach which goes straight to the facts via a
learned embedding space. We demonstrate state-of-the-art results on the
challenging recently introduced fact-based visual question answering dataset,
outperforming competing methods by more than 5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_M/0/1/0/all/0/1&quot;&gt;Medhini Narasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1&quot;&gt;Alexander G. Schwing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07243">
<title>Personalizing Dialogue Agents: I have a dog, do you have pets too?. (arXiv:1801.07243v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07243</link>
<description rdf:parseType="Literal">&lt;p&gt;Chit-chat models are known to have several problems: they lack specificity,
do not display a consistent personality and are often not very captivating. In
this work we present the task of making chit-chat more engaging by conditioning
on profile information. We collect data and train models to (i) condition on
their given profile information; and (ii) information about the person they are
talking to, resulting in improved dialogues, as measured by next utterance
prediction. Since (ii) is initially unknown our model is trained to engage its
partner with personal topics, and we show the resulting dialogue can be used to
predict profile information about the interlocutors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Saizheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1&quot;&gt;Emily Dinan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1&quot;&gt;Jack Urbanek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1&quot;&gt;Arthur Szlam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1&quot;&gt;Douwe Kiela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jason Weston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00538">
<title>Investigating Capsule Networks with Dynamic Routing for Text Classification. (arXiv:1804.00538v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00538</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we explore capsule networks with dynamic routing for text
classification. We propose three strategies to stabilize the dynamic routing
process to alleviate the disturbance of some noise capsules which may contain
&quot;background&quot; information or have not been successfully trained. A series of
experiments are conducted with capsule networks on six text classification
benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets,
which shows the effectiveness of capsule networks for text classification. We
additionally show that capsule networks exhibit significant improvement when
transfer single-label to multi-label text classification over strong baseline
methods. To the best of our knowledge, this is the first work that capsule
networks have been empirically investigated for text modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jianbo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zeyang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Suofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08010">
<title>Multi-Modal Coreference Resolution with the Correlation between Space Structures. (arXiv:1804.08010v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08010</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal data is becoming more common in big data background. Finding the
semantically similar objects from different modality is one of the heart
problems of multi-modal learning. Most of the current methods try to learn the
inter-modal correlation with extrinsic supervised information, while intrinsic
structural information of each modality is neglected. The performance of these
methods heavily depends on the richness of training samples. However, obtaining
the multi-modal training samples is still a labor and cost intensive work. In
this paper, we bring a extrinsic correlation between the space structures of
each modalities in coreference resolution. With this correlation, a
semi-supervised learning model for multi-modal coreference resolution is
proposed. We firstly extract high-level features of images and text, then
compute the distances of each object from some reference points to build the
space structure of each modality. With a shared reference point set, the space
structures of each modality are correlated. We employ the correlation to build
a commonly shared space that the semantic distance between multi-modal objects
can be computed directly. The experiments on two multi-modal datasets show that
our model performs better than the existing methods with insufficient training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qibin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1&quot;&gt;Xingchun Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jianjun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaolei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongmei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10938">
<title>Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond. (arXiv:1804.10938v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10938</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic understanding of human affect using visual signals is of great
importance in everyday human-machine interactions. Appraising human emotional
states, behaviors and reactions displayed in real-world settings, can be
accomplished using latent continuous dimensions (e.g., the circumplex model of
affect). Valence (i.e., how positive or negative is an emotion) and arousal
(i.e., power of the activation of the emotion) constitute the most popular and
effective affect representations. Nevertheless, the majority of collected
datasets this far, although containing naturalistic emotional states, have been
captured in highly controlled recording conditions. In this paper, we introduce
the Aff-Wild benchmark for training and evaluating affect recognition
algorithms. We also report on the results of the First Affect-in-the-wild
Challenge (Aff-Wild Challenge) that was recently organized on the Aff-Wild
database, and was the first ever challenge on the estimation of valence and
arousal in-the-wild. Furthermore, we design and extensively train an end-to-end
deep neural architecture which performs prediction of continuous emotion
dimensions based on visual cues. The proposed deep learning architecture,
AffWildNet, includes convolutional and recurrent neural network (CNN-RNN)
layers, exploiting the invariant properties of convolutional features, while
also modeling temporal dynamics that arise in human behavior via the recurrent
layers. The AffWildNet produced state-of-the-art results on the Aff-Wild
Challenge. We then exploit the AffWild database for learning features, which
can be used as priors for achieving best performances both for dimensional, as
well as categorical emotion recognition, using the RECOLA, AFEW-VA and EmotiW
2017 datasets, compared to all other methods designed for the same goal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1&quot;&gt;Dimitrios Kollias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzirakis_P/0/1/0/all/0/1&quot;&gt;Panagiotis Tzirakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolaou_M/0/1/0/all/0/1&quot;&gt;Mihalis A. Nicolaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papaioannou_A/0/1/0/all/0/1&quot;&gt;Athanasios Papaioannou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guoying Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotsia_I/0/1/0/all/0/1&quot;&gt;Irene Kotsia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01509">
<title>SURREAL: SUbgraph Robust REpresentAtion Learning. (arXiv:1805.01509v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01509</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of graph embeddings or node representation learning in a variety
of downstream tasks, such as node classification, link prediction, and
recommendation systems, has led to their popularity in recent years.
Representation learning algorithms aim to preserve local and global network
structure by identifying node neighborhood notions. However, many existing
algorithms generate embeddings that fail to properly preserve the network
structure, or lead to unstable representations due to random processes (e.g.,
random walks to generate context) and, thus, cannot generate to multi-graph
problems. In this paper, we propose RECS, a novel, stable graph embedding
algorithmic framework. RECS learns graph representations using connection
subgraphs by employing the analogy of graphs with electrical circuits. It
preserves both local and global connectivity patterns, and addresses the issue
of high-degree nodes. Further, it exploits the strength of weak ties and
meta-data that have been neglected by baselines. The experiments show that RECS
outperforms state-of-the-art algorithms by up to 36.85% on multi-label
classification problem. Further, in contrast to baselines, RECS, being
deterministic, is completely stable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Sayouri_S/0/1/0/all/0/1&quot;&gt;Saba A. Al-Sayouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1&quot;&gt;Danai Koutra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_S/0/1/0/all/0/1&quot;&gt;Sarah S. Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10071">
<title>Learning Existing Social Conventions in Markov Games. (arXiv:1806.10071v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10071</link>
<description rdf:parseType="Literal">&lt;p&gt;In order for artificial agents to coordinate effectively with people, they
must act consistently with existing conventions (e.g. how to navigate in
traffic, which language to speak, or how to work with teammates). A group&apos;s
conventions can be viewed as a choice of equilibrium in a coordination game. We
consider the problem of an agent learning a policy for a coordination game in a
simulated environment and then using this policy when it enters an existing
group. When there are multiple possible conventions we show that learning a
policy via multi-agent reinforcement learning (MARL) is likely to find policies
which achieve high payoffs at training time but fail to coordinate with the
real group into which the agent enters. We assume access to a small number of
samples of behavior from the true convention and show that we can augment the
MARL objective to help it find policies consistent with the real group&apos;s
convention. In three environments from the literature - traffic, communication,
and team coordination - we observe that augmenting MARL with a small amount of
imitation learning greatly increases the probability that the strategy found by
MARL fits well with the existing social convention. We show that this works
even in an environment where standard training methods very rarely find the
true convention of the agent&apos;s partners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1&quot;&gt;Adam Lerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peysakhovich_A/0/1/0/all/0/1&quot;&gt;Alexander Peysakhovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06570">
<title>Detecting cognitive impairments by agreeing on interpretations of linguistic features. (arXiv:1808.06570v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06570</link>
<description rdf:parseType="Literal">&lt;p&gt;Linguistic features have shown promising applications for detecting various
cognitive impairments. To improve detection accuracies, increasing the amount
of data or the number of linguistic features have been two applicable
approaches. However, acquiring additional clinical data could be expensive, and
hand-carving features are burdensome. In this paper, we take a third approach,
putting forward Consensus Networks (CN), a framework to classify after reaching
agreements between modalities. We divide the linguistic features into
non-overlapping subsets according to their modalities, let neural networks
learn low-dimensional representations that agree with each other. These
representations are passed into a classifier network. All neural networks are
optimized iteratively. In this paper, we also present two methods that
empirically improve the performance of CN. We then present ablation studies to
illustrate the effectiveness of modality division. To understand further what
happens in Consensus Networks, we visualize the interpretation vectors during
training procedures. They demonstrate symmetry in an aggregate manner. Overall,
using all of the 413 linguistic features, our models significantly outperform
traditional classifiers, which are used by the state-of-the-art papers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zining Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1&quot;&gt;Jekaterina Novikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1&quot;&gt;Frank Rudzicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08003">
<title>Approximate Distribution Matching for Sequence-to-Sequence Learning. (arXiv:1808.08003v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08003</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequence-to-Sequence models were introduced to tackle many real-life problems
like machine translation, summarization, image captioning, etc. The standard
optimization algorithms are mainly based on example-to-example matching like
maximum likelihood estimation, which is known to suffer from data sparsity
problem. Here we present an alternate view to explain sequence-to-sequence
learning as a distribution matching problem, where each source or target
example is viewed to represent a local latent distribution in the source or
target domain. Then, we interpret sequence-to-sequence learning as learning a
transductive model to transform the source local latent distributions to match
their corresponding target distributions. In our framework, we approximate both
the source and target latent distributions with recurrent neural networks
(augmenter). During training, the parallel augmenters learn to better
approximate the local latent distributions, while the sequence prediction model
learns to minimize the KL-divergence of the transformed source distributions
and the approximated target distributions. This algorithm can alleviate the
data sparsity issues in sequence learning by locally augmenting more unseen
data pairs and increasing the model&apos;s robustness. Experiments conducted on
machine translation and image captioning consistently demonstrate the
superiority of our proposed algorithm over the other competing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shujie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09333">
<title>Bridging Knowledge Gaps in Neural Entailment via Symbolic Models. (arXiv:1808.09333v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09333</link>
<description rdf:parseType="Literal">&lt;p&gt;Most textual entailment models focus on lexical gaps between the premise text
and the hypothesis, but rarely on knowledge gaps. We focus on filling these
knowledge gaps in the Science Entailment task, by leveraging an external
structured knowledge base (KB) of science facts. Our new architecture combines
standard neural entailment models with a knowledge lookup module. To facilitate
this lookup, we propose a fact-level decomposition of the hypothesis, and
verifying the resulting sub-facts against both the textual premise and the
structured KB. Our model, NSnet, learns to aggregate predictions from these
heterogeneous data formats. On the SciTail dataset, NSnet outperforms a simpler
combination of the two predictions by 3% and the base entailment model by 5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dongyeop Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1&quot;&gt;Tushar Khot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1&quot;&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00052">
<title>Your Actions or Your Associates? Predicting Certification and Dropout in MOOCs with Behavioral and Social Features. (arXiv:1809.00052v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1809.00052</link>
<description rdf:parseType="Literal">&lt;p&gt;The high level of attrition and low rate of certification in Massive Open
Online Courses (MOOCs) has prompted a great deal of research. Prior researchers
have focused on predicting dropout based upon behavioral features such as
student confusion, click-stream patterns, and social interactions. However, few
studies have focused on combining student logs with forum data.
&lt;/p&gt;
&lt;p&gt;In this work, we use data from two different offerings of the same MOOC. We
conduct a survival analysis to identify likely dropouts. We then examine two
classes of features, social and behavioral, and apply a combination of modeling
and feature-selection methods to identify the most relevant features to predict
both dropout and certification. We examine the utility of three different model
types and we consider the impact of different definitions of dropout on the
predictors. Finally, we assess the reliability of the models over time by
evaluating whether or not models from week 1 can predict dropout in week 2, and
so on. The outcomes of this study will help instructors identify students
likely to fail or dropout as soon as the first two weeks and provide them with
more support.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gitinabard_N/0/1/0/all/0/1&quot;&gt;Niki Gitinabard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoshnevisan_F/0/1/0/all/0/1&quot;&gt;Farzaneh Khoshnevisan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1&quot;&gt;Collin F. Lynch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1&quot;&gt;Elle Yuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00065">
<title>MULDEF: Multi-model-based Defense Against Adversarial Examples for Neural Networks. (arXiv:1809.00065v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00065</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite being popularly used in many application domains such as image
recognition and classification, neural network models have been found to be
vulnerable to adversarial examples: given a model and an example correctly
classified by the model, an adversarial example is a new example formed by
applying small perturbation (imperceptible to human) on the given example so
that the model misclassifies the new example. Adversarial examples can pose
potential risks on safety or security in real-world applications. In recent
years, given a vulnerable model, defense approaches, such as adversarial
training and defensive distillation, improve the model to make it more robust
against adversarial examples. However, based on the improved model, attackers
can still generate adversarial examples to successfully attack the model. To
address such limitation, we propose a new defense approach, named MULDEF, based
on the design principle of diversity. Given a target model (as a seed model)
and an attack approach to be defended against, MULDEF constructs additional
models (from the seed model) together with the seed model to form a family of
models, such that the models are complementary to each other to accomplish
robustness diversity (i.e., one model&apos;s adversarial examples typically do not
become other models&apos; adversarial examples), while maintaining about the same
accuracy for normal examples. At runtime, given an input example, MULDEF
randomly selects a model from the family to be applied on the given example.
The robustness diversity of the model family and the random selection of a
model from the family together lower the success rate of attacks. Our
evaluation results show that MULDEF substantially improves the target model&apos;s
accuracy on adversarial examples by 35-50% and 2-10% in the white-box and
black-box attack scenarios, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srisakaokul_S/0/1/0/all/0/1&quot;&gt;Siwakorn Srisakaokul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zexuan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tao Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00068">
<title>Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection. (arXiv:1809.00068v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00068</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring domain relevance of data and identifying or selecting well-fit
domain data for machine translation (MT) is a well-studied topic, but denoising
is not yet. Denoising is concerned with a different type of data quality and
tries to reduce the negative impact of data noise on MT training, in
particular, neural MT (NMT) training. This paper generalizes methods for
measuring and selecting data for domain MT and applies them to denoising NMT
training. The proposed approach uses trusted data and a denoising curriculum
realized by online data selection. Intrinsic and extrinsic evaluations of the
approach show its significant effectiveness for NMT to train on data with
severe noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1&quot;&gt;Taro Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Macduff Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakagawa_T/0/1/0/all/0/1&quot;&gt;Tetsuji Nakagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chelba_C/0/1/0/all/0/1&quot;&gt;Ciprian Chelba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00101">
<title>Attentive Crowd Flow Machines. (arXiv:1809.00101v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00101</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic flow prediction is crucial for urban traffic management and public
safety. Its key challenges lie in how to adaptively integrate the various
factors that affect the flow changes. In this paper, we propose a unified
neural network module to address this problem, called Attentive Crowd Flow
Machine~(ACFM), which is able to infer the evolution of the crowd flow by
learning dynamic representations of temporally-varying data with an attention
mechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units
connected with a convolutional layer for spatial weight prediction. The first
LSTM takes the sequential flow density representation as input and generates a
hidden state at each time-step for attention map inference, while the second
LSTM aims at learning the effective spatial-temporal feature expression from
attentionally weighted crowd flow features. Based on the ACFM, we further build
a deep architecture with the application to citywide crowd flow prediction,
which naturally incorporates the sequential and periodic data as well as other
external influences. Extensive experiments on two standard benchmarks (i.e.,
crowd flow in Beijing and New York City) show that the proposed method achieves
significant improvements over the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiefeng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bowen Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00175">
<title>Hyperparameter Learning for Conditional Mean Embeddings with Rademacher Complexity Bounds. (arXiv:1809.00175v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.00175</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional mean embeddings are nonparametric models that encode conditional
expectations in a reproducing kernel Hilbert space. While they provide a
flexible and powerful framework for probabilistic inference, their performance
is highly dependent on the choice of kernel and regularization hyperparameters.
Nevertheless, current hyperparameter tuning methods predominantly rely on
expensive cross validation or heuristics that is not optimized for the
inference task. For conditional mean embeddings with categorical targets and
arbitrary inputs, we propose a hyperparameter learning framework based on
Rademacher complexity bounds to prevent overfitting by balancing data fit
against model complexity. Our approach only requires batch updates, allowing
scalable kernel hyperparameter tuning without invoking kernel approximations.
Experiments demonstrate that our learning framework outperforms competing
methods, and can be further extended to incorporate and learn deep neural
network weights to improve generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsu_K/0/1/0/all/0/1&quot;&gt;Kelvin Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramos_F/0/1/0/all/0/1&quot;&gt;Fabio Ramos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00338">
<title>Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition. (arXiv:1809.00338v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00338</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable progress in face recognition related technologies,
reliably recognizing faces across ages still remains a big challenge. The
appearance of a human face changes substantially over time, resulting in
significant intra-class variations. As opposed to current techniques for
age-invariant face recognition, which either directly extract age-invariant
features for recognition, or first synthesize a face that matches target age
before feature extraction, we argue that it is more desirable to perform both
tasks jointly so that they can leverage each other. To this end, we propose a
deep Age-Invariant Model (AIM) for face recognition in the wild with three
distinct novelties. First, AIM presents a novel unified deep architecture
jointly performing cross-age face synthesis and recognition in a mutual
boosting way. Second, AIM achieves continuous face rejuvenation/aging with
remarkable photorealistic and identity-preserving properties, avoiding the
requirement of paired data and the true age of testing samples. Third, we
develop effective and novel training strategies for end-to-end learning the
whole deep architecture, which generates powerful age-invariant face
representations explicitly disentangled from the age variation. Moreover, we
propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset
to facilitate existing efforts and push the frontiers of age-invariant face
recognition research. Extensive experiments on both our CAFR and several other
cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the
proposed AIM model over the state-of-the-arts. Benchmarking our model on one of
the most popular unconstrained face recognition datasets IJB-C additionally
verifies the promising generalizability of AIM in recognizing faces in the
wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1&quot;&gt;Haochong Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Fang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Lin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pranata_S/0/1/0/all/0/1&quot;&gt;Sugiri Pranata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Shengmei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hengzhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00403">
<title>Effective Exploration for Deep Reinforcement Learning via Bootstrapped Q-Ensembles under Tsallis Entropy Regularization. (arXiv:1809.00403v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00403</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently deep reinforcement learning (DRL) has achieved outstanding success
on solving many difficult and large-scale RL problems. However the high sample
cost required for effective learning often makes DRL unaffordable in
resource-limited applications. With the aim of improving sample efficiency, we
will develop a new DRL algorithm in this paper that seamless integrates
entropy-induced and bootstrap-induced techniques for efficient and deep
exploration of the learning environment. Specifically, a general form of
Tsallis entropy regularizer will be utilized to drive entropy-induced
exploration based on efficient approximation of optimal action-selection
policies. Different from many existing works that rely on action dithering
strategies for exploration, our algorithm is efficient in exploring actions
with clear exploration value. Meanwhile, by employing an ensemble of Q-networks
under varied Tsallis entropy regularization, the diversity of the ensemble can
be further enhanced to enable effective bootstrap-induced exploration.
Experiments on Atari game playing tasks clearly demonstrate that our new
algorithm can achieve more efficient and effective exploration for DRL, in
comparison to recently proposed exploration methods including Bootstrapped Deep
Q-Network and UCB Q-Ensemble.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yiming Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00758">
<title>End-to-end Multimodal Emotion and Gender Recognition with Dynamic Weights of Joint Loss. (arXiv:1809.00758v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00758</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning (MTL) is one of the method for improving generalizability
of multiple tasks. In order to perform multiple classification tasks with one
neural network model, the losses of each task should be combined. Previous
studies have mostly focused on prediction of multiple tasks using joint loss
with static weights for training model. Choosing weights between tasks have not
taken any considerations while it is set by uniformly or empirically. In this
study, we propose a method to make joint loss using dynamic weights to improve
total performance not an individual performance of tasks, and apply this method
to end-to-end multimodal emotion and gender recognition model using audio and
video data. This approach provides proper weights for each loss of the tasks
when training ends. In our experiment, a performance of emotion and gender
recognition with proposed method shows lower joint loss which is computed as
negative log-likelihood than the one with static weights of joint loss. Also,
our proposed model shows better generalizability than compared models. In our
best knowledge, this research shows the strength of dynamic weights of joint
loss for maximizing total performance at first in emotion and gender
recognition task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_M/0/1/0/all/0/1&quot;&gt;Myungsu Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tae-Ho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Young Hoon Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jun-Woo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soo-Young Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00770">
<title>Transferring Deep Reinforcement Learning with Adversarial Objective and Augmentation. (arXiv:1809.00770v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00770</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past few years, deep reinforcement learning has been proven to solve
problems which have complex states like video games or board games. The next
step of intelligent agents would be able to generalize between tasks, and using
prior experience to pick up new skills more quickly. However, most
reinforcement learning algorithms for now are often suffering from catastrophic
forgetting even when facing a very similar target task. Our approach enables
the agents to generalize knowledge from a single source task, and boost the
learning progress with a semisupervised learning method when facing a new task.
We evaluate this approach on Atari games, which is a popular reinforcement
learning benchmark, and show that it outperforms common baselines based on
pre-training and fine-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_S/0/1/0/all/0/1&quot;&gt;Shu-Hsuan Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_I/0/1/0/all/0/1&quot;&gt;I-Chao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bing-Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00800">
<title>Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions. (arXiv:1809.00800v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00800</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new kernel-based co-occurrence measure that can
be applied to sparse linguistic expressions (e.g., sentences) with a very short
learning time, as an alternative to pointwise mutual information (PMI). As well
as deriving PMI from mutual information, we derive this new measure from the
Hilbert--Schmidt independence criterion (HSIC); thus, we call the new measure
the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of
PMI that allows various similarity metrics (e.g., sentence embeddings) to be
plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast
(linear in the size of the data) matrix calculations regardless of whether we
use linear or nonlinear kernels. Empirically, in a dialogue response selection
task, PHSIC is learned thousands of times faster than an RNN-based PMI while
outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is
beneficial as a criterion of a data selection task for machine translation
owing to its ability to give high (low) scores to a consistent (inconsistent)
pair with other pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1&quot;&gt;Sho Yokoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1&quot;&gt;Sosuke Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukumizu_K/0/1/0/all/0/1&quot;&gt;Kenji Fukumizu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1&quot;&gt;Jun Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1&quot;&gt;Kentaro Inui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00836">
<title>A Recurrent Neural Network for Sentiment Quantification. (arXiv:1809.00836v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00836</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantification is a supervised learning task that consists in predicting,
given a set of classes C and a set D of unlabelled items, the prevalence (or
relative frequency) p(c|D) of each class c in C. Quantification can in
principle be solved by classifying all the unlabelled items and counting how
many of them have been attributed to each class. However, this &quot;classify and
count&quot; approach has been shown to yield suboptimal quantification accuracy;
this has established quantification as a task of its own, and given rise to a
number of methods specifically devised for it. We propose a recurrent neural
network architecture for quantification (that we call QuaNet) that observes the
classification predictions to learn higher-order &quot;quantification embeddings&quot;,
which are then refined by incorporating quantification predictions of simple
classify-and-count-like methods. We test {QuaNet on sentiment quantification on
text, showing that it substantially outperforms several state-of-the-art
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esuli_A/0/1/0/all/0/1&quot;&gt;Andrea Esuli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_A/0/1/0/all/0/1&quot;&gt;Alejandro Moreo Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1&quot;&gt;Fabrizio Sebastiani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00846">
<title>Understanding Regularization in Batch Normalization. (arXiv:1809.00846v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00846</link>
<description rdf:parseType="Literal">&lt;p&gt;Batch Normalization (BN) makes output of hidden neuron had zero mean and unit
variance, improving convergence and generalization when training neural
networks. This work understands these phenomena theoretically. We analyze BN by
using a building block of neural networks, which consists of a weight layer, a
BN layer, and a nonlinear activation function. This simple network helps us
understand the characteristics of BN, where the results are generalized to deep
models in numerical studies. We explore BN in three aspects. First, by viewing
BN as a stochastic process, an analytical form of regularization inherited in
BN is derived. Second, the optimization dynamic with this regularization shows
that BN enables training converged with large maximum and effective learning
rates. Third, BN&apos;s generalization with regularization is explored by using
random matrix theory and statistical mechanics. Both simulations and
experiments support our analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhanglin Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00862">
<title>Handwriting styles: benchmarks and evaluation metrics. (arXiv:1809.00862v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00862</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the style of handwriting generation is a challenging problem,
since it is not well defined. It is a key component in order to develop in
developing systems with more personalized experiences with humans. In this
paper, we propose baseline benchmarks, in order to set anchors to estimate the
relative quality of different handwriting style methods. This will be done
using deep learning techniques, which have shown remarkable results in
different machine learning tasks, learning classification, regression, and most
relevant to our work, generating temporal sequences. We discuss the challenges
associated with evaluating our methods, which is related to evaluation of
generative models in general. We then propose evaluation metrics, which we find
relevant to this problem, and we discuss how we evaluate the evaluation
metrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,
there is no work done before in generating handwriting (either in terms of
methodology or the performance metrics), our in exploring styles using this
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammed_O/0/1/0/all/0/1&quot;&gt;Omar Mohammed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailly_G/0/1/0/all/0/1&quot;&gt;Gerard Bailly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellier_D/0/1/0/all/0/1&quot;&gt;Damien Pellier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00934">
<title>A Deep Neural Network Sentence Level Classification Method with Context Information. (arXiv:1809.00934v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.00934</link>
<description rdf:parseType="Literal">&lt;p&gt;In the sentence classification task, context formed from sentences adjacent
to the sentence being classified can provide important information for
classification. This context is, however, often ignored. Where methods do make
use of context, only small amounts are considered, making it difficult to
scale. We present a new method for sentence classification, Context-LSTM-CNN,
that makes use of potentially large contexts. The method also utilizes
long-range dependencies within the sentence being classified, using an LSTM,
and short-span features, using a stacked CNN. Our experiments demonstrate that
this approach consistently improves over previous methods on two different
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xingyi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrak_J/0/1/0/all/0/1&quot;&gt;Johann Petrak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1&quot;&gt;Angus Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00957">
<title>Road User Abnormal Trajectory Detection using a Deep Autoencoder. (arXiv:1809.00957v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00957</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on the development of a method that detects abnormal
trajectories of road users at traffic intersections. The main difficulty with
this is the fact that there are very few abnormal data and the normal ones are
insufficient for the training of any kinds of machine learning model. To tackle
these problems, we proposed the solution of using a deep autoencoder network
trained solely through augmented data considered as normal. By generating
artificial abnormal trajectories, our method is tested on four different
outdoor urban users scenes and performs better compared to some classical
outlier detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1&quot;&gt;Pankaj Raj Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1&quot;&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00977">
<title>DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal Convolutional Autoencoders. (arXiv:1809.00977v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00977</link>
<description rdf:parseType="Literal">&lt;p&gt;Human falls rarely occur; however, detecting falls is very important from the
health and safety perspective. Due to the rarity of falls, it is difficult to
employ supervised classification techniques to detect them. Moreover, in these
highly skewed situations it is also difficult to extract domain specific
features to identify falls. In this paper, we present a novel framework,
\textit{DeepFall}, which formulates the fall detection problem as an anomaly
detection problem. The \textit{DeepFall} framework presents the novel use of
deep spatio-temporal convolutional autoencoders to learn spatial and temporal
features from normal activities using non-invasive sensing modalities. We also
present a new anomaly scoring method that combines the reconstruction score of
frames across a video sequences to detect unseen falls. We tested the
\textit{DeepFall} framework on three publicly available datasets collected
through non-invasive sensing modalities, thermal camera and depth cameras and
show superior results in comparison to traditional autoencoder and
convolutional autoencoder methods to identify unseen falls.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogas_J/0/1/0/all/0/1&quot;&gt;Jacob Nogas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shehroz S. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihailidis_A/0/1/0/all/0/1&quot;&gt;Alex Mihailidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01018">
<title>Parameter Transfer Extreme Learning Machine based on Projective Model. (arXiv:1809.01018v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01018</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years, transfer learning has attracted much attention in the community
of machine learning. In this paper, we mainly focus on the tasks of parameter
transfer under the framework of extreme learning machine (ELM). Unlike the
existing parameter transfer approaches, which incorporate the source model
information into the target by regularizing the di erence between the source
and target domain parameters, an intuitively appealing projective-model is
proposed to bridge the source and target model parameters. Specifically, we
formulate the parameter transfer in the ELM networks by the means of parameter
projection, and train the model by optimizing the projection matrix and
classifier parameters jointly. Further more, the `L2,1-norm structured sparsity
penalty is imposed on the source domain parameters, which encourages the joint
feature selection and parameter transfer. To evaluate the e ectiveness of the
proposed method, comprehensive experiments on several commonly used domain
adaptation datasets are presented. The results show that the proposed method
significantly outperforms the non-transfer ELM networks and other classical
transfer learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Boyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xinyu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01079">
<title>Chi-Square Test Neural Network: A New Binary Classifier based on Backpropagation Neural Network. (arXiv:1809.01079v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01079</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the chi-square test neural network: a single hidden layer
backpropagation neural network using chi-square test theorem to redefine the
cost function and the error function. The weights and thresholds are modified
using standard backpropagation algorithm. The proposed approach has the
advantage of making consistent data distribution over training and testing
sets. It can be used for binary classification. The experimental results on
real world data sets indicate that the proposed algorithm can significantly
improve the classification accuracy comparing to related approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01090">
<title>A Quantum Spatial Graph Convolutional Neural Network using Quantum Passing Information. (arXiv:1809.01090v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01090</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a new Quantum Spatial Graph Convolutional Neural
Network (QSGCNN) model that can directly learn a classification function for
graphs of arbitrary sizes. Unlike state-of-the-art Graph Convolutional Neural
Network (GCN) models, the proposed QSGCNN model incorporates the process of
identifying transitive aligned vertices between graphs, and transforms
arbitrary sized graphs into fixed-sized aligned vertex grid structures. To
further learn representative graph characteristics, a new quantum spatial graph
convolution is proposed and employed to extract multi-scale vertex features, in
terms of quantum passing information between grid vertices of each graph. Since
the quantum spatial convolution preserves the property of the input grid
structures, the proposed QSGCNN model allows to directly employ the traditional
convolutional neural network to further learn from the global graph topology,
providing an end-to-end deep learning architecture that integrates the graph
representation and learning in the quantum spatial graph convolution layer and
the traditional convolutional layer for graph classifications. We demonstrate
the effectiveness of the proposed QSGCNN model in terms of the theoretical
connections to state-of-the-art methods. The proposed QSGCNN model addresses
the shortcomings of information loss and imprecise information representation
arising in existing GCN models associated with SortPooling or SumPooling
layers. Experimental results on benchmark graph classification datasets
demonstrate the effectiveness of the proposed QSGCNN model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yuhang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_L/0/1/0/all/0/1&quot;&gt;Luca Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Lixin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hancock_E/0/1/0/all/0/1&quot;&gt;Edwin R. Hancock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01093">
<title>Adversarial Attacks on Node Embeddings. (arXiv:1809.01093v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01093</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of network representation learning is to learn low-dimensional node
embeddings that capture the graph structure and are useful for solving
downstream tasks. However, despite the proliferation of such methods there is
currently no study of their robustness to adversarial attacks. We provide the
first adversarial vulnerability analysis on the widely used family of methods
based on random walks. We derive efficient adversarial perturbations that
poison the network structure and have a negative effect on both the quality of
the embeddings and the downstream tasks. We further show that our attacks are
transferable -- they generalize to many models -- and are successful even when
the attacker has restricted actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojcheski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Bojcheski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.10277">
<title>Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge. (arXiv:1611.10277v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1611.10277</link>
<description rdf:parseType="Literal">&lt;p&gt;While generative models such as Latent Dirichlet Allocation (LDA) have proven
fruitful in topic modeling, they often require detailed assumptions and careful
specification of hyperparameters. Such model complexity issues only compound
when trying to generalize generative models to incorporate human input. We
introduce Correlation Explanation (CorEx), an alternative approach to topic
modeling that does not assume an underlying generative model, and instead
learns maximally informative topics through an information-theoretic framework.
This framework naturally generalizes to hierarchical and semi-supervised
extensions with no additional modeling assumptions. In particular, word-level
domain knowledge can be flexibly incorporated within CorEx through anchor
words, allowing topic separability and representation to be promoted with
minimal human intervention. Across a variety of datasets, metrics, and
experiments, we demonstrate that CorEx produces topics that are comparable in
quality to those produced by unsupervised and semi-supervised variants of LDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallagher_R/0/1/0/all/0/1&quot;&gt;Ryan J. Gallagher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reing_K/0/1/0/all/0/1&quot;&gt;Kyle Reing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_D/0/1/0/all/0/1&quot;&gt;David Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00864">
<title>The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings. (arXiv:1703.00864v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00864</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine a class of embeddings based on structured random matrices with
orthogonal rows which can be applied in many machine learning applications
including dimensionality reduction and kernel approximation. For both the
Johnson-Lindenstrauss transform and the angular kernel, we show that we can
select matrices yielding guaranteed improved performance in accuracy and/or
speed compared to earlier methods. We introduce matrices with complex entries
which give significant further accuracy improvement. We provide geometric and
Markov chain-based perspectives to help understand the benefits, and empirical
results which suggest that the approach is helpful in a wider range of
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Choromanski_K/0/1/0/all/0/1&quot;&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1&quot;&gt;Mark Rowland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07287">
<title>Pairing an arbitrary regressor with an artificial neural network estimating aleatoric uncertainty. (arXiv:1707.07287v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07287</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest a general approach to quantification of different forms of
aleatoric uncertainty in regression tasks performed by artificial neural
networks. It is based on the simultaneous training of two neural networks with
a joint loss function and a specific hyperparameter $\lambda&amp;gt;0$ that allows for
automatically detecting noisy and clean regions in the input space and
controlling their {\em relative contribution} to the loss and its gradients.
After the model has been trained, one of the networks performs predictions and
the other quantifies the uncertainty of these predictions by estimating the
locally averaged loss of the first one. Unlike in many classical uncertainty
quantification methods, we do not assume any a priori knowledge of the ground
truth probability distribution, neither do we, in general, maximize the
likelihood of a chosen parametric family of distributions. We analyze the
learning process and the influence of clean and noisy regions of the input
space on the loss surface, depending on $\lambda$. In particular, we show that
small values of $\lambda$ increase the relative contribution of clean regions
to the loss and its gradients. This explains why choosing small $\lambda$
allows for better predictions compared with neural networks without uncertainty
counterparts and those based on classical likelihood maximization. Finally, we
demonstrate that one can naturally form ensembles of pairs of our networks and
thus capture both aleatoric and epistemic uncertainty and avoid overfitting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gurevich_P/0/1/0/all/0/1&quot;&gt;Pavel Gurevich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stuke_H/0/1/0/all/0/1&quot;&gt;Hannes Stuke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07169">
<title>Large-scale Nonlinear Variable Selection via Kernel Random Features. (arXiv:1804.07169v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07169</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method for input variable selection in nonlinear regression.
The method is embedded into a kernel regression machine that can model general
nonlinear functions, not being a priori limited to additive models. This is the
first kernel-based variable selection method applicable to large datasets. It
sidesteps the typical poor scaling properties of kernel methods by mapping the
inputs into a relatively low-dimensional space of random features. The
algorithm discovers the variables relevant for the regression task together
with learning the prediction model through learning the appropriate nonlinear
random feature maps. We demonstrate the outstanding performance of our method
on a set of large-scale synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregorova_M/0/1/0/all/0/1&quot;&gt;Magda Gregorov&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1&quot;&gt;Alexandros Kalousis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchand_Maillet_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Marchand-Maillet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06546">
<title>Joint Classification and Prediction CNN Framework for Automatic Sleep Stage Classification. (arXiv:1805.06546v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06546</link>
<description rdf:parseType="Literal">&lt;p&gt;Correctly identifying sleep stages is important in diagnosing and treating
sleep disorders. This work proposes a joint classification-and-prediction
framework based on CNNs for automatic sleep staging, and, subsequently,
introduces a simple yet efficient CNN architecture to power the framework.
Given a single input epoch, the novel framework jointly determines its label
(classification) and its neighboring epochs&apos; labels (prediction) in the
contextual output. While the proposed framework is orthogonal to the widely
adopted classification schemes, which take one or multiple epochs as contextual
inputs and produce a single classification decision on the target epoch, we
demonstrate its advantages in several ways. First, it leverages the dependency
among consecutive sleep epochs while surpassing the problems experienced with
the common classification schemes. Second, even with a single model, the
framework has the capacity to produce multiple decisions, which are essential
in obtaining a good performance as in ensemble-of-models methods, with very
little induced computational overhead. Probabilistic aggregation techniques are
then proposed to leverage the availability of multiple decisions. We conducted
experiments on two public datasets: Sleep-EDF Expanded with 20 subjects, and
Montreal Archive of Sleep Studies dataset with 200 subjects. The proposed
framework yields an overall classification accuracy of 82.3% and 83.6%,
respectively. We also show that the proposed framework not only is superior to
the baselines based on the common classification schemes but also outperforms
existing deep-learning approaches. To our knowledge, this is the first work
going beyond the standard single-output classification to consider multitask
neural networks for automatic sleep staging. This framework provides avenues
for further studies of different neural-network architectures for automatic
sleep staging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1&quot;&gt;Huy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreotti_F/0/1/0/all/0/1&quot;&gt;Fernando Andreotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooray_N/0/1/0/all/0/1&quot;&gt;Navin Cooray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1&quot;&gt;Oliver Y. Ch&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1&quot;&gt;Maarten De Vos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07898">
<title>SmoothOut: Smoothing Out Sharp Minima to Improve Generalization in Deep Learning. (arXiv:1805.07898v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07898</link>
<description rdf:parseType="Literal">&lt;p&gt;In Deep Learning, Stochastic Gradient Descent (SGD) is usually selected as
the training method because of its efficiency and scalability; however,
recently, a problem in SGD gains research interest: sharp minima in Deep Neural
Networks (DNNs) have poor generalization [1][2]; especially, large-batch SGD
tends to converge to sharp minima. It becomes an open question whether escaping
sharp minima can improve the generalization. To answer this question, we
proposed SmoothOut to smooth out sharp minima in DNNs and thereby improve
generalization. In a nutshell, SmoothOut perturbs multiple copies of the DNN by
noise injection and averages these copies. Injecting noises to SGD is widely
for exploration, but SmoothOut differs in lots of ways: (1) de-noising process
is applied before parameter updating; (2) uniform noises are injected instead
of Gaussian noises; (3) the goal is to obtain an auxiliary function without
sharp minima for better generalization, instead of higher exploration. We prove
that SmoothOut can eliminate sharp minima. Training multiple DNN copies is
inefficient, we further propose a stochastic version of SmoothOut which only
introduces the overhead of noise injecting and de-noising per batch. We prove
that the Stochastic SmoothOut is an unbiased approximation of the original
SmoothOut. In experiments on a variety of DNNs and datasets, SmoothOut
consistently improve generalization in both small-batch and large-batch
training on the top of state-of-the-art solutions. Our source code is in
https://github.com/wenwei202/smoothout
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wei Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yandan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1&quot;&gt;Feng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chunpeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00804">
<title>NAM: Non-Adversarial Unsupervised Domain Mapping. (arXiv:1806.00804v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00804</link>
<description rdf:parseType="Literal">&lt;p&gt;Several methods were recently proposed for the task of translating images
between domains without prior knowledge in the form of correspondences. The
existing methods apply adversarial learning to ensure that the distribution of
the mapped source domain is indistinguishable from the target domain, which
suffers from known stability issues. In addition, most methods rely heavily on
`cycle&apos; relationships between the domains, which enforce a one-to-one mapping.
In this work, we introduce an alternative method: Non-Adversarial Mapping
(NAM), which separates the task of target domain generative modeling from the
cross-domain mapping task. NAM relies on a pre-trained generative model of the
target domain, and aligns each source image with an image synthesized from the
target domain, while jointly optimizing the domain mapping function. It has
several key advantages: higher quality and resolution image translations,
simpler and more stable training and reusable target models. Extensive
experiments are presented validating the advantages of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1&quot;&gt;Yedid Hoshen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07215">
<title>A Machine Learning Approach for Detecting Students at Risk of Low Academic Achievement. (arXiv:1807.07215v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07215</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to predict whether a primary school student will perform in the `below
standard&apos; band of a national standardized test. We exploit a data set
containing test performance on the National Assessment Program - Literacy and
Numeracy (NAPLAN); a test given annually to all Australian school students in
grades 3, 5, 7, and 9. We separate the analysis into students in grade 5 and
above, for which previous achievement may be used as a predictor; and students
in grade 3, which must rely on family- and school-level predictors only. We
train and compare a set of classifiers for reading and numeracy learning areas
respectively. The classifiers achieve good predictive power in terms of area
under the ROC curve, suggesting that it is feasible for schools to more
accurately screen a large number of students for academic risk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cornell_Farrow_S/0/1/0/all/0/1&quot;&gt;Sarah Cornell-Farrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garrard_R/0/1/0/all/0/1&quot;&gt;Robert Garrard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01630">
<title>A Review of Learning with Deep Generative Models from perspective of graphical modeling. (arXiv:1808.01630v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01630</link>
<description rdf:parseType="Literal">&lt;p&gt;This document aims to provide a review on learning with deep generative
models (DGMs), which is an highly-active area in machine learning and more
generally, artificial intelligence. This review is not meant to be a tutorial,
but when necessary, we provide self-contained derivations for completeness.
This review has two features. First, though there are different perspectives to
classify DGMs, we choose to organize this review from the perspective of
graphical modeling, because the learning methods for directed DGMs and
undirected DGMs are fundamentally different. Second, we differentiate model
definitions from model learning algorithms, since different learning algorithms
can be applied to solve the learning problem on the same model, and an
algorithm can be applied to learn different models. We thus separate model
definition and model learning, with more emphasis on reviewing, differentiating
and connecting different learning algorithms. We also discuss promising future
research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1&quot;&gt;Zhijian Ou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07233">
<title>Neural Architecture Optimization. (arXiv:1808.07233v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07233</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic neural architecture design has shown its potential in discovering
powerful neural network architectures. Existing methods, no matter based on
reinforcement learning or evolutionary algorithms (EA), conduct architecture
search in a discrete space, which is highly inefficient. In this paper, we
propose a simple and efficient method to automatic neural architecture design
based on continuous optimization. We call this new approach neural architecture
optimization (NAO). There are three key components in our proposed approach:
(1) An encoder embeds/maps neural network architectures into a continuous
space. (2) A predictor takes the continuous representation of a network as
input and predicts its accuracy. (3) A decoder maps a continuous representation
of a network back to its architecture. The performance predictor and the
encoder enable us to perform gradient based optimization in the continuous
space to find the embedding of a new architecture with potentially better
accuracy. Such a better embedding is then decoded to a network by the decoder.
Experiments show that the architecture discovered by our method is very
competitive for image classification task on CIFAR-10 and language modeling
task on PTB, outperforming or on par with the best results of previous
architecture search methods with a significantly reduction of computational
resources. Specifically we obtain $2.07\%$ test set error rate for CIFAR-10
image classification task and $55.9$ test set perplexity of PTB language
modeling task. The best discovered architectures on both tasks are successfully
transferred to other tasks such as CIFAR-100 and WikiText-2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Renqian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1&quot;&gt;Fei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_a/0/1/0/all/0/1&quot;&gt;and Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>