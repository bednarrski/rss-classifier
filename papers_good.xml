<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.06037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.01780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02025"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02281"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02334"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02384"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02471"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.05176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01973"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02149"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02227"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02610"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07433"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1608.06037">
<title>Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures. (arXiv:1608.06037v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1608.06037</link>
<description rdf:parseType="Literal">&lt;p&gt;Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet,
ResNet, GoogleNet, include tens to hundreds of millions of parameters, which
impose considerable computation and memory overhead. This limits their
practical use for training, optimization and memory efficiency. On the
contrary, light-weight architectures, being proposed to address this issue,
mainly suffer from low accuracy. These inefficiencies mostly stem from
following an ad hoc procedure. We propose a simple architecture, called
SimpleNet, based on a set of designing principles and we empirically show that
SimpleNet provides a good tradeoff between the computation/memory efficiency
and the accuracy. Our simple 13-layer architecture outperforms most of the
deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet
on several well-known benchmarks while having 2 to 25 times fewer number of
parameters and operations. This makes it very handy for embedded system or
system with computational and memory limitations. We achieved state-of-the-art
result on CIFAR10 outperforming several heavier architectures, near state of
the art on MNIST and competitive results on CIFAR100 and SVHN. Models are made
available at: https://github.com/Coderx7/SimpleNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasanpour_S/0/1/0/all/0/1&quot;&gt;Seyyed Hossein Hasanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouhani_M/0/1/0/all/0/1&quot;&gt;Mohammad Rouhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1&quot;&gt;Mohsen Fayyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1&quot;&gt;Mohammad Sabokrou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.01780">
<title>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. (arXiv:1703.01780v5 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1703.01780</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently proposed Temporal Ensembling has achieved state-of-the-art
results in several semi-supervised learning benchmarks. It maintains an
exponential moving average of label predictions on each training example, and
penalizes predictions that are inconsistent with this target. However, because
the targets change only once per epoch, Temporal Ensembling becomes unwieldy
when learning large datasets. To overcome this problem, we propose Mean
Teacher, a method that averages model weights instead of label predictions. As
an additional benefit, Mean Teacher improves test accuracy and enables training
with fewer labels than Temporal Ensembling. Without changing the network
architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250
labels, outperforming Temporal Ensembling trained with 1000 labels. We also
show that a good network architecture is crucial to performance. Combining Mean
Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with
4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels
from 35.24% to 9.11%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarvainen_A/0/1/0/all/0/1&quot;&gt;Antti Tarvainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valpola_H/0/1/0/all/0/1&quot;&gt;Harri Valpola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10380">
<title>Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning. (arXiv:1710.10380v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10380</link>
<description rdf:parseType="Literal">&lt;p&gt;Context information plays an important role in human language understanding,
and it is also useful for machines to learn vector representations of language.
In this paper, we explore an asymmetric encoder-decoder structure for
unsupervised context-based sentence representation learning. As a result, we
build an encoder-decoder architecture with an RNN encoder and a CNN decoder. We
further combine a suite of effective designs to significantly improve model
efficiency while also achieving better performance. Our model is trained on two
different large unlabeled corpora, and in both cases transferability is
evaluated on a set of downstream language understanding tasks. We empirically
show that our model is simple and fast while producing rich sentence
representations that excel in downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shuai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hailin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_V/0/1/0/all/0/1&quot;&gt;Virginia R. de Sa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08314">
<title>Benchmarking Decoupled Neural Interfaces with Synthetic Gradients. (arXiv:1712.08314v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08314</link>
<description rdf:parseType="Literal">&lt;p&gt;Artifical Neural Networks are a particular class of learning systems modeled
after biological neural functions with an interesting penchant for Hebbian
learning, that is &quot;neurons that wire together, fire together&quot;. However, unlike
their natural counterparts, artificial neural networks have a close and
stringent coupling between the modules of neurons in the network. This coupling
or locking imposes upon the network a strict and inflexible structure that
prevent layers in the network from updating their weights until a full
feed-forward and backward pass has occurred. Such a constraint though may have
sufficed for a while, is now no longer feasible in the era of very-large-scale
machine learning, coupled with the increased desire for parallelization of the
learning process across multiple computing infrastructures. To solve this
problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are
introduced as a viable alternative to the backpropagation algorithm. This paper
performs a speed benchmark to compare the speed and accuracy capabilities of
SG-DNI as opposed to a standard neural interface using multilayer perceptron
MLP. SG-DNI shows good promise, in that it not only captures the learning
problem, it is also over 3-fold faster due to it asynchronous learning
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisong_E/0/1/0/all/0/1&quot;&gt;Ekaba Bisong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01078">
<title>Recent Advances in Recurrent Neural Networks. (arXiv:1801.01078v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01078</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are capable of learning features and long
term dependencies from sequential and time-series data. The RNNs have a stack
of non-linear units where at least one connection between units forms a
directed cycle. A well-trained RNN can model any dynamical system; however,
training RNNs is mostly plagued by issues in learning long-term dependencies.
In this paper, we present a survey on RNNs and several new advances for
newcomers and professionals in the field. The fundamentals and recent advances
are explained and the research challenges are introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehinejad_H/0/1/0/all/0/1&quot;&gt;Hojjat Salehinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baarbe_J/0/1/0/all/0/1&quot;&gt;Julianne Baarbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_S/0/1/0/all/0/1&quot;&gt;Sharan Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barfett_J/0/1/0/all/0/1&quot;&gt;Joseph Barfett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1&quot;&gt;Errol Colak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valaee_S/0/1/0/all/0/1&quot;&gt;Shahrokh Valaee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01944">
<title>Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. (arXiv:1801.01944v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.01944</link>
<description rdf:parseType="Literal">&lt;p&gt;We construct targeted audio adversarial examples on automatic speech
recognition. Given any audio waveform, we can produce another that is over
99.9% similar, but transcribes as any phrase we choose (at a rate of up to 50
characters per second). We apply our iterative optimization-based attack to
Mozilla&apos;s implementation DeepSpeech end-to-end, and show it has a 100% success
rate. The feasibility of this attack introduce a new domain to study
adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01957">
<title>From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots. (arXiv:1801.01957v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.01957</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational systems have come a long way after decades of research and
development, from Eliza and Parry in the 60&apos;s and 70&apos;s, to task-completion
systems as in the ATIS project, to intelligent personal assistants such as
Siri, and to today&apos;s social chatbots like XiaoIce. Social chatbots&apos; appeal lies
in not only their ability to respond to users&apos; diverse requests, but also in
being able to establish an emotional connection with users. The latter is done
by satisfying the users&apos; essential needs for communication, affection, and
social belonging. The design of social chatbots must focus on user engagement
and take both intellectual quotient (IQ) and emotional quotient (EQ) into
account. Users should want to engage with the social chatbot; as such, we
define the success metric for social chatbots as conversation-turns per session
(CPS). Using XiaoIce as an illustrative example, we discuss key technologies in
building social chatbots from core chat to visual sense to skills. We also show
how XiaoIce can dynamically recognize emotion and engage the user throughout
long conversations with appropriate interpersonal responses. As we become the
first generation of humans ever living with AI, social chatbots that are
well-designed to be both useful and empathic will soon be ubiquitous.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Heung-Yeung Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaodong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Di Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01968">
<title>Faster Deep Q-learning using Neural Episodic Control. (arXiv:1801.01968v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.01968</link>
<description rdf:parseType="Literal">&lt;p&gt;The Research on deep reinforcement learning to estimate Q-value by deep
learning has been active in recent years. In deep reinforcement learning, it is
important to efficiently learn the experiences that a agent has collected by
exploring the environment. In this research, we propose NEC2DQN that improves
learning speed of a algorithm with poor sample efficiency by using a algorithm
with good one at the beginning of learning, and we demonstrate it in
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishio_D/0/1/0/all/0/1&quot;&gt;Daichi Nishio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamane_S/0/1/0/all/0/1&quot;&gt;Satoshi Yamane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01972">
<title>Distance formulas capable of unifying Euclidian space and probability space. (arXiv:1801.01972v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.01972</link>
<description rdf:parseType="Literal">&lt;p&gt;For pattern recognition like image recognition, it has become clear that each
machine-learning dictionary data actually became data in probability space
belonging to Euclidean space. However, the distances in the Euclidean space and
the distances in the probability space are separated and ununified when machine
learning is introduced in the pattern recognition. There is still a problem
that it is impossible to directly calculate an accurate matching relation
between the sampling data of the read image and the learned dictionary data. In
this research, we focused on the reason why the distance is changed and the
extent of change when passing through the probability space from the original
Euclidean distance among data belonging to multiple probability spaces
containing Euclidean space. By finding the reason of the cause of the distance
error and finding the formula expressing the error quantitatively, a possible
distance formula to unify Euclidean space and probability space is found. Based
on the results of this research, the relationship between machine-learning
dictionary data and sampling data was clearly understood for pattern
recognition. As a result, the calculation of collation among data and
machine-learning to compete mutually between data are cleared, and complicated
calculations became unnecessary. Finally, using actual pattern recognition
data, experimental demonstration of a possible distance formula to unify
Euclidean space and probability space discovered by this research was carried
out, and the effectiveness of the result was confirmed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1&quot;&gt;Zecang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Ling Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02025">
<title>Robot Localisation and 3D Position Estimation Using a Free-Moving Camera and Cascaded Convolutional Neural Networks. (arXiv:1801.02025v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1801.02025</link>
<description rdf:parseType="Literal">&lt;p&gt;Many works in collaborative robotics and human-robot interaction focuses on
identifying and predicting human behaviour while considering the information
about the robot itself as given. This can be the case when sensors and the
robot are calibrated in relation to each other and often the reconfiguration of
the system is not possible, or extra manual work is required. We present a deep
learning based approach to remove the constraint of having the need for the
robot and the vision sensor to be fixed and calibrated in relation to each
other. The system learns the visual cues of the robot body and is able to
localise it, as well as estimate the position of robot joints in 3D space by
just using a 2D color image. The method uses a cascaded convolutional neural
network, and we present the structure of the network, describe our own
collected dataset, explain the network training and achieved results. A fully
trained system shows promising results in providing an accurate mask of where
the robot is located and a good estimate of its joints positions in 3D. The
accuracy is not good enough for visual servoing applications yet, however, it
can be sufficient for general safety and some collaborative tasks not requiring
very high precision. The main benefit of our method is the possibility of the
vision sensor to move freely. This allows it to be mounted on moving objects,
for example, a body of the person or a mobile robot working in the same
environment as the robots are operating in.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miseikis_J/0/1/0/all/0/1&quot;&gt;Justinas Miseikis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knobelreiter_P/0/1/0/all/0/1&quot;&gt;Patrick Knobelreiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brijacak_I/0/1/0/all/0/1&quot;&gt;Inka Brijacak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahyanejad_S/0/1/0/all/0/1&quot;&gt;Saeed Yahyanejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glette_K/0/1/0/all/0/1&quot;&gt;Kyrre Glette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elle_O/0/1/0/all/0/1&quot;&gt;Ole Jakob Elle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1&quot;&gt;Jim Torresen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02268">
<title>Sample-Efficient Reinforcement Learning through Transfer and Architectural Priors. (arXiv:1801.02268v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02268</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in deep reinforcement learning has allowed algorithms to learn
complex tasks such as Atari 2600 games just from the reward provided by the
game, but these algorithms presently require millions of training steps in
order to learn, making them approximately five orders of magnitude slower than
humans. One reason for this is that humans build robust shared representations
that are applicable to collections of problems, making it much easier to
assimilate new variants. This paper first introduces the idea of
automatically-generated game sets to aid in transfer learning research, and
then demonstrates the utility of shared representations by showing that models
can substantially benefit from the incorporation of relevant architectural
priors. This technique affords a remarkable 50x positive transfer on a toy
problem-set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spector_B/0/1/0/all/0/1&quot;&gt;Benjamin Spector&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02281">
<title>Winograd Schema - Knowledge Extraction Using Narrative Chains. (arXiv:1801.02281v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.02281</link>
<description rdf:parseType="Literal">&lt;p&gt;The Winograd Schema Challenge (WSC) is a test of machine intelligence,
designed to be an improvement on the Turing test. A Winograd Schema consists of
a sentence and a corresponding question. To successfully answer these
questions, one requires the use of commonsense knowledge and reasoning. This
work focuses on extracting common sense knowledge which can be used to generate
answers for the Winograd schema challenge. Common sense knowledge is extracted
based on events (or actions) and their participants; called Event-Based
Conditional Commonsense (ECC). I propose an approach using Narrative Event
Chains [Chambers et al., 2008] to extract ECC knowledge. These are stored in
templates, to be later used for answering the WSC questions. This approach
works well with respect to a subset of WSC tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_V/0/1/0/all/0/1&quot;&gt;Vatsal Mahajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02334">
<title>A generalized concept-cognitive learning: A machine learning viewpoint. (arXiv:1801.02334v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.02334</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept-cognitive learning (CCL) is a hot topic in recent years, and it has
attracted much attention from the communities of formal concept analysis,
granular computing and cognitive computing. However, the relationship among
cognitive computing (CC), conceptcognitive computing (CCC), and CCL is not
clearly described. To this end, we explain the relationship of CC, CCC, and
CCL. Then, we propose a generalized CCL from the point of view of machine
learning. Finally, experiments on seven data sets are conducted to evaluate
concept formation and concept-cognitive processes of the proposed generalized
CCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_Y/0/1/0/all/0/1&quot;&gt;Yunlong Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinhai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiabin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_b/0/1/0/all/0/1&quot;&gt;biao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02384">
<title>Attacking Speaker Recognition With Deep Generative Models. (arXiv:1801.02384v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1801.02384</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we investigate the ability of generative adversarial networks
(GANs) to synthesize spoofing attacks on modern speaker recognition systems. We
first show that samples generated with SampleRNN and WaveNet are unable to fool
a CNN-based speaker recognition system. We propose a modification of the
Wasserstein GAN objective function to make use of data that is real but not
from the class being learned. Our semi-supervised learning method is able to
perform both targeted and untargeted attacks, raising questions related to
security in speaker authentication systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Wilson Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_A/0/1/0/all/0/1&quot;&gt;Anish Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valle_R/0/1/0/all/0/1&quot;&gt;Rafael Valle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02471">
<title>Gated Recurrent Networks for Seizure Detection. (arXiv:1801.02471v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1801.02471</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent Neural Networks (RNNs) with sophisticated units that implement a
gating mechanism have emerged as powerful technique for modeling sequential
signals such as speech or electroencephalography (EEG). The latter is the focus
on this paper. A significant big data resource, known as the TUH EEG Corpus
(TUEEG), has recently become available for EEG research, creating a unique
opportunity to evaluate these recurrent units on the task of seizure detection.
In this study, we compare two types of recurrent units: long short-term memory
units (LSTM) and gated recurrent units (GRU). These are evaluated using a state
of the art hybrid architecture that integrates Convolutional Neural Networks
(CNNs) with RNNs. We also investigate a variety of initialization methods and
show that initialization is crucial since poorly initialized networks cannot be
trained. Furthermore, we explore regularization of these convolutional gated
recurrent networks to address the problem of overfitting. Our experiments
revealed that convolutional LSTM networks can achieve significantly better
performance than convolutional GRU networks. The convolutional LSTM
architecture with proper initialization and regularization delivers 30%
sensitivity at 6 false alarms per 24 hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ziyabari_S/0/1/0/all/0/1&quot;&gt;Saeedeh Ziyabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Vinit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weltin_E/0/1/0/all/0/1&quot;&gt;Eva Von Weltin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Campbell_C/0/1/0/all/0/1&quot;&gt;Christopher Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.05176">
<title>Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking. (arXiv:1707.05176v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.05176</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new neural architecture for collaborative ranking with
implicit feedback. Our model, LRML (\textit{Latent Relational Metric Learning})
is a novel metric learning approach for recommendation. More specifically,
instead of simple push-pull mechanisms between user and item pairs, we propose
to learn latent relations that describe each user item interaction. This helps
to alleviate the potential geometric inflexibility of existing metric learing
approaches. This enables not only better performance but also a greater extent
of modeling capability, allowing our model to scale to a larger number of
interactions. In order to do so, we employ a augmented memory module and learn
to attend over these memory blocks to construct latent relations. The
memory-based attention module is controlled by the user-item interaction,
making the learned relation vector specific to each user-item pair. Hence, this
can be interpreted as learning an exclusive and optimal relational translation
for each user-item interaction. The proposed architecture demonstrates the
state-of-the-art performance across multiple recommendation benchmarks. LRML
outperforms other metric learning models by $6\%-7.5\%$ in terms of Hits@10 and
nDCG@10 on large datasets such as Netflix and MovieLens20M. Moreover,
qualitative studies also demonstrate evidence that our proposed model is able
to infer and encode explicit sentiment, temporal and attribute information
despite being only trained on implicit feedback. As such, this ascertains the
ability of LRML to uncover hidden relational structure within implicit
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siu Cheung Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01952">
<title>Generating Neural Networks with Neural Networks. (arXiv:1801.01952v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.01952</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypernetworks are neural networks that transform a random input vector into
weights for a specified target neural network. We formulate the hypernetwork
training objective as a compromise between accuracy and diversity, where the
diversity takes into account trivial symmetry transformations of the target
network. We show that this formulation naturally arises as a relaxation of an
optimistic probability distribution objective for the generated networks, and
we explain how it is related to variational inference. We use multi-layered
perceptrons to form the mapping from the low dimensional input random vector to
the high dimensional weight space, and demonstrate how to reduce the number of
parameters in this mapping by weight sharing. We perform experiments on a four
layer convolutional target network which classifies MNIST images, and show that
the generated weights are diverse and have interesting distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deutsch_L/0/1/0/all/0/1&quot;&gt;Lior Deutsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01973">
<title>A Note on the Inception Score. (arXiv:1801.01973v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.01973</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models are powerful tools that have produced impressive
results in recent years. These advances have been for the most part empirically
driven, making it essential that we use high quality evaluation metrics. In
this paper, we provide new insights into the Inception Score, a recently
proposed and widely used evaluation metric for generative models, and
demonstrate that it fails to provide useful guidance when comparing models. We
discuss both suboptimalities of the metric itself and issues with its
application. Finally, we call for researchers to be more systematic and careful
when evaluating and comparing generative models, as the advancement of the
field depends upon it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barratt_S/0/1/0/all/0/1&quot;&gt;Shane Barratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Rishi Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02149">
<title>Applying an Ensemble Learning Method for Improving Multi-label Classification Performance. (arXiv:1801.02149v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02149</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, multi-label classification problem has become a
controversial issue. In this kind of classification, each sample is associated
with a set of class labels. Ensemble approaches are supervised learning
algorithms in which an operator takes a number of learning algorithms, namely
base-level algorithms and combines their outcomes to make an estimation. The
simplest form of ensemble learning is to train the base-level algorithms on
random subsets of data and then let them vote for the most popular
classifications or average the predictions of the base-level algorithms. In
this study, an ensemble learning method is proposed for improving multi-label
classification evaluation criteria. We have compared our method with well-known
base-level algorithms on some data sets. Experiment results show the proposed
approach outperforms the base well-known classifiers for the multi-label
classification problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_Shahri_A/0/1/0/all/0/1&quot;&gt;Amirreza Mahdavi-Shahri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houshmand_M/0/1/0/all/0/1&quot;&gt;Mahboobeh Houshmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaghoobi_M/0/1/0/all/0/1&quot;&gt;Mahdi Yaghoobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalali_M/0/1/0/all/0/1&quot;&gt;Mehrdad Jalali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02227">
<title>Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models. (arXiv:1801.02227v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02227</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new technique that boosts the convergence of training generative
adversarial networks. Generally, the rate of training deep models reduces
severely after multiple iterations. A key reason for this phenomenon is that a
deep network is expressed using a highly non-convex finite-dimensional model,
and thus the parameter gets stuck in a local optimum. Because of this, methods
often suffer not only from degeneration of the convergence speed but also from
limitations in the representational power of the trained network. To overcome
this issue, we propose an additional layer called the gradient layer to seek a
descent direction in an infinite-dimensional space. Because the layer is
constructed in the infinite-dimensional space, we are not restricted by the
specific model structure of finite-dimensional models. As a result, we can get
out of the local optima in finite-dimensional models and move towards the
global optimal function more directly. In this paper, this phenomenon is
explained from the functional gradient method perspective of the gradient
layer. Interestingly, the optimization procedure using the gradient layer
naturally constructs the deep structure of the network. Moreover, we
demonstrate that this procedure can be regarded as a discretization method of
the gradient flow that naturally reduces the objective function. Finally, the
method is tested using several numerical experiments, which show its fast
convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1&quot;&gt;Atsushi Nitanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1&quot;&gt;Taiji Suzuki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02294">
<title>Learning Tree-based Deep Model for Recommender Systems. (arXiv:1801.02294v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02294</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel recommendation method based on tree. With user behavior
data, the tree based model can capture user interests from coarse to fine, by
traversing nodes top down and make decisions whether to pick up each node to
user. Compared to traditional model-based methods like matrix factorization
(MF), our tree based model does not have to fetch and estimate each item in the
entire set. Instead, candidates are drawn from subsets corresponding to user&apos;s
high-level interests, which is defined by the tree structure. Meanwhile,
finding candidates from the entire corpus brings more novelty than
content-based approaches like item-based collaborative filtering.Moreover, in
this paper, we show that the tree structure can also act to refine user
interests distribution, to benefit both training and prediction. The
experimental results in both open dataset and Taobao display advertising
dataset indicate that the proposed method outperforms existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Han Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guozheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jie He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02328">
<title>Deep Nearest Class Mean Model for Incremental Odor Classification. (arXiv:1801.02328v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02328</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, more and more machine learning algorithms have been applied
to odor recognition. These odor recognition algorithms usually assume that the
training dataset is static. However, for some odor recognition tasks, the odor
dataset is dynamically growing where not only the training samples but also the
number of classes increase over time. Motivated by this concern, we proposed a
deep nearest class mean (DNCM) model which combines the deep learning framework
and nearest class mean (NCM) method. DNCM not only can leverage deep neural
network to extract deep features, but also well suited for integrating new
classes. Experiments demonstrate that the proposed DNCM model is effective and
efficient for incremental odor classification, especially for new classes with
only a small number of training examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Angus Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1&quot;&gt;Kevin Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weitong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02610">
<title>Generating adversarial examples with adversarial networks. (arXiv:1801.02610v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1801.02610</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been found to be vulnerable to adversarial
examples resulting from adding small-magnitude perturbations to inputs. Such
adversarial examples can mislead DNNs to produce adversary-selected results.
Different attack strategies have been proposed to generate adversarial
examples, but how to produce them with high perceptual quality and more
efficiently requires more research efforts. In this paper, we propose AdvGAN to
generate adversarial examples with generative adversarial networks (GANs),
which can learn and approximate the distribution of original instances. For
AdvGAN, once the generator is trained, it can generate adversarial
perturbations efficiently for any instance, so as to potentially accelerate
adversarial training as defenses. We apply AdvGAN in both semi-whitebox and
black-box attack settings. In semi-whitebox attacks, there is no need to access
the original target model after the generator is trained, in contrast to
traditional white-box attacks. In black-box attacks, we dynamically train a
distilled model for the black-box model and optimize the generator accordingly.
Adversarial examples generated by AdvGAN on different target models have high
attack success rate under state-of-the-art defenses compared to other attacks.
Our attack has placed the first with 92.76% accuracy on a public MNIST
black-box attack challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Warren He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07433">
<title>Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples. (arXiv:1704.07433v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07433</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-paced learning and hard example mining re-weight training instances to
improve learning accuracy. This paper presents two improved alternatives based
on lightweight estimates of sample uncertainty in stochastic gradient descent
(SGD): the variance in predicted probability of the correct class across
iterations of mini-batch SGD, and the proximity of the correct class
probability to the decision threshold. Extensive experimental results on six
datasets show that our methods reliably improve accuracy in various network
architectures, including additional gains on top of other popular training
techniques, such as residual learning, momentum, ADAM, batch normalization,
dropout, and distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Haw-Shiuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Learned_Miller_E/0/1/0/all/0/1&quot;&gt;Erik Learned-Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCallum_A/0/1/0/all/0/1&quot;&gt;Andrew McCallum&lt;/a&gt;</dc:creator>
</item></rdf:RDF>