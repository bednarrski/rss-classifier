<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.04780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07637"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07722"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02311"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07247"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07441"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07492"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07863"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10321"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04223"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.07550">
<title>Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?. (arXiv:1806.07550v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07550</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary neural networks (BNN) have been studied extensively since they run
dramatically faster at lower memory and power consumption than floating-point
networks, thanks to the efficiency of bit operations. However, contemporary
BNNs whose weights and activations are both single bits suffer from severe
accuracy degradation. To understand why, we investigate the representation
ability, speed and bias/variance of BNNs through extensive experiments. We
conclude that the error of BNNs is predominantly caused by the intrinsic
instability (training time) and non-robustness (train \&amp;amp; test time). Inspired
by this investigation, we propose the Binary Ensemble Neural Network (BENN)
which leverages ensemble methods to improve the performance of BNNs with
limited efficiency cost. While ensemble techniques have been broadly believed
to be only marginally helpful for strong classifiers such as deep neural
networks, our analyses and experiments show that they are naturally a perfect
fit to boost BNNs. We find that our BENN, which is faster and much more robust
than state-of-the-art binary networks, can even surpass the accuracy of the
full-precision floating number network with the same architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shilin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07572">
<title>Neural Tangent Kernel: Convergence and Generalization in Neural Networks. (arXiv:1806.07572v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07572</link>
<description rdf:parseType="Literal">&lt;p&gt;At initialization, artificial neural networks (ANNs) are equivalent to
Gaussian processes in the infinite-width limit, thus connecting them to kernel
methods. We prove that the evolution of an ANN during training can also be
described by a kernel: during gradient descent on the parameters of an ANN, the
network function $f_\theta$ (which maps input vectors to output vectors)
follows the kernel gradient of the functional cost (which is convex, in
contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel
(NTK). This kernel is central to describe the generalization features of ANNs.
While the NTK is random at initialization and varies during training, in the
infinite-width limit it converges to an explicit limiting kernel and it stays
constant during training. This makes it possible to study the training of ANNs
in function space instead of parameter space. Convergence of the training can
then be related to the positive-definiteness of the limiting NTK. We prove the
positive-definiteness of the limiting NTK when the data is supported on the
sphere and the non-linearity is non-polynomial. We then focus on the setting of
least-squares regression and show that in the infinite-width limit, the network
function $f_\theta$ follows a linear differential equation during training. The
convergence is fastest along the largest kernel principal components of the
input data with respect to the NTK, hence suggesting a theoretical motivation
for early stopping. Finally we study the NTK numerically, observe its behavior
for wide networks, and compare it to the infinite-width limit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacot_A/0/1/0/all/0/1&quot;&gt;Arthur Jacot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_F/0/1/0/all/0/1&quot;&gt;Franck Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hongler_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Hongler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.04780">
<title>Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science. (arXiv:1707.04780v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1707.04780</link>
<description rdf:parseType="Literal">&lt;p&gt;Through the success of deep learning in various domains, artificial neural
networks are currently among the most used artificial intelligence methods.
Taking inspiration from the network properties of biological neural networks
(e.g. sparsity, scale-freeness), we argue that (contrary to general practice)
artificial neural networks, too, should not have fully-connected layers. Here
we propose sparse evolutionary training of artificial neural networks, an
algorithm which evolves an initial sparse topology (Erd\H{o}s-R\&apos;enyi random
graph) of two consecutive layers of neurons into a scale-free topology, during
learning. Our method replaces artificial neural networks fully-connected layers
with sparse ones before training, reducing quadratically the number of
parameters, with no decrease in accuracy. We demonstrate our claims on
restricted Boltzmann machines, multi-layer perceptrons, and convolutional
neural networks for unsupervised and supervised learning on 15 datasets. Our
approach has the potential to enable artificial neural networks to scale up
beyond what is currently possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1&quot;&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1&quot;&gt;Elena Mocanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibescu_M/0/1/0/all/0/1&quot;&gt;Madeleine Gibescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liotta_A/0/1/0/all/0/1&quot;&gt;Antonio Liotta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02257">
<title>Interoceptive robustness through environment-mediated morphological development. (arXiv:1804.02257v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02257</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically, AI researchers and roboticists try to realize intelligent behavior
in machines by tuning parameters of a predefined structure (body plan and/or
neural network architecture) using evolutionary or learning algorithms. Another
but not unrelated longstanding property of these systems is their brittleness
to slight aberrations, as highlighted by the growing deep learning literature
on adversarial examples. Here we show robustness can be achieved by evolving
the geometry of soft robots, their control systems, and how their material
properties develop in response to one particular interoceptive stimulus
(engineering stress) during their lifetimes. By doing so we realized robots
that were equally fit but more robust to extreme material defects (such as
might occur during fabrication or by damage thereafter) than robots that did
not develop during their lifetimes, or developed in response to a different
interoceptive stimulus (pressure). This suggests that the interplay between
changes in the containing systems of agents (body plan and/or neural
architecture) at different temporal scales (evolutionary and developmental)
along different modalities (geometry, material properties, synaptic weights)
and in response to different signals (interoceptive and external perception)
all dictate those agents&apos; abilities to evolve or learn capable and robust
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kriegman_S/0/1/0/all/0/1&quot;&gt;Sam Kriegman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corucci_F/0/1/0/all/0/1&quot;&gt;Francesco Corucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh C. Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06242">
<title>Conversational Analysis using Utterance-level Attention-based Bidirectional Recurrent Neural Networks. (arXiv:1805.06242v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06242</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent approaches for dialogue act recognition have shown that context from
preceding utterances is important to classify the subsequent one. It was shown
that the performance improves rapidly when the context is taken into account.
We propose an utterance-level attention-based bidirectional recurrent neural
network (Utt-Att-BiRNN) model to analyze the importance of preceding utterances
to classify the current one. In our setup, the BiRNN is given the input set of
current and preceding utterances. Our model outperforms previous models that
use only preceding utterances as context on the used corpus. Another
contribution of the article is to discover the amount of information in each
utterance to classify the subsequent one and to show that context-based
learning not only improves the performance but also achieves higher confidence
in the classification. We use character- and word-level features to represent
the utterances. The results are presented for character and word feature
representations and as an ensemble model of both representations. We found that
when classifying short utterances, the closest preceding utterances contributes
to a higher degree.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bothe_C/0/1/0/all/0/1&quot;&gt;Chandrakant Bothe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magg_S/0/1/0/all/0/1&quot;&gt;Sven Magg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1&quot;&gt;Cornelius Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07637">
<title>Skilled Experience Catalogue: A Skill-Balancing Mechanism for Non-Player Characters using Reinforcement Learning. (arXiv:1806.07637v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07637</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a skill-balancing mechanism for adversarial
non-player characters (NPCs), called Skilled Experience Catalogue (SEC). The
objective of this mechanism is to approximately match the skill level of an NPC
to an opponent in real-time. We test the technique in the context of a
First-Person Shooter (FPS) game. Specifically, the technique adjusts a
reinforcement learning NPC&apos;s proficiency with a weapon based on its current
performance against an opponent. Firstly, a catalogue of experience, in the
form of stored learning policies, is built up by playing a series of training
games. Once the NPC has been sufficiently trained, the catalogue acts as a
timeline of experience with incremental knowledge milestones in the form of
stored learning policies. If the NPC is performing poorly, it can jump to a
later stage in the learning timeline to be equipped with more informed
decision-making. Likewise, if it is performing significantly better than the
opponent, it will jump to an earlier stage. The NPC continues to learn in
real-time using reinforcement learning but its policy is adjusted, as required,
by loading the most suitable milestones for the current circumstances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavin_F/0/1/0/all/0/1&quot;&gt;Frank G. Glavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madden_M/0/1/0/all/0/1&quot;&gt;Michael G. Madden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07697">
<title>Self-weighted Multiple Kernel Learning for Graph-based Clustering and Semi-supervised Classification. (arXiv:1806.07697v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07697</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple kernel learning (MKL) method is generally believed to perform better
than single kernel method. However, some empirical studies show that this is
not always true: the combination of multiple kernels may even yield an even
worse performance than using a single kernel. There are two possible reasons
for the failure: (i) most existing MKL methods assume that the optimal kernel
is a linear combination of base kernels, which may not hold true; and (ii) some
kernel weights are inappropriately assigned due to noises and carelessly
designed algorithms. In this paper, we propose a novel MKL framework by
following two intuitive assumptions: (i) each kernel is a perturbation of the
consensus kernel; and (ii) the kernel that is close to the consensus kernel
should be assigned a large weight. Impressively, the proposed method can
automatically assign an appropriate weight to each kernel without introducing
additional parameters, as existing methods do. The proposed framework is
integrated into a unified framework for graph-based clustering and
semi-supervised classification. We have conducted experiments on multiple
benchmark datasets and our empirical results verify the superiority of the
proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kang_Z/0/1/0/all/0/1&quot;&gt;Zhao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07722">
<title>Stylized innovation: interrogating incrementally available randomised dictionaries. (arXiv:1806.07722v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07722</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by recent work of Fink, Reeves, Palma and Farr (2017) on innovation
in language, gastronomy, and technology, I study how new symbol discovery
manifests itself in terms of additional &quot;word&quot; vocabulary being available from
dictionaries generated from a finite number of symbols. Several distinct
dictionary generation models are investigated using numerical simulation, with
emphasis on the scaling of knowledge as dictionary generators and parameters
are varied, and the role of which order the symbols are discovered in.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinsler_P/0/1/0/all/0/1&quot;&gt;Paul Kinsler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07822">
<title>Learning Neural Parsers with Deterministic Differentiable Imitation Learning. (arXiv:1806.07822v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07822</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of spatial segmentation of a 2D object in the context
of a robotic system for painting, where an optimal segmentation depends on both
the appearance of the object and the size of each segment. Since each segment
must take into account appearance features at several scales, we take a
hierarchical grammar-based parsing approach to decompose the object into 2D
segments for painting. Since there are many ways to segment an object the
solution space is extremely large and it is very challenging to utilize an
exploration based optimization approach like reinforcement learning. Instead,
we pose the segmentation problem as an imitation learning problem by using a
segmentation algorithm in the place of an expert, that has access to a small
dataset with known foreground-background segmentations. During the imitation
learning process, we learn to imitate the oracle (segmentation algorithm) using
only the image of the object, without the use of the known
foreground-background segmentations. We introduce a novel deterministic policy
gradient update, DRAG, in the form of a deterministic actor-critic variant of
AggreVaTeD, to train our neural network based object parser. We will also show
that our approach can be seen as extending DDPG to the Imitation Learning
scenario. Training our neural parser to imitate the oracle via DRAG allow our
neural parser to outperform several existing imitation learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_T/0/1/0/all/0/1&quot;&gt;Tanmay Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1&quot;&gt;Nicholas Rhinehart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muelling_K/0/1/0/all/0/1&quot;&gt;Katharina Muelling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris M. Kitani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07840">
<title>Edge Intelligence: On-Demand Deep Learning Model Co-Inference with Device-Edge Synergy. (arXiv:1806.07840v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1806.07840</link>
<description rdf:parseType="Literal">&lt;p&gt;As the backbone technology of machine learning, deep neural networks (DNNs)
have have quickly ascended to the spotlight. Running DNNs on
resource-constrained mobile devices is, however, by no means trivial, since it
incurs high performance and energy overhead. While offloading DNNs to the cloud
for execution suffers unpredictable performance, due to the uncontrolled long
wide-area network latency. To address these challenges, in this paper, we
propose Edgent, a collaborative and on-demand DNN co-inference framework with
device-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that
adaptively partitions DNN computation between device and edge, in order to
leverage hybrid computation resources in proximity for real-time DNN inference.
(2) DNN right-sizing that accelerates DNN inference through early-exit at a
proper intermediate DNN layer to further reduce the computation latency. The
prototype implementation and extensive evaluations based on Raspberry Pi
demonstrate Edgent&apos;s effectiveness in enabling on-demand low-latency edge
intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1&quot;&gt;En Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00107">
<title>Learned in Translation: Contextualized Word Vectors. (arXiv:1708.00107v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00107</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision has benefited from initializing multiple deep layers with
weights pretrained on large supervised training sets like ImageNet. Natural
language processing (NLP) typically sees initialization of only the lowest
layer of deep models with pretrained word vectors. In this paper, we use a deep
LSTM encoder from an attentional sequence-to-sequence model trained for machine
translation (MT) to contextualize word vectors. We show that adding these
context vectors (CoVe) improves performance over using only unsupervised word
and character vectors on a wide variety of common NLP tasks: sentiment analysis
(SST, IMDb), question classification (TREC), entailment (SNLI), and question
answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe
improves performance of our baseline models to the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCann_B/0/1/0/all/0/1&quot;&gt;Bryan McCann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1&quot;&gt;James Bradbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00538">
<title>Investigating Capsule Networks with Dynamic Routing for Text Classification. (arXiv:1804.00538v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00538</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we explore capsule networks with dynamic routing for text
classification. We propose three strategies to stabilize the dynamic routing
process to alleviate the disturbance of some noise capsules which may contain
&quot;background&quot; information or have not been successfully trained. A series of
experiments are conducted with capsule networks on six text classification
benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets,
which shows the effectiveness of capsule networks for text classification. We
additionally show that capsule networks exhibit significant improvement when
transfer single-label to multi-label text classification over strong baseline
methods. To the best of our knowledge, this is the first work that capsule
networks have been empirically investigated for text modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jianbo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zeyang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Suofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02311">
<title>Unsupervised Attention-guided Image to Image Translation. (arXiv:1806.02311v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02311</link>
<description rdf:parseType="Literal">&lt;p&gt;Current unsupervised image-to-image translation techniques struggle to focus
their attention on individual objects without altering the background or the
way multiple objects interact within a scene. Motivated by the important role
of attention in human perception, we tackle this limitation by introducing
unsupervised attention mechanisms that are jointly adversarialy trained with
the generators and discriminators. We demonstrate qualitatively and
quantitatively that our approach is able to attend to relevant regions in the
image without requiring supervision, and that by doing so it achieves more
realistic mappings compared to recent approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejjati_Y/0/1/0/all/0/1&quot;&gt;Youssef A. Mejjati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1&quot;&gt;Christian Richardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1&quot;&gt;James Tompkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1&quot;&gt;Darren Cosker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang In Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07247">
<title>Tensor-Tensor Product Toolbox. (arXiv:1806.07247v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07247</link>
<description rdf:parseType="Literal">&lt;p&gt;The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011]
is a natural generalization of matrix multiplication. Based on t-product, many
operations on matrix can be extended to tensor cases, including tensor SVD,
tensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many
others. The linear algebraic structure of tensors are similar to the matrix
cases. We develop a Matlab toolbox to implement several basic operations on
tensors based on t-product. The toolbox is available at
https://github.com/canyilu/tproduct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Canyi Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07385">
<title>Detecting and interpreting myocardial infarctions using fully convolutional neural networks. (arXiv:1806.07385v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1806.07385</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the detection of myocardial infarction in electrocardiography
(ECG) data as provided by the PTB ECG database without non-trivial
preprocessing. The classification is carried out using deep neural networks in
a comparative study involving convolutional as well as recurrent neural network
architectures. The best architecture, an ensemble of fully convolutional
architectures, beats state-of-the-art results on this dataset and reaches 93.3%
sensitivity and 89.7% specificity evaluated with 10-fold crossvalidation, which
is the performance level of human cardiologists for this task. We investigate
questions relevant for clinical applications such as the dependence of the
classification results on the considered data channels and the considered
subdiagnoses. Finally, we apply attribution methods to gain an understanding of
the network&apos;s decision criteria on an exemplary basis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1&quot;&gt;Nils Strodthoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strodthoff_C/0/1/0/all/0/1&quot;&gt;Claas Strodthoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07406">
<title>Contrastive Hebbian Learning with Random Feedback Weights. (arXiv:1806.07406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07406</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are commonly trained to make predictions through learning
algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by
gradient backpropagation, is based on Hebb&apos;s rule and the contrastive
divergence algorithm. It operates in two phases, the forward (or free) phase,
where the data are fed to the network, and a backward (or clamped) phase, where
the target signals are clamped to the output layer of the network and the
feedback signals are transformed through the transpose synaptic weight
matrices. This implies symmetries at the synaptic level, for which there is no
evidence in the brain. In this work, we propose a new variant of the algorithm,
called random contrastive Hebbian learning, which does not rely on any synaptic
weights symmetries. Instead, it uses random matrices to transform the feedback
signals during the clamped phase, and the neural dynamics are described by
first order non-linear differential equations. The algorithm is experimentally
verified by solving a Boolean logic task, classification tasks (handwritten
digits and letters), and an autoencoding task. This article also shows how the
parameters affect learning, especially the random matrices. We use the
pseudospectra analysis to investigate further how random matrices impact the
learning process. Finally, we discuss the biological plausibility of the
proposed algorithm, and how it can give rise to better computational models for
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Detorakis_G/0/1/0/all/0/1&quot;&gt;Georgios Detorakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartley_T/0/1/0/all/0/1&quot;&gt;Travis Bartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neftci_E/0/1/0/all/0/1&quot;&gt;Emre Neftci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07441">
<title>Wall Stress Estimation of Cerebral Aneurysm based on Zernike Convolutional Neural Networks. (arXiv:1806.07441v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07441</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (ConvNets) have demonstrated an exceptional
capacity to discern visual patterns from digital images and signals.
Unfortunately, such powerful ConvNets do not generalize well to
arbitrary-shaped manifolds, where data representation does not fit into a
tensor-like grid. Hence, many fields of science and engineering, where data
points possess some manifold structure, cannot enjoy the full benefits of the
recent advances in ConvNets. The aneurysm wall stress estimation problem
introduced in this paper is one of many such problems. The problem is
well-known to be of a paramount clinical importance, but yet, traditional
ConvNets cannot be applied due to the manifold structure of the data, neither
does the state-of-the-art geometric ConvNets perform well. Motivated by this,
we propose a new geometric ConvNet method named ZerNet, which builds upon our
novel mathematical generalization of convolution and pooling operations on
manifolds. Our study shows that the ZerNet outperforms the other
state-of-the-art geometric ConvNets in terms of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jia Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Stephen Baek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07464">
<title>Exploring the Semantic Content of Unsupervised Graph Embeddings: An Empirical Study. (arXiv:1806.07464v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07464</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph embeddings have become a key and widely used technique within the field
of graph mining, proving to be successful across a broad range of domains
including social, citation, transportation and biological. Graph embedding
techniques aim to automatically create a low-dimensional representation of a
given graph, which captures key structural elements in the resulting embedding
space. However, to date, there has been little work exploring exactly which
topological structures are being learned in the embeddings process. In this
paper, we investigate if graph embeddings are approximating something analogous
with traditional vertex level graph features. If such a relationship can be
found, it could be used to provide a theoretical insight into how graph
embedding approaches function. We perform this investigation by predicting
known topological features, using supervised and unsupervised methods, directly
from the embedding space. If a mapping between the embeddings and topological
features can be found, then we argue that the structural information
encapsulated by the features is represented in the embedding space. To explore
this, we present extensive experimental evaluation from five state-of-the-art
unsupervised graph embedding techniques, across a range of empirical graph
datasets, measuring a selection of topological features. We demonstrate that
several topological features are indeed being approximated by the embedding
space, allowing key insight into how graph embeddings create good
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonner_S/0/1/0/all/0/1&quot;&gt;Stephen Bonner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kureshi_I/0/1/0/all/0/1&quot;&gt;Ibad Kureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brennan_J/0/1/0/all/0/1&quot;&gt;John Brennan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodoropoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Theodoropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1&quot;&gt;Andrew Stephen McGough&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obara_B/0/1/0/all/0/1&quot;&gt;Boguslaw Obara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07492">
<title>On the Learning of Deep Local Features for Robust Face Spoofing Detection. (arXiv:1806.07492v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07492</link>
<description rdf:parseType="Literal">&lt;p&gt;Biometrics emerged as a robust solution for security systems. However, given
the widespread of biometric applications, criminals are developing techniques
to circumvent them by simulating physical or behavioral traits of legal users
(spoofing attacks). Despite face being a promising characteristic due to its
universality, acceptability and presence of cameras almost everywhere, face
recognition systems are extremely vulnerable to such frauds since they can be
easily fooled with common printed facial photographs. State-of-the-art
approaches, based on Convolutional Neural Networks (CNNs), present good results
in face spoofing detection. However, these methods do not exploit the
importance of learning deep local features from each facial region, even though
it is known from face recognition that different face regions have much
different visual aspects, that can also be exploited for face spoofing
detection. In this work we propose a novel CNN architecture trained in two
steps for such task. Initially, each part of the neural network learns features
from a given facial region. After, the whole model is fine-tuned on the whole
facial images. Results show that such pretraining step allows the CNN to learn
different local spoofing cues, improving the performance and convergence speed
of the final model, outperforming the state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_G/0/1/0/all/0/1&quot;&gt;Gustavo Botelho de Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Paulo Papa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marana_A/0/1/0/all/0/1&quot;&gt;Aparecido Nilceu Marana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07528">
<title>Uncertainty in Multitask Transfer Learning. (arXiv:1806.07528v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;Using variational Bayes neural networks, we develop an algorithm capable of
accumulating knowledge into a prior from multiple different tasks. The result
is a rich and meaningful prior capable of few-shot learning on new tasks. The
posterior can go beyond the mean field approximation and yields good
uncertainty on the performed experiments. Analysis on toy tasks shows that it
can learn from significantly different tasks while finding similarities among
them. Experiments of Mini-Imagenet yields the new state of the art with 74.5%
accuracy on 5 shot learning. Finally, we provide experiments showing that other
existing methods can fail to perform well in different benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oreshkin_B/0/1/0/all/0/1&quot;&gt;Boris Oreshkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chung_W/0/1/0/all/0/1&quot;&gt;Wonchang Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boquet_T/0/1/0/all/0/1&quot;&gt;Thomas Boquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rostamzadeh_N/0/1/0/all/0/1&quot;&gt;Negar Rostamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07538">
<title>Towards Robust Interpretability with Self-Explaining Neural Networks. (arXiv:1806.07538v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07538</link>
<description rdf:parseType="Literal">&lt;p&gt;Most recent work on interpretability of complex machine learning models has
focused on estimating $\textit{a posteriori}$ explanations for previously
trained models around specific predictions. $\textit{Self-explaining}$ models
where interpretability plays a key role already during learning have received
much less attention. We propose three desiderata for explanations in general --
explicitness, faithfulness, and stability -- and show that existing methods do
not satisfy them. In response, we design self-explaining models in stages,
progressively generalizing linear classifiers to complex yet architecturally
explicit models. Faithfulness and stability are enforced via regularization
specifically tailored to such models. Experimental results across various
benchmark datasets show that our framework offers a promising direction for
reconciling model complexity and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1&quot;&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi S. Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07692">
<title>Reinforcement Learning using Augmented Neural Networks. (arXiv:1806.07692v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07692</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks allow Q-learning reinforcement learning agents such as deep
Q-networks (DQN) to approximate complex mappings from state spaces to value
functions. However, this also brings drawbacks when compared to other function
approximators such as tile coding or their generalisations, radial basis
functions (RBF) because they introduce instability due to the side effect of
globalised updates present in neural networks. This instability does not even
vanish in neural networks that do not have any hidden layers. In this paper, we
show that simple modifications to the structure of the neural network can
improve stability of DQN learning when a multi-layer perceptron is used for
function approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shannon_J/0/1/0/all/0/1&quot;&gt;Jack Shannon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grzes_M/0/1/0/all/0/1&quot;&gt;Marek Grzes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07751">
<title>Versatile Auxiliary Classifier with Generative Adversarial Network (VAC+GAN), Multi Class Scenarios. (arXiv:1806.07751v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07751</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional generators learn the data distribution for each class in a
multi-class scenario and generate samples for a specific class given the right
input from the latent space. In this work, a method known as &quot;Versatile
Auxiliary Classifier with Generative Adversarial Network&quot; for multi-class
scenarios is presented. In this technique, the Generative Adversarial Networks
(GAN)&apos;s generator is turned into a conditional generator by placing a
multi-class classifier in parallel with the discriminator network and
backpropagate the classification error through the generator. This technique is
versatile enough to be applied to any GAN implementation. The results on two
databases and comparisons with other method are provided as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazrafkan_S/0/1/0/all/0/1&quot;&gt;Shabab Bazrafkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07789">
<title>Quaternion Convolutional Neural Networks for End-to-End Automatic Speech Recognition. (arXiv:1806.07789v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.07789</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the connectionist temporal classification (CTC) model coupled with
recurrent (RNN) or convolutional neural networks (CNN), made it easier to train
speech recognition systems in an end-to-end fashion. However in real-valued
models, time frame components such as mel-filter-bank energies and the cepstral
coefficients obtained from them, together with their first and second order
derivatives, are processed as individual elements, while a natural alternative
is to process such components as composed entities. We propose to group such
elements in the form of quaternions and to process these quaternions using the
established quaternion algebra. Quaternion numbers and quaternion neural
networks have shown their efficiency to process multidimensional inputs as
entities, to encode internal dependencies, and to solve many tasks with less
learning parameters than real-valued models. This paper proposes to integrate
multiple feature views in quaternion-valued convolutional neural network
(QCNN), to be used for sequence-to-sequence mapping with the CTC model.
Promising results are reported using simple QCNNs in phoneme recognition
experiments with the TIMIT corpus. More precisely, QCNNs obtain a lower phoneme
error rate (PER) with less learning parameters than a competing model based on
real-valued CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1&quot;&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morchid_M/0/1/0/all/0/1&quot;&gt;Mohamed Morchid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trabelsi_C/0/1/0/all/0/1&quot;&gt;Chiheb Trabelsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linares_G/0/1/0/all/0/1&quot;&gt;Georges Linar&amp;#xe8;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_R/0/1/0/all/0/1&quot;&gt;Renato De Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07808">
<title>Learning One-hidden-layer ReLU Networks via Gradient Descent. (arXiv:1806.07808v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07808</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning one-hidden-layer neural networks with
Rectified Linear Unit (ReLU) activation function, where the inputs are sampled
from standard Gaussian distribution and the outputs are generated from a noisy
teacher network. We analyze the performance of gradient descent for training
such kind of neural networks based on empirical risk minimization, and provide
algorithm-dependent guarantees. In particular, we prove that tensor
initialization followed by gradient descent can converge to the ground-truth
parameters at a linear rate up to some statistical error. To the best of our
knowledge, this is the first work characterizing the recovery guarantee for
practical learning of one-hidden-layer ReLU networks with multiple neurons.
Numerical experiments verify our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Quanquan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07863">
<title>Learning ReLU Networks via Alternating Minimization. (arXiv:1806.07863v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07863</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and analyze a new family of algorithms for training neural
networks with ReLU activations. Our algorithms are based on the technique of
alternating minimization: estimating the activation patterns of each ReLU for
all given samples, interleaved with weight updates via a least-squares step. We
consider three different cases of this model: (i) a single ReLU; (ii) 1-hidden
layer networks with $k$ hidden ReLUs (iii) 2-hidden layer networks. We show
that under standard distributional assumptions on the input data, our algorithm
provably recovers the true &quot;ground truth&quot; parameters in a linearly convergent
fashion; furthermore, our method exhibits requires only $O(d)$ samples for the
single ReLU case and $\widetilde{O}(dk^2)$ samples in the 1-hidden layer case.
We also extend this framework to deeper networks, and empirically demonstrate
its convergence to a global minimum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagatap_G/0/1/0/all/0/1&quot;&gt;Gauri Jagatap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10321">
<title>Learning Structural Node Embeddings Via Diffusion Wavelets. (arXiv:1710.10321v4 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10321</link>
<description rdf:parseType="Literal">&lt;p&gt;Nodes residing in different parts of a graph can have similar structural
roles within their local network topology. The identification of such roles
provides key insight into the organization of networks and can be used for a
variety of machine learning tasks. However, learning structural representations
of nodes is a challenging problem, and it has typically involved manually
specifying and tailoring topological features for each node. In this paper, we
develop GraphWave, a method that represents each node&apos;s network neighborhood
via a low-dimensional embedding by leveraging heat wavelet diffusion patterns.
Instead of training on hand-selected features, GraphWave learns these
embeddings in an unsupervised way. We mathematically prove that nodes with
similar network neighborhoods will have similar GraphWave embeddings even
though these nodes may reside in very different parts of the network, and our
method scales linearly with the number of edges. Experiments in a variety of
different settings demonstrate GraphWave&apos;s real-world potential for capturing
structural roles in networks, and our approach outperforms existing
state-of-the-art baselines in every experiment, by as much as 137%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donnat_C/0/1/0/all/0/1&quot;&gt;Claire Donnat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallac_D/0/1/0/all/0/1&quot;&gt;David Hallac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06788">
<title>MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks. (arXiv:1711.06788v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06788</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MinimalRNN, a new recurrent neural network architecture that
achieves comparable performance as the popular gated RNNs with a simplified
structure. It employs minimal updates within RNN, which not only leads to
efficient learning and testing but more importantly better interpretability and
trainability. We demonstrate that by endorsing the more restrictive update
rule, MinimalRNN learns disentangled RNN states. We further examine the
learning dynamics of different RNN structures using input-output Jacobians, and
show that MinimalRNN is able to capture longer range dependencies than existing
RNN architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minmin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04223">
<title>SparseMAP: Differentiable Sparse Structured Inference. (arXiv:1802.04223v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04223</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured prediction requires searching over a combinatorial number of
structures. To tackle it, we introduce SparseMAP: a new method for sparse
structured inference, and its natural loss function. SparseMAP automatically
selects only a few global structures: it is situated between MAP inference,
which picks a single structure, and marginal inference, which assigns
probability mass to all structures, including implausible ones. Importantly,
SparseMAP can be computed using only calls to a MAP oracle, making it
applicable to problems with intractable marginal inference, e.g., linear
assignment. Sparsity makes gradient backpropagation efficient regardless of the
structure, enabling us to augment deep neural networks with generic and sparse
structured hidden layers. Experiments in dependency parsing and natural
language inference reveal competitive accuracy, improved interpretability, and
the ability to capture natural language ambiguities, which is attractive for
pipeline systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niculae_V/0/1/0/all/0/1&quot;&gt;Vlad Niculae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martins_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blondel_M/0/1/0/all/0/1&quot;&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cardie_C/0/1/0/all/0/1&quot;&gt;Claire Cardie&lt;/a&gt;</dc:creator>
</item></rdf:RDF>