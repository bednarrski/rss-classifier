<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05859"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1410.0736"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00109"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00245"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00361"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00532"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00645"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08290"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00140"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00218"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00236"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00338"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00408"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00425"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00448"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03656"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10122"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.00222">
<title>Learning Unsupervised Learning Rules. (arXiv:1804.00222v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00222</link>
<description rdf:parseType="Literal">&lt;p&gt;A major goal of unsupervised learning is to discover data representations
that are useful for subsequent tasks, without access to supervised labels
during training. Typically, this goal is approached by minimizing a surrogate
objective, such as the negative log likelihood of a generative model, with the
hope that representations useful for subsequent tasks will arise as a side
effect. In this work, we propose instead to directly target a later desired
task by meta-learning an unsupervised learning rule, which leads to
representations useful for that task. Here, our desired task (meta-objective)
is the performance of the representation on semi-supervised classification, and
we meta-learn an algorithm -- an unsupervised weight update rule -- that
produces representations that perform well under this meta-objective.
Additionally, we constrain our unsupervised update rule to a be a
biologically-motivated, neuron-local function, which enables it to generalize
to novel neural network architectures. We show that the meta-learned update
rule produces useful features and sometimes outperforms existing unsupervised
learning techniques. We show that the meta-learned unsupervised update rule
generalizes to train networks with different widths, depths, and
nonlinearities. It also generalizes to train on data with randomly permuted
input dimensions and even generalizes from image datasets to a text task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1&quot;&gt;Luke Metz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheswaranathan_N/0/1/0/all/0/1&quot;&gt;Niru Maheswaranathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1&quot;&gt;Brian Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00497">
<title>$\mu$Net: A Highly Compact Deep Convolutional Neural Network Architecture for Real-time Embedded Traffic Sign Classification. (arXiv:1804.00497v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00497</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic sign recognition is a very important computer vision task for a
number of real-world applications such as intelligent transportation
surveillance and analysis. While deep neural networks have been demonstrated in
recent years to provide state-of-the-art performance traffic sign recognition,
a key challenge for enabling the widespread deployment of deep neural networks
for embedded traffic sign recognition is the high computational and memory
requirements of such networks. As a consequence, there are significant benefits
in investigating compact deep neural network architectures for traffic sign
recognition that are better suited for embedded devices. In this paper, we
introduce $\mu$Net, a highly compact deep convolutional neural network for
real-time embedded traffic sign recognition based on macroarchitecture design
principles as well as microarchitecture optimization strategies. The resulting
overall architecture of $\mu$Net is thus designed with as few parameters and
computations as possible while maintaining recognition performance. The
resulting $\mu$Net possesses a model size of just ~1MB and ~510,000 parameters
(~27x fewer parameters than state-of-the-art) while still achieving a human
performance level top-1 accuracy of 98.9% on the German traffic sign
recognition benchmark. Furthermore, $\mu$Net requires just $\sim$10 million
multiply-accumulate operations to perform inference. These experimental results
show that highly compact, optimized deep neural network architectures can be
designed for real-time traffic sign recognition that are well-suited for
embedded scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jules_M/0/1/0/all/0/1&quot;&gt;Michael St. Jules&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05859">
<title>Neural Network Quine. (arXiv:1803.05859v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05859</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-replication is a key aspect of biological life that has been largely
overlooked in Artificial Intelligence systems. Here we describe how to build
and train self-replicating neural networks. The network replicates itself by
learning to output its own weights. The network is designed using a loss
function that can be optimized with either gradient-based or non-gradient-based
methods. We also describe a method we call regeneration to train the network
without explicit optimization, by injecting the network with predictions of its
own parameters. The best solution for a self-replicating network was found by
alternating between regeneration and optimization steps. Finally, we describe a
design for a self-replicating neural network that can solve an auxiliary task
such as MNIST image classification. We observe that there is a trade-off
between the network&apos;s ability to classify images and its ability to replicate,
but training is biased towards increasing its specialization at image
classification at the expense of replication. This is analogous to the
trade-off between reproduction and other tasks observed in nature. We suggest
that a self-replication mechanism for artificial intelligence is useful because
it introduces the possibility of continual improvement through natural
selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_O/0/1/0/all/0/1&quot;&gt;Oscar Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1410.0736">
<title>HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition. (arXiv:1410.0736v4 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1410.0736</link>
<description rdf:parseType="Literal">&lt;p&gt;In image classification, visual separability between different object
categories is highly uneven, and some categories are more difficult to
distinguish than others. Such difficult categories demand more dedicated
classifiers. However, existing deep convolutional neural networks (CNN) are
trained as flat N-way classifiers, and few efforts have been made to leverage
the hierarchical structure of categories. In this paper, we introduce
hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category
hierarchy. An HD-CNN separates easy classes using a coarse category classifier
while distinguishing difficult classes using fine category classifiers. During
HD-CNN training, component-wise pretraining is followed by global finetuning
with a multinomial logistic loss regularized by a coarse category consistency
term. In addition, conditional executions of fine category classifiers and
layer parameter compression make HD-CNNs scalable for large-scale visual
recognition. We achieve state-of-the-art results on both CIFAR100 and
large-scale ImageNet 1000-class benchmark datasets. In our experiments, we
build up three different HD-CNNs and they lower the top-1 error of the standard
CNNs by 2.65%, 3.1% and 1.1%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1&quot;&gt;Robinson Piramuthu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagadeesh_V/0/1/0/all/0/1&quot;&gt;Vignesh Jagadeesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeCoste_D/0/1/0/all/0/1&quot;&gt;Dennis DeCoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_W/0/1/0/all/0/1&quot;&gt;Wei Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00062">
<title>Visual Robot Task Planning. (arXiv:1804.00062v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.00062</link>
<description rdf:parseType="Literal">&lt;p&gt;Prospection, the act of predicting the consequences of many possible futures,
is intrinsic to human planning and action, and may even be at the root of
consciousness. Surprisingly, this idea has been explored comparatively little
in robotics. In this work, we propose a neural network architecture and
associated planning algorithm that (1) learns a representation of the world
useful for generating prospective futures after the application of high-level
actions, (2) uses this generative model to simulate the result of sequences of
high-level actions in a variety of environments, and (3) uses this same
representation to evaluate these actions and perform tree search to find a
sequence of high-level actions in a new environment. Models are trained via
imitation learning on a variety of domains, including navigation,
pick-and-place, and a surgical robotics task. Our approach allows us to
visualize intermediate motion goals and learn to plan complex activity from
visual information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1&quot;&gt;Chris Paxton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnoy_Y/0/1/0/all/0/1&quot;&gt;Yotam Barnoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katyal_K/0/1/0/all/0/1&quot;&gt;Kapil Katyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1&quot;&gt;Gregory D. Hager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00109">
<title>QDEE: Question Difficulty and Expertise Estimation in Community Question Answering Sites. (arXiv:1804.00109v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1804.00109</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a framework for Question Difficulty and Expertise
Estimation (QDEE) in Community Question Answering sites (CQAs) such as Yahoo!
Answers and Stack Overflow, which tackles a fundamental challenge in
crowdsourcing: how to appropriately route and assign questions to users with
the suitable expertise. This problem domain has been the subject of much
research and includes both language-agnostic as well as language conscious
solutions. We bring to bear a key language-agnostic insight: that users gain
expertise and therefore tend to ask as well as answer more difficult questions
over time. We use this insight within the popular competition (directed) graph
model to estimate question difficulty and user expertise by identifying key
hierarchical structure within said model. An important and novel contribution
here is the application of &quot;social agony&quot; to this problem domain. Difficulty
levels of newly posted questions (the cold-start problem) are estimated by
using our QDEE framework and additional textual features. We also propose a
model to route newly posted questions to appropriate users based on the
difficulty level of the question and the expertise of the user. Extensive
experiments on real world CQAs such as Yahoo! Answers and Stack Overflow data
demonstrate the improved efficacy of our approach over contemporary
state-of-the-art models. The QDEE framework also allows us to characterize user
expertise in novel ways by identifying interesting patterns and roles played by
different users in such CQAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosavi_S/0/1/0/all/0/1&quot;&gt;Sobhan Moosavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramnath_R/0/1/0/all/0/1&quot;&gt;Rajiv Ramnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1&quot;&gt;Srinivasan Parthasarathy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00168">
<title>Learning to Navigate in Cities Without a Map. (arXiv:1804.00168v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00168</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating through unstructured environments is a basic capability of
intelligent creatures, and thus is of fundamental interest in the study and
development of artificial intelligence. Long-range navigation is a complex
cognitive task that relies on developing an internal representation of space,
grounded by recognisable landmarks and robust visual processing, that can
simultaneously support continuous self-localisation (&quot;I am here&quot;) and a
representation of the goal (&quot;I am going there&quot;). Building upon recent research
that applies deep reinforcement learning to maze navigation problems, we
present an end-to-end deep reinforcement learning approach that can be applied
on a city scale. Recognising that successful navigation relies on integration
of general policies with locale-specific knowledge, we propose a dual pathway
architecture that allows locale-specific features to be encapsulated, while
still enabling transfer to multiple cities. We present an interactive
navigation environment that uses Google StreetView for its photographic content
and worldwide coverage, and demonstrate that our learning method allows agents
to learn to navigate multiple cities and to traverse to target destinations
that may be kilometres away. A video summarizing our research and showing the
trained agent in diverse city environments as well as on the transfer task is
available at: goo.gl/ESUfho.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirowski_P/0/1/0/all/0/1&quot;&gt;Piotr Mirowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimes_M/0/1/0/all/0/1&quot;&gt;Matthew Koichi Grimes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1&quot;&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermann_K/0/1/0/all/0/1&quot;&gt;Karl Moritz Hermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_K/0/1/0/all/0/1&quot;&gt;Keith Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teplyashin_D/0/1/0/all/0/1&quot;&gt;Denis Teplyashin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1&quot;&gt;Karen Simonyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kavukcuoglu_K/0/1/0/all/0/1&quot;&gt;Koray Kavukcuoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1&quot;&gt;Raia Hadsell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00245">
<title>Modeling Individual Differences in Game Behavior using HMM. (arXiv:1804.00245v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00245</link>
<description rdf:parseType="Literal">&lt;p&gt;Player modeling is an important concept that has gained much attention in
game research due to its utility in developing adaptive techniques to target
better designs for engagement and retention. Previous work has explored
modeling individual differences using machine learning algorithms per- formed
on aggregated game actions. However, players&apos; individual differences may be
better manifested through sequential patterns of the in-game player&apos;s actions.
While few works have explored sequential analysis of player data, none have
explored the use of Hidden Markov Models (HMM) to model individual differences,
which is the topic of this paper. In par- ticular, we developed a modeling
approach using data col- lected from players playing a Role-Playing Game (RPG).
Our proposed approach is two fold: 1. We present a Hidden Markov Model (HMM) of
player in-game behaviors to model individual differences, and 2. using the
output of the HMM, we generate behavioral features used to classify real world
players&apos; characteristics, including game expertise and the big five personality
traits. Our results show predictive power for some of personality traits, such
as game expertise and conscientiousness, but the most influential factor was
game expertise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunian_S/0/1/0/all/0/1&quot;&gt;Sara Bunian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canossa_A/0/1/0/all/0/1&quot;&gt;Alessandro Canossa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colvin_R/0/1/0/all/0/1&quot;&gt;Randy Colvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Nasr_M/0/1/0/all/0/1&quot;&gt;Magy Seif El-Nasr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00293">
<title>Attentional Multilabel Learning over Graphs - A message passing approach. (arXiv:1804.00293v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00293</link>
<description rdf:parseType="Literal">&lt;p&gt;We address a largely open problem of multilabel classification over graphs.
Unlike traditional vector input, a graph has rich variable-size structures,
that suggests complex relationships between labels and subgraphs. Uncovering
these relations might hold the keys of classification performance and
explainability. To this end, we design GAML (Graph Attentional Multi-Label
learning), a graph neural network that models the relations present in the
input graph, in the label set, and across graph-labels by leveraging the
message passing algorithm and attention mechanism. Representation of labels and
input nodes is refined iteratively through multiple steps, during which
interesting subgraph-label patterns emerge. In addition, GAML is highly
flexible by allowing explicit label dependencies to be incorporated easily. It
also scales linearly with the number of labels and graph size thanks to our
proposed hierarchical attention. These properties open a wide range of
applications seen in the real world. We evaluate GAML on an extensive set of
experiments with both graph inputs (for predicting drug-protein binding, and
drug-cancer response), and classical unstructured inputs. The results are
significantly better than well-known multilabel learning techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1&quot;&gt;Kien Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Truyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thin Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00361">
<title>Learning to Run challenge solutions: Adapting reinforcement learning methods for neuromusculoskeletal environments. (arXiv:1804.00361v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00361</link>
<description rdf:parseType="Literal">&lt;p&gt;In the NIPS 2017 Learning to Run challenge, participants were tasked with
building a controller for a musculoskeletal model to make it run as fast as
possible through an obstacle course. Top participants were invited to describe
their algorithms. In this work, we present eight solutions that used deep
reinforcement learning approaches, based on algorithms such as Deep
Deterministic Policy Gradient, Proximal Policy Optimization, and Trust Region
Policy Optimization. Many solutions use similar relaxations and heuristics,
such as reward shaping, frame skipping, discretization of the action space,
symmetry, and policy blending. However, each of the eight teams implemented
different modifications of the known algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kidzinski_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Kidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1&quot;&gt;Sharada Prasanna Mohanty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1&quot;&gt;Carmichael Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhewei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuchang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenko_A/0/1/0/all/0/1&quot;&gt;Anton Pechenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stelmaszczyk_A/0/1/0/all/0/1&quot;&gt;Adam Stelmaszczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jarosik_P/0/1/0/all/0/1&quot;&gt;Piotr Jarosik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlov_M/0/1/0/all/0/1&quot;&gt;Mikhail Pavlov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolesnikov_S/0/1/0/all/0/1&quot;&gt;Sergey Kolesnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1&quot;&gt;Sergey Plis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhibo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiale Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhuobin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhihui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1&quot;&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1&quot;&gt;Piotr Mi&amp;#x142;o&amp;#x15b;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1&quot;&gt;B&amp;#x142;a&amp;#x17c;ej Osi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1&quot;&gt;Andrew Melnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schilling_M/0/1/0/all/0/1&quot;&gt;Malte Schilling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1&quot;&gt;Helge Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carroll_S/0/1/0/all/0/1&quot;&gt;Sean Carroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hicks_J/0/1/0/all/0/1&quot;&gt;Jennifer Hicks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salathe_M/0/1/0/all/0/1&quot;&gt;Marcel Salath&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delp_S/0/1/0/all/0/1&quot;&gt;Scott Delp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00532">
<title>Predictions of short-term driving intention using recurrent neural network on sequential data. (arXiv:1804.00532v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00532</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictions of driver&apos;s intentions and their behaviors using the road is of
great importance for planning and decision making processes of autonomous
driving vehicles. In particular, relatively short-term driving intentions are
the fundamental units that constitute more sophisticated driving goals,
behaviors, such as overtaking the slow vehicle in front, exit or merge onto a
high way, etc. While it is not uncommon that most of the time human driver can
rationalize, in advance, various on-road behaviors, intentions, as well as the
associated risks, aggressiveness, reciprocity characteristics, etc., such
reasoning skills can be challenging and difficult for an autonomous driving
system to learn. In this article, we demonstrate a disciplined methodology that
can be used to build and train a predictive drive system, therefore to learn
the on-road characteristics aforementioned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhou Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Fei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00538">
<title>Investigating Capsule Networks with Dynamic Routing for Text Classification. (arXiv:1804.00538v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.00538</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we explore capsule networks with dynamic routing for text
classification. We propose three strategies to stabilize the dynamic routing
process to alleviate the disturbance of some noise capsules which may contain
&quot;background&quot; information or have not been successfully trained. A series of
experiments are conducted with capsule networks on six text classification
benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets,
which shows the effectiveness of capsule networks for text classification. We
additionally show that capsule networks exhibit significant improvement when
transfer single-label to multi-label text classification over strong baseline
methods. To the best of our knowledge, this is the first work that capsule
networks have been empirically investigated for text modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jianbo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zeyang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Suofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00596">
<title>Learning to Prove with Tactics. (arXiv:1804.00596v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00596</link>
<description rdf:parseType="Literal">&lt;p&gt;We implement a automated tactical prover TacticToe on top of the HOL4
interactive theorem prover. TacticToe learns from human proofs which
mathematical technique is suitable in each proof situation. This knowledge is
then used in a Monte Carlo tree search algorithm to explore promising
tactic-level proof paths. On a single CPU, with a time limit of 60 seconds,
TacticToe proves 66.4 percent of the 7164 theorems in HOL4&apos;s standard library,
whereas E prover with auto-schedule solves 34.5 percent. The success rate rises
to 69.0 percent by combining the results of TacticToe and E prover.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauthier_T/0/1/0/all/0/1&quot;&gt;Thibault Gauthier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1&quot;&gt;Cezary Kaliszyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1&quot;&gt;Josef Urban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Ramana Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norrish_M/0/1/0/all/0/1&quot;&gt;Michael Norrish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00645">
<title>Universal Planning Networks. (arXiv:1804.00645v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00645</link>
<description rdf:parseType="Literal">&lt;p&gt;A key challenge in complex visuomotor control is learning abstract
representations that are effective for specifying goals, planning, and
generalization. To this end, we introduce universal planning networks (UPN).
UPNs embed differentiable planning within a goal-directed policy. This planning
computation unrolls a forward model in a latent space and infers an optimal
action plan through gradient descent trajectory optimization. The
plan-by-gradient-descent process and its underlying representations are learned
end-to-end to directly optimize a supervised imitation learning objective. We
find that the representations learned are not only effective for goal-directed
visual imitation via gradient-based trajectory optimization, but can also
provide a metric for specifying goals using images. The learned representations
can be leveraged to specify distance-based rewards to reach new target states
for model-free reinforcement learning, resulting in substantially more
effective learning when solving new tasks described via image-based goals. We
were able to achieve successful transfer of visuomotor planning strategies
across robots with significantly different morphologies and actuation
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1&quot;&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabri_A/0/1/0/all/0/1&quot;&gt;Allan Jabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06744">
<title>Learning to Organize Knowledge with N-Gram Machines. (arXiv:1711.06744v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06744</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) had great success on NLP tasks such as language
modeling, machine translation and certain question answering (QA) tasks.
However, the success is limited at more knowledge intensive tasks such as QA
from a big corpus. Existing end-to-end deep QA models (Miller et al., 2016;
Weston et al., 2014) need to read the entire text after observing the question,
and therefore their complexity in responding a question is linear in the text
size. This is prohibitive for practical tasks such as QA from Wikipedia, a
novel, or the Web. We propose to solve this scalability issue by using symbolic
meaning representations, which can be indexed and retrieved efficiently with
complexity that is independent of the text size. More specifically, we use
sequence-to-sequence models to encode knowledge symbolically and generate
programs to answer questions from the encoded knowledge. We apply our approach,
called the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a
special version of them (&quot;life-long bAbI&quot;) which has stories of up to 10
million sentences. Our experiments show that NGM can successfully solve both of
these tasks accurately and efficiently. Unlike fully differentiable memory
models, NGM&apos;s time complexity and answering quality are not affected by the
story length. The whole system of NGM is trained end-to-end with REINFORCE
(Williams, 1992). To avoid high variance in gradient estimation, which is
typical in discrete latent variable models, we use beam search instead of
sampling. To tackle the exponentially large search space, we use a stabilized
auto-encoding objective and a structure tweak procedure to iteratively reduce
and refine the search space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiazhong Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1&quot;&gt;William W. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1&quot;&gt;Ni Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08290">
<title>CSGNet: Neural Shape Parser for Constructive Solid Geometry. (arXiv:1712.08290v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08290</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural architecture that takes as input a 2D or 3D shape and
outputs a program that generates the shape. The instructions in our program are
based on constructive solid geometry principles, i.e., a set of boolean
operations on shape primitives defined recursively. Bottom-up techniques for
this shape parsing task rely on primitive detection and are inherently slow
since the search space over possible primitive combinations is large. In
contrast, our model uses a recurrent neural network that parses the input shape
in a top-down manner, which is significantly faster and yields a compact and
easy-to-interpret sequence of modeling instructions. Our model is also more
effective as a shape detector compared to existing state-of-the-art detection
techniques. We finally demonstrate that our network can be trained on novel
datasets without ground-truth program annotations through policy gradient
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gopal Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_R/0/1/0/all/0/1&quot;&gt;Rishabh Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Difan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1&quot;&gt;Evangelos Kalogerakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1&quot;&gt;Subhransu Maji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00085">
<title>Learning Structural Weight Uncertainty for Sequential Decision-Making. (arXiv:1801.00085v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00085</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning probability distributions on the weights of neural networks (NNs)
has recently proven beneficial in many applications. Bayesian methods, such as
Stein variational gradient descent (SVGD), offer an elegant framework to reason
about NN model uncertainty. However, by assuming independent Gaussian priors
for the individual NN weights (as often applied), SVGD does not impose prior
knowledge that there is often structural information (dependence) among
weights. We propose efficient posterior learning of structural weight
uncertainty, within an SVGD framework, by employing matrix variate Gaussian
priors on NN parameters. We further investigate the learned structural
uncertainty in sequential decision-making problems, including contextual
bandits and reinforcement learning. Experiments on several synthetic and real
datasets indicate the superiority of our model, compared with state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03774">
<title>Learning Multiple Levels of Representations with Kernel Machines. (arXiv:1802.03774v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03774</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a connectionist-inspired kernel machine model with three key
advantages over traditional kernel machines. First, it is capable of learning
distributed and hierarchical representations. Second, its performance is highly
robust to the choice of kernel function. Third, the solution space is not
limited to the span of images of training data in reproducing kernel Hilbert
space (RKHS). Together with the architecture, we propose a greedy learning
algorithm that allows the proposed multilayer network to be trained layer-wise
without backpropagation by optimizing the geometric properties of images in
RKHS. With a single fixed generic kernel for each layer and two layers in
total, our model compares favorably with state-of-the-art multiple kernel
learning algorithms using significantly more kernels and popular deep
architectures on widely used classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shiyu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunmei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00021">
<title>Hierarchical Transfer Convolutional Neural Networks for Image Classification. (arXiv:1804.00021v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00021</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the issue of how to enhance the generalization
performance of convolutional neural networks (CNN) in the early learning stage
for image classification. This is motivated by real-time applications that
require the generalization performance of CNN to be satisfactory within limited
training time. In order to achieve this, a novel hierarchical transfer CNN
framework is proposed. It consists of a group of shallow CNNs and a cloud CNN,
where the shallow CNNs are trained firstly and then the first layers of the
trained shallow CNNs are used to initialize the first layer of the cloud CNN.
This method will boost the generalization performance of the cloud CNN
significantly, especially during the early stage of training. Experiments using
CIFAR-10 and ImageNet datasets are performed to examine the proposed method.
Results demonstrate the improvement of testing accuracy is 12% on average and
as much as 20% for the CIFAR-10 case while 5% testing accuracy improvement for
the ImageNet case during the early stage of learning. It is also shown that
universal improvements of testing accuracy are obtained across different
settings of dropout and number of shallow CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xishuang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hsiang-Huang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuzhong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Lijun Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00097">
<title>Adversarial Attacks and Defences Competition. (arXiv:1804.00097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00097</link>
<description rdf:parseType="Literal">&lt;p&gt;To accelerate research on adversarial examples and robustness of machine
learning classifiers, Google Brain organized a NIPS 2017 competition that
encouraged researchers to develop new methods to generate adversarial examples
as well as to develop new ways to defend against them. In this chapter, we
describe the structure and organization of the competition and the solutions
developed by several of the top-placing teams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurakin_A/0/1/0/all/0/1&quot;&gt;Alexey Kurakin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1&quot;&gt;Samy Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_F/0/1/0/all/0/1&quot;&gt;Fangzhou Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1&quot;&gt;Ming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhou Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sangxia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhonglin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;Junjiajia Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berdibekov_Y/0/1/0/all/0/1&quot;&gt;Yerkebulan Berdibekov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akiba_T/0/1/0/all/0/1&quot;&gt;Takuya Akiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokui_S/0/1/0/all/0/1&quot;&gt;Seiya Tokui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abe_M/0/1/0/all/0/1&quot;&gt;Motoki Abe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00140">
<title>Generative Adversarial Networks (GANs): What it can generate and What it cannot?. (arXiv:1804.00140v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00140</link>
<description rdf:parseType="Literal">&lt;p&gt;Why are Generative Adversarial Networks (GANs) so popular? What is the
purpose of designing GANs? Can we justify functioning of GANs theoretically?
How are the theoretical guarantees? Are there any shortcomings?
&lt;/p&gt;
&lt;p&gt;With the popularity of GANs, the researchers across the globe have been
perplexed by these questions. In the last year (2017), a plethora of research
papers attempted to answer the above questions. In this article, we put in our
best efforts to compare and contrast different results and put forth a summary
of theoretical contributions about GANs with focus on image/visual
applications. Our main aim is to highlight the primary issues related to GANs
that each of these papers examine. Besides we provide insight into how each of
the discussed articles solve the concerned problems. We expect this summary
paper to give a bird&apos;s eye view to a person wishing to understand the theory
behind GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manisha_P/0/1/0/all/0/1&quot;&gt;P Manisha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1&quot;&gt;Sujit Gujar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00218">
<title>Synthesis of Differentiable Functional Programs for Lifelong Learning. (arXiv:1804.00218v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00218</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neurosymbolic approach to the lifelong learning of algorithmic
tasks that mix perception and procedural reasoning. Reusing highlevel concepts
across domains and learning complex procedures are two key challenges in
lifelong learning. We show that a combination of gradientbased learning and
symbolic program synthesis can be a more effective response to these challenges
than purely neural methods. Concretely, our approach, called HOUDINI,
represents neural networks as strongly typed, end-to-end differentiable
functional programs that use symbolic higher-order combinators to compose a
library of neural functions. Our learning algorithm consists of: (1) a program
synthesizer that performs a type-directed search over programs in this
language, and decides on the library functions that should be reused and the
architectures that should be used to combine them; and (2) a neural module that
trains synthesized programs using stochastic gradient descent. We evaluate our
approach on three algorithmic tasks. Our experiments show that our
type-directed search technique is able to significantly prune the search space
of programs, and that the overall approach transfers high-level concepts more
effectively than monolithic neural networks as well as traditional transfer
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valkov_L/0/1/0/all/0/1&quot;&gt;Lazar Valkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_D/0/1/0/all/0/1&quot;&gt;Dipak Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1&quot;&gt;Charles Sutton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00236">
<title>Recognizing Challenging Handwritten Annotations with Fully Convolutional Networks. (arXiv:1804.00236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00236</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a very challenging dataset of historic German documents
and evaluates Fully Convolutional Neural Network (FCNN) based methods to locate
handwritten annotations of any kind in these documents. The handwritten
annotations can appear in form of underlines and text by using various writing
instruments, e.g., the use of pencils makes the data more challenging. We train
and evaluate various end-to-end semantic segmentation approaches and report the
results. The task is to classify the pixels of documents into two classes:
background and handwritten annotation. The best model achieves a mean
Intersection over Union (IoU) score of 95.6% on the test documents of the
presented dataset. We also present a comparison of different strategies used
for data augmentation and training on our presented dataset. For evaluation, we
use the Layout Analysis Evaluator for the ICDAR 2017 Competition on Layout
Analysis for Challenging Medieval Manuscripts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolsch_A/0/1/0/all/0/1&quot;&gt;Andreas K&amp;#xf6;lsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Ashutosh Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshneya_S/0/1/0/all/0/1&quot;&gt;Saurabh Varshneya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1&quot;&gt;Marcus Liwicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00306">
<title>Revisiting Skip-Gram Negative Sampling Model with Regularization. (arXiv:1804.00306v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.00306</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit skip-gram negative sampling (SGNS), a popular neural-network based
approach to learning distributed word representation. We first point out the
ambiguity issue undermining the SGNS model, in the sense that the word vectors
can be entirely distorted without changing the objective value. To resolve this
issue, we rectify the SGNS model with quadratic regularization. A theoretical
justification, which provides a novel insight into quadratic regularization, is
presented. Preliminary experiments are also conducted on Google&apos;s analytical
reasoning task to support the modified SGNS model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1&quot;&gt;Cun Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00335">
<title>Online learning with graph-structured feedback against adaptive adversaries. (arXiv:1804.00335v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00335</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive upper and lower bounds for the policy regret of $T$-round online
learning problems with graph-structured feedback, where the adversary is
nonoblivious but assumed to have a bounded memory. We obtain upper bounds of
$\widetilde O(T^{2/3})$ and $\widetilde O(T^{3/4})$ for strongly-observable and
weakly-observable graphs, respectively, based on analyzing a variant of the
Exp3 algorithm. When the adversary is allowed a bounded memory of size 1, we
show that a matching lower bound of $\widetilde\Omega(T^{2/3})$ is achieved in
the case of full-information feedback. We also study the particular loss
structure of an oblivious adversary with switching costs, and show that in such
a setting, non-revealing strongly-observable feedback graphs achieve a lower
bound of $\widetilde\Omega(T^{2/3})$, as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhili Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loh_P/0/1/0/all/0/1&quot;&gt;Po-Ling Loh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00338">
<title>Towards Intelligent Vehicular Networks: A Machine Learning Framework. (arXiv:1804.00338v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1804.00338</link>
<description rdf:parseType="Literal">&lt;p&gt;As wireless networks evolve towards high mobility and providing better
support for connected vehicles, a number of new challenges arise due to the
resulting high dynamics in vehicular environments and thus motive rethinking of
traditional wireless design methodologies. Future intelligent vehicles, which
are at the heart of high mobility networks, are increasingly equipped with
multiple advanced onboard sensors and keep generating large volumes of data.
Machine learning, as an effective approach to artificial intelligence, can
provide a rich set of tools to exploit such data for the benefit of the
networks. In this article, we first identify the distinctive characteristics of
high mobility vehicular networks and motivate the use of machine learning to
address the resulting challenges. After a brief introduction of the major
concepts of machine learning, we discuss its applications to learn the dynamics
of vehicular networks and make informed decisions to optimize network
performance. In particular, we discuss in greater detail the application of
reinforcement learning in managing network resources as an alternative to the
prevalent optimization approach. Finally, some open issues worth further
investigation are highlighted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Le Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Geoffrey Ye Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00403">
<title>A Note on Kaldi&apos;s PLDA Implementation. (arXiv:1804.00403v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00403</link>
<description rdf:parseType="Literal">&lt;p&gt;Some explanations to Kaldi&apos;s PLDA implementation to make formula derivation
easier to catch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1&quot;&gt;Ke Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00408">
<title>Sparse Gaussian ICA. (arXiv:1804.00408v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00408</link>
<description rdf:parseType="Literal">&lt;p&gt;Independent component analysis (ICA) is a cornerstone of modern data
analysis. Its goal is to recover a latent random vector S with independent
components from samples of X=AS where A is an unknown mixing matrix.
Critically, all existing methods for ICA rely on and exploit strongly the
assumption that S is not Gaussian as otherwise A becomes unidentifiable. In
this paper, we show that in fact one can handle the case of Gaussian components
by imposing structure on the matrix A. Specifically, we assume that A is sparse
and generic in the sense that it is generated from a sparse Bernoulli-Gaussian
ensemble. Under this condition, we give an efficient algorithm to recover the
columns of A given only the covariance matrix of X as input even when S has
several Gaussian components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abrahamsen_N/0/1/0/all/0/1&quot;&gt;Nilin Abrahamsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rigollet_P/0/1/0/all/0/1&quot;&gt;Philippe Rigollet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00425">
<title>High-quality nonparallel voice conversion based on cycle-consistent adversarial network. (arXiv:1804.00425v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.00425</link>
<description rdf:parseType="Literal">&lt;p&gt;Although voice conversion (VC) algorithms have achieved remarkable success
along with the development of machine learning, superior performance is still
difficult to achieve when using nonparallel data. In this paper, we propose
using a cycle-consistent adversarial network (CycleGAN) for nonparallel
data-based VC training. A CycleGAN is a generative adversarial network (GAN)
originally developed for unpaired image-to-image translation. A subjective
evaluation of inter-gender conversion demonstrated that the proposed method
significantly outperformed a method based on the Merlin open source neural
network speech synthesis system (a parallel VC system adapted for our setup)
and a GAN-based parallel VC system. This is the first research to show that the
performance of a nonparallel VC method can exceed that of state-of-the-art
parallel VC methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fuming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Echizen_I/0/1/0/all/0/1&quot;&gt;Isao Echizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00448">
<title>Fixed-sized representation learning from Offline Handwritten Signatures of different sizes. (arXiv:1804.00448v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00448</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for learning feature representations for Offline Handwritten
Signature Verification have been successfully proposed in recent literature,
using Deep Convolutional Neural Networks to learn representations from
signature pixels. Such methods reported large performance improvements compared
to handcrafted feature extractors. However, they also introduced an important
constraint: the inputs to the neural networks must have a fixed size, while
signatures vary significantly in size between different users. In this paper we
propose addressing this issue by learning a fixed-sized representation from
variable-sized signatures by modifying the network architecture, using Spatial
Pyramid Pooling. We also investigate the impact of the resolution of the images
used for training, and the impact of adapting (fine-tuning) the representations
to new operating conditions (different acquisition protocols, such as writing
instruments and scan resolution). On the GPDS dataset, we achieve results
comparable with the state-of-the-art, while removing the constraint of having a
maximum size for the signatures to be processed. We also show that using higher
resolutions (300 or 600dpi) can improve performance when skilled forgeries from
a subset of users are available for feature learning, but lower resolutions
(around 100dpi) can be used if only genuine signatures are used. Lastly, we
show that fine-tuning can improve performance when the operating conditions
change.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafemann_L/0/1/0/all/0/1&quot;&gt;Luiz G. Hafemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1&quot;&gt;Luiz S. Oliveira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00550">
<title>On Unifying Deep Generative Models. (arXiv:1706.00550v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00550</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have achieved impressive success in recent years.
Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as
powerful frameworks for deep generative model learning, have largely been
considered as two distinct paradigms and received extensive independent studies
respectively. This paper aims to establish formal connections between GANs and
VAEs through a new formulation of them. We interpret sample generation in GANs
as performing posterior inference, and show that GANs and VAEs involve
minimizing KL divergences of respective posterior and inference distributions
with opposite directions, extending the two learning phases of classic
wake-sleep algorithm, respectively. The unified view provides a powerful tool
to analyze a diverse set of existing model variants, and enables to transfer
techniques across research lines in a principled way. For example, we apply the
importance weighting method in VAE literatures for improved GAN learning, and
enhance VAEs with an adversarial mechanism that leverages generated samples.
Experiments show generality and effectiveness of the transfered techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03656">
<title>p-FP: Extraction, Classification, and Prediction of Website Fingerprints with Deep Learning. (arXiv:1711.03656v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03656</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in learning Deep Neural Network (DNN) architectures have
received a great deal of attention due to their ability to outperform
state-of-the-art classifiers across a wide range of applications, with little
or no feature engineering. In this paper, we broadly study the applicability of
deep learning to website fingerprinting. We show that unsupervised DNNs can be
used to extract low-dimensional feature vectors that improve the performance of
state-of-the-art website fingerprinting attacks. When used as classifiers, we
show that they can match or exceed performance of existing attacks across a
range of application scenarios, including fingerprinting Tor website traces,
fingerprinting search engine queries over Tor, defeating fingerprinting
defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs
can be used to predict the fingerprintability of a website based on its
contents, achieving 99% accuracy on a data set of 4500 website downloads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Se Eun Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkam_S/0/1/0/all/0/1&quot;&gt;Saikrishna Sunkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hopper_N/0/1/0/all/0/1&quot;&gt;Nicholas Hopper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04386">
<title>Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches. (arXiv:1803.04386v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04386</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic neural net weights are used in a variety of contexts, including
regularization, Bayesian neural nets, exploration in reinforcement learning,
and evolution strategies. Unfortunately, due to the large number of weights,
all the examples in a mini-batch typically share the same weight perturbation,
thereby limiting the variance reduction effect of large mini-batches. We
introduce flipout, an efficient method for decorrelating the gradients within a
mini-batch by implicitly sampling pseudo-independent weight perturbations for
each example. Empirically, flipout achieves the ideal linear variance reduction
for fully connected networks, convolutional networks, and RNNs. We find
significant speedups in training neural networks with multiplicative Gaussian
perturbations. We show that flipout is effective at regularizing LSTMs, and
outperforms previous methods. Flipout also enables us to vectorize evolution
strategies: in our experiments, a single GPU with flipout can handle the same
throughput as at least 40 CPU cores using existing methods, equivalent to a
factor-of-4 cost reduction on Amazon Web Services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yeming Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicol_P/0/1/0/all/0/1&quot;&gt;Paul Vicol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1&quot;&gt;Jimmy Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09733">
<title>Domain transfer convolutional attribute embedding. (arXiv:1803.09733v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09733</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of transfer learning with the attribute
data. In the transfer learning problem, we want to leverage the data of the
auxiliary and the target domains to build an effective model for the
classification problem in the target domain. Meanwhile, the attributes are
naturally stable cross different domains. This strongly motives us to learn
effective domain transfer attribute representations. To this end, we proposed
to embed the attributes of the data to a common space by using the powerful
convolutional neural network (CNN) model. The convolutional representations of
the data points are mapped to the corresponding attributes so that they can be
effective embedding of the attributes. We also represent the data of different
domains by a domain-independent CNN, ant a domain-specific CNN, and combine
their outputs with the attribute embedding to build the classification model.
An joint learning framework is constructed to minimize the classification
errors, the attribute mapping error, the mismatching of the domain-independent
representations cross different domains, and to encourage the the neighborhood
smoothness of representations in the target domain. The minimization problem is
solved by an iterative algorithm based on gradient descent. Experiments over
benchmark data sets of person re-identification, bankruptcy prediction, and
spam email detection, show the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1&quot;&gt;Fang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing-Yan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10122">
<title>World Models. (arXiv:1803.10122v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10122</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore building generative neural network models of popular reinforcement
learning environments. Our world model can be trained quickly in an
unsupervised manner to learn a compressed spatial and temporal representation
of the environment. By using features extracted from the world model as inputs
to an agent, we can train a very compact and simple policy that can solve the
required task. We can even train our agent entirely inside of its own
hallucinated dream generated by its world model, and transfer this policy back
into the actual environment.
&lt;/p&gt;
&lt;p&gt;An interactive version of this paper is available at
https://worldmodels.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1&quot;&gt;David Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item></rdf:RDF>