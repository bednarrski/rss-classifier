<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04098"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03478"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03486"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06593"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02799"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03870"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03915"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04015"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04109"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04162"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04241"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.03046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10352"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03697"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.03952">
<title>Shortening Time Required for Adaptive Structural Learning Method of Deep Belief Network with Multi-Modal Data Arrangement. (arXiv:1807.03952v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03952</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Deep Learning has been applied in the techniques of artificial
intelligence. Especially, Deep Learning performed good results in the field of
image recognition. Most new Deep Learning architectures are naturally developed
in image recognition. For this reason, not only the numerical data and text
data but also the time-series data are transformed to the image data format.
Multi-modal data consists of two or more kinds of data such as picture and
text. The arrangement in a general method is formed in the squared array with
no specific aim. In this paper, the data arrangement are modified according to
the similarity of input-output pattern in Adaptive Structural Learning method
of Deep Belief Network. The similarity of output signals of hidden neurons is
made by the order rearrangement of hidden neurons. The experimental results for
the data rearrangement in squared array showed the shortening time required for
DBN learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03953">
<title>Adaptive Learning Method of Recurrent Temporal Deep Belief Network to Analyze Time Series Data. (arXiv:1807.03953v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03953</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning has the hierarchical network architecture to represent the
complicated features of input patterns. Such architecture is well known to
represent higher learning capability compared with some conventional models if
the best set of parameters in the optimal network structure is found. We have
been developing the adaptive learning method that can discover the optimal
network structure in Deep Belief Network (DBN). The learning method can
construct the network structure with the optimal number of hidden neurons in
each Restricted Boltzmann Machine and with the optimal number of layers in the
DBN during learning phase. The network structure of the learning method can be
self-organized according to given input patterns of big data set. In this
paper, we embed the adaptive learning method into the recurrent temporal RBM
and the self-generated layer into DBN. In order to verify the effectiveness of
our proposed method, the experimental results are higher classification
capability than the conventional methods in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03954">
<title>Knowledge Extracted from Recurrent Deep Belief Network for Real Time Deterministic Control. (arXiv:1807.03954v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.03954</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the market on deep learning including not only software but also
hardware is developing rapidly. Big data is collected through IoT devices and
the industry world will analyze them to improve their manufacturing process.
Deep Learning has the hierarchical network architecture to represent the
complicated features of input patterns. Although deep learning can show the
high capability of classification, prediction, and so on, the implementation on
GPU devices are required. We may meet the trade-off between the higher
precision by deep learning and the higher cost with GPU devices. We can success
the knowledge extraction from the trained deep learning with high
classification capability. The knowledge that can realize faster inference of
pre-trained deep network is extracted as IF-THEN rules from the network signal
flow given input data. Some experiment results with benchmark tests for time
series data sets showed the effectiveness of our proposed method related to the
computational speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04065">
<title>Recurrent Neural Networks with Flexible Gates using Kernel Activation Functions. (arXiv:1807.04065v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.04065</link>
<description rdf:parseType="Literal">&lt;p&gt;Gated recurrent neural networks have achieved remarkable results in the
analysis of sequential data. Inside these networks, gates are used to control
the flow of information, allowing to model even very long-term dependencies in
the data. In this paper, we investigate whether the original gate equation (a
linear projection followed by an element-wise sigmoid) can be improved. In
particular, we design a more flexible architecture, with a small number of
adaptable parameters, which is able to model a wider range of gating functions
than the classical one. To this end, we replace the sigmoid function in the
standard gate with a non-parametric formulation extending the recently proposed
kernel activation function (KAF), with the addition of a residual
skip-connection. A set of experiments on sequential variants of the MNIST
dataset shows that the adoption of this novel gate allows to improve accuracy
with a negligible cost in terms of computational power and with a large
speed-up in the number of training iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1&quot;&gt;Simone Scardapane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaerenbergh_S/0/1/0/all/0/1&quot;&gt;Steven Van Vaerenbergh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comminiello_D/0/1/0/all/0/1&quot;&gt;Danilo Comminiello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Totaro_S/0/1/0/all/0/1&quot;&gt;Simone Totaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uncini_A/0/1/0/all/0/1&quot;&gt;Aurelio Uncini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04098">
<title>A Recurrent Neural Network Survival Model: Predicting Web User Return Time. (arXiv:1807.04098v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04098</link>
<description rdf:parseType="Literal">&lt;p&gt;The size of a website&apos;s active user base directly affects its value. Thus, it
is important to monitor and influence a user&apos;s likelihood to return to a site.
Essential to this is predicting when a user will return. Current state of the
art approaches to solve this problem come in two flavors: (1) Recurrent Neural
Network (RNN) based solutions and (2) survival analysis methods. We observe
that both techniques are severely limited when applied to this problem.
Survival models can only incorporate aggregate representations of users instead
of automatically learning a representation directly from a raw time series of
user actions. RNNs can automatically learn features, but can not be directly
trained with examples of non-returning users who have no target value for their
return time. We develop a novel RNN survival model that removes the limitations
of the state of the art methods. We demonstrate that this model can
successfully be applied to return time prediction on a large e-commerce dataset
with a superior ability to discriminate between returning and non-returning
users than either method applied in isolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grob_G/0/1/0/all/0/1&quot;&gt;Georg L. Grob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_A/0/1/0/all/0/1&quot;&gt;&amp;#xc2;ngelo Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;C. H. Bryan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_D/0/1/0/all/0/1&quot;&gt;Duncan A. Little&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamberlain_B/0/1/0/all/0/1&quot;&gt;Benjamin Paul Chamberlain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06687">
<title>A Directionally Selective Small Target Motion Detecting Visual Neural Network in Cluttered Backgrounds. (arXiv:1801.06687v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06687</link>
<description rdf:parseType="Literal">&lt;p&gt;Discriminating targets moving against a cluttered background is a huge
challenge, let alone detecting a target as small as one or a few pixels and
tracking it in flight. In the fly&apos;s visual system, a class of specific neurons,
called small target motion detectors (STMDs), have been identified as showing
exquisite selectivity for small target motion. Some of the STMDs have also
demonstrated directional selectivity which means these STMDs respond strongly
only to their preferred motion direction. Directional selectivity is an
important property of these STMD neurons which could contribute to tracking
small targets such as mates in flight. However, little has been done on
systematically modeling these directional selective STMD neurons. In this
paper, we propose a directional selective STMD-based neural network (DSTMD) for
small target detection in a cluttered background. In the proposed neural
network, a new correlation mechanism is introduced for direction selectivity
via correlating signals relayed from two pixels. Then, a lateral inhibition
mechanism is implemented on the spatial field for size selectivity of STMD
neurons. Extensive experiments showed that the proposed neural network not only
is in accord with current biological findings, i.e. showing directional
preferences, but also worked reliably in detecting small targets against
cluttered backgrounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jigen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1&quot;&gt;Shigang Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00188">
<title>Benchmarking the Hill-Valley Evolutionary Algorithm for the GECCO 2018 Competition on Niching Methods Multimodal Optimization. (arXiv:1807.00188v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00188</link>
<description rdf:parseType="Literal">&lt;p&gt;This report presents benchmarking results of the latest version of the
Hill-Valley Evolutionary Algorithm (HillVallEA) on the CEC2013 niching
benchmark suite. The benchmarking follows restrictions required by the GECCO
2018 competition on Niching methods for Multimodal Optimization. In particular,
no problem dependent parameter tuning is performed. A number of adjustments
have been made to original publication of HillVallEA that are discussed in this
report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maree_S/0/1/0/all/0/1&quot;&gt;S.C. Maree&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1&quot;&gt;T. Alderliesten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thierens_D/0/1/0/all/0/1&quot;&gt;D. Thierens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1&quot;&gt;P.A.N. Bosman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03478">
<title>An Adaptive Learning Method of Restricted Boltzmann Machine by Neuron Generation and Annihilation Algorithm. (arXiv:1807.03478v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03478</link>
<description rdf:parseType="Literal">&lt;p&gt;Restricted Boltzmann Machine (RBM) is a generative stochastic energy-based
model of artificial neural network for unsupervised learning. Recently, RBM is
well known to be a pre-training method of Deep Learning. In addition to visible
and hidden neurons, the structure of RBM has a number of parameters such as the
weights between neurons and the coefficients for them. Therefore, we may meet
some difficulties to determine an optimal network structure to analyze big
data. In order to evade the problem, we investigated the variance of parameters
to find an optimal structure during learning. For the reason, we should check
the variance of parameters to cause the fluctuation for energy function in RBM
model. In this paper, we propose the adaptive learning method of RBM that can
discover an optimal number of hidden neurons according to the training
situation by applying the neuron generation and annihilation algorithm. In this
method, a new hidden neuron is generated if the energy function is not still
converged and the variance of the parameters is large. Moreover, the
inactivated hidden neuron will be annihilated if the neuron does not affect the
learning situation. The experimental results for some benchmark data sets were
discussed in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03486">
<title>An Adaptive Learning Method of Deep Belief Network by Layer Generation Algorithm. (arXiv:1807.03486v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03486</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Belief Network (DBN) has a deep architecture that represents multiple
features of input patterns hierarchically with the pre-trained Restricted
Boltzmann Machines (RBM). A traditional RBM or DBN model cannot change its
network structure during the learning phase. Our proposed adaptive learning
method can discover the optimal number of hidden neurons and weights and/or
layers according to the input space. The model is an important method to take
account of the computational cost and the model stability. The regularities to
hold the sparse structure of network is considerable problem, since the
extraction of explicit knowledge from the trained network should be required.
In our previous research, we have developed the hybrid method of adaptive
structural learning method of RBM and Learning Forgetting method to the trained
RBM. In this paper, we propose the adaptive learning method of DBN that can
determine the optimal number of layers during the learning. We evaluated our
proposed model on some benchmark data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03487">
<title>Fine Tuning Method by using Knowledge Acquisition from Deep Belief Network. (arXiv:1807.03487v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03487</link>
<description rdf:parseType="Literal">&lt;p&gt;We developed an adaptive structure learning method of Restricted Boltzmann
Machine (RBM) which can generate/annihilate neurons by self-organizing learning
method according to input patterns. Moreover, the adaptive Deep Belief Network
(DBN) in the assemble process of pre-trained RBM layer was developed. The
proposed method presents to score a great success to the training data set for
big data benchmark test such as CIFAR-10. However, the classification
capability of the test data set, which are included unknown patterns, is high,
but does not lead perfect correct solution. We investigated the wrong specified
data and then some characteristic patterns were found. In this paper, the
knowledge related to the patterns is embedded into the classification algorithm
of trained DBN. As a result, the classification capability can achieve a great
success (97.1\% to unknown data set).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04178">
<title>Explainable Security. (arXiv:1807.04178v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1807.04178</link>
<description rdf:parseType="Literal">&lt;p&gt;The Defense Advanced Research Projects Agency (DARPA) recently launched the
Explainable Artificial Intelligence (XAI) program that aims to create a suite
of new AI techniques that enable end users to understand, appropriately trust,
and effectively manage the emerging generation of AI systems.
&lt;/p&gt;
&lt;p&gt;In this paper, inspired by DARPA&apos;s XAI program, we propose a new paradigm in
security research: Explainable Security (XSec). We discuss the ``Six Ws&apos;&apos; of
XSec (Who? What? Where? When? Why? and How?) and argue that XSec has unique and
complex characteristics: XSec involves several different stakeholders (i.e.,
the system&apos;s developers, analysts, users and attackers) and is multi-faceted by
nature (as it requires reasoning about system model, threat model and
properties of security, privacy and trust as well as about concrete attacks,
vulnerabilities and countermeasures). We define a roadmap for XSec that
identifies several possible research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vigano_L/0/1/0/all/0/1&quot;&gt;Luca Vigan&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1&quot;&gt;Daniele Magazzeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04813">
<title>Triangular Architecture for Rare Language Translation. (arXiv:1805.04813v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04813</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Machine Translation (NMT) performs poor on the low-resource language
pair $(X,Z)$, especially when $Z$ is a rare language. By introducing another
rich language $Y$, we propose a novel triangular training architecture (TA-NMT)
to leverage bilingual data $(Y,Z)$ (may be small) and $(X,Y)$ (can be rich) to
improve the translation performance of low-resource pairs. In this triangular
architecture, $Z$ is taken as the intermediate latent variable, and translation
models of $Z$ are jointly optimized with a unified bidirectional EM algorithm
under the goal of maximizing the translation likelihood of $(X,Y)$. Empirical
results demonstrate that our method significantly improves the translation
quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even
better performance combining back-translation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shuo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shujie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuai Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06593">
<title>Cross-Target Stance Classification with Self-Attention Networks. (arXiv:1805.06593v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06593</link>
<description rdf:parseType="Literal">&lt;p&gt;In stance classification, the target on which the stance is made defines the
boundary of the task, and a classifier is usually trained for prediction on the
same target. In this work, we explore the potential for generalizing
classifiers between different targets, and propose a neural model that can
apply what has been learned from a source target to a destination target. We
show that our model can find useful information shared between relevant targets
which improves generalization in certain scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paris_C/0/1/0/all/0/1&quot;&gt;Cecile Paris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1&quot;&gt;Surya Nepal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1&quot;&gt;Ross Sparks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01270">
<title>Reaching Human-level Performance in Automatic Grammatical Error Correction: An Empirical Study. (arXiv:1807.01270v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01270</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural sequence-to-sequence (seq2seq) approaches have proven to be successful
in grammatical error correction (GEC). Based on the seq2seq framework, we
propose a novel fluency boost learning and inference mechanism. Fluency
boosting learning generates diverse error-corrected sentence pairs during
training, enabling the error correction model to learn how to improve a
sentence&apos;s fluency from more instances, while fluency boosting inference allows
the model to correct a sentence incrementally with multiple inference steps.
Combining fluency boost learning and inference with convolutional seq2seq
models, our approach achieves the state-of-the-art performance: 75.72 (F_{0.5})
on CoNLL-2014 10 annotation dataset and 62.42 (GLEU) on JFLEG test set
respectively, becoming the first GEC system that reaches human-level
performance (72.58 for CoNLL and 62.37 for JFLEG) on both of the benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02799">
<title>Distillation Techniques for Pseudo-rehearsal Based Incremental Learning. (arXiv:1807.02799v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02799</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to learn from incrementally arriving data is essential for any
life-long learning system. However, standard deep neural networks forget the
knowledge about the old tasks, a phenomenon called catastrophic forgetting,
when trained on incrementally arriving data. We discuss the biases in current
Generative Adversarial Networks (GAN) based approaches that learn the
classifier by knowledge distillation from previously trained classifiers. These
biases cause the trained classifier to perform poorly. We propose an approach
to remove these biases by distilling knowledge from the classifier of AC-GAN.
Experiments on MNIST and CIFAR10 show that this method is comparable to current
state of the art rehearsal based approaches. The code for this paper is
available at https://bit.ly/incremental-learning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1&quot;&gt;Haseeb Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_K/0/1/0/all/0/1&quot;&gt;Khurram Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafait_F/0/1/0/all/0/1&quot;&gt;Faisal Shafait&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03819">
<title>Universal Transformers. (arXiv:1807.03819v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.03819</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-attentive feed-forward sequence models have been shown to achieve
impressive results on sequence modeling tasks, thereby presenting a compelling
alternative to recurrent neural networks (RNNs) which has remained the de-facto
standard architecture for many sequence modeling problems to date. Despite
these successes, however, feed-forward sequence models like the Transformer
fail to generalize in many tasks that recurrent models handle with ease (e.g.
copying when the string lengths exceed those observed at training time).
Moreover, and in contrast to RNNs, the Transformer model is not computationally
universal, limiting its theoretical expressivity. In this paper we propose the
Universal Transformer which addresses these practical and theoretical
shortcomings and we show that it leads to improved performance on several
tasks. Instead of recurring over the individual symbols of sequences like RNNs,
the Universal Transformer repeatedly revises its representations of all symbols
in the sequence with each recurrent step. In order to combine information from
different parts of a sequence, it employs a self-attention mechanism in every
recurrent step. Assuming sufficient memory, its recurrence makes the Universal
Transformer computationally universal. We further employ an adaptive
computation time (ACT) mechanism to allow the model to dynamically adjust the
number of times the representation of each position in a sequence is revised.
Beyond saving computation, we show that ACT can improve the accuracy of the
model. Our experiments show that on various algorithmic tasks and a diverse set
of large-scale language understanding tasks the Universal Transformer
generalizes significantly better and outperforms both a vanilla Transformer and
an LSTM in machine translation, and achieves a new state of the art on the bAbI
linguistic reasoning task and the challenging LAMBADA language modeling task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouws_S/0/1/0/all/0/1&quot;&gt;Stephan Gouws&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1&quot;&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Kaiser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03870">
<title>Learning Implicit Generative Models by Teaching Explicit Ones. (arXiv:1807.03870v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03870</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit generative models are difficult to train as no explicit probability
density functions are defined. The well-known minimax framework proposed by
generative adversarial nets (GANs) is equivalent to minimizing the
Jensen-Shannon divergence and suffers from mode collapse in practice. In this
paper, we propose learning by teaching (LBT) framework to train implicit
generative models via incorporating an auxiliary explicit model. In LBT, an
explicit model is introduced to learn the distribution defined by the implicit
model and the later one&apos;s goal is to teach the explicit model to cover the
training data. Formally, our method is formulated as a bilevel optimization
problem, whose optimum implies that we obatin the MLE of the implicit model. We
also adopt the unrolling trick to make the optimization problem differentiable
with respect to the implicit model&apos;s parameters. Experimental results
demonstrate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03888">
<title>A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. (arXiv:1807.03888v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03888</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting test samples drawn sufficiently far away from the training
distribution statistically or adversarially is a fundamental requirement to
deploying a good classifier in many real-world machine learning applications.
However, deep neural networks with the softmax classifier are known to produce
highly overconfident posterior distributions even for such abnormal samples. In
this paper, we propose a simple yet effective method for detecting any abnormal
samples, which is applicable to any pre-trained softmax neural classifier. We
obtain the class conditionalGaussian distributions with respect to (low- and
upper-level) features of the deep models under Gaussian discriminant analysis,
which result in a confidence score based on the Mahalanobis distance. While
most prior methods have been evaluated for detecting either out-of-distribution
or adversarial samples, but not both, the proposed method achieves the
state-of-art performances for both cases in our experiments. Moreover, we found
that our proposed method is more robust in extreme cases, e.g., when the
training dataset has noisy labels or small number of samples. Finally, we show
that the proposed method enjoys broader usage by applying it to class
incremental learning: whenever out-of-distribution samples are detected, our
classification rule can incorporate new classes well without further training
deep models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kimin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kibok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03915">
<title>Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis. (arXiv:1807.03915v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.03915</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal machine learning is a core research area spanning the language,
visual and acoustic modalities. The central challenge in multimodal learning
involves learning representations that can process and relate information from
multiple modalities. In this paper, we propose two methods for unsupervised
learning of joint multimodal representations using sequence to sequence
(Seq2Seq) methods: a \textit{Seq2Seq Modality Translation Model} and a
\textit{Hierarchical Seq2Seq Modality Translation Model}. We also explore
multiple different variations on the multimodal inputs and outputs of these
seq2seq models. Our experiments on multimodal sentiment analysis using the
CMU-MOSI dataset indicate that our methods learn informative multimodal
representations that outperform the baselines and achieve improved performance
on multimodal sentiment analysis, specifically in the Bimodal case where our
model is able to improve F1 Score by twelve points. We also discuss future
directions for multimodal Seq2Seq methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hai Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzini_T/0/1/0/all/0/1&quot;&gt;Thomas Manzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnabas Poczos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03920">
<title>Discovering Interesting Plots in Production Yield Data Analytics. (arXiv:1807.03920v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03920</link>
<description rdf:parseType="Literal">&lt;p&gt;An analytic process is iterative between two agents, an analyst and an
analytic toolbox. Each iteration comprises three main steps: preparing a
dataset, running an analytic tool, and evaluating the result, where dataset
preparation and result evaluation, conducted by the analyst, are largely
domain-knowledge driven. In this work, the focus is on automating the result
evaluation step. The underlying problem is to identify plots that are deemed
interesting by an analyst. We propose a methodology to learn such analyst&apos;s
intent based on Generative Adversarial Networks (GANs) and demonstrate its
applications in the context of production yield optimization using data
collected from several product lines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nero_M/0/1/0/all/0/1&quot;&gt;Matthew Nero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1&quot;&gt;Chuanhe Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li-C. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumikawa_N/0/1/0/all/0/1&quot;&gt;Nik Sumikawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04015">
<title>On catastrophic forgetting and mode collapse in Generative Adversarial Networks. (arXiv:1807.04015v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04015</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GAN) are one of the most prominent tools for
learning complicated distributions. However, problems such as mode collapse and
catastrophic forgetting, prevent GAN from learning the target distribution.
These problems are usually studied independently from each other. In this
paper, we show that both problems are present in GAN and their combined effect
makes the training of GAN unstable. We also show that methods such as gradient
penalties and momentum based optimizers can improve the stability of GAN by
effectively preventing these problems from happening. Finally, we study a
mechanism for mode collapse to occur and propagate in feedforward neural
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thanh_Tung_H/0/1/0/all/0/1&quot;&gt;Hoang Thanh-Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Truyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04081">
<title>Proactive Intervention to Downtrend Employee Attrition using Artificial Intelligence Techniques. (arXiv:1807.04081v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04081</link>
<description rdf:parseType="Literal">&lt;p&gt;To predict the employee attrition beforehand and to enable management to take
individualized preventive action. Using Ensemble classification modeling
techniques and Linear Regression. Model could predict over 91% accurate
employee prediction, lead-time in separation and individual reasons causing
attrition. Prior intimation of employee attrition enables manager to take
preventive actions to retain employee or to manage the business consequences of
attrition. Once deployed this will model can help in downtrend Employee
Attrition, will help manager to manage team more effectively. Model does not
cover the natural calamities, and unforeseen events occurring at an individual
level like accident, death etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barvey_A/0/1/0/all/0/1&quot;&gt;Aasheesh Barvey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kapila_J/0/1/0/all/0/1&quot;&gt;Jitin Kapila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pathak_K/0/1/0/all/0/1&quot;&gt;Kumarjit Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04109">
<title>Modeling and Soft-fault Diagnosis of Underwater Thrusters with Recurrent Neural Networks. (arXiv:1807.04109v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.04109</link>
<description rdf:parseType="Literal">&lt;p&gt;Noncritical soft-faults and model deviations are a challenge for Fault
Detection and Diagnosis (FDD) of resident Autonomous Underwater Vehicles
(AUVs). Such systems may have a faster performance degradation due to the
permanent exposure to the marine environment, and constant monitoring of
component conditions is required to ensure their reliability. This works
presents an evaluation of Recurrent Neural Networks (RNNs) for a data-driven
fault detection and diagnosis scheme for underwater thrusters with empirical
data. The nominal behavior of the thruster was modeled using the measured
control input, voltage, rotational speed and current signals. We evaluated the
performance of fault classification using all the measured signals compared to
using the computed residuals from the nominal model as features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_S/0/1/0/all/0/1&quot;&gt;Samy Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1&quot;&gt;Matias Valdenegro-Toro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04162">
<title>TherML: Thermodynamics of Machine Learning. (arXiv:1807.04162v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we offer a framework for reasoning about a wide class of
existing objectives in machine learning. We develop a formal correspondence
between this work and thermodynamics and discuss its implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1&quot;&gt;Alexander A. Alemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1&quot;&gt;Ian Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04225">
<title>Measuring abstract reasoning in neural networks. (arXiv:1807.04225v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04225</link>
<description rdf:parseType="Literal">&lt;p&gt;Whether neural networks can learn abstract reasoning or whether they merely
rely on superficial statistics is a topic of recent debate. Here, we propose a
dataset and challenge designed to probe abstract reasoning, inspired by a
well-known human IQ test. To succeed at this challenge, models must cope with
various generalisation `regimes&apos; in which the training and test data differ in
clearly-defined ways. We show that popular models such as ResNets perform
poorly, even when the training and test sets differ only minimally, and we
present a novel architecture, with a structure designed to encourage reasoning,
that does significantly better. When we vary the way in which the test
questions and training data differ, we find that our model is notably
proficient at certain forms of generalisation, but notably weak at others. We
further show that the model&apos;s ability to generalise improves markedly if it is
trained to predict symbolic explanations for its answers. Altogether, we
introduce and explore ways to both measure and induce stronger abstract
reasoning in neural networks. Our freely-available dataset should motivate
further progress in this direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_D/0/1/0/all/0/1&quot;&gt;David G.T. Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1&quot;&gt;Felix Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1&quot;&gt;Adam Santoro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1&quot;&gt;Ari S. Morcos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1&quot;&gt;Timothy Lillicrap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04241">
<title>DeepMove: Learning Place Representations through Large Scale Movement Data. (arXiv:1807.04241v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04241</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding and reasoning about places and their relationships are critical
for many applications. Places are traditionally curated by a small group of
people as place gazetteers and are represented by an ID with spatial extent,
category, and other descriptions. However, a place context is described to a
large extent by movements made from/to other places. Places are linked and
related to each other by these movements. This important context is missing
from the traditional representation.
&lt;/p&gt;
&lt;p&gt;We present DeepMove, a novel approach for learning latent representations of
places. DeepMove advances the current deep learning based place representations
by directly model movements between places. We demonstrate DeepMove&apos;s latent
representations on place categorization and clustering tasks on large place and
movement datasets with respect to important parameters. Our results show that
DeepMove outperforms state-of-the-art baselines. DeepMove&apos;s representations can
provide up to 15% higher than competing methods in matching rate of place
category and result in up to 39% higher silhouette coefficient value for place
clusters.
&lt;/p&gt;
&lt;p&gt;DeepMove is spatial and temporal context aware. It is scalable. It
outperforms competing models using much smaller training dataset (a month or
1/12 of data). These qualities make it suitable for a broad class of real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00550">
<title>On Unifying Deep Generative Models. (arXiv:1706.00550v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00550</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have achieved impressive success in recent years.
Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as
emerging families for generative model learning, have largely been considered
as two distinct paradigms and received extensive independent studies
respectively. This paper aims to establish formal connections between GANs and
VAEs through a new formulation of them. We interpret sample generation in GANs
as performing posterior inference, and show that GANs and VAEs involve
minimizing KL divergences of respective posterior and inference distributions
with opposite directions, extending the two learning phases of classic
wake-sleep algorithm, respectively. The unified view provides a powerful tool
to analyze a diverse set of existing model variants, and enables to transfer
techniques across research lines in a principled way. For example, we apply the
importance weighting method in VAE literatures for improved GAN learning, and
enhance VAEs with an adversarial mechanism that leverages generated samples.
Experiments show generality and effectiveness of the transferred techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.03046">
<title>When Is the First Spurious Variable Selected by Sequential Regression Procedures?. (arXiv:1708.03046v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1708.03046</link>
<description rdf:parseType="Literal">&lt;p&gt;Applied statisticians use sequential regression procedures to produce a
ranking of explanatory variables and, in settings of low correlations between
variables and strong true effect sizes, expect that variables at the very top
of this ranking are truly relevant to the response. In a regime of certain
sparsity levels, however, three examples of sequential procedures--forward
stepwise, the lasso, and least angle regression--are shown to include the first
spurious variable unexpectedly early. We derive a rigorous, sharp prediction of
the rank of the first spurious variable for these three procedures,
demonstrating that the first spurious variable occurs earlier and earlier as
the regression coefficients become denser. This counterintuitive phenomenon
persists for statistically independent Gaussian random designs and an
arbitrarily large magnitude of the true effects. We gain a better understanding
of the phenomenon by identifying the underlying cause and then leverage the
insights to introduce a simple visualization tool termed the double-ranking
diagram to improve on sequential methods. As a byproduct of these findings, we
obtain the first provable result certifying the exact equivalence between the
lasso and least angle regression in the early stages of solution paths beyond
orthogonal designs. This equivalence can seamlessly carry over many important
model selection results concerning the lasso to least angle regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie J. Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01216">
<title>Hierarchical-Pointer Generator Memory Network for Task Oriented Dialog. (arXiv:1805.01216v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01216</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end networks trained for task-oriented dialog, such as for
recommending restaurants to a user, suffer from out-of-vocabulary (OOV) problem
-- the entities in the Knowledge Base (KB) may not be seen by the network at
training time, making it hard to use them in dialog. We propose a novel
Hierarchical Pointer Generator Memory Network (HyP-MN), in which the next word
may be generated from the decode vocabulary or copied from a hierarchical
memory maintaining KB results and previous utterances. This hierarchical memory
layout along with a novel KB dropout helps to alleviate the OOV problem.
Evaluating over the dialog bAbI tasks, we find that HyP-MN outperforms
state-of-the-art results, with considerable improvements (10% on OOV test set).
HyP-MN also achieves competitive performances on various real-world datasets
such as CamRest676 and In-car assistant dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1&quot;&gt;Dinesh Raghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Nikhil Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1&quot;&gt;Mausam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10352">
<title>Tensorized Spectrum Preserving Compression for Neural Networks. (arXiv:1805.10352v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10352</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern neural networks can have tens of millions of parameters, and are often
ill-suited for smartphones or IoT devices. In this paper, we describe an
efficient mechanism for compressing large networks by {\em tensorizing\/}
network layers: i.e. mapping layers on to high-order matrices, for which we
introduce new tensor decomposition methods. Compared to previous compression
methods, some of which use tensor decomposition, our techniques preserve more
of the networks invariance structure. Coupled with a new data
reconstruction-based learning method, we show that tensorized compression
outperforms existing techniques for both convolutional and fully-connected
layers on state-of-the art networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jiahao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhattacharjee_B/0/1/0/all/0/1&quot;&gt;Bobby Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10961">
<title>Automatic Exploration of Machine Learning Experiments on OpenML. (arXiv:1806.10961v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10961</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the influence of hyperparameters on the performance of a
machine learning algorithm is an important scientific topic in itself and can
help to improve automatic hyperparameter tuning procedures. Unfortunately,
experimental meta data for this purpose is still rare. This paper presents a
large, free and open dataset addressing this problem, containing results on 38
OpenML data sets, six different machine learning algorithms and many different
hyperparameter configurations. Result where generated by an automated random
sampling strategy, termed the OpenML Random Bot. Each algorithm was
cross-validated up to 20.000 times per dataset with different hyperparameters
settings, resulting in a meta dataset of around 2.5 million experiments
overall.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuhn_D/0/1/0/all/0/1&quot;&gt;Daniel K&amp;#xfc;hn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Probst_P/0/1/0/all/0/1&quot;&gt;Philipp Probst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thomas_J/0/1/0/all/0/1&quot;&gt;Janek Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03697">
<title>Deep Learning for Audio Transcription on Low-Resource Datasets. (arXiv:1807.03697v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03697</link>
<description rdf:parseType="Literal">&lt;p&gt;In training a deep learning system to perform audio transcription, two
practical problems may arise. Firstly, most datasets are weakly labelled,
having only a list of events present in each recording without any temporal
information for training. Secondly, deep neural networks need a very large
amount of labelled training data to achieve good quality performance, yet in
practice it is difficult to collect enough samples for most classes of
interest. In this paper, we propose factorising the final task of audio
transcription into multiple intermediate tasks in order to improve the training
performance when dealing with this kind of low-resource datasets. We evaluate
three data-efficient approaches of training a stacked convolutional and
recurrent neural network for the intermediate tasks. Our results show that
different methods of training have different advantages and disadvantages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morfi_V/0/1/0/all/0/1&quot;&gt;Veronica Morfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item></rdf:RDF>