<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03351"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03223"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03249"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03390"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03719"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03931"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.03438"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.01161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07903"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03067"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08859"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03281"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03337"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03353"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03428"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03471"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03480"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03483"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03605"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03607"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03660"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03847"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03878"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1505.05629"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04395"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06786"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08413"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00961"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.03351">
<title>Peephole: Predicting Network Performance Before Training. (arXiv:1712.03351v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03351</link>
<description rdf:parseType="Literal">&lt;p&gt;The quest for performant networks has been a significant force that drives
the advancements of deep learning in recent years. While rewarding, improving
network design has never been an easy journey. The large design space combined
with the tremendous cost required for network training poses a major obstacle
to this endeavor. In this work, we propose a new approach to this problem,
namely, predicting the performance of a network before training, based on its
architecture. Specifically, we develop a unified way to encode individual
layers into vectors and bring them together to form an integrated description
via LSTM. Taking advantage of the recurrent network&apos;s strong expressive power,
this method can reliably predict the performances of various network
architectures. Our empirical studies showed that it not only achieved accurate
predictions but also produced consistent rankings across datasets -- a key
desideratum in performance prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Boyang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junjie Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03541">
<title>An Architecture Combining Convolutional Neural Network (CNN) and Support Vector Machine (SVM) for Image Classification. (arXiv:1712.03541v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.03541</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) are similar to &quot;ordinary&quot; neural
networks in the sense that they are made up of hidden layers consisting of
neurons with &quot;learnable&quot; parameters. These neurons receive inputs, performs a
dot product, and then follows it with a non-linearity. The whole network
expresses the mapping between raw image pixels and their class scores.
Conventionally, the Softmax function is the classifier used at the last layer
of this network. However, there have been studies (Alalshekmubarak and Smith,
2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited
studies introduce the usage of linear support vector machine (SVM) in an
artificial neural network architecture. This project is yet another take on the
subject, and is inspired by (Tang, 2013). Empirical data has shown that the
CNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST
dataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax
was able to achieve a test accuracy of ~99.23% using the same dataset. Both
models were also tested on the recently-published Fashion-MNIST dataset (Xiao,
Rasul, and Vollgraf, 2017), which is suppose to be a more difficult image
classification dataset than MNIST (Zalandoresearch, 2017). This proved to be
the case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax
reached a test accuracy of ~91.86%. The said results may be improved if data
preprocessing techniques were employed on the datasets, and if the base CNN
model was a relatively more sophisticated than the one used in this study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1&quot;&gt;Abien Fred Agarap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07215">
<title>On Convergence and Stability of GANs. (arXiv:1705.07215v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07215</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose studying GAN training dynamics as regret minimization, which is in
contrast to the popular view that there is consistent minimization of a
divergence between real and generated distributions. We analyze the convergence
of GAN training from this new point of view to understand why mode collapse
happens. We hypothesize the existence of undesirable local equilibria in this
non-convex game to be responsible for mode collapse. We observe that these
local equilibria often exhibit sharp gradients of the discriminator function
around some real data points. We demonstrate that these degenerate local
equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show
that DRAGAN enables faster training, achieves improved stability with fewer
mode collapses, and leads to generator networks with better modeling
performance across a variety of architectures and objective functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodali_N/0/1/0/all/0/1&quot;&gt;Naveen Kodali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1&quot;&gt;Jacob Abernethy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1&quot;&gt;James Hays&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06257">
<title>A Flow Model of Neural Networks. (arXiv:1708.06257v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06257</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on a natural connection between ResNet and transport equation or its
characteristic equation, we propose a continuous flow model for both ResNet and
plain net. Through this continuous model, a ResNet can be explicitly
constructed as a refinement of a plain net. The flow model provides an
alternative perspective to understand phenomena in deep neural networks, such
as why it is necessary and sufficient to use 2-layer blocks in ResNets, why
deeper is better, and why ResNets are even deeper, and so on. It also opens a
gate to bring in more tools from the huge area of differential equations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zuoqiang Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03223">
<title>S-Shaped vs. V-Shaped Transfer Functions for Antlion Optimization Algorithm in Feature Selection Problems. (arXiv:1712.03223v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.03223</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection is an important preprocessing step for classification
problems. It deals with selecting near optimal features in the original
dataset. Feature selection is an NP-hard problem, so meta-heuristics can be
more efficient than exact methods. In this work, Ant Lion Optimizer (ALO),
which is a recent metaheuristic algorithm, is employed as a wrapper feature
selection method. Six variants of ALO are proposed where each employ a transfer
function to map a continuous search space to a discrete search space. The
performance of the proposed approaches is tested on eighteen UCI datasets and
compared to a number of existing approaches in the literature: Particle Swarm
Optimization, Gravitational Search Algorithm, and two existing ALO-based
approaches. Computational experiments show that the proposed approaches
efficiently explore the feature space and select the most informative features,
which help to improve the classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mafarja_M/0/1/0/all/0/1&quot;&gt;Majdi Mafarja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirjalili_S/0/1/0/all/0/1&quot;&gt;Seyedali Mirjalili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03249">
<title>Social Emotion Mining Techniques for Facebook Posts Reaction Prediction. (arXiv:1712.03249v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.03249</link>
<description rdf:parseType="Literal">&lt;p&gt;As of February 2016 Facebook allows users to express their experienced
emotions about a post by using five so-called `reactions&apos;. This research paper
proposes and evaluates alternative methods for predicting these reactions to
user posts on public pages of firms/companies (like supermarket chains). For
this purpose, we collected posts (and their reactions) from Facebook pages of
large supermarket chains and constructed a dataset which is available for other
researches. In order to predict the distribution of reactions of a new post,
neural network architectures (convolutional and recurrent neural networks) were
tested using pretrained word embeddings. Results of the neural networks were
improved by introducing a bootstrapping approach for sentiment and emotion
mining on the comments for each post. The final model (a combination of neural
network and a baseline emotion miner) is able to predict the reaction
distribution on Facebook posts with a mean squared error (or misclassification
rate) of 0.135.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krebs_F/0/1/0/all/0/1&quot;&gt;Florian Krebs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lubascher_B/0/1/0/all/0/1&quot;&gt;Bruno Lubascher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moers_T/0/1/0/all/0/1&quot;&gt;Tobias Moers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaap_P/0/1/0/all/0/1&quot;&gt;Pieter Schaap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spanakis_G/0/1/0/all/0/1&quot;&gt;Gerasimos Spanakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03280">
<title>Nintendo Super Smash Bros. Melee: An &quot;Untouchable&quot; Agent. (arXiv:1712.03280v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.03280</link>
<description rdf:parseType="Literal">&lt;p&gt;Nintendo&apos;s Super Smash Bros. Melee fighting game can be emulated on modern
hardware allowing us to inspect internal memory states, such as character
positions. We created an AI that avoids being hit by training using these
internal memory states and outputting controller button presses. After training
on a month&apos;s worth of Melee matches, our best agent learned to avoid the
toughest AI built into the game for a full minute 74.6% of the time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parr_B/0/1/0/all/0/1&quot;&gt;Ben Parr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilipkumar_D/0/1/0/all/0/1&quot;&gt;Deepak Dilipkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03333">
<title>Bayesian Q-learning with Assumed Density Filtering. (arXiv:1712.03333v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03333</link>
<description rdf:parseType="Literal">&lt;p&gt;While off-policy temporal difference methods have been broadly used in
reinforcement learning due to their efficiency and simple implementation, their
Bayesian counterparts have been relatively understudied. This is mainly because
the max operator in the Bellman optimality equation brings non-linearity and
inconsistent distributions over value function. In this paper, we introduce a
new Bayesian approach to off-policy TD methods using Assumed Density Filtering,
called ADFQ, which updates beliefs on action-values (Q) through an online
Bayesian inference method. Uncertainty measures in the beliefs not only are
used in exploration but they provide a natural regularization in the belief
updates. We also present a connection between ADFQ and Q-learning. Our
empirical results show the proposed ADFQ algorithms outperform comparing
algorithms in several task domains. Moreover, our algorithms improve general
drawbacks in BRL such as computational complexity, usage of uncertainty, and
nonlinearity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Heejin Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Daniel D. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03390">
<title>NAG: Network for Adversary Generation. (arXiv:1712.03390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.03390</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial perturbations can pose a serious threat for deploying machine
learning systems. Recent works have shown existence of image-agnostic
perturbations that can fool classifiers over most natural images. Existing
methods present optimization approaches that solve for a fooling objective with
an imperceptibility constraint to craft the perturbations. However, for a given
classifier, they generate one perturbation at a time, which is a single
instance from the manifold of adversarial perturbations. Also, in order to
build robust models, it is essential to explore the manifold of adversarial
perturbations. In this paper, we propose for the first time, a generative
approach to model the distribution of adversarial perturbations. The
architecture of the proposed model is inspired from that of GANs and is trained
using fooling and diversity objectives. Our trained generator network attempts
to capture the distribution of adversarial perturbations for a given classifier
and readily generates a wide variety of such perturbations. Our experimental
evaluation demonstrates that perturbations crafted by our model (i) achieve
state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver
excellent cross model generalizability. Our work can be deemed as an important
step in the process of inferring about the complex manifolds of adversarial
perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1&quot;&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_U/0/1/0/all/0/1&quot;&gt;Utkarsh Ojha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1&quot;&gt;Utsav Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1&quot;&gt;R. Venkatesh Babu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03608">
<title>Towards Fully Environment-Aware UAVs: Real-Time Path Planning with Online 3D Wind Field Prediction in Complex Terrain. (arXiv:1712.03608v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1712.03608</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, low-altitude fixed-wing Unmanned Aerial Vehicles (UAVs) are largely
limited to primitively follow user-defined waypoints. To allow fully-autonomous
remote missions in complex environments, real-time environment-aware navigation
is required both with respect to terrain and strong wind drafts. This paper
presents two relevant initial contributions: First, the literature&apos;s first-ever
3D wind field prediction method which can run in real time onboard a UAV is
presented. The approach retrieves low-resolution global weather data, and uses
potential flow theory to adjust the wind field such that terrain boundaries,
mass conservation, and the atmospheric stratification are observed. A
comparison with 1D LIDAR data shows an overall wind error reduction of 23% with
respect to the zero-wind assumption that is mostly used for UAV path planning
today. However, given that the vertical winds are not resolved accurately
enough further research is required and identified. Second, a sampling-based
path planner that considers the aircraft dynamics in non-uniform wind
iteratively via Dubins airplane paths is presented. Performance optimizations,
e.g. obstacle-aware sampling and fast 2.5D-map collision checks, render the
planner 50% faster than the Open Motion Planning Library (OMPL) implementation.
Test cases in Alpine terrain show that the wind-aware planning performs up to
50x less iterations than shortest-path planning and is thus slower in low
winds, but that it tends to deliver lower-cost paths in stronger winds. More
importantly, in contrast to the shortest-path planner, it always delivers
collision-free paths. Overall, our initial research demonstrates the
feasibility of 3D wind field prediction from a UAV and the advantages of
wind-aware planning. This paves the way for follow-up research on
fully-autonomous environment-aware navigation of UAVs in real-life missions and
complex terrain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oettershagen_P/0/1/0/all/0/1&quot;&gt;Philipp Oettershagen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achermann_F/0/1/0/all/0/1&quot;&gt;Florian Achermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1&quot;&gt;Benjamin M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1&quot;&gt;Daniel Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1&quot;&gt;Roland Siegwart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03632">
<title>Robust Deep Reinforcement Learning with Adversarial Attacks. (arXiv:1712.03632v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03632</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes adversarial attacks for Reinforcement Learning (RL) and
then improves the robustness of Deep Reinforcement Learning algorithms (DRL) to
parameter uncertainties with the help of these attacks. We show that even a
naively engineered attack successfully degrades the performance of DRL
algorithm. We further improve the attack using gradient information of an
engineered loss function which leads to further degradation in performance.
These attacks are then leveraged during training to improve the robustness of
RL within robust control framework. We show that this adversarial training of
DRL algorithms like Deep Double Q learning and Deep Deterministic Policy
Gradients leads to significant increase in robustness to parameter variations
for RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah
environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pattanaik_A/0/1/0/all/0/1&quot;&gt;Anay Pattanaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuijing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bommannan_G/0/1/0/all/0/1&quot;&gt;Gautham Bommannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1&quot;&gt;Girish Chowdhary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03719">
<title>Novel model-based heuristics for energy optimal motion planning of an autonomous vehicle using A*. (arXiv:1712.03719v1 [math.OC])</title>
<link>http://arxiv.org/abs/1712.03719</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive motion planning is the key to achieve energy-efficient driving,
which is one of the main benefits of automated driving. Researchers have been
studying the planning of velocity trajectories, a simpler form of motion
planning, for over a decade now and many different methods are available.
Dynamic programming has shown to be the most common choice due to its numerical
background and ability to include nonlinear constraints and models. Although
planning of optimal trajectory is done in a systematic way, dynamic programming
doesn&apos;t use any knowledge about the considered problem to guide the exploration
and therefore explores all possible trajectories. A* is an algorithm which
enables using knowledge about the problem to guide the exploration to the most
promising solutions first. Knowledge has to be represented in a form of a
heuristic function, which gives an optimistic estimate of cost for
transitioning between two states, which is not a straightforward task. This
paper presents a novel heuristics incorporating air drag and auxiliary power as
well as operational costs of the vehicle, besides kinetic and potential energy
and rolling resistance known in the literature. Furthermore, optimal cruising
velocity, which depends on vehicle aerodynamic properties and auxiliary power,
is derived. Results are compared for different variants of heuristic functions
and dynamic programming as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ajanovic_Z/0/1/0/all/0/1&quot;&gt;Zlatan Ajanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stolz_M/0/1/0/all/0/1&quot;&gt;Michael Stolz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Horn_M/0/1/0/all/0/1&quot;&gt;Martin Horn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03724">
<title>Cogniculture: Towards a Better Human-Machine Co-evolution. (arXiv:1712.03724v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1712.03724</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in Artificial Intelligence is breaking technology barriers every
day. New algorithms and high performance computing are making things possible
which we could only have imagined earlier. Though the enhancements in AI are
making life easier for human beings day by day, there is constant fear that AI
based systems will pose a threat to humanity. People in AI community have
diverse set of opinions regarding the pros and cons of AI mimicking human
behavior. Instead of worrying about AI advancements, we propose a novel idea of
cognitive agents, including both human and machines, living together in a
complex adaptive ecosystem, collaborating on human computation for producing
essential social goods while promoting sustenance, survival and evolution of
the agents&apos; life cycle. We highlight several research challenges and technology
barriers in achieving this goal. We propose a governance mechanism around this
ecosystem to ensure ethical behaviors of all cognitive agents. Along with a
novel set of use-cases of Cogniculture, we discuss the road map ahead for this
journey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimplikar_R/0/1/0/all/0/1&quot;&gt;Rakesh R Pimplikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_K/0/1/0/all/0/1&quot;&gt;Kushal Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parija_G/0/1/0/all/0/1&quot;&gt;Gyana Parija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwakarma_H/0/1/0/all/0/1&quot;&gt;Harit Vishwakarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanam_R/0/1/0/all/0/1&quot;&gt;Ramasuri Narayanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1&quot;&gt;Sarthak Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vallam_R/0/1/0/all/0/1&quot;&gt;Rohith D Vallam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_R/0/1/0/all/0/1&quot;&gt;Ritwik Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1&quot;&gt;Joydeep Mondal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03779">
<title>Artificial Intelligence and Statistics. (arXiv:1712.03779v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03779</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) is intrinsically data-driven. It calls for the
application of statistical concepts through human-machine collaboration during
generation of data, development of algorithms, and evaluation of results. This
paper discusses how such human-machine collaboration can be approached through
the statistical concepts of population, question of interest,
representativeness of training data, and scrutiny of results (PQRS). The PQRS
workflow provides a conceptual framework for integrating statistical ideas with
human input into AI products and research. These ideas include experimental
design principles of randomization and local control as well as the principle
of stability to gain reproducibility and interpretability of algorithms and
data results. We discuss the use of these principles in the contexts of
self-driving cars, automated medical diagnoses, and examples from the authors&apos;
collaborative research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumbier_K/0/1/0/all/0/1&quot;&gt;Karl Kumbier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03781">
<title>Learning Nested Sparse Structures in Deep Neural Networks. (arXiv:1712.03781v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.03781</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there have been increasing demands to construct compact deep
architectures to remove unnecessary redundancy and to improve the inference
speed. While many recent works focus on reducing the redundancy by eliminating
unneeded weight parameters, it is not possible to apply a single deep
architecture for multiple devices with different resources. When a new device
or circumstantial condition requires a new deep architecture, it is necessary
to construct and train a new network from scratch. In this work, we propose a
novel deep learning framework, called a nested sparse network, which exploits
an n-in-1-type nested structure in a neural network. A nested sparse network
consists of multiple levels of networks with a different sparsity ratio
associated with each level, and higher level networks share parameters with
lower level networks to enable stable nested learning. The proposed framework
realizes a resource-aware versatile architecture as the same network can meet
diverse resource requirements. Moreover, the proposed nested network can learn
different forms of knowledge in its internal networks at different levels,
enabling multiple tasks using a single network, such as coarse-to-fine
hierarchical classification. In order to train the proposed nested sparse
network, we propose efficient weight connection learning and channel and layer
scheduling strategies. We evaluate our network in multiple tasks, including
adaptive deep compression, knowledge distillation, and learning class
hierarchy, and demonstrate that nested sparse networks perform competitively,
but more efficiently, than existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Eunwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_C/0/1/0/all/0/1&quot;&gt;Chanho Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Songhwai Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03931">
<title>MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments. (arXiv:1712.03931v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03931</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MINOS, a simulator designed to support the development of
multisensory models for goal-directed navigation in complex indoor
environments. The simulator leverages large datasets of complex 3D environments
and supports flexible configuration of multimodal sensor suites. We use MINOS
to benchmark deep-learning-based navigation methods, to analyze the influence
of environmental complexity on navigation performance, and to carry out a
controlled study of multimodality in sensorimotor learning. The experiments
show that current deep reinforcement learning approaches fail in large
realistic environments. The experiments also indicate that multimodality is
beneficial in learning to navigate cluttered scenes. MINOS is released
open-source to the research community at &lt;a href=&quot;http://minosworld.org&quot;&gt;this http URL&lt;/a&gt; . A video that
shows MINOS can be found at https://youtu.be/c0mL9K64q84
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1&quot;&gt;Manolis Savva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Angel X. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1&quot;&gt;Alexey Dosovitskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1&quot;&gt;Thomas Funkhouser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1&quot;&gt;Vladlen Koltun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.03438">
<title>Reactive Multi-Context Systems: Heterogeneous Reasoning in Dynamic Environments. (arXiv:1609.03438v3 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/1609.03438</link>
<description rdf:parseType="Literal">&lt;p&gt;Managed multi-context systems (mMCSs) allow for the integration of
heterogeneous knowledge sources in a modular and very general way. They were,
however, mainly designed for static scenarios and are therefore not well-suited
for dynamic environments in which continuous reasoning over such heterogeneous
knowledge with constantly arriving streams of data is necessary. In this paper,
we introduce reactive multi-context systems (rMCSs), a framework for reactive
reasoning in the presence of heterogeneous knowledge sources and data streams.
We show that rMCSs are indeed well-suited for this purpose by illustrating how
several typical problems arising in the context of stream reasoning can be
handled using them, by showing how inconsistencies possibly occurring in the
integration of multiple knowledge sources can be handled, and by arguing that
the potential non-determinism of rMCSs can be avoided if needed using an
alternative, more skeptical well-founded semantics instead with beneficial
computational properties. We also investigate the computational complexity of
various reasoning problems related to rMCSs. Finally, we discuss related work,
and show that rMCSs do not only generalize mMCSs to dynamic settings, but also
capture/extend relevant approaches w.r.t. dynamics in knowledge representation
and stream reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brewka_G/0/1/0/all/0/1&quot;&gt;Gerhard Brewka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellmauthaler_S/0/1/0/all/0/1&quot;&gt;Stefan Ellmauthaler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_R/0/1/0/all/0/1&quot;&gt;Ricardo Gon&amp;#xe7;alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knorr_M/0/1/0/all/0/1&quot;&gt;Matthias Knorr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leite_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Leite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puhrer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg P&amp;#xfc;hrer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.01161">
<title>Finite Sample Analyses for TD(0) with Function Approximation. (arXiv:1704.01161v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1704.01161</link>
<description rdf:parseType="Literal">&lt;p&gt;TD(0) is one of the most commonly used algorithms in reinforcement learning.
Despite this, there is no existing finite sample analysis for TD(0) with
function approximation, even for the linear case. Our work is the first to
provide such results. Existing convergence rates for Temporal Difference (TD)
methods apply only to somewhat modified versions, e.g., projected variants or
ones where stepsizes depend on unknown problem parameters. Our analyses obviate
these artificial alterations by exploiting strong properties of TD(0). We
provide convergence rates both in expectation and with high-probability. The
two are obtained via different approaches that use relatively unknown, recently
developed stochastic approximation techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_G/0/1/0/all/0/1&quot;&gt;Gal Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szorenyi_B/0/1/0/all/0/1&quot;&gt;Bal&amp;#xe1;zs Sz&amp;#xf6;r&amp;#xe9;nyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoppe_G/0/1/0/all/0/1&quot;&gt;Gugan Thoppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06975">
<title>Generating Visual Representations for Zero-Shot Classification. (arXiv:1708.06975v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06975</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the task of learning an image clas-sifier when some
categories are defined by semantic descriptions only (e.g. visual attributes)
while the others are defined by exemplar images as well. This task is often
referred to as the Zero-Shot classification task (ZSC). Most of the previous
methods rely on learning a common embedding space allowing to compare visual
features of unknown categories with semantic descriptions. This paper argues
that these approaches are limited as i) efficient discrimi-native classifiers
can&apos;t be used ii) classification tasks with seen and unseen categories
(Generalized Zero-Shot Classification or GZSC) can&apos;t be addressed efficiently.
In contrast , this paper suggests to address ZSC and GZSC by i) learning a
conditional generator using seen classes ii) generate artificial training
examples for the categories without exemplars. ZSC is then turned into a
standard supervised learning problem. Experiments with 4 generative models and
5 datasets experimentally validate the approach, giving state-of-the-art
results on both ZSC and GZSC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucher_M/0/1/0/all/0/1&quot;&gt;Maxime Bucher&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herbin_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Herbin&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Jurie&lt;/a&gt; ((1) Palaiseau)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07903">
<title>The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes. (arXiv:1710.07903v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07903</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the never-worse relation (NWR) for Markov decision processes with an
infinite-horizon reachability objective. A state q is never worse than a state
p if the maximal probability of reaching the target set of states from p is at
most the same value from q, regardless of the probabilities labelling the
transitions. Extremal-probability states, end components, and essential states
are all special cases of the equivalence relation induced by the NWR. Using the
NWR, states in the same equivalence class can be collapsed. Then, actions
leading to sub-optimal states can be removed. We show the natural decision
problem associated to computing the NWR is coNP-complete. Finally, we extend a
known incomplete polynomial-time iterative algorithm to under-approximate the
NWR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roux_S/0/1/0/all/0/1&quot;&gt;Stephane Le Roux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03067">
<title>Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03067</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding methods such as word embedding have become pillars for many
applications containing discrete structures. Conventional embedding methods
directly associate each symbol with a continuous embedding vector, which is
equivalent to applying linear transformation based on &quot;one-hot&quot; encoding of the
discrete symbols. Despite its simplicity, such approach yields number of
parameters that grows linearly with the vocabulary size and can lead to
overfitting. In this work we propose a much more compact K-way D-dimensional
discrete encoding scheme to replace the &quot;one-hot&quot; encoding. In &quot;KD encoding&quot;,
each symbol is represented by a $D$-dimensional code, and each of its dimension
has a cardinality of $K$. The final symbol embedding vector can be generated by
composing the code embedding vectors. To learn the semantically meaningful
code, we derive a relaxed discrete optimization technique based on stochastic
gradient descent. By adopting the new coding system, the efficiency of
parameterization can be significantly improved (from linear to logarithmic),
and this can also mitigate the over-fitting problem. In our experiments with
language modeling, the number of embedding parameters can be reduced by 97\%
while achieving similar or better performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1&quot;&gt;Martin Renqiang Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08859">
<title>Exploring Approximations for Floating-Point Arithmetic using UppSAT. (arXiv:1711.08859v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08859</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of solving floating-point constraints obtained from
software verification. We present UppSAT --- a new implementation of a
systematic approximation refinement framework [ZWR17] as an abstract SMT
solver. Provided with an approximation and a decision procedure (implemented in
an off-the-shelf SMT solver), UppSAT yields an approximating SMT solver.
Additionally, UppSAT includes a library of predefined approximation components
which can be combined and extended to define new encodings, orderings and
solving strategies. We propose that UppSAT can be used as a sandbox for easy
and flexible exploration of new approximations. To substantiate this, we
explore several approximations of floating-point arithmetic. Approximations can
be viewed as a composition of an encoding into a target theory, a precision
ordering, and a number of strategies for model reconstruction and precision (or
approximation) refinement. We present encodings of floating-point arithmetic
into reduced precision floating-point arithmetic, real-arithmetic, and
fixed-point arithmetic (encoded in the theory of bit-vectors). In an
experimental evaluation, we compare the advantages and disadvantages of
approximating solvers obtained by combining various encodings and decision
procedures (based on existing state-of-the-art SMT solvers for floating-point,
real, and bit-vector arithmetic).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeljic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Zeljic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backeman_P/0/1/0/all/0/1&quot;&gt;Peter Backeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wintersteiger_C/0/1/0/all/0/1&quot;&gt;Christoph M. Wintersteiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruemmer_P/0/1/0/all/0/1&quot;&gt;Philipp Ruemmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11068">
<title>Happiness Pursuit: Personality Learning in a Society of Agents. (arXiv:1711.11068v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11068</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling personality is a challenging problem with applications spanning
computer games, virtual assistants, online shopping and education. Many
techniques have been tried, ranging from neural networks to computational
cognitive architectures. However, most approaches rely on examples with
hand-crafted features and scenarios. Here, we approach learning a personality
by training agents using a Deep Q-Network (DQN) model on rewards based on
psychoanalysis, against hand-coded AI in the game of Pong. As a result, we
obtain 4 agents, each with its own personality. Then, we define happiness of an
agent, which can be seen as a measure of alignment with agent&apos;s objective
function, and study it when agents play both against hand-coded AI, and against
each other. We find that the agents that achieve higher happiness during
testing against hand-coded AI, have lower happiness when competing against each
other. This suggests that higher happiness in testing is a sign of overfitting
in learning to interact with hand-coded AI, and leads to worse performance
against agents with different personalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muszynski_R/0/1/0/all/0/1&quot;&gt;Rafa&amp;#x142; Muszy&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03043">
<title>A Heuristic Search Algorithm Using the Stability of Learning Algorithms in Certain Scenarios as the Fitness Function: An Artificial General Intelligence Engineering Approach. (arXiv:1712.03043v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03043</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a non-manual design engineering method based on heuristic
search algorithm to search for candidate agents in the solution space which
formed by artificial intelligence agents modeled on the base of
bionics.Compared with the artificial design method represented by meta-learning
and the bionics method represented by the neural architecture chip,this method
is more feasible for realizing artificial general intelligence,and it has a
much better interaction with cognitive neuroscience;at the same time,the
engineering method is based on the theoretical hypothesis that the final
learning algorithm is stable in certain scenarios,and has generalization
ability in various scenarios.The paper discusses the theory preliminarily and
proposes the possible correlation between the theory and the fixed-point
theorem in the field of mathematics.Limited by the author&apos;s knowledge
level,this correlation is proposed only as a kind of conjecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zengkun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03281">
<title>Fast Low-Rank Matrix Estimation without the Condition Number. (arXiv:1712.03281v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03281</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the general problem of optimizing a convex function
$F(L)$ over the set of $p \times p$ matrices, subject to rank constraints on
$L$. However, existing first-order methods for solving such problems either are
too slow to converge, or require multiple invocations of singular value
decompositions. On the other hand, factorization-based non-convex algorithms,
while being much faster, require stringent assumptions on the \emph{condition
number} of the optimum. In this paper, we provide a novel algorithmic framework
that achieves the best of both worlds: asymptotically as fast as factorization
methods, while requiring no dependency on the condition number.
&lt;/p&gt;
&lt;p&gt;We instantiate our general framework for three important matrix estimation
problems that impact several practical applications; (i) a \emph{nonlinear}
variant of affine rank minimization, (ii) logistic PCA, and (iii) precision
matrix estimation in probabilistic graphical model learning. We then derive
explicit bounds on the sample complexity as well as the running time of our
approach, and show that it achieves the best possible bounds for both cases. We
also provide an extensive range of experimental results, and demonstrate that
our algorithm provides a very attractive tradeoff between estimation accuracy
and running time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soltani_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Soltani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03298">
<title>Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks. (arXiv:1712.03298v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03298</link>
<description rdf:parseType="Literal">&lt;p&gt;Progress in deep learning is slowed by the days or weeks it takes to train
large models. The natural solution of using more hardware is limited by
diminishing returns, and leads to inefficient use of additional resources. In
this paper, we present a large batch, stochastic optimization algorithm that is
both faster than widely used algorithms for fixed amounts of computation, and
also scales up substantially better as more computational resources become
available. Our algorithm implicitly computes the inverse Hessian of each
mini-batch to produce descent directions; we do so without either an explicit
approximation to the Hessian or Hessian-vector products. We demonstrate the
effectiveness of our algorithm by successfully training large ImageNet models
(Inception-V3, Resnet-50, Resnet-101 and Inception-Resnet-V2) with mini-batch
sizes of up to 32000 with no loss in validation error relative to current
baselines, and no increase in the total number of steps. At smaller mini-batch
sizes, our optimizer improves the validation error in these models by 0.8-0.9%.
Alternatively, we can trade off this accuracy to reduce the number of training
steps needed by roughly 10-30%. Our work is practical and easily usable by
others -- only one hyperparameter (learning rate) needs tuning, and
furthermore, the algorithm is as computationally cheap as the commonly used
Adam optimizer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1&quot;&gt;Shankar Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Ying Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1&quot;&gt;Rif A. Saurous&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03337">
<title>Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise. (arXiv:1712.03337v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.03337</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix decomposition is a popular and fundamental approach in machine
learning and data mining. It has been successfully applied into various fields.
Most matrix decomposition methods focus on decomposing a data matrix from one
single source. However, it is common that data are from different sources with
heterogeneous noise. A few of matrix decomposition methods have been extended
for such multi-view data integration and pattern discovery. While only few
methods were designed to consider the heterogeneity of noise in such multi-view
data for data integration explicitly. To this end, we propose a joint matrix
decomposition framework (BJMD), which models the heterogeneity of noise by
Gaussian distribution in a Bayesian framework. We develop two algorithms to
solve this model: one is a variational Bayesian inference algorithm, which
makes full use of the posterior distribution; and another is a maximum a
posterior algorithm, which is more scalable and can be easily paralleled.
Extensive experiments on synthetic and real-world datasets demonstrate that
BJMD considering the heterogeneity of noise is superior or competitive to the
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03353">
<title>Variational Inference over Non-differentiable Cardiac Simulators using Bayesian Optimization. (arXiv:1712.03353v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03353</link>
<description rdf:parseType="Literal">&lt;p&gt;Performing inference over simulators is generally intractable as their
runtime means we cannot compute a marginal likelihood. We develop a
likelihood-free inference method to infer parameters for a cardiac simulator,
which replicates electrical flow through the heart to the body surface. We
improve the fit of a state-of-the-art simulator to an electrocardiogram (ECG)
recorded from a real patient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCarthy_A/0/1/0/all/0/1&quot;&gt;Adam McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodriguez_B/0/1/0/all/0/1&quot;&gt;Blanca Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Minchole_A/0/1/0/all/0/1&quot;&gt;Ana Minchole&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03412">
<title>Elastic-net regularized High-dimensional Negative Binomial Regression: Consistency and Weak Signals Detection. (arXiv:1712.03412v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03412</link>
<description rdf:parseType="Literal">&lt;p&gt;We study sparse high-dimensional negative binomial regression problem for
count data regression by showing non-asymptotic merits of the Elastic-net
regularized estimator. With the KKT conditions, we derive two types of
non-asymptotic oracle inequalities for the elastic net estimates of negative
binomial regression by utilizing Compatibility factor and Stabil Condition,
respectively. Based on oracle inequalities we proposed, we firstly show the
sign consistency property of the Elastic-net estimators provided that the
non-zero components in sparse true vector are large than a proper choice of the
weakest signal detection threshold, and the second application is that we give
an oracle inequality for bounding the grouping effect with high probability,
thirdly, under some assumptions of design matrix, we can recover the true
variable set with high probability if the weakest signal detection threshold is
large than 3 times the value of turning parameter, at last, we briefly discuss
the de-biased Elastic-net estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinzhu Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03428">
<title>Cost-Sensitive Approach to Batch Size Adaptation for Gradient Descent. (arXiv:1712.03428v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03428</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach to automatically determine the
batch size in stochastic gradient descent methods. The choice of the batch size
induces a trade-off between the accuracy of the gradient estimate and the cost
in terms of samples of each update. We propose to determine the batch size by
optimizing the ratio between a lower bound to a linear or quadratic Taylor
approximation of the expected improvement and the number of samples used to
estimate the gradient. The performance of the proposed approach is empirically
compared with related methods on popular classification tasks.
&lt;/p&gt;
&lt;p&gt;The work was presented at the NIPS workshop on Optimizing the Optimizers.
Barcelona, Spain, 2016.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1&quot;&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Restelli_M/0/1/0/all/0/1&quot;&gt;Marcello Restelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03471">
<title>Identifiability of Kronecker-structured Dictionaries for Tensor Data. (arXiv:1712.03471v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03471</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper derives sufficient conditions for reliable recovery of coordinate
dictionaries comprising a Kronecker-structured dictionary that is used for
representing $K$th-order tensor data. Tensor observations are generated by a
Kronecker-structured dictionary and sparse coefficient tensors that follow the
separable sparsity model. This work provides sufficient conditions on the
underlying coordinate dictionaries, coefficient and noise distributions, and
number of samples that guarantee recovery of the individual coordinate
dictionaries up to a specified error with high probability. In particular, the
sample complexity to recover $K$ coordinate dictionaries with dimensions
$m_k\times p_k$ up to estimation error $r_k$ is shown to be $\max_{k \in
[K]}\mathcal{O}(m_kp_k^3r_k^{-2})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shakeri_Z/0/1/0/all/0/1&quot;&gt;Zahra Shakeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarwate_A/0/1/0/all/0/1&quot;&gt;Anand D. Sarwate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bajwa_W/0/1/0/all/0/1&quot;&gt;Waheed U. Bajwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03480">
<title>Capsule Network Performance on Complex Data. (arXiv:1712.03480v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03480</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, convolutional neural networks (CNN) have played an important
role in the field of deep learning. Variants of CNN&apos;s have proven to be very
successful in classification tasks across different domains. However, there are
two big drawbacks to CNN&apos;s: their failure to take into account of important
spatial hierarchies between features, and their lack of rotational invariance.
As long as certain key features of an object are present in the test data,
CNN&apos;s classify the test data as the object, disregarding features&apos; relative
spatial orientation to each other. This causes false positives. The lack of
rotational invariance in CNN&apos;s would cause the network to incorrectly assign
the object another label, causing false negatives. To address this concern,
Hinton et al. propose a novel type of neural network using the concept of
capsules in a recent paper. With the use of dynamic routing and reconstruction
regularization, the capsule network model would be both rotation invariant and
spatially aware. The capsule network has shown its potential by achieving a
state-of-the-art result of 0.25% test error on MNIST without data augmentation
such as rotation and scaling, better than the previous baseline of 0.39%. To
further test out the application of capsule networks on data with higher
dimensionality, we attempt to find the best set of configurations that yield
the optimal test error on CIFAR10 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_E/0/1/0/all/0/1&quot;&gt;Edgar Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bing_S/0/1/0/all/0/1&quot;&gt;Selina Bing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yang Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03483">
<title>Improving Malware Detection Accuracy by Extracting Icon Information. (arXiv:1712.03483v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1712.03483</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting PE malware files is now commonly approached using statistical and
machine learning models. While these models commonly use features extracted
from the structure of PE files, we propose that icons from these files can also
help better predict malware. We propose an innovative machine learning approach
to extract information from icons. Our proposed approach consists of two steps:
1) extracting icon features using summary statics, histogram of gradients
(HOG), and a convolutional autoencoder, 2) clustering icons based on the
extracted icon features. Using publicly available data and by using machine
learning experiments, we show our proposed icon clusters significantly boost
the efficacy of malware prediction models. In particular, our experiments show
an average accuracy increase of 10% when icon clusters are used in the
prediction model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1&quot;&gt;Pedro Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhavan_Masouleh_S/0/1/0/all/0/1&quot;&gt;Sepehr Akhavan-Masouleh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03553">
<title>Causal Inference for Observational Time-Series with Encoder-Decoder Networks. (arXiv:1712.03553v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03553</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a method for estimating the causal effect of a discrete
intervention in observational time-series data using encoder-decoder recurrent
neural networks (RNNs). Encoder-decoder networks, which are special class of
RNNs suitable for handling variable-length sequential data, are used to predict
a counterfactual time-series of treated unit outcomes. The proposed method does
not rely on pretreatment covariates and encoder-decoder networks are capable of
learning nonconvex combinations of control unit outcomes to construct a
counterfactual. To demonstrate the proposed method, I extend a field experiment
studying the effect of radio advertisements on electoral competition to
observational time-series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poulos_J/0/1/0/all/0/1&quot;&gt;Jason Poulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03563">
<title>DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model. (arXiv:1712.03563v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03563</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) can be applied to graph similarity
matching, in which case they are called graph CNNs. Graph CNNs are attracting
increasing attention due to their effectiveness and efficiency. However, the
existing convolution approaches focus only on regular data forms and require
the transfer of the graph or key node neighborhoods of the graph into the same
fixed form. During this transfer process, structural information of the graph
can be lost, and some redundant information can be incorporated. To overcome
this problem, we propose the disordered graph convolutional neural network
(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a
preprocessing layer called the disordered graph convolutional layer (DGCL). The
DGCL uses a mixed Gaussian function to realize the mapping between the
convolution kernel and the nodes in the neighborhood of the graph. The output
of the DGCL is the input of the CNN. We further implement a
backward-propagation optimization process of the convolutional layer by which
we incorporate the feature-learning model of the irregular node neighborhood
structure into the network. Thereafter, the optimization of the convolution
kernel becomes part of the neural network learning process. The DGCNN can
accept arbitrary scaled and disordered neighborhood graph structures as the
receptive fields of CNNs, which reduces information loss during graph
transformation. Finally, we perform experiments on multiple standard graph
datasets. The results show that the proposed method outperforms the
state-of-the-art methods in graph classification and retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_B/0/1/0/all/0/1&quot;&gt;Bo Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03605">
<title>Sensitivity Analysis for Predictive Uncertainty in Bayesian Neural Networks. (arXiv:1712.03605v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03605</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive a novel sensitivity analysis of input variables for predictive
epistemic and aleatoric uncertainty. We use Bayesian neural networks with
latent variables as a model class and illustrate the usefulness of our
sensitivity analysis on real-world datasets. Our method increases the
interpretability of complex black-box probabilistic models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Depeweg_S/0/1/0/all/0/1&quot;&gt;Stefan Depeweg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Udluft_S/0/1/0/all/0/1&quot;&gt;Steffen Udluft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Runkler_T/0/1/0/all/0/1&quot;&gt;Thomas Runkler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03607">
<title>Gradient Normalization &amp; Depth Based Decay For Deep Learning. (arXiv:1712.03607v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03607</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a novel method of gradient normalization and decay
with respect to depth. Our method leverages the simple concept of normalizing
all gradients in a deep neural network, and then decaying said gradients with
respect to their depth in the network. Our proposed normalization and decay
techniques can be used in conjunction with most current state of the art
optimizers and are a very simple addition to any network. This method, although
simple, showed improvements in convergence time on state of the art networks
such as DenseNet and ResNet on image classification tasks, as well as on an
LSTM for natural language processing tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwiatkowski_R/0/1/0/all/0/1&quot;&gt;Robert Kwiatkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_O/0/1/0/all/0/1&quot;&gt;Oscar Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03638">
<title>The PhaseLift for Non-quadratic Gaussian Measurements. (arXiv:1712.03638v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03638</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of recovering a structured signal $\mathbf{x}_0$ from
high-dimensional measurements of the form $y=f(\mathbf{a}^T\mathbf{x}_0)$ for
some nonlinear function $f$. When the measurement vector $\mathbf a$ is iid
Gaussian, Brillinger observed in his 1982 paper that $\mu_\ell\cdot\mathbf{x}_0
= \min_{\mathbf{x}}\mathbb{E}(y - \mathbf{a}^T\mathbf{x})^2$, where
$\mu_\ell=\mathbb{E}_{\gamma}[\gamma f(\gamma)]$ with $\gamma$ being a standard
Gaussian random variable. Based on this simple observation, he showed that, in
the classical statistical setting, the least-squares method is consistent. More
recently, Plan \&amp;amp; Vershynin extended this result to the high-dimensional
setting and derived error bounds for the generalized Lasso. Unfortunately, both
least-squares and the Lasso fail to recover $\mathbf{x}_0$ when $\mu_\ell=0$.
For example, this includes all even link functions. We resolve this issue by
proposing and analyzing an appropriate generic semidefinite-optimization based
method. In a nutshell, our idea is to treat such link functions as if they were
linear in a lifted space of higher-dimension. An appealing feature of our error
analysis is that it captures the effect of the nonlinearity in a few simple
summary parameters, which can be particularly useful in system design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rawat_A/0/1/0/all/0/1&quot;&gt;Ankit Singh Rawat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03660">
<title>Distributed Mapper. (arXiv:1712.03660v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.03660</link>
<description rdf:parseType="Literal">&lt;p&gt;The construction of Mapper has emerged in the last decade as a powerful and
effective topological data analysis tool that approximates and generalizes
other topological summaries, such as the Reeb graph, the contour tree, split,
and joint trees. In this paper we study the parallel analysis of the
construction of Mapper. We give a provably correct algorithm to distribute
Mapper on a set of processors and discuss the performance results that compare
our approach to a reference sequential Mapper implementation. We report the
performance experiments that demonstrate the efficiency of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1&quot;&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assiri_B/0/1/0/all/0/1&quot;&gt;Basem Assiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosen_P/0/1/0/all/0/1&quot;&gt;Paul Rosen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03834">
<title>Crime prediction through urban metrics and statistical learning. (arXiv:1712.03834v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/1712.03834</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the causes of crime is a longstanding issue in researcher&apos;s
agenda. While it is a hard task to extract causality from data, several linear
models have been proposed to predict crime through the existing correlations
between crime and urban metrics. However, because of non-Gaussian distributions
and multicollinearity in urban indicators, it is common to find controversial
conclusions about the influence of some urban indicators on crime. Machine
learning ensemble-based algorithms can handle well such problems. Here, we use
a random forest regressor to predict crime and quantify the influence of urban
indicators on homicides. Our approach can have up to $97\%$ of accuracy on
crime prediction and the importance of urban indicators is ranked and clustered
in groups of equal influence, which are robust under slightly changes in the
data sample analyzed. Our results determine the rank of importance of urban
indicators to predict crime, unveiling that unemployment and illiteracy are the
most important variables for describing homicides in Brazilian cities. We
further believe that our approach helps in producing more robust conclusions
regarding the effects of urban indicators on crime, having potential
applications for guiding public policies for crime control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Alves_L/0/1/0/all/0/1&quot;&gt;Luiz G A Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ribeiro_H/0/1/0/all/0/1&quot;&gt;Haroldo V Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rodrigues_F/0/1/0/all/0/1&quot;&gt;Francisco A Rodrigues&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03847">
<title>On Quadratic Penalties in Elastic Weight Consolidation. (arXiv:1712.03847v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.03847</link>
<description rdf:parseType="Literal">&lt;p&gt;Elastic weight consolidation (EWC, Kirkpatrick et al, 2017) is a novel
algorithm designed to safeguard against catastrophic forgetting in neural
networks. EWC can be seen as an approximation to Laplace propagation (Eskin et
al, 2004), and this view is consistent with the motivation given by Kirkpatrick
et al (2017). In this note, I present an extended derivation that covers the
case when there are more than two tasks. I show that the quadratic penalties in
EWC are inconsistent with this derivation and might lead to double-counting
data from earlier tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huszar_F/0/1/0/all/0/1&quot;&gt;Ferenc Husz&amp;#xe1;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03878">
<title>Generalized Zero-Shot Learning via Synthesized Examples. (arXiv:1712.03878v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.03878</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a generative framework for generalized zero-shot learning where
the training and test classes are not necessarily disjoint. Built upon a
variational autoencoder based architecture, consisting of a probabilistic
encoder and a probabilistic conditional decoder, our model can generate novel
exemplars from seen/unseen classes, given their respective class attributes.
These exemplars can subsequently be used to train any off-the-shelf
classification model. One of the key aspects of our encoder-decoder
architecture is a feedback-driven mechanism in which a discriminator (a
multivariate regressor) learns to map the generated exemplars to the
corresponding class attribute vectors, leading to an improved generator. Our
model&apos;s ability to generate and leverage examples from unseen classes to train
the classification model naturally helps to mitigate the bias towards
predicting seen classes in generalized zero-shot learning settings. Through a
comprehensive set of experiments, we show that our model outperforms several
state-of-the-art methods, on several benchmark datasets, for both standard as
well as generalized zero-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1&quot;&gt;Gundeep Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1&quot;&gt;Vinay Kumar Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Ashish Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1&quot;&gt;Piyush Rai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03893">
<title>Towards reduction of autocorrelation in HMC by machine learning. (arXiv:1712.03893v1 [hep-lat])</title>
<link>http://arxiv.org/abs/1712.03893</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose new algorithm to reduce autocorrelation in Markov
chain Monte-Carlo algorithms for euclidean field theories on the lattice. Our
proposing algorithm is the Hybrid Monte-Carlo algorithm (HMC) with restricted
Boltzmann machine. We examine the validity of the algorithm by employing the
phi-fourth theory in three dimension. We observe reduction of the
autocorrelation both in symmetric and broken phase as well. Our proposing
algorithm provides consistent central values of expectation values of the
action density and one-point Green&apos;s function with ones from the original HMC
in both the symmetric phase and broken phase within the statistical error. On
the other hand, two-point Green&apos;s functions have slight difference between one
calculated by the HMC and one by our proposing algorithm in the symmetric
phase. Furthermore, near the criticality, the distribution of the one-point
Green&apos;s function differs from the one from HMC. We discuss the origin of
discrepancies and its improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-lat/1/au:+Tanaka_A/0/1/0/all/0/1&quot;&gt;Akinori Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-lat/1/au:+Tomiya_A/0/1/0/all/0/1&quot;&gt;Akio Tomiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1505.05629">
<title>Regulating Greed Over Time. (arXiv:1505.05629v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1505.05629</link>
<description rdf:parseType="Literal">&lt;p&gt;In retail, there are predictable yet dramatic time-dependent patterns in
customer behavior, such as periodic changes in the number of visitors, or
increases in visitors just before major holidays. The current paradigm of
multi-armed bandit analysis does not take these known patterns into account.
This means that for applications in retail, where prices are fixed for periods
of time, current bandit algorithms will not suffice. This work provides a
remedy that takes the time-dependent patterns into account, and we show how
this remedy is implemented in the UCB and {\epsilon}-greedy methods and we
introduce a new policy called the variable arm pool method. In the corrected
methods, exploitation (greed) is regulated over time, so that more exploitation
occurs during higher reward periods, and more exploration occurs in periods of
low reward. In order to understand why regret is reduced with the corrected
methods, we present a set of bounds that provide insight into why we would want
to exploit during periods of high reward, and discuss the impact on regret. Our
proposed methods perform well in experiments, and were inspired by a
high-scoring entry in the Exploration and Exploitation 3 contest using data
from Yahoo! Front Page. That entry heavily used time-series methods to regulate
greed over time, which was substantially more effective than other contextual
bandit methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Traca_S/0/1/0/all/0/1&quot;&gt;Stefano Trac&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00326">
<title>Discriminative k-shot learning using probabilistic models. (arXiv:1706.00326v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00326</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a probabilistic framework for k-shot image
classification. The goal is to generalise from an initial large-scale
classification task to a separate task comprising new classes and small numbers
of examples. The new approach not only leverages the feature-based
representation learned by a neural network from the initial task
(representational transfer), but also information about the classes (concept
transfer). The concept information is encapsulated in a probabilistic model for
the final layer weights of the neural network which acts as a prior for
probabilistic k-shot learning. We show that even a simple probabilistic model
achieves state-of-the-art on a standard k-shot learning dataset by a large
margin. Moreover, it is able to accurately model uncertainty, leading to well
calibrated classifiers, and is easily extensible and flexible, unlike many
recent approaches to k-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bauer_M/0/1/0/all/0/1&quot;&gt;Matthias Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rojas_Carulla_M/0/1/0/all/0/1&quot;&gt;Mateo Rojas-Carulla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Swiatkowski_J/0/1/0/all/0/1&quot;&gt;Jakub Bart&amp;#x142;omiej &amp;#x15a;wi&amp;#x105;tkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04395">
<title>Tight Semi-Nonnegative Matrix Factorization. (arXiv:1709.04395v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04395</link>
<description rdf:parseType="Literal">&lt;p&gt;The nonnegative matrix factorization is a widely used, flexible matrix
decomposition, finding applications in biology, image and signal processing and
information retrieval, among other areas. Here we present a related matrix
factorization. A multi-objective optimization problem finds conical
combinations of templates that approximate a given data matrix. The templates
are chosen so that as far as possible only the initial data set can be
represented this way. However, the templates are not required to be nonnegative
nor convex combinations of the original data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dreisigmeyer_D/0/1/0/all/0/1&quot;&gt;David W Dreisigmeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04673">
<title>Conditions for Stability and Convergence of Set-Valued Stochastic Approximations: Applications to Approximate Value and Fixed point Iterations. (arXiv:1709.04673v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04673</link>
<description rdf:parseType="Literal">&lt;p&gt;The main aim of this paper is the development of easily verifiable sufficient
conditions for stability (almost sure boundedness) and convergence of
stochastic approximation algorithms (SAAs) with set-valued mean-fields, a class
of model-free algorithms that have become important in recent times. In this
paper we provide a complete analysis of such algorithms under three different,
yet related sets of sufficient conditions, based on the existence of an
associated global/local Lyapunov function. Unlike previous Lyapunov function
based approaches, we provide a simple recipe for explicitly constructing the
Lyapunov function, needed for analysis. Our work builds on the works of
Abounadi, Bertsekas and Borkar (2002), Munos (2005), and Ramaswamy and
Bhatnagar (2016). An important motivation for the flavor of our assumptions
comes from the need to understand dynamic programming and reinforcement
learning algorithms, that use deep neural networks (DNNs) for function
approximations and parameterizations. These algorithms are popularly known as
deep learning algorithms. As an important application of our theory, we provide
a complete analysis of the stochastic approximation counterpart of approximate
value iteration (AVI), an important dynamic programming method designed to
tackle Bellman&apos;s curse of dimensionality. Further, the assumptions involved are
significantly weaker, easily verifiable and truly model-free. The theory
presented in this paper is also used to develop and analyze the first SAA for
finding fixed points of contractive set-valued maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramaswamy_A/0/1/0/all/0/1&quot;&gt;Arunselvan Ramaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1&quot;&gt;Shalabh Bhatnagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11268">
<title>Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection. (arXiv:1710.11268v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11268</link>
<description rdf:parseType="Literal">&lt;p&gt;The mean field variational Bayes method is becoming increasingly popular in
statistics and machine learning. Its iterative Coordinate Ascent Variational
Inference algorithm has been widely applied to large scale Bayesian inference.
See Blei et al. (2017) for a recent comprehensive review. Despite the
popularity of the mean field method there exist remarkably little fundamental
theoretical justifications. To the best of our knowledge, the iterative
algorithm has never been investigated for any high dimensional and complex
model. In this paper, we study the mean field method for community detection
under the Stochastic Block Model. For an iterative Batch Coordinate Ascent
Variational Inference algorithm, we show that it has a linear convergence rate
and converges to the minimax rate within $\log n$ iterations. This complements
the results of Bickel et al. (2013) which studied the global minimum of the
mean field variational Bayes and obtained asymptotic normal estimation of
global model parameters. In addition, we obtain similar optimality results for
Gibbs sampling and an iterative procedure to calculate maximum likelihood
estimation, which can be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anderson Y. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Harrison H. Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06786">
<title>Measuring Territorial Control in Civil Wars Using Hidden Markov Models: A Data Informatics-Based Approach. (arXiv:1711.06786v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06786</link>
<description rdf:parseType="Literal">&lt;p&gt;Territorial control is a key aspect shaping the dynamics of civil war.
Despite its importance, we lack data on territorial control that are
fine-grained enough to account for subnational spatio-temporal variation and
that cover a large set of conflicts. To resolve this issue, we propose a
theoretical model of the relationship between territorial control and tactical
choice in civil war and outline how Hidden Markov Models (HMMs) are suitable to
capture theoretical intuitions and estimate levels of territorial control. We
discuss challenges of using HMMs in this application and mitigation strategies
for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anders_T/0/1/0/all/0/1&quot;&gt;Therese Anders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Cheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumar_T/0/1/0/all/0/1&quot;&gt;T. K. Satish Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07468">
<title>Glitch Classification and Clustering for LIGO with Deep Transfer Learning. (arXiv:1711.07468v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07468</link>
<description rdf:parseType="Literal">&lt;p&gt;The detection of gravitational waves with LIGO and Virgo requires a detailed
understanding of the response of these instruments in the presence of
environmental and instrumental noise. Of particular interest is the study of
anomalous non-Gaussian noise transients known as glitches, since their high
occurrence rate in LIGO/Virgo data can obscure or even mimic true gravitational
wave signals. Therefore, successfully identifying and excising glitches is of
utmost importance to detect and characterize gravitational waves. In this
article, we present the first application of Deep Learning combined with
Transfer Learning for glitch classification, using real data from LIGO&apos;s first
discovery campaign labeled by Gravity Spy, showing that knowledge from
pre-trained models for real-world object recognition can be transferred for
classifying spectrograms of glitches. We demonstrate that this method enables
the optimal use of very deep convolutional neural networks for glitch
classification given small unbalanced training datasets, significantly reduces
the training time, and achieves state-of-the-art accuracy above 98.8%. Once
trained via transfer learning, we show that the networks can be truncated and
used as feature extractors for unsupervised clustering to automatically group
together new classes of glitches and anomalies. This novel capability is of
critical importance to identify and remove new types of glitches which will
occur as the LIGO/Virgo detectors gradually attain design sensitivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+George_D/0/1/0/all/0/1&quot;&gt;Daniel George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hongyu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Huerta_E/0/1/0/all/0/1&quot;&gt;E. A. Huerta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08413">
<title>SolarisNet: A Deep Regression Network for Solar Radiation Prediction. (arXiv:1711.08413v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08413</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective utilization of photovoltaic (PV) plants requires weather
variability robust global solar radiation (GSR) forecasting models. Random
weather turbulence phenomena coupled with assumptions of clear sky model as
suggested by Hottel pose significant challenges to parametric &amp;amp; non-parametric
models in GSR conversion rate estimation. Also, a decent GSR estimate requires
costly high-tech radiometer and expert dependent instrument handling and
measurements, which are subjective. As such, a computer aided monitoring (CAM)
system to evaluate PV plant operation feasibility by employing smart grid past
data analytics and deep learning is developed. Our algorithm, SolarisNet is a
6-layer deep neural network trained on data collected at two weather stations
located near Kalyani metrological site, West Bengal, India. The daily GSR
prediction performance using SolarisNet outperforms the existing state of art
and its efficacy in inferring past GSR data insights to comprehend daily and
seasonal GSR variability along with its competence for short term forecasting
is discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1&quot;&gt;Subhadip Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratiher_S/0/1/0/all/0/1&quot;&gt;Sawon Pratiher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Saon Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_C/0/1/0/all/0/1&quot;&gt;Chanchal Kumar Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00961">
<title>Learning Independent Causal Mechanisms. (arXiv:1712.00961v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Independent causal mechanisms are a central concept in the study of causality
with implications for machine learning tasks. In this work we develop an
algorithm to recover a set of (inverse) independent mechanisms relating a
distribution transformed by the mechanisms to a reference distribution. The
approach is fully unsupervised and based on a set of experts that compete for
data to specialize and extract the mechanisms. We test and analyze the proposed
method on a series of experiments based on image transformations. Each expert
successfully maps a subset of the transformed data to the original domain, and
the learned mechanisms generalize to other domains. We discuss implications for
domain transfer and links to recent trends in generative modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_Carulla_M/0/1/0/all/0/1&quot;&gt;Mateo Rojas-Carulla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1&quot;&gt;Niki Kilbertus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item></rdf:RDF>