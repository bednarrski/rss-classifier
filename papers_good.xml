<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07336"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11545"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11838"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11914"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11919"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01863"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09853"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11682"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11698"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05428"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10478"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11393"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.11551">
<title>Deep Recurrent Neural Networks for ECG Signal Denoising. (arXiv:1807.11551v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.11551</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to denoise electrocardiographic signals (ECG),
utilizing deep recurrent neural network built of Long-Short Term Memory (LSTM)
units. The network is trained using synthetic data, generated using a dynamic
model proposed by McSharry et al. as well as real data from Physionet PDB
database of ECG signals. The results show that a 6-layer DRNN has a mean
squared error as low as 0.0121 for denoising real signals with white noise of
amplitude 0.2 mV, making it a viable alternative for other commonly used
methods. We also investigate the impact of synthetic data on the network
performance on real signals. Our findings show that networks trained with more
synthetic data have better results than trained with more real data. We propose
to explain this by means of the transfer learning framework and the analogy to
human cognitive process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antczak_K/0/1/0/all/0/1&quot;&gt;Karol Antczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02464">
<title>Differentiable plasticity: training plastic neural networks with backpropagation. (arXiv:1804.02464v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02464</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we build agents that keep learning from experience, quickly and
efficiently, after their initial training? Here we take inspiration from the
main mechanism of learning in biological brains: synaptic plasticity, carefully
tuned by evolution to produce efficient lifelong learning. We show that
plasticity, just like connection weights, can be optimized by gradient descent
in large (millions of parameters) recurrent networks with Hebbian plastic
connections. First, recurrent plastic networks with more than two million
parameters can be trained to memorize and reconstruct sets of novel,
high-dimensional 1000+ pixels natural images not seen during training.
Crucially, traditional non-plastic recurrent networks fail to solve this task.
Furthermore, trained plastic networks can also solve generic meta-learning
tasks such as the Omniglot task, with competitive results and little parameter
overhead. Finally, in reinforcement learning settings, plastic networks
outperform a non-plastic equivalent in a maze exploration task. We conclude
that differentiable plasticity may provide a powerful novel approach to the
learning-to-learn problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miconi_T/0/1/0/all/0/1&quot;&gt;Thomas Miconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07336">
<title>Neural Code Comprehension: A Learnable Representation of Code Semantics. (arXiv:1806.07336v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07336</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent success of embeddings in natural language processing,
research has been conducted into applying similar methods to code analysis.
Most works attempt to process the code directly or use a syntactic tree
representation, treating it like sentences written in a natural language.
However, none of the existing methods are sufficient to comprehend program
semantics robustly, due to structural features such as function calls,
branching, and interchangeable order of statements. In this paper, we propose a
novel processing technique to learn code semantics, and apply it to a variety
of program analysis tasks. In particular, we stipulate that a robust
distributional hypothesis of code applies to both human- and machine-generated
programs. Following this hypothesis, we define an embedding space, inst2vec,
based on an Intermediate Representation (IR) of the code that is independent of
the source programming language. We provide a novel definition of contextual
flow for this IR, leveraging both the underlying data- and control-flow of the
program. We then analyze the embeddings qualitatively using analogies and
clustering, and evaluate the learned representation on three different
high-level tasks. We show that with a single RNN architecture and pre-trained
fixed embeddings, inst2vec outperforms specialized approaches for performance
prediction (compute device mapping, optimal thread coarsening); and algorithm
classification from raw code (104 classes), where we set a new
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Nun_T/0/1/0/all/0/1&quot;&gt;Tal Ben-Nun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakobovits_A/0/1/0/all/0/1&quot;&gt;Alice Shoshana Jakobovits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11545">
<title>Call Detail Records Driven Anomaly Detection and Traffic Prediction in Mobile Cellular Networks. (arXiv:1807.11545v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11545</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile networks possess information about the users as well as the network.
Such information is useful for making the network end-to-end visible and
intelligent. Big data analytics can efficiently analyze user and network
information, unearth meaningful insights with the help of machine learning
tools. Utilizing big data analytics and machine learning, this work contributes
in three ways. First, we utilize the call detail records (CDR) data to detect
anomalies in the network. For authentication and verification of anomalies, we
use k-means clustering, an unsupervised machine learning algorithm. Through
effective detection of anomalies, we can proceed to suitable design for
resource distribution as well as fault detection and avoidance. Second, we
prepare anomaly-free data by removing anomalous activities and train a neural
network model. By passing anomaly and anomaly-free data through this model, we
observe the effect of anomalous activities in training of the model and also
observe mean square error of anomaly and anomaly free data. Lastly, we use an
autoregressive integrated moving average (ARIMA) model to predict future
traffic for a user. Through simple visualization, we show that anomaly free
data better generalizes the learning models and performs better on prediction
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sultan_K/0/1/0/all/0/1&quot;&gt;Kashif Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1&quot;&gt;Hazrat Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongshan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11567">
<title>Neural Sentence Embedding using Only In-domain Sentences for Out-of-domain Sentence Detection in Dialog Systems. (arXiv:1807.11567v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.11567</link>
<description rdf:parseType="Literal">&lt;p&gt;To ensure satisfactory user experience, dialog systems must be able to
determine whether an input sentence is in-domain (ID) or out-of-domain (OOD).
We assume that only ID sentences are available as training data because
collecting enough OOD sentences in an unbiased way is a laborious and
time-consuming job. This paper proposes a novel neural sentence embedding
method that represents sentences in a low-dimensional continuous vector space
that emphasizes aspects that distinguish ID cases from OOD cases. We first used
a large set of unlabeled text to pre-train word representations that are used
to initialize neural sentence embedding. Then we used domain-category analysis
as an auxiliary task to train neural sentence embedding for OOD sentence
detection. After the sentence representations were learned, we used them to
train an autoencoder aimed at OOD sentence detection. We evaluated our method
by experimentally comparing it to the state-of-the-art methods in an
eight-domain dialog system; our proposed method achieved the highest accuracy
in all tests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1&quot;&gt;Seonghan Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seokhwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Junhwi Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hwanjo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gary Geunbae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11838">
<title>Extensible Grounding of Speech for Robot Instruction. (arXiv:1807.11838v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.11838</link>
<description rdf:parseType="Literal">&lt;p&gt;Spoken language is a convenient interface for commanding a mobile robot. Yet
for this to work a number of base terms must be grounded in perceptual and
motor skills. We detail the language processing used on our robot ELI and
explain how this grounding is performed, how it interacts with user gestures,
and how it handles phenomena such as anaphora. More importantly, however, there
are certain concepts which the robot cannot be preprogrammed with, such as the
names of various objects in a household or the nature of specific tasks it may
be requested to perform. In these cases it is vital that there exist a method
for extending the grounding, essentially &quot;learning by being told&quot;. We describe
how this was successfully implemented for learning new nouns and verbs in a
tabletop setting. Creating this language learning kernel may be the last
explicit programming the robot ever needs - the core mechanism could eventually
be used for imparting a vast amount of knowledge, much as a child learns from
its parents and teachers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Connell_J/0/1/0/all/0/1&quot;&gt;Jonathan Connell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11914">
<title>Computing the Strategy to Commit to in Polymatrix Games (Extended Version). (arXiv:1807.11914v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11914</link>
<description rdf:parseType="Literal">&lt;p&gt;Leadership games provide a powerful paradigm to model many real-world
settings. Most literature focuses on games with a single follower who acts
optimistically, breaking ties in favour of the leader. Unfortunately, for
real-world applications, this is unlikely. In this paper, we look for
efficiently solvable games with multiple followers who play either
optimistically or pessimistically, i.e., breaking ties in favour or against the
leader. We study the computational complexity of finding or approximating an
optimistic or pessimistic leader-follower equilibrium in specific classes of
succinct games---polymatrix like---which are equivalent to 2-player Bayesian
games with uncertainty over the follower, with interdependent or independent
types. Furthermore, we provide an exact algorithm to find a pessimistic
equilibrium for those game classes. Finally, we show that in general polymatrix
games the computation is harder even when players are forced to play pure
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nittis_G/0/1/0/all/0/1&quot;&gt;Giuseppe De Nittis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchesi_A/0/1/0/all/0/1&quot;&gt;Alberto Marchesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatti_N/0/1/0/all/0/1&quot;&gt;Nicola Gatti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11919">
<title>Efficiency, Sequenceability and Deal-Optimality in Fair Division of Indivisible Goods. (arXiv:1807.11919v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11919</link>
<description rdf:parseType="Literal">&lt;p&gt;In fair division of indivisible goods, using sequences of sincere choices (or
picking sequences) is a natural way to allocate the objects. The idea is as
follows: at each stage, a designated agent picks one object among those that
remain. Another intuitive way to obtain an allocation is to give objects to
agents in the first place, and to let agents exchange them as long as such
&quot;deals&quot; are beneficial. This paper investigates these notions, when agents have
additive preferences over objects, and unveils surprising connections between
them, and with other efficiency and fairness notions. In particular, we show
that an allocation is sequenceable iff it is optimal for a certain type of
deals, namely cycle deals involving a single object. Furthermore, any
Pareto-optimal allocation is sequenceable, but not the converse. Regarding
fairness, we show that an allocation can be envy-free and non-sequenceable, but
that every competitive equilibrium with equal incomes is sequenceable. To
complete the picture, we show how some domain restrictions may affect the
relations between these notions. Finally, we experimentally explore the links
between the scales of efficiency and fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beynier_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lie Beynier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouveret_S/0/1/0/all/0/1&quot;&gt;Sylvain Bouveret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemaitre_M/0/1/0/all/0/1&quot;&gt;Michel Lema&amp;#xee;tre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maudet_N/0/1/0/all/0/1&quot;&gt;Nicolas Maudet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rey_S/0/1/0/all/0/1&quot;&gt;Simon Rey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11926">
<title>What am I searching for?. (arXiv:1807.11926v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11926</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we infer intentions and goals from a person&apos;s actions? As an example of
this family of problems, we consider here whether it is possible to decipher
what a person is searching for by decoding their eye movement behavior. We
conducted two human psychophysics experiments on object arrays and natural
images where we monitored subjects&apos; eye movements while they were looking for a
target object. Using as input the pattern of &quot;error&quot; fixations on non-target
objects before the target was found, we developed a model (InferNet) whose goal
was to infer what the target was. &quot;Error&quot; fixations share similar features with
the sought target. The Infernet model uses a pre-trained 2D convolutional
architecture to extract features from the error fixations and computes a 2D
similarity map between the error fixation and all locations across the search
image by modulating the search image via convolution across layers. InferNet
consolidates the modulated response maps across layers via max pooling to keep
track of the sub-patterns highly similar to features at error fixations and
integrates these maps across all error fixations. InferNet successfully
identifies the subject&apos;s goal and outperforms all the competitive null models,
even without any object-specific training on the inference task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Joo Hwee Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1&quot;&gt;Gabriel Kreiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01863">
<title>Marmara Turkish Coreference Corpus and Coreference Resolution Baseline. (arXiv:1706.01863v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01863</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe the Marmara Turkish Coreference Corpus, which is an annotation of
the whole METU-Sabanci Turkish Treebank with mentions and coreference chains.
Collecting eight or more independent annotations for each document allowed for
fully automatic adjudication. We provide a baseline system for Turkish mention
detection and coreference resolution and evaluate it on the corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_P/0/1/0/all/0/1&quot;&gt;Peter Sch&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cingilli_K/0/1/0/all/0/1&quot;&gt;K&amp;#xfc;bra C&amp;#x131;ng&amp;#x131;ll&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuncer_F/0/1/0/all/0/1&quot;&gt;Ferit Tun&amp;#xe7;er&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surmeli_B/0/1/0/all/0/1&quot;&gt;Bar&amp;#x131;&amp;#x15f; G&amp;#xfc;n S&amp;#xfc;rmeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pekel_A/0/1/0/all/0/1&quot;&gt;Ay&amp;#x15f;eg&amp;#xfc;l Pekel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatay_A/0/1/0/all/0/1&quot;&gt;Ay&amp;#x15f;e Hande Karatay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karakas_H/0/1/0/all/0/1&quot;&gt;Hacer Ezgi Karaka&amp;#x15f;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01244">
<title>Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory. (arXiv:1711.01244v7 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01244</link>
<description rdf:parseType="Literal">&lt;p&gt;In meta-learning an agent extracts knowledge from observed tasks, aiming to
facilitate learning of novel future tasks. Under the assumption that future
tasks are &apos;related&apos; to previous tasks, the accumulated knowledge should be
learned in a way which captures the common structure across learned tasks,
while allowing the learner sufficient flexibility to adapt to novel aspects of
new tasks. We present a framework for meta-learning that is based on
generalization error bounds, allowing us to extend various PAC-Bayes bounds to
meta-learning. Learning takes place through the construction of a distribution
over hypotheses based on the observed tasks, and its utilization for learning a
new task. Thus, prior knowledge is incorporated through setting an
experience-dependent prior for novel tasks. We develop a gradient-based
algorithm which minimizes an objective function derived from the bounds and
demonstrate its effectiveness numerically with deep neural networks. In
addition to establishing the improved performance available through
meta-learning, we demonstrate the intuitive way by which prior information is
manifested at different levels of the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amit_R/0/1/0/all/0/1&quot;&gt;Ron Amit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meir_R/0/1/0/all/0/1&quot;&gt;Ron Meir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04406">
<title>Which Training Methods for GANs do actually Converge?. (arXiv:1801.04406v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04406</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has shown local convergence of GAN training for absolutely
continuous data and generator distributions. In this paper, we show that the
requirement of absolute continuity is necessary: we describe a simple yet
prototypical counterexample showing that in the more realistic case of
distributions that are not absolutely continuous, unregularized GAN training is
not always convergent. Furthermore, we discuss regularization strategies that
were recently proposed to stabilize GAN training. Our analysis shows that GAN
training with instance noise or zero-centered gradient penalties converges. On
the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number
of discriminator updates per generator update do not always converge to the
equilibrium point. We discuss these results, leading us to a new explanation
for the stability problems of GAN training. Based on our analysis, we extend
our convergence results to more general GANs and prove local convergence for
simplified gradient penalties even if the generator and data distribution lie
on lower dimensional manifolds. We find these penalties to work well in
practice and use them to learn high-resolution generative image models for a
variety of datasets with little hyperparameter tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mescheder_L/0/1/0/all/0/1&quot;&gt;Lars Mescheder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowozin_S/0/1/0/all/0/1&quot;&gt;Sebastian Nowozin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09853">
<title>Generative Design in Minecraft (GDMC), Settlement Generation Competition. (arXiv:1803.09853v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09853</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the settlement generation competition for Minecraft,
the first part of the Generative Design in Minecraft challenge. The settlement
generation competition is about creating Artificial Intelligence (AI) agents
that can produce functional, aesthetically appealing and believable settlements
adapted to a given Minecraft map - ideally at a level that can compete with
human created designs. The aim of the competition is to advance procedural
content generation for games, especially in overcoming the challenges of
adaptive and holistic PCG. The paper introduces the technical details of the
challenge, but mostly focuses on what challenges this competition provides and
why they are scientifically relevant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salge_C/0/1/0/all/0/1&quot;&gt;Christoph Salge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1&quot;&gt;Michael Cerny Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canaan_R/0/1/0/all/0/1&quot;&gt;Rodrigo Canaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01077">
<title>DOCK: Detecting Objects by transferring Common-sense Knowledge. (arXiv:1804.01077v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01077</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a scalable approach for Detecting Objects by transferring
Common-sense Knowledge (DOCK) from source to target categories. In our setting,
the training data for the source categories have bounding box annotations,
while those for the target categories only have image-level annotations.
Current state-of-the-art approaches focus on image-level visual or semantic
similarity to adapt a detector trained on the source categories to the new
target categories. In contrast, our key idea is to (i) use similarity not at
the image-level, but rather at the region-level, and (ii) leverage richer
common-sense (based on attribute, spatial, etc.) to guide the algorithm towards
learning the correct detections. We acquire such common-sense cues
automatically from readily-available knowledge bases without any extra human
effort. On the challenging MS COCO dataset, we find that common-sense knowledge
can substantially improve detection performance over existing transfer-learning
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divvala_S/0/1/0/all/0/1&quot;&gt;Santosh Divvala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10965">
<title>Ontology-Grounded Topic Modeling for Climate Science Research. (arXiv:1807.10965v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10965</link>
<description rdf:parseType="Literal">&lt;p&gt;In scientific disciplines where research findings have a strong impact on
society, reducing the amount of time it takes to understand, synthesize and
exploit the research is invaluable. Topic modeling is an effective technique
for summarizing a collection of documents to find the main themes among them
and to classify other documents that have a similar mixture of co-occurring
words. We show how grounding a topic model with an ontology, extracted from a
glossary of important domain phrases, improves the topics generated and makes
them easier to understand. We apply and evaluate this method to the climate
science domain. The result improves the topics generated and supports faster
research understanding, discovery of social networks among researchers, and
automatic ontology generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sleeman_J/0/1/0/all/0/1&quot;&gt;Jennifer Sleeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finin_T/0/1/0/all/0/1&quot;&gt;Tim Finin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halem_M/0/1/0/all/0/1&quot;&gt;Milton Halem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11573">
<title>State-of-the-art and gaps for deep learning on limited training data in remote sensing. (arXiv:1807.11573v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11573</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning usually requires big data, with respect to both volume and
variety. However, most remote sensing applications only have limited training
data, of which a small subset is labeled. Herein, we review three
state-of-the-art approaches in deep learning to combat this challenge. The
first topic is transfer learning, in which some aspects of one domain, e.g.,
features, are transferred to another domain. The next is unsupervised learning,
e.g., autoencoders, which operate on unlabeled data. The last is generative
adversarial networks, which can generate realistic looking data that can fool
the likes of both a deep learning network and human. The aim of this article is
to raise awareness of this dilemma, to direct the reader to existing work and
to highlight current gaps that need solving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_J/0/1/0/all/0/1&quot;&gt;John E. Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Derek T. Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Pan Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11582">
<title>A Hierarchical Approach to Neural Context-Aware Modeling. (arXiv:1807.11582v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.11582</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new recurrent neural network topology to enhance
state-of-the-art machine learning systems by incorporating a broader context.
Our approach overcomes recent limitations with extended narratives through a
multi-layered computational approach to generate an abstract context
representation. Therefore, the developed system captures the narrative on
word-level, sentence-level, and context-level. Through the hierarchical set-up,
our proposed model summarizes the most salient information on each level and
creates an abstract representation of the extended context. We subsequently use
this representation to enhance neural language processing systems on the task
of semantic error detection. To show the potential of the newly introduced
topology, we compare the approach against a context-agnostic set-up including a
standard neural language model and a supervised binary classification network.
The performance measures on the error detection task show the advantage of the
hierarchical context-aware topologies, improving the baseline by 12.75%
relative for unsupervised models and 20.37% relative for supervised models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1&quot;&gt;Patrick Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1&quot;&gt;Jan Niehues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1&quot;&gt;Alex Waibel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11655">
<title>Security and Privacy Issues in Deep Learning. (arXiv:1807.11655v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1807.11655</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of machine learning, expectations for artificial
intelligence (AI) technology are increasing day by day. In particular, deep
learning has shown enriched performance results in a variety of fields. There
are many applications that are closely related to our daily life, such as
making significant decisions in application area based on predictions or
classifications, in which a deep learning (DL) model could be relevant. Hence,
if a DL model causes mispredictions or misclassifications due to malicious
external influences, it can cause very large difficulties in real life.
Moreover, training deep learning models involves relying on an enormous amount
of data and the training data often includes sensitive information. Therefore,
deep learning models should not expose the privacy of such data. In this paper,
we reviewed the threats and developed defense methods on the security of the
models and the data privacy under the notion of SPAI: Secure and Private AI. We
also discuss current challenges and open issues.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1&quot;&gt;Ho Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jaehee Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1&quot;&gt;Dahuin Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1&quot;&gt;Hyemi Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1&quot;&gt;Heonseok Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11682">
<title>Deep Belief Networks Based Feature Generation and Regression for Predicting Wind Power. (arXiv:1807.11682v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11682</link>
<description rdf:parseType="Literal">&lt;p&gt;Wind energy forecasting helps to manage power production, and hence, reduces
energy cost. Deep Neural Networks (DNN) mimics hierarchical learning in the
human brain and thus possesses hierarchical, distributed, and multi-task
learning capabilities. Based on aforementioned characteristics, we report Deep
Belief Network (DBN) based forecast engine for wind power prediction because of
its good generalization and unsupervised pre-training attributes. The proposed
DBN-WP forecast engine, which exhibits stochastic feature generation
capabilities and is composed of multiple Restricted Boltzmann Machines,
generates suitable features for wind power prediction using atmospheric
properties as input. DBN-WP, due to its unsupervised pre-training of RBM layers
and generalization capabilities, is able to learn the fluctuations in the
meteorological properties and thus is able to perform effective mapping of the
wind power. In the deep network, a regression layer is appended at the end to
predict sort-term wind power. It is experimentally shown that the deep learning
and unsupervised pre-training capabilities of DBN based model has comparable
and in some cases better results than hybrid and complex learning techniques
proposed for wind power prediction. The proposed prediction system based on
DBN, achieves mean values of RMSE, MAE and SDE as 0.124, 0.083 and 0.122,
respectively. Statistical analysis of several independent executions of the
proposed DBN-WP wind power prediction system demonstrates the stability of the
system. The proposed DBN-WP architecture is easy to implement and offers
generalization as regards the change in location of the wind farm is concerned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zameer_A/0/1/0/all/0/1&quot;&gt;Aneela Zameer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1&quot;&gt;Tauseef Jamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1&quot;&gt;Ahmad Raza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11698">
<title>Rank and Rate: Multi-task Learning for Recommender Systems. (arXiv:1807.11698v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.11698</link>
<description rdf:parseType="Literal">&lt;p&gt;The two main tasks in the Recommender Systems domain are the ranking and
rating prediction tasks. The rating prediction task aims at predicting to what
extent a user would like any given item, which would enable to recommend the
items with the highest predicted scores. The ranking task on the other hand
directly aims at recommending the most valuable items for the user. Several
previous approaches proposed learning user and item representations to optimize
both tasks simultaneously in a multi-task framework. In this work we propose a
novel multi-task framework that exploits the fact that a user does a two-phase
decision process - first decides to interact with an item (ranking task) and
only afterward to rate it (rating prediction task). We evaluated our framework
on two benchmark datasets, on two different configurations and showed its
superiority over state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadash_G/0/1/0/all/0/1&quot;&gt;Guy Hadash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalom_O/0/1/0/all/0/1&quot;&gt;Oren Sar Shalom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osadchy_R/0/1/0/all/0/1&quot;&gt;Rita Osadchy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05428">
<title>A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music. (arXiv:1803.05428v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05428</link>
<description rdf:parseType="Literal">&lt;p&gt;The Variational Autoencoder (VAE) has proven to be an effective model for
producing semantically meaningful latent representations for natural data.
However, it has thus far seen limited application to sequential data, and, as
we demonstrate, existing recurrent VAE models have difficulty modeling
sequences with long-term structure. To address this issue, we propose the use
of a hierarchical decoder, which first outputs embeddings for subsequences of
the input and then uses these embeddings to generate each subsequence
independently. This structure encourages the model to utilize its latent code,
thereby avoiding the &quot;posterior collapse&quot; problem which remains an issue for
recurrent VAEs. We apply this architecture to modeling sequences of musical
notes and find that it exhibits dramatically better sampling, interpolation,
and reconstruction performance than a &quot;flat&quot; baseline model. An implementation
of our &quot;MusicVAE&quot; is available online at &lt;a href=&quot;http://g.co/magenta/musicvae-code.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1&quot;&gt;Adam Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1&quot;&gt;Jesse Engel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1&quot;&gt;Curtis Hawthorne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1&quot;&gt;Douglas Eck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10222">
<title>Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization. (arXiv:1805.10222v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10222</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest a general oracle-based framework that captures different parallel
stochastic optimization settings described by a dependency graph, and derive
generic lower bounds in terms of this graph. We then use the framework and
derive lower bounds for several specific parallel optimization settings,
including delayed updates and parallel processing with intermittent
communication. We highlight gaps between lower and upper bounds on the oracle
complexity, and cases where the &quot;natural&quot; algorithms are not known to be
optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Woodworth_B/0/1/0/all/0/1&quot;&gt;Blake Woodworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+McMahan_B/0/1/0/all/0/1&quot;&gt;Brendan McMahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Srebro_N/0/1/0/all/0/1&quot;&gt;Nathan Srebro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05975">
<title>Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors. (arXiv:1806.05975v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05975</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Neural Networks (BNNs) have recently received increasing attention
for their ability to provide well-calibrated posterior uncertainties. However,
model selection---even choosing the number of nodes---remains an open question.
Recent work has proposed the use of a horseshoe prior over node pre-activations
of a Bayesian neural network, which effectively turns off nodes that do not
help explain the data. In this work, we propose several modeling and inference
advances that consistently improve the compactness of the model learned while
maintaining predictive performance, especially in smaller-sample settings
including reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiayu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05748">
<title>Learning Stochastic Differential Equations With Gaussian Processes Without Gradient Matching. (arXiv:1807.05748v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05748</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel paradigm for learning non-parametric drift and diffusion
functions for stochastic differential equation (SDE). The proposed model learns
to simulate path distributions that match observations with non-uniform time
increments and arbitrary sparseness, which is in contrast with gradient
matching that does not optimize simulated responses. We formulate sensitivity
equations for learning and demonstrate that our general stochastic distribution
optimisation leads to robust and efficient learning of SDE systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yildiz_C/0/1/0/all/0/1&quot;&gt;Cagatay Yildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Intosalmi_J/0/1/0/all/0/1&quot;&gt;Jukka Intosalmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mannerstrom_H/0/1/0/all/0/1&quot;&gt;Henrik Mannerstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lahdesmaki_H/0/1/0/all/0/1&quot;&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10478">
<title>Interpreting RNN behaviour via excitable network attractors. (arXiv:1807.10478v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10478</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning has become a basic tool in scientific research and for the
development of technologies with significant impact on society. In fact, such
methods allow to discover regularities in data and make predictions without
explicit knowledge of the rules governing the system under analysis. However, a
price must be paid for exploiting such a modeling flexibility: machine learning
methods are usually black-box, meaning that it is difficult to fully understand
what the machine is doing and how. This poses constraints on the applicability
of such methods, neglecting the possibility to gather novel scientific insights
from experimental data. Our research aims to open the black-box of recurrent
neural networks, an important family of neural networks suitable to process
sequential data. Here, we propose a novel methodology that allows to provide a
mechanistic interpretation of their behaviour when used to solve computational
tasks. The methodology is based on mathematical constructs called excitable
network attractors, which are models represented as networks in phase space
composed by stable attractors and excitable connections between them. As the
behaviour of recurrent neural networks depends on training and inputs driving
the autonomous system, we introduce an algorithm to extract network attractors
directly from a trajectory generated by the neural network while solving tasks.
Simulations conducted on a controlled benchmark highlight the relevance of the
proposed methodology for interpreting the behaviour of recurrent neural
networks on tasks that involve learning a finite number of stable states.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceni_A/0/1/0/all/0/1&quot;&gt;Andrea Ceni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashwin_P/0/1/0/all/0/1&quot;&gt;Peter Ashwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Livi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11158">
<title>Robust Student Network Learning. (arXiv:1807.11158v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11158</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks bring in impressive accuracy in various applications,
but the success often relies on the heavy network architecture. Taking
well-trained heavy networks as teachers, classical teacher-student learning
paradigm aims to learn a student network that is lightweight yet accurate. In
this way, a portable student network with significantly fewer parameters can
achieve a considerable accuracy which is comparable to that of teacher network.
However, beyond accuracy, robustness of the learned student network against
perturbation is also essential for practical uses. Existing teacher-student
learning frameworks mainly focus on accuracy and compression ratios, but ignore
the robustness. In this paper, we make the student network produce more
confident predictions with the help of the teacher network, and analyze the
lower bound of the perturbation that will destroy the confidence of the student
network. Two important objectives regarding prediction scores and gradients of
examples are developed to maximize this lower bound, so as to enhance the
robustness of the student network without sacrificing the performance.
Experiments on benchmark datasets demonstrate the efficiency of the proposed
approach to learn robust student networks which have satisfying accuracy and
compact sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shiyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11393">
<title>Making Classifier Chains Resilient to Class Imbalance. (arXiv:1807.11393v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11393</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance is an intrinsic characteristic of multi-label data. Most of
the labels in multi-label data sets are associated with a small number of
training examples, much smaller compared to the size of the data set. Class
imbalance poses a key challenge that plagues most multi-label learning methods.
Ensemble of Classifier Chains (ECC), one of the most prominent multi-label
learning methods, is no exception to this rule, as each of the binary models it
builds is trained from all positive and negative examples of a label. To make
ECC resilient to class imbalance, we first couple it with random undersampling.
We then present two extensions of this basic approach, where we build a varying
number of binary models per label and construct chains of different sizes, in
order to improve the exploitation of majority examples with approximately the
same computational budget. Experimental results on 16 multi-label datasets
demonstrate the effectiveness of the proposed approaches in a variety of
evaluation metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1&quot;&gt;Grigorios Tsoumakas&lt;/a&gt;</dc:creator>
</item></rdf:RDF>