<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08142"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05075"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05088"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05134"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05236"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05407"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11029"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02610"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.05156">
<title>Empirical Explorations in Training Networks with Discrete Activations. (arXiv:1801.05156v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.05156</link>
<description rdf:parseType="Literal">&lt;p&gt;We present extensive experiments training and testing hidden units in deep
networks that emit only a predefined, static, number of discretized values.
These units provide benefits in real-world deployment in systems in which
memory and/or computation may be limited. Additionally, they are particularly
well suited for use in large recurrent network models that require the
maintenance of large amounts of internal state in memory. Surprisingly, we find
that despite reducing the number of values that can be represented in the
output activations from $2^{32}-2^{64}$ to between 64 and 256, there is little
to no degradation in network performance across a variety of different
settings. We investigate simple classification and regression tasks, as well as
memorization and compression problems. We compare the results with more
standard activations, such as tanh and relu. Unlike previous discretization
studies which often concentrate only on binary units, we examine the effects of
varying the number of allowed activation levels. Compared to existing
approaches for discretization, the approach presented here is both conceptually
and programatically simple, has no stochastic component, and allows the
training, testing, and usage phases to be treated in exactly the same manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baluja_S/0/1/0/all/0/1&quot;&gt;Shumeet Baluja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08142">
<title>Learning what to share between loosely related tasks. (arXiv:1705.08142v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08142</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning is motivated by the observation that humans bring to bear
what they know about related problems when solving new ones. Similarly, deep
neural networks can profit from related tasks by sharing parameters with other
networks. However, humans do not consciously decide to transfer knowledge
between tasks. In Natural Language Processing (NLP), it is hard to predict if
sharing will lead to improvements, particularly if tasks are only loosely
related. To overcome this, we introduce Sluice Networks, a general framework
for multi-task learning where trainable parameters control the amount of
sharing. Our framework generalizes previous proposals in enabling sharing of
all combinations of subspaces, layers, and skip connections. We perform
experiments on three task pairs, and across seven different domains, using data
from OntoNotes 5.0, and achieve up to 15% average error reductions over common
approaches to multi-task learning. We show that a) label entropy is predictive
of gains in sluice networks, confirming findings for hard parameter sharing and
b) while sluice networks easily fit noise, they are robust across domains in
practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bingel_J/0/1/0/all/0/1&quot;&gt;Joachim Bingel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Augenstein_I/0/1/0/all/0/1&quot;&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sogaard_A/0/1/0/all/0/1&quot;&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05032">
<title>AliMe Assist: An Intelligent Assistant for Creating an Innovative E-commerce Experience. (arXiv:1801.05032v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.05032</link>
<description rdf:parseType="Literal">&lt;p&gt;We present AliMe Assist, an intelligent assistant designed for creating an
innovative online shopping experience in E-commerce. Based on question
answering (QA), AliMe Assist offers assistance service, customer service, and
chatting service. It is able to take voice and text input, incorporate context
to QA, and support multi-round interaction. Currently, it serves millions of
customer questions per day and is able to address 85% of them. In this paper,
we demonstrate the system, present the underlying techniques, and share our
experience in dealing with real-world QA in the E-commerce field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng-Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1&quot;&gt;Minghui Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haiqing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Juwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongzhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weipeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1&quot;&gt;Guwei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wei Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05075">
<title>A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning. (arXiv:1801.05075v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1801.05075</link>
<description rdf:parseType="Literal">&lt;p&gt;In order for people to be able to trust and take advantage of the results of
advanced machine learning and artificial intelligence solutions for real
decision making, people need to be able to understand the machine rationale for
given output. Research in explain artificial intelligence (XAI) addresses the
aim, but there is a need for evaluation of human relevance and
understandability of explanations. Our work contributes a novel methodology for
evaluating the quality or human interpretability of explanations for machine
learning models. We present an evaluation benchmark for instance explanations
from text and image classifiers. The explanation meta-data in this benchmark is
generated from user annotations of image and text samples. We describe the
benchmark and demonstrate its utility by a quantitative evaluation on
explanations generated from a recent machine learning algorithm. This research
demonstrates how human-grounded evaluation could be used as a measure to
qualify local machine-learning explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohseni_S/0/1/0/all/0/1&quot;&gt;Sina Mohseni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragan_E/0/1/0/all/0/1&quot;&gt;Eric D. Ragan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05088">
<title>Real-time Road Traffic Information Detection Through Social Media. (arXiv:1801.05088v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.05088</link>
<description rdf:parseType="Literal">&lt;p&gt;In current study, a mechanism to extract traffic related information such as
congestion and incidents from textual data from the internet is proposed. The
current source of data is Twitter. As the data being considered is extremely
large in size automated models are developed to stream, download, and mine the
data in real-time. Furthermore, if any tweet has traffic related information
then the models should be able to infer and extract this data.
&lt;/p&gt;
&lt;p&gt;Currently, the data is collected only for United States and a total of
120,000 geo-tagged traffic related tweets are extracted, while six million
geo-tagged non-traffic related tweets are retrieved and classification models
are trained. Furthermore, this data is used for various kinds of spatial and
temporal analysis. A mechanism to calculate level of traffic congestion,
safety, and traffic perception for cities in U.S. is proposed. Traffic
congestion and safety rankings for the various urban areas are obtained and
then they are statistically validated with existing widely adopted rankings.
Traffic perception depicts the attitude and perception of people towards the
traffic.
&lt;/p&gt;
&lt;p&gt;It is also seen that traffic related data when visualized spatially and
temporally provides the same pattern as the actual traffic flows for various
urban areas. When visualized at the city level, it is clearly visible that the
flow of tweets is similar to flow of vehicles and that the traffic related
tweets are representative of traffic within the cities. With all the findings
in current study, it is shown that significant amount of traffic related
information can be extracted from Twitter and other sources on internet.
Furthermore, Twitter and these data sources are freely available and are not
bound by spatial and temporal limitations. That is, wherever there is a user
there is a potential for data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_C/0/1/0/all/0/1&quot;&gt;Chandra Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_M/0/1/0/all/0/1&quot;&gt;Michael Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujimoto_R/0/1/0/all/0/1&quot;&gt;Richard Fujimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_K/0/1/0/all/0/1&quot;&gt;Kari Watkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05159">
<title>GitGraph - Architecture Search Space Creation through Frequent Computational Subgraph Mining. (arXiv:1801.05159v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.05159</link>
<description rdf:parseType="Literal">&lt;p&gt;The dramatic success of deep neural networks across multiple application
areas often relies on experts painstakingly designing a network architecture
specific to each task. To simplify this process and make it more accessible, an
emerging research effort seeks to automate the design of neural network
architectures, using e.g. evolutionary algorithms or reinforcement learning or
simple search in a constrained space of neural modules.
&lt;/p&gt;
&lt;p&gt;Considering the typical size of the search space (e.g. $10^{10}$ candidates
for a $10$-layer network) and the cost of evaluating a single candidate,
current architecture search methods are very restricted. They either rely on
static pre-built modules to be recombined for the task at hand, or they define
a static hand-crafted framework within which they can generate new
architectures from the simplest possible operations.
&lt;/p&gt;
&lt;p&gt;In this paper, we relax these restrictions, by capitalizing on the collective
wisdom contained in the plethora of neural networks published in online code
repositories. Concretely, we (a) extract and publish GitGraph, a corpus of
neural architectures and their descriptions; (b) we create problem-specific
neural architecture search spaces, implemented as a textual search mechanism
over GitGraph; (c) we propose a method of identifying unique common subgraphs
within the architectures solving each problem (e.g., image processing,
reinforcement learning), that can then serve as modules in the newly created
problem specific neural search space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennani_Smires_K/0/1/0/all/0/1&quot;&gt;Kamil Bennani-Smires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1&quot;&gt;Claudiu Musat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossmann_A/0/1/0/all/0/1&quot;&gt;Andreea Hossmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baeriswyl_M/0/1/0/all/0/1&quot;&gt;Michael Baeriswyl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05387">
<title>StressedNets: Efficient Feature Representations via Stress-induced Evolutionary Synthesis of Deep Neural Networks. (arXiv:1801.05387v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.05387</link>
<description rdf:parseType="Literal">&lt;p&gt;The computational complexity of leveraging deep neural networks for
extracting deep feature representations is a significant barrier to its
widespread adoption, particularly for use in embedded devices. One particularly
promising strategy to addressing the complexity issue is the notion of
evolutionary synthesis of deep neural networks, which was demonstrated to
successfully produce highly efficient deep neural networks while retaining
modeling performance. Here, we further extend upon the evolutionary synthesis
strategy for achieving efficient feature extraction via the introduction of a
stress-induced evolutionary synthesis framework, where stress signals are
imposed upon the synapses of a deep neural network during training to induce
stress and steer the synthesis process towards the production of more efficient
deep neural networks over successive generations and improved model fidelity at
a greater efficiency. The proposed stress-induced evolutionary synthesis
approach is evaluated on a variety of different deep neural network
architectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object
classification and object detection) to synthesize efficient StressedNets over
multiple generations. Experimental results demonstrate the efficacy of the
proposed framework to synthesize StressedNets with significant improvement in
network architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and
speed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra
X1 mobile processor).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chwyl_B/0/1/0/all/0/1&quot;&gt;Brendan Chwyl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Francis Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rongyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1&quot;&gt;Michelle Karg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1&quot;&gt;Christian Scharfenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05062">
<title>Multi-Label Learning from Medical Plain Text with Convolutional Residual Models. (arXiv:1801.05062v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.05062</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting diagnoses from Electronic Health Records (EHRs) is an important
medical application of multi-label learning. We propose a convolutional
residual model for multi-label classification from doctor notes in EHR data. A
given patient may have multiple diagnoses, and therefore multi-label learning
is required. We employ a Convolutional Neural Network (CNN) to encode plain
text into a fixed-length sentence embedding vector. Since diagnoses are
typically correlated, a deep residual network is employed on top of the CNN
encoder, to capture label (diagnosis) dependencies and incorporate information
directly from the encoded sentence vector. A real EHR dataset is considered,
and we compare the proposed model with several well-known baselines, to predict
diagnoses based on doctor notes. Experimental results demonstrate the
superiority of the proposed convolutional residual model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Henao_R/0/1/0/all/0/1&quot;&gt;Ricardo Henao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gan_Z/0/1/0/all/0/1&quot;&gt;Zhe Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yitong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05134">
<title>Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift. (arXiv:1801.05134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.05134</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper first answers the question &quot;why do the two most powerful
techniques Dropout and Batch Normalization (BN) often lead to a worse
performance when they are combined together?&quot; in both theoretical and
statistical aspects. Theoretically, we find that Dropout would shift the
variance of a specific neural unit when we transfer the state of that network
from train to test. However, BN would maintain its statistical variance, which
is accumulated from the entire learning procedure, in the test phase. The
inconsistency of that variance (we name this scheme as &quot;variance shift&quot;) causes
the unstable numerical behavior in inference that leads to more erroneous
predictions finally, when applying Dropout before BN. Thorough experiments on
DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to
the uncovered mechanism, we next explore several strategies that modifies
Dropout and try to overcome the limitations of their combination by avoiding
the variance shift risks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05236">
<title>MORF: A Framework for MOOC Predictive Modeling and Replication At Scale. (arXiv:1801.05236v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1801.05236</link>
<description rdf:parseType="Literal">&lt;p&gt;The MOOC Replication Framework (MORF) is a novel software system for feature
extraction, model training/testing, and evaluation of predictive dropout models
in Massive Open Online Courses (MOOCs). MORF makes large-scale replication of
complex machine-learned models tractable and accessible for researchers, and
enables public research on privacy-protected data. It does so by focusing on
the high-level operations of an \emph{extract-train-test-evaluate} workflow,
and enables researchers to encapsulate their implementations in portable, fully
reproducible software containers which are executed on data with a known
schema. MORF&apos;s workflow allows researchers to use data in analysis without
providing them access to the underlying data directly, preserving privacy and
data security. During execution, containers are sandboxed for security and data
leakage and parallelized for efficiency, allowing researchers to create and
test new models rapidly, on large-scale multi-institutional datasets that were
previously inaccessible to most researchers. MORF is provided both as a Python
API (the MORF Software), for institutions to use on their own MOOC data) or in
a platform-as-a-service (PaaS) model with a web API and a high-performance
computing environment (the MORF Platform).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_C/0/1/0/all/0/1&quot;&gt;Christopher Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andres_J/0/1/0/all/0/1&quot;&gt;Juan Miguel L. Andres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1&quot;&gt;Ryan Baker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05407">
<title>Deep Canonically Correlated LSTMs. (arXiv:1801.05407v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.05407</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear
transformations of variable length sequences and embed them into a correlated,
fixed dimensional space. We use LSTMs to transform multi-view time-series data
non-linearly while learning temporal relationships within the data. We then
perform correlation analysis on the outputs of these neural networks to find a
correlated subspace through which we get our final representation via
projection. This work follows from previous work done on Deep Canonical
Correlation (DCCA), in which deep feed-forward neural networks were used to
learn nonlinear transformations of data while maximizing correlation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mallinar_N/0/1/0/all/0/1&quot;&gt;Neil Mallinar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosset_C/0/1/0/all/0/1&quot;&gt;Corbin Rosset&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11029">
<title>Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. (arXiv:1710.11029v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11029</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient descent (SGD) is widely believed to perform implicit
regularization when used to train deep neural networks, but the precise manner
in which this occurs has thus far been elusive. We prove that SGD minimizes an
average potential over the posterior distribution of weights along with an
entropic regularization term. This potential is however not the original loss
function in general. So SGD does perform variational inference, but for a
different loss than the one used to compute the gradients. Even more
surprisingly, SGD does not even converge in the classical sense: we show that
the most likely trajectories of SGD for deep networks do not behave like
Brownian motion around critical points. Instead, they resemble closed loops
with deterministic components. We prove that such &quot;out-of-equilibrium&quot; behavior
is a consequence of highly non-isotropic gradient noise in SGD; the covariance
matrix of mini-batch gradients for deep networks has a rank as small as 1% of
its dimension. We provide extensive empirical validation of these claims,
proven in the appendix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08277">
<title>Unleashing the Potential of CNNs for Interpretable Few-Shot Learning. (arXiv:1711.08277v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08277</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have been generally acknowledged as one
of the driving forces for the advancement of computer vision. Despite their
promising performances on many tasks, CNNs still face major obstacles on the
road to achieving ideal machine intelligence. One is that CNNs are complex and
hard to interpret. Another is that standard CNNs require large amounts of
annotated data, which is sometimes very hard to obtain, and it is desirable to
be able to learn them from few examples. In this work, we address these
limitations of CNNs by developing novel, simple, and interpretable models for
few-shot learning. Our models are based on the idea of encoding objects in
terms of visual concepts, which are interpretable visual cues represented by
the feature vectors within CNNs. We first adapt the learning of visual concepts
to the few-shot setting and then uncover two key properties of feature encoding
using visual concepts, which we call category sensitivity and spatial pattern.
Motivated by these properties, we present two intuitive models for the problem
of few-shot learning. Experiments show that our models achieve competitive
performances, while being much more flexible and interpretable than alternative
state-of-the-art few-shot learning methods. We conclude that using visual
concepts helps expose the natural capability of CNNs for few-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Boyang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02610">
<title>Generating Adversarial Examples with Adversarial Networks. (arXiv:1801.02610v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02610</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been found to be vulnerable to adversarial
examples resulting from adding small-magnitude perturbations to inputs. Such
adversarial examples can mislead DNNs to produce adversary-selected results.
Different attack strategies have been proposed to generate adversarial
examples, but how to produce them with high perceptual quality and more
efficiently requires more research efforts. In this paper, we propose AdvGAN to
generate adversarial examples with generative adversarial networks (GANs),
which can learn and approximate the distribution of original instances. For
AdvGAN, once the generator is trained, it can generate adversarial
perturbations efficiently for any instance, so as to potentially accelerate
adversarial training as defenses. We apply AdvGAN in both semi-whitebox and
black-box attack settings. In semi-whitebox attacks, there is no need to access
the original target model after the generator is trained, in contrast to
traditional white-box attacks. In black-box attacks, we dynamically train a
distilled model for the black-box model and optimize the generator accordingly.
Adversarial examples generated by AdvGAN on different target models have high
attack success rate under state-of-the-art defenses compared to other attacks.
Our attack has placed the first with 92.76% accuracy on a public MNIST
black-box attack challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Warren He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item></rdf:RDF>