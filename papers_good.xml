<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03645"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03796"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.07655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.02302"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.08694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03361"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03417"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03455"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03671"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03793"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03960"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1503.04333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07121"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04676"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04325"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01261"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03720"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03723"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03903"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03925"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03945"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04000"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.03581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10140"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09039"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01466"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01660"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.03645">
<title>Deep Curiosity Loops in Social Environments. (arXiv:1806.03645v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.03645</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by infants&apos; intrinsic motivation to learn, which values informative
sensory channels contingent on their immediate social environment, we developed
a deep curiosity loop (DCL) architecture. The DCL is composed of a learner,
which attempts to learn a forward model of the agent&apos;s state-action transition,
and a novel reinforcement-learning (RL) component, namely, an
Action-Convolution Deep Q-Network, which uses the learner&apos;s prediction error as
reward. The environment for our agent is composed of visual social scenes,
composed of sitcom video streams, thereby both the learner and the RL are
constructed as deep convolutional neural networks. The agent&apos;s learner learns
to predict the zero-th order of the dynamics of visual scenes, resulting in
intrinsic rewards proportional to changes within its social environment. The
sources of these socially informative changes within the sitcom are
predominantly motions of faces and hands, leading to the unsupervised
curiosity-based learning of social interaction features. The face and hand
detection is represented by the value function and the social interaction
optical-flow is represented by the policy. Our results suggest that face and
hand detection are emergent properties of curiosity-based learning embedded in
social environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barkan_J/0/1/0/all/0/1&quot;&gt;Jonatan Barkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1&quot;&gt;Goren Gordon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03751">
<title>State Space Representations of Deep Neural Networks. (arXiv:1806.03751v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.03751</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with neural networks as dynamical systems governed by
differential or difference equations. It shows that the introduction of skip
connections into network architectures, such as residual networks and dense
networks, turns a system of static equations into a system of dynamical
equations with varying levels of smoothness on the layer-wise transformations.
Closed form solutions for the state space representations of general dense
networks, as well as $k^{th}$ order smooth networks, are found in general
settings. Furthermore, it is shown that imposing $k^{th}$ order smoothness on a
network architecture with $d$-many nodes per layer increases the state space
dimension by a multiple of $k$, and so the effective embedding dimension of the
data manifold is $k \cdot d$-many dimensions. It follows that network
architectures of these types reduce the number of parameters needed to maintain
the same embedding dimension by a factor of $k^2$ when compared to an
equivalent first-order, residual network, significantly motivating the
development of network architectures of these types. Numerical simulations were
run to validate parts of the developed theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauser_M/0/1/0/all/0/1&quot;&gt;Michael Hauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunn_S/0/1/0/all/0/1&quot;&gt;Sean Gunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saab_S/0/1/0/all/0/1&quot;&gt;Samer Saab Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Asok Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03796">
<title>Generative Adversarial Network Architectures For Image Synthesis Using Capsule Networks. (arXiv:1806.03796v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.03796</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose Generative Adversarial Network (GAN) architectures
using dynamically-routed Capsule Networks for conditional and random
image-synthesis. The architectures benefit from the fact that Capsule Networks
encode the properties and spatial relationships of the features in the images.
Capsule Networks being a more effective critic provide multiple benefits when
replacing Convolutional Neural Network discriminators being used in the current
work-horses for image synthesis - DCGANs. Our architectures use a loss
analogous to Wasserstein loss and demonstrate that they can encode a dataset&apos;s
representation faster than currently existing GANs resulting in lesser number
of training samples required. Our experiments show that the generator is pushed
to give better and more diverse results in significantly lesser number of
epochs without over-fitting. We have used MNIST, Fashion-MNIST and their
variants for demonstrating the results achieved from this architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_Y/0/1/0/all/0/1&quot;&gt;Yash Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrater_P/0/1/0/all/0/1&quot;&gt;Paul Schrater&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03934">
<title>When and where do feed-forward neural networks learn localist representations?. (arXiv:1806.03934v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.03934</link>
<description rdf:parseType="Literal">&lt;p&gt;According to parallel distributed processing (PDP) theory in psychology,
neural networks (NN) learn distributed rather than interpretable localist
representations. This view has been held so strongly that few researchers have
analysed single units to determine if this assumption is correct. However,
recent results from psychology, neuroscience and computer science have shown
the occasional existence of local codes emerging in artificial and biological
neural networks. In this paper, we undertake the first systematic survey of
when local codes emerge in a feed-forward neural network, using generated input
and output data with known qualities. We find that the number of local codes
that emerge from a NN follows a well-defined distribution across the number of
hidden layer neurons, with a peak determined by the size of input data, number
of examples presented and the sparsity of input data. Using a 1-hot output code
drastically decreases the number of local codes on the hidden layer. The number
of emergent local codes increases with the percentage of dropout applied to the
hidden layer, suggesting that the localist encoding may offer a resilience to
noisy networks. This data suggests that localist coding can emerge from
feed-forward PDP networks and suggests some of the conditions that may lead to
interpretable localist representations in the cortex. The findings highlight
how local codes should not be dismissed out of hand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gale_E/0/1/0/all/0/1&quot;&gt;Ella M. Gale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_N/0/1/0/all/0/1&quot;&gt;Nicolas Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1&quot;&gt;Jeffrey S. Bowers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.07655">
<title>ASP: Learning to Forget with Adaptive Synaptic Plasticity in Spiking Neural Networks. (arXiv:1703.07655v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1703.07655</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental feature of learning in animals is the &quot;ability to forget&quot; that
allows an organism to perceive, model and make decisions from disparate streams
of information and adapt to changing environments. Against this backdrop, we
present a novel unsupervised learning mechanism ASP (Adaptive Synaptic
Plasticity) for improved recognition with Spiking Neural Networks (SNNs) for
real time on-line learning in a dynamic environment. We incorporate an adaptive
weight decay mechanism with the traditional Spike Timing Dependent Plasticity
(STDP) learning to model adaptivity in SNNs. The leak rate of the synaptic
weights is modulated based on the temporal correlation between the spiking
patterns of the pre- and post-synaptic neurons. This mechanism helps in gradual
forgetting of insignificant data while retaining significant, yet old,
information. ASP, thus, maintains a balance between forgetting and immediate
learning to construct a stable-plastic self-adaptive SNN for continuously
changing inputs. We demonstrate that the proposed learning methodology
addresses catastrophic forgetting while yielding significantly improved
accuracy over the conventional STDP learning method for digit recognition
applications. Additionally, we observe that the proposed learning model
automatically encodes selective attention towards relevant features in the
input data while eliminating the influence of background noise (or denoising)
further improving the robustness of the ASP learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1&quot;&gt;Priyadarshini Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allred_J/0/1/0/all/0/1&quot;&gt;Jason M. Allred&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanathan_S/0/1/0/all/0/1&quot;&gt;Shriram Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.02302">
<title>Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions. (arXiv:1705.02302v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.02302</link>
<description rdf:parseType="Literal">&lt;p&gt;The driving force behind convolutional networks - the most successful deep
learning architecture to date, is their expressive power. Despite its wide
acceptance and vast empirical evidence, formal analyses supporting this belief
are scarce. The primary notions for formally reasoning about expressiveness are
efficiency and inductive bias. Expressive efficiency refers to the ability of a
network architecture to realize functions that require an alternative
architecture to be much larger. Inductive bias refers to the prioritization of
some functions over others given prior knowledge regarding a task at hand. In
this paper we overview a series of works written by the authors, that through
an equivalence to hierarchical tensor decompositions, analyze the expressive
efficiency and inductive bias of various convolutional network architectural
features (depth, width, strides and more). The results presented shed light on
the demonstrated effectiveness of convolutional networks, and in addition,
provide new tools for network design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1&quot;&gt;Nadav Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1&quot;&gt;Or Sharir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1&quot;&gt;Yoav Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamari_R/0/1/0/all/0/1&quot;&gt;Ronen Tamari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakira_D/0/1/0/all/0/1&quot;&gt;David Yakira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1&quot;&gt;Amnon Shashua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.08694">
<title>Natasha 2: Faster Non-Convex Optimization Than SGD. (arXiv:1708.08694v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1708.08694</link>
<description rdf:parseType="Literal">&lt;p&gt;We design a stochastic algorithm to train any smooth neural network to
$\varepsilon$-approximate local minima, using $O(\varepsilon^{-3.25})$
backpropagations. The best result was essentially $O(\varepsilon^{-4})$ by SGD.
&lt;/p&gt;
&lt;p&gt;More broadly, it finds $\varepsilon$-approximate local minima of any smooth
nonconvex function in rate $O(\varepsilon^{-3.25})$, with only oracle access to
stochastic gradients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Allen_Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Allen-Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02114">
<title>Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02114</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the complexity of deep neural networks (DNN) that represent
piecewise linear (PWL) functions. In particular, we study the number of linear
regions, i.e. pieces, that a PWL function represented by a DNN can attain, both
theoretically and empirically. We present (i) tighter upper and lower bounds
for the maximum number of linear regions on rectifier networks, which are exact
for inputs of dimension one; (ii) a first upper bound for multi-layer maxout
networks; and (iii) a first method to perform exact enumeration or counting of
the number of regions by modeling the DNN with a mixed-integer linear
formulation. These bounds come from leveraging the dimension of the space
defining each linear region. The results also indicate that a deep rectifier
network can only have more linear regions than every shallow counterpart with
same number of neurons if that number exceeds the dimension of the input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_T/0/1/0/all/0/1&quot;&gt;Thiago Serra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjandraatmadja_C/0/1/0/all/0/1&quot;&gt;Christian Tjandraatmadja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1&quot;&gt;Srikumar Ramalingam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04924">
<title>Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks. (arXiv:1802.04924v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04924</link>
<description rdf:parseType="Literal">&lt;p&gt;The past few years have witnessed growth in the computational requirements
for training deep convolutional neural networks. Current approaches parallelize
training onto multiple devices by applying a single parallelization strategy
(e.g., data or model parallelism) to all layers in a network. Although easy to
reason about, these approaches result in suboptimal runtime performance in
large-scale distributed training, since different layers in a network may
prefer different parallelization strategies. In this paper, we propose
layer-wise parallelism that allows each layer in a network to use an individual
parallelization strategy. We jointly optimize how each layer is parallelized by
solving a graph search problem. Our evaluation shows that layer-wise
parallelism outperforms state-of-the-art approaches by increasing training
throughput, reducing communication costs, achieving better scalability to
multiple GPUs, while maintaining original network accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Zhihao Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Sina Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1&quot;&gt;Charles R. Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aiken_A/0/1/0/all/0/1&quot;&gt;Alex Aiken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03361">
<title>A Content-Based Late Fusion Approach Applied to Pedestrian Detection. (arXiv:1806.03361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.03361</link>
<description rdf:parseType="Literal">&lt;p&gt;The variety of pedestrians detectors proposed in recent years has encouraged
some works to fuse pedestrian detectors to achieve a more accurate detection.
The intuition behind is to combine the detectors based on its spatial
consensus. We propose a novel method called Content-Based Spatial Consensus
(CSBC), which, in addition to relying on spatial consensus, considers the
content of the detection windows to learn a weighted-fusion of pedestrian
detectors. The result is a reduction in false alarms and an enhancement in the
detection. In this work, we also demonstrate that there is small influence of
the feature used to learn the contents of the windows of each detector, which
enables our method to be efficient even employing simple features. The CSBC
overcomes state-of-the-art fusion methods in the ETH dataset and in the Caltech
dataset. Particularly, our method is more efficient since fewer detectors are
necessary to achieve expressive results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sena_J/0/1/0/all/0/1&quot;&gt;Jessica Sena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordao_A/0/1/0/all/0/1&quot;&gt;Artur Jordao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1&quot;&gt;William Robson Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03417">
<title>Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry. (arXiv:1806.03417v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.03417</link>
<description rdf:parseType="Literal">&lt;p&gt;We are concerned with the discovery of hierarchical relationships from
large-scale unstructured similarity scores. For this purpose, we study
different models of hyperbolic space and find that learning embeddings in the
Lorentz model is substantially more efficient than in the Poincar\&apos;e-ball
model. We show that the proposed approach allows us to learn high-quality
embeddings of large taxonomies which yield improvements over Poincar\&apos;e
embeddings, especially in low dimensions. Lastly, we apply our model to
discover hierarchies in two real-world datasets: we show that an embedding in
hyperbolic space can reveal important aspects of a company&apos;s organizational
structure as well as reveal historical relationships between language families.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nickel_M/0/1/0/all/0/1&quot;&gt;Maximilian Nickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1&quot;&gt;Douwe Kiela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03455">
<title>A Preliminary Exploration of Floating Point Grammatical Evolution. (arXiv:1806.03455v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.03455</link>
<description rdf:parseType="Literal">&lt;p&gt;Current GP frameworks are highly effective on a range of real and simulated
benchmarks. However, due to the high dimensionality of the genotypes for GP,
the task of visualising the fitness landscape for GP search can be difficult.
This paper describes a new framework: Floating Point Grammatical Evolution
(FP-GE) which uses a single floating point genotype to encode an individual
program. This encoding permits easier visualisation of the fitness landscape
arbitrary problems by providing a way to map fitness against a single
dimension. The new framework also makes it trivially easy to apply continuous
search algorithms, such as Differential Evolution, to the search problem. In
this work, the FP-GE framework is tested against several regression problems,
visualising the search landscape for these and comparing different search
meta-heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_B/0/1/0/all/0/1&quot;&gt;Brad Alexander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03536">
<title>Representation Learning on Graphs with Jumping Knowledge Networks. (arXiv:1806.03536v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03536</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent deep learning approaches for representation learning on graphs follow
a neighborhood aggregation procedure. We analyze some important properties of
these models, and propose a strategy to overcome those. In particular, the
range of &quot;neighboring&quot; nodes that a node&apos;s representation draws from strongly
depends on the graph structure, analogous to the spread of a random walk. To
adapt to local neighborhood properties and tasks, we explore an architecture --
jumping knowledge (JK) networks -- that flexibly leverages, for each node,
different neighborhood ranges to enable better structure-aware representation.
In a number of experiments on social, bioinformatics and citation networks, we
demonstrate that our model achieves state-of-the-art performance. Furthermore,
combining the JK framework with models like Graph Convolutional Networks,
GraphSAGE and Graph Attention Networks consistently improves those models&apos;
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Keyulu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengtao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonglong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonobe_T/0/1/0/all/0/1&quot;&gt;Tomohiro Sonobe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawarabayashi_K/0/1/0/all/0/1&quot;&gt;Ken-ichi Kawarabayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03561">
<title>What Knowledge is Needed to Solve the RTE5 Textual Entailment Challenge?. (arXiv:1806.03561v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.03561</link>
<description rdf:parseType="Literal">&lt;p&gt;This document gives a knowledge-oriented analysis of about 20 interesting
Recognizing Textual Entailment (RTE) examples, drawn from the 2005 RTE5
competition test set. The analysis ignores shallow statistical matching
techniques between T and H, and rather asks: What would it take to reasonably
infer that T implies H? What world knowledge would be needed for this task?
Although such knowledge-intensive techniques have not had much success in RTE
evaluations, ultimately an intelligent system should be expected to know and
deploy this kind of world knowledge required to perform this kind of reasoning.
&lt;/p&gt;
&lt;p&gt;The selected examples are typically ones which our RTE system (called BLUE)
got wrong and ones which require world knowledge to answer. In particular, the
analysis covers cases where there was near-perfect lexical overlap between T
and H, yet the entailment was NO, i.e., examples that most likely all current
RTE systems will have got wrong. A nice example is #341 (page 26), that
requires inferring from &quot;a river floods&quot; that &quot;a river overflows its banks&quot;.
Seems it should be easy, right? Enjoy!
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03568">
<title>Explainable Recommendation via Multi-Task Learning in Opinionated Text Data. (arXiv:1806.03568v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.03568</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining automatically generated recommendations allows users to make more
informed and accurate decisions about which results to utilize, and therefore
improves their satisfaction. In this work, we develop a multi-task learning
solution for explainable recommendation. Two companion learning tasks of user
preference modeling for recommendation} and \textit{opinionated content
modeling for explanation are integrated via a joint tensor factorization. As a
result, the algorithm predicts not only a user&apos;s preference over a list of
items, i.e., recommendation, but also how the user would appreciate a
particular item at the feature level, i.e., opinionated textual explanation.
Extensive experiments on two large collections of Amazon and Yelp reviews
confirmed the effectiveness of our solution in both recommendation and
explanation tasks, compared with several existing recommendation algorithms.
And our extensive user study clearly demonstrates the practical value of the
explainable recommendations generated by our algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yiling Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yue Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03671">
<title>The Impact of Humanoid Affect Expression on Human Behavior in a Game-Theoretic Setting. (arXiv:1806.03671v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1806.03671</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of robot and other intelligent and autonomous
agents, how a human could be influenced by a robot&apos;s expressed mood when making
decisions becomes a crucial question in human-robot interaction. In this pilot
study, we investigate (1) in what way a robot can express a certain mood to
influence a human&apos;s decision making behavioral model; (2) how and to what
extent the human will be influenced in a game theoretic setting. More
specifically, we create an NLP model to generate sentences that adhere to a
specific affective expression profile. We use these sentences for a humanoid
robot as it plays a Stackelberg security game against a human. We investigate
the behavioral model of the human player.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1&quot;&gt;Aaron M. Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1&quot;&gt;Umang Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amin_T/0/1/0/all/0/1&quot;&gt;Tamara Amin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doryab_A/0/1/0/all/0/1&quot;&gt;Afsaneh Doryab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1&quot;&gt;Manuela Veloso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03793">
<title>Context-Aware Policy Reuse. (arXiv:1806.03793v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.03793</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning can greatly speed up reinforcement learning for a new task
by leveraging policies of relevant tasks.
&lt;/p&gt;
&lt;p&gt;Existing works of policy reuse either focus on only selecting a single best
source policy for transfer without considering contexts, or cannot guarantee to
learn an optimal policy for a target task.
&lt;/p&gt;
&lt;p&gt;To improve transfer efficiency and guarantee optimality, we develop a novel
policy reuse method, called {\em Context-Aware Policy reuSe} (CAPS), that
enables multi-policy transfer. Our method learns when and which source policy
is best for reuse, as well as when to terminate its reuse. CAPS provides
theoretical guarantees in convergence and optimality for both source policy
selection and target task learning. Empirical results on a grid-based
navigation domain and the Pygame Learning Environment demonstrate that CAPS
significantly outperforms other state-of-the-art policy reuse methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1&quot;&gt;Fangda Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guangxiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03806">
<title>Greybox fuzzing as a contextual bandits problem. (arXiv:1806.03806v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.03806</link>
<description rdf:parseType="Literal">&lt;p&gt;Greybox fuzzing is one of the most useful and effective techniques for the
bug detection in large scale application programs. It uses minimal amount of
instrumentation. American Fuzzy Lop (AFL) is a popular coverage based
evolutionary greybox fuzzing tool. AFL performs extremely well in fuzz testing
large applications and finding critical vulnerabilities, but AFL involves a lot
of heuristics while deciding the favored test case(s), skipping test cases
during fuzzing, assigning fuzzing iterations to test case(s). In this work, we
aim at replacing the heuristics the AFL uses while assigning the fuzzing
iterations to a test case during the random fuzzing. We formalize this problem
as a `contextual bandit problem&apos; and we propose an algorithm to solve this
problem. We have implemented our approach on top of the AFL. We modify the
AFL&apos;s heuristics with our learned model through the policy gradient method. Our
learning algorithm selects the multiplier of the number of fuzzing iterations
to be assigned to a test case during random fuzzing, given a fixed length
substring of the test case to be fuzzed. We fuzz the substring with this new
energy value and continuously updates the policy based upon the interesting
test cases it produces on fuzzing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_K/0/1/0/all/0/1&quot;&gt;Ketan Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1&quot;&gt;Aditya Kanade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03960">
<title>AGIL: Learning Attention from Human for Visuomotor Tasks. (arXiv:1806.03960v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.03960</link>
<description rdf:parseType="Literal">&lt;p&gt;When intelligent agents learn visuomotor behaviors from human demonstrations,
they may benefit from knowing where the human is allocating visual attention,
which can be inferred from their gaze. A wealth of information regarding
intelligent decision making is conveyed by human gaze allocation; hence,
exploiting such information has the potential to improve the agents&apos;
performance. With this motivation, we propose the AGIL (Attention Guided
Imitation Learning) framework. We collect high-quality human action and gaze
data while playing Atari games in a carefully controlled experimental setting.
Using these data, we first train a deep neural network that can predict human
gaze positions and visual attention with high accuracy (the gaze network) and
then train another network to predict human actions (the policy network).
Incorporating the learned attention model from the gaze network into the policy
network significantly improves the action prediction accuracy and task
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuode Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whritner_J/0/1/0/all/0/1&quot;&gt;Jake A. Whritner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Karl S. Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayhoe_M/0/1/0/all/0/1&quot;&gt;Mary M. Hayhoe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballard_D/0/1/0/all/0/1&quot;&gt;Dana H. Ballard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1503.04333">
<title>A More Human Way to Play Computer Chess. (arXiv:1503.04333v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1503.04333</link>
<description rdf:parseType="Literal">&lt;p&gt;The idea of dynamic move chains has been described in a preceding paper [13].
Re-using an earlier piece of search allows the tree to be forward-pruned, which
is known to be dangerous, because that can remove new information which would
only be realised through a more exhaustive search process. This paper has added
to the forward-pruning technique by using &apos;Move Tables&apos; that can act in the
same way as Transposition Tables, but for moves not positions. They use an
efficient memory structure and has put the design into the context of short and
long-term memories. The long-term memory uses a play path with weight
reinforcement, while the short-term memory can be immediately added or removed.
Therefore, &apos;long branches&apos; can play a short path, before returning to a full
search at the resulting leaf nodes. Automatic feature analysis is now central
to the search algorithm, where key squares and related squares can be generated
automatically and used to guide the search process. Also using the analysis, if
a search result is inferior, it can re-insert un-played moves that cover these
key squares, thereby correcting those tactical errors. In particular, a type of
move that the forward-pruning will fail on is recognised and a solution to that
problem is suggested. This process reduces some gains made by forward-pruning
but has created an intelligent framework that could make it more reliable. This
has completed the theory of an earlier paper and resulted in a more human-like
approach to searching for a chess move.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07121">
<title>Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets. (arXiv:1704.07121v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07121</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering (Visual QA) has attracted a lot of attention
lately, seen essentially as a form of (visual) Turing test that artificial
intelligence should strive to achieve. In this paper, we study a crucial
component of this task: how can we design good datasets for the task? We focus
on the design of multiple-choice based datasets where the learner has to select
the right answer from a set of candidate ones including the target (\ie the
correct one) and the decoys (\ie the incorrect ones). Through careful analysis
of the results attained by state-of-the-art learning models and human
annotators on existing datasets, we show that the design of the decoy answers
has a significant impact on how and what the learning models learn from the
datasets. In particular, the resulting learner can ignore the visual
information, the question, or both while still doing well on the task. Inspired
by this, we propose automatic procedures to remedy such design deficiencies. We
apply the procedures to re-construct decoy answers for two popular Visual QA
datasets as well as to create a new Visual QA dataset from the Visual Genome
project, resulting in the largest dataset for this task. Extensive empirical
studies show that the design deficiencies have been alleviated in the remedied
datasets and the performance on them is likely a more faithful indicator of the
difference among learning models. The datasets are released and publicly
available via &lt;a href=&quot;http://www.teds.usc.edu/website_vqa/.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hexiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1&quot;&gt;Fei Sha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04676">
<title>KBLRN : End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features. (arXiv:1709.04676v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04676</link>
<description rdf:parseType="Literal">&lt;p&gt;We present KBLRN, a framework for end-to-end learning of knowledge base
representations from latent, relational, and numerical features. KBLRN
integrates feature types with a novel combination of neural representation
learning and probabilistic product of experts models. To the best of our
knowledge, KBLRN is the first approach that learns representations of knowledge
bases by integrating latent, relational, and numerical features. We show that
instances of KBLRN outperform existing methods on a range of knowledge base
completion tasks. We contribute a novel data sets enriching commonly used
knowledge base completion benchmarks with numerical features. The data sets are
available under a permissive BSD-3 license. We also investigate the impact
numerical features have on the KB completion performance of KBLRN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garcia-Duran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04406">
<title>Which Training Methods for GANs do actually Converge?. (arXiv:1801.04406v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04406</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has shown local convergence of GAN training for absolutely
continuous data and generator distributions. In this paper, we show that the
requirement of absolute continuity is necessary: we describe a simple yet
prototypical counterexample showing that in the more realistic case of
distributions that are not absolutely continuous, unregularized GAN training is
not always convergent. Furthermore, we discuss regularization strategies that
were recently proposed to stabilize GAN training. Our analysis shows that GAN
training with instance noise or zero-centered gradient penalties converges. On
the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number
of discriminator updates per generator update do not always converge to the
equilibrium point. We discuss these results, leading us to a new explanation
for the stability problems of GAN training. Based on our analysis, we extend
our convergence results to more general GANs and prove local convergence for
simplified gradient penalties even if the generator and data distribution lie
on lower dimensional manifolds. We find these penalties to work well in
practice and use them to learn high-resolution generative image models for a
variety of datasets with little hyperparameter tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mescheder_L/0/1/0/all/0/1&quot;&gt;Lars Mescheder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowozin_S/0/1/0/all/0/1&quot;&gt;Sebastian Nowozin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04325">
<title>Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation. (arXiv:1802.04325v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04325</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern reinforcement learning algorithms reach super-human performance on
many board and video games, but they are sample inefficient, i.e. they
typically require significantly more playing experience than humans to reach an
equal performance level. To improve sample efficiency, an agent may build a
model of the environment and use planning methods to update its policy. In this
article we introduce Variational State Tabulation (VaST), which maps an
environment with a high-dimensional state space (e.g. the space of visual
inputs) to an abstract tabular model. Prioritized sweeping with small backups,
a highly efficient planning method, can then be used to update state-action
values. We show how VaST can rapidly learn to maximize reward in tasks like 3D
navigation and efficiently adapt to sudden changes in rewards or transition
probabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corneil_D/0/1/0/all/0/1&quot;&gt;Dane Corneil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1&quot;&gt;Wulfram Gerstner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brea_J/0/1/0/all/0/1&quot;&gt;Johanni Brea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07090">
<title>Intriguing Properties of Learned Representations. (arXiv:1804.07090v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07090</link>
<description rdf:parseType="Literal">&lt;p&gt;A key feature of neural networks, particularly deep convolutional neural
networks, is their ability to &quot;learn&quot; useful representations from data. The
very last layer of a neural network is then simply a linear model trained on
these &quot;learned&quot; representations. Despite their numerous applications in other
tasks such as classification, retrieval, clustering etc., a.k.a. transfer
learning, not much work has been published that investigates the structure of
these representations or indeed whether structure can be imposed on them during
the training process.
&lt;/p&gt;
&lt;p&gt;In this paper, we study the effective dimensionality of the learned
representations by models that have proved highly successful for image
classification. We focus on ResNet-18, ResNet-50 and VGG-19 and observe that
when trained on CIFAR10 or CIFAR100, the learned representations exhibit a
fairly low rank structure. We propose a modification to the training procedure,
which further encourages low rank structure on learned activations.
Empirically, we show that this has implications for robustness to adversarial
examples and compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1&quot;&gt;Amartya Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1&quot;&gt;Varun Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H. S. Torr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06502">
<title>First Experiments with Neural Translation of Informal to Formal Mathematics. (arXiv:1805.06502v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06502</link>
<description rdf:parseType="Literal">&lt;p&gt;We report on our experiments to train deep neural networks that automatically
translate informalized LaTeX-written Mizar texts into the formal Mizar
language. To the best of our knowledge, this is the first time when neural
networks have been adopted in the formalization of mathematics. Using Luong et
al.&apos;s neural machine translation model (NMT), we tested our aligned
informal-formal corpora against various hyperparameters and evaluated their
results. Our experiments show that our best performing model configurations are
able to generate correct Mizar statements on 65.73\% of the inference data,
with the union of all models covering 79.17\%. These results indicate that
formalization through artificial neural network is a promising approach for
automated formalization of mathematics. We present several case studies to
illustrate our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1&quot;&gt;Cezary Kaliszyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1&quot;&gt;Josef Urban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10503">
<title>Deep Learning Topological Invariants of Band Insulators. (arXiv:1805.10503v2 [cond-mat.str-el] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10503</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we design and train deep neural networks to predict topological
invariants for one-dimensional four-band insulators in AIII class whose
topological invariant is the winding number, and two-dimensional two-band
insulators in A class whose topological invariant is the Chern number. Given
Hamiltonians in the momentum space as the input, neural networks can predict
topological invariants for both classes with accuracy close to or higher than
90%, even for Hamiltonians whose invariants are beyond the training data set.
Despite the complexity of the neural network, we find that the output of
certain intermediate hidden layers resembles either the winding angle for
models in AIII class or the solid angle (Berry curvature) for models in A
class, indicating that neural networks essentially capture the mathematical
formula of topological invariants. Our work demonstrates the ability of neural
networks to predict topological invariants for complicated models with local
Hamiltonians as the only input, and offers an example that even a deep neural
network is understandable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sun_N/0/1/0/all/0/1&quot;&gt;Ning Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinmin Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huitao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhai_H/0/1/0/all/0/1&quot;&gt;Hui Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01261">
<title>Relational inductive biases, deep learning, and graph networks. (arXiv:1806.01261v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01261</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) has undergone a renaissance recently, making
major progress in key domains such as vision, language, control, and
decision-making. This has been due, in part, to cheap data and cheap compute
resources, which have fit the natural strengths of deep learning. However, many
defining characteristics of human intelligence, which developed under much
different pressures, remain out of reach for current approaches. In particular,
generalizing beyond one&apos;s experiences--a hallmark of human intelligence from
infancy--remains a formidable challenge for modern AI.
&lt;/p&gt;
&lt;p&gt;The following is part position paper, part review, and part unification. We
argue that combinatorial generalization must be a top priority for AI to
achieve human-like abilities, and that structured representations and
computations are key to realizing this objective. Just as biology uses nature
and nurture cooperatively, we reject the false choice between
&quot;hand-engineering&quot; and &quot;end-to-end&quot; learning, and instead advocate for an
approach which benefits from their complementary strengths. We explore how
using relational inductive biases within deep learning architectures can
facilitate learning about entities, relations, and rules for composing them. We
present a new building block for the AI toolkit with a strong relational
inductive bias--the graph network--which generalizes and extends various
approaches for neural networks that operate on graphs, and provides a
straightforward interface for manipulating structured knowledge and producing
structured behaviors. We discuss how graph networks can support relational
reasoning and combinatorial generalization, laying the foundation for more
sophisticated, interpretable, and flexible patterns of reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1&quot;&gt;Peter W. Battaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamrick_J/0/1/0/all/0/1&quot;&gt;Jessica B. Hamrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bapst_V/0/1/0/all/0/1&quot;&gt;Victor Bapst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1&quot;&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambaldi_V/0/1/0/all/0/1&quot;&gt;Vinicius Zambaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1&quot;&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tacchetti_A/0/1/0/all/0/1&quot;&gt;Andrea Tacchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raposo_D/0/1/0/all/0/1&quot;&gt;David Raposo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1&quot;&gt;Adam Santoro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faulkner_R/0/1/0/all/0/1&quot;&gt;Ryan Faulkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1&quot;&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1&quot;&gt;Francis Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballard_A/0/1/0/all/0/1&quot;&gt;Andrew Ballard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilmer_J/0/1/0/all/0/1&quot;&gt;Justin Gilmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_G/0/1/0/all/0/1&quot;&gt;George Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1&quot;&gt;Ashish Vaswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_K/0/1/0/all/0/1&quot;&gt;Kelsey Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1&quot;&gt;Charles Nash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langston_V/0/1/0/all/0/1&quot;&gt;Victoria Langston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1&quot;&gt;Chris Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1&quot;&gt;Nicolas Heess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierstra_D/0/1/0/all/0/1&quot;&gt;Daan Wierstra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1&quot;&gt;Matt Botvinick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01825">
<title>The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces. (arXiv:1806.01825v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Dyna is an architecture for reinforcement learning agents that interleaves
planning, acting, and learning in an online setting. This architecture aims to
make fuller use of limited experience to achieve better performance with fewer
environmental interactions. Dyna has been well studied in problems with a
tabular representation of states, and has also been extended to some settings
with larger state spaces that require function approximation. However, little
work has studied Dyna in environments with high-dimensional state spaces like
images. In Dyna, the environment model is typically used to generate one-step
transitions from selected start states. We applied one-step Dyna to several
games from the Arcade Learning Environment and found that the model-based
updates offered surprisingly little benefit, even with a perfect model.
However, when the model was used to generate longer trajectories of simulated
experience, performance improved dramatically. This observation also holds when
using a model that is learned from experience; even though the learned model is
flawed, it can still be used to accelerate learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holland_G/0/1/0/all/0/1&quot;&gt;G. Zacharias Holland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talvitie_E/0/1/0/all/0/1&quot;&gt;Erik Talvitie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowling_M/0/1/0/all/0/1&quot;&gt;Michael Bowling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03316">
<title>Adversarial Meta-Learning. (arXiv:1806.03316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03316</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-learning enables a model to learn from very limited data to undertake a
new task. In this paper, we study the general meta-learning with adversarial
samples. We present a meta-learning algorithm, ADML (ADversarial Meta-Learner),
which leverages clean and adversarial samples to optimize the initialization of
a learning model in an adversarial manner. ADML leads to the following
desirable properties: 1) it turns out to be very effective even in the cases
with only clean samples; 2) it is model-agnostic, i.e., it is compatible with
any learning model that can be trained with gradient descent; and most
importantly, 3) it is robust to adversarial samples, i.e., unlike other
meta-learning methods, it only leads to a minor performance degradation when
there are adversarial samples. We show via extensive experiments that ADML
delivers the state-of-the-art performance on two widely-used image datasets,
MiniImageNet and CIFAR100, in terms of both accuracy and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chengxiang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03514">
<title>Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. (arXiv:1806.03514v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03514</link>
<description rdf:parseType="Literal">&lt;p&gt;Click-through rate (CTR) prediction is a critical task in online display
advertising. The data involved in CTR prediction are typically multi-field
categorical data, i.e., every feature is categorical and belongs to one and
only one field. One of the interesting characteristics of such data is that
features from one field often interact differently with features from different
other fields. Recently, Field-aware Factorization Machines (FFMs) have been
among the best performing models for CTR prediction by explicitly modeling such
difference. However, the number of parameters in FFMs is in the order of
feature number times field number, which is unacceptable in the real-world
production systems. In this paper, we propose Field-weighted Factorization
Machines (FwFMs) to model the different feature interactions between different
fields in a much more memory-efficient way. Our experimental evaluations show
that FwFMs can achieve competitive prediction performance with only as few as
4% parameters of FFMs. When using the same number of parameters, FwFMs can
bring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Junwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1&quot;&gt;Alfonso Lobos Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shengjun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Quan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03720">
<title>Stochastic seismic waveform inversion using generative adversarial networks as a geological prior. (arXiv:1806.03720v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/1806.03720</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an application of deep generative models in the context of
partial-differential equation (PDE) constrained inverse problems. We combine a
generative adversarial network (GAN) representing an a priori model that
creates subsurface geological structures and their petrophysical properties,
with the numerical solution of the PDE governing the propagation of acoustic
waves within the earth&apos;s interior. We perform Bayesian inversion using an
approximate Metropolis-adjusted Langevin algorithm (MALA) to sample from the
posterior given seismic observations. Gradients with respect to the model
parameters governing the forward problem are obtained by solving the adjoint of
the acoustic wave equation. Gradients of the mismatch with respect to the
latent variables are obtained by leveraging the differentiable nature of the
deep neural network used to represent the generative model. We show that
approximate MALA sampling allows efficient Bayesian inversion of model
parameters obtained from a prior represented by a deep generative model,
obtaining a diverse set of realizations that reflect the observed seismic
response.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mosser_L/0/1/0/all/0/1&quot;&gt;Lukas Mosser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dubrule_O/0/1/0/all/0/1&quot;&gt;Olivier Dubrule&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Blunt_M/0/1/0/all/0/1&quot;&gt;Martin J. Blunt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03723">
<title>Smallify: Learning Network Size while Training. (arXiv:1806.03723v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03723</link>
<description rdf:parseType="Literal">&lt;p&gt;As neural networks become widely deployed in different applications and on
different hardware, it has become increasingly important to optimize inference
time and model size along with model accuracy. Most current techniques optimize
model size, model accuracy and inference time in different stages, resulting in
suboptimal results and computational inefficiency. In this work, we propose a
new technique called Smallify that optimizes all three of these metrics at the
same time. Specifically we present a new method to simultaneously optimize
network size and model performance by neuron-level pruning during training.
Neuron-level pruning not only produces much smaller networks but also produces
dense weight matrices that are amenable to efficient inference. By applying our
technique to convolutional as well as fully connected models, we show that
Smallify can reduce network size by 35X with a 6X improvement in inference time
with similar accuracy as models found by traditional training techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leclerc_G/0/1/0/all/0/1&quot;&gt;Guillaume Leclerc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vartak_M/0/1/0/all/0/1&quot;&gt;Manasi Vartak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fernandez_R/0/1/0/all/0/1&quot;&gt;Raul Castro Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kraska_T/0/1/0/all/0/1&quot;&gt;Tim Kraska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Madden_S/0/1/0/all/0/1&quot;&gt;Samuel Madden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03791">
<title>The Effect of Network Width on the Performance of Large-batch Training. (arXiv:1806.03791v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03791</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed implementations of mini-batch stochastic gradient descent (SGD)
suffer from communication overheads, attributed to the high frequency of
gradient updates inherent in small-batch training. Training with large batches
can reduce these overheads; however, large batches can affect the convergence
properties and generalization performance of SGD. In this work, we take a first
step towards analyzing how the structure (width and depth) of a neural network
affects the performance of large-batch training. We present new theoretical
results which suggest that--for a fixed number of parameters--wider networks
are more amenable to fast large-batch training compared to deeper ones. We
provide extensive experiments on residual and fully-connected neural networks
which suggest that wider networks can be trained using larger batches without
incurring a convergence slow-down, unlike their deeper variants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinman Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koutris_P/0/1/0/all/0/1&quot;&gt;Paraschos Koutris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03857">
<title>Deep Learning for Classification Tasks on Geospatial Vector Polygons. (arXiv:1806.03857v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03857</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we evaluate the accuracy of deep learning approaches on
geospatial vector geometry classification tasks. The purpose of this evaluation
is to investigate the ability of deep learning models to learn from geometry
coordinates directly. Previous machine learning research applied to geospatial
polygon data did not use geometries directly, but derived properties thereof.
These are produced by way of extracting geometry properties such as Fourier
descriptors. Instead, our introduced deep neural net architectures are able to
learn on sequences of coordinates mapped directly from polygons. In three
classification tasks we show that the deep learning architectures are
competitive with common learning algorithms that require extracted features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veer_R/0/1/0/all/0/1&quot;&gt;Rein van &amp;#x27;t Veer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bloem_P/0/1/0/all/0/1&quot;&gt;Peter Bloem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Folmer_E/0/1/0/all/0/1&quot;&gt;Erwin Folmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03903">
<title>Multi-task learning of daily work and study round-trips from survey data. (arXiv:1806.03903v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03903</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a machine learning approach to infer the worker and
student mobility flows on daily basis from static censuses. The rapid
urbanization has made the estimation of the human mobility flows a critical
task for transportation and urban planners. The primary objective of this paper
is to complete individuals&apos; census data with working and studying trips,
allowing its merging with other mobility data to better estimate the complete
origin-destination matrices. Worker and student mobility flows are among the
most weekly regular displacements and consequently generate road congestion
problems. Estimating their round-trips eases the decision-making processes for
local authorities. Worker and student censuses often contain home location,
work places and educational institutions. We thus propose a neural network
model that learns the temporal distribution of displacements from other
mobility sources and tries to predict them on new censuses data. The inclusion
of multi-task learning in our neural network results in a significant error
rate control in comparison to single task learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katranji_M/0/1/0/all/0/1&quot;&gt;Mehdi Katranji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraiem_S/0/1/0/all/0/1&quot;&gt;Sami Kraiem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moalic_L/0/1/0/all/0/1&quot;&gt;Laurent Moalic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanmarty_G/0/1/0/all/0/1&quot;&gt;Guilhem Sanmarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caminada_A/0/1/0/all/0/1&quot;&gt;Alexandre Caminada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selem_F/0/1/0/all/0/1&quot;&gt;Fouad Hadj Selem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03925">
<title>Gear Training: A new way to implement high-performance model-parallel training. (arXiv:1806.03925v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03925</link>
<description rdf:parseType="Literal">&lt;p&gt;The training of Deep Neural Networks usually needs tremendous computing
resources. Therefore many deep models are trained in large cluster instead of
single machine or GPU. Though major researchs at present try to run whole model
on all machines by using asynchronous asynchronous stochastic gradient descent
(ASGD), we present a new approach to train deep model parallely -- split the
model and then seperately train different parts of it in different speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dongchang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Di Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03945">
<title>A Fast and Easy Regression Technique for k-NN Classification Without Using Negative Pairs. (arXiv:1806.03945v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03945</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes an inexpensive way to learn an effective dissimilarity
function to be used for $k$-nearest neighbor ($k$-NN) classification. Unlike
Mahalanobis metric learning methods that map both query (unlabeled) objects and
labeled objects to new coordinates by a single transformation, our method
learns a transformation of labeled objects to new points in the feature space
whereas query objects are kept in their original coordinates. This method has
several advantages over existing distance metric learning methods: (i) In
experiments with large document and image datasets, it achieves $k$-NN
classification accuracy better than or at least comparable to the
state-of-the-art metric learning methods. (ii) The transformation can be
learned efficiently by solving a standard ridge regression problem. For
document and image datasets, training is often more than two orders of
magnitude faster than the fastest metric learning methods tested. This speed-up
is also due to the fact that the proposed method eliminates the optimization
over &quot;negative&quot; object pairs, i.e., objects whose class labels are different.
(iii) The formulation has a theoretical justification in terms of reducing
hubness in data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shigeto_Y/0/1/0/all/0/1&quot;&gt;Yutaro Shigeto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimbo_M/0/1/0/all/0/1&quot;&gt;Masashi Shimbo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1&quot;&gt;Yuji Matsumoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03972">
<title>Multi-task Learning for Maritime Traffic Surveillance from AIS Data Streams. (arXiv:1806.03972v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03972</link>
<description rdf:parseType="Literal">&lt;p&gt;In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular time-sampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadaine_R/0/1/0/all/0/1&quot;&gt;Rodolphe Vadaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajduch_G/0/1/0/all/0/1&quot;&gt;Guillaume Hajduch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garello_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Garello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04000">
<title>Aggregating Predictions on Multiple Non-disclosed Datasets using Conformal Prediction. (arXiv:1806.04000v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.04000</link>
<description rdf:parseType="Literal">&lt;p&gt;Conformal Prediction is a machine learning methodology that produces valid
prediction regions under mild conditions. In this paper, we explore the
application of making predictions over multiple data sources of different sizes
without disclosing data between the sources. We propose that each data source
applies a transductive conformal predictor independently using the local data,
and that the individual predictions are then aggregated to form a combined
prediction region. We demonstrate the method on several data sets, and show
that the proposed method produces conservatively valid predictions and reduces
the variance in the aggregated predictions. We also study the effect that the
number of data sources and size of each source has on aggregated predictions,
as compared with equally sized sources and pooled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spjuth_O/0/1/0/all/0/1&quot;&gt;Ola Spjuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carlsson_L/0/1/0/all/0/1&quot;&gt;Lars Carlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gauraha_N/0/1/0/all/0/1&quot;&gt;Niharika Gauraha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04016">
<title>Baselines and a datasheet for the Cerema AWP dataset. (arXiv:1806.04016v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.04016</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the recently published Cerema AWP (Adverse Weather
Pedestrian) dataset for various machine learning tasks and its exports in
machine learning friendly format. We explain why this dataset can be
interesting (mainly because it is a greatly controlled and fully annotated
image dataset) and present baseline results for various tasks. Moreover, we
decided to follow the very recent suggestions of datasheets for dataset, trying
to standardize all the available information of the dataset, with a
transparency objective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seck_I/0/1/0/all/0/1&quot;&gt;Isma&amp;#xef;la Seck&lt;/a&gt; (LIMOS, LITIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahmane_K/0/1/0/all/0/1&quot;&gt;Khouloud Dahmane&lt;/a&gt; (Cerema), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duthon_P/0/1/0/all/0/1&quot;&gt;Pierre Duthon&lt;/a&gt; (Cerema), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loosli_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;lle Loosli&lt;/a&gt; (LIMOS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.03581">
<title>Polya Urn Latent Dirichlet Allocation: a doubly sparse massively parallel sampler. (arXiv:1704.03581v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.03581</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Dirichlet Allocation (LDA) is a topic model widely used in natural
language processing and machine learning. Most approaches to training the model
rely on iterative algorithms, which makes it difficult to run LDA on big
corpora that are best analyzed in parallel and distributed computational
environments. Indeed, current approaches to parallel inference either don&apos;t
converge to the correct posterior or require storage of large dense matrices in
memory. We present a novel sampler that overcomes both problems, and we show
that this sampler is faster, both empirically and theoretically, than previous
Gibbs samplers for LDA. We do so by employing a novel P\&apos;olya-urn-based
approximation in the sparse partially collapsed sampler for LDA. We prove that
the approximation error vanishes with data size, making our algorithm
asymptotically exact, a property of importance for large-scale topic models. In
addition, we show, via an explicit example, that -- contrary to popular belief
in the topic modeling literature -- partially collapsed samplers can be more
efficient than fully collapsed samplers. We conclude by comparing the
performance of our algorithm with that of other approaches on well-known
corpora.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1&quot;&gt;Alexander Terenin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Magnusson_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;ns Magnusson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jonsson_L/0/1/0/all/0/1&quot;&gt;Leif Jonsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Draper_D/0/1/0/all/0/1&quot;&gt;David Draper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02664">
<title>Geometry Score: A Method For Comparing Generative Adversarial Networks. (arXiv:1802.02664v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02664</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the biggest challenges in the research of generative adversarial
networks (GANs) is assessing the quality of generated samples and detecting
various levels of mode collapse. In this work, we construct a novel measure of
performance of a GAN by comparing geometrical properties of the underlying data
manifold and the generated one, which provides both qualitative and
quantitative means for evaluation. Our algorithm can be applied to datasets of
an arbitrary nature and is not limited to visual data. We test the obtained
metric on various real-life models and datasets and demonstrate that our method
provides new insights into properties of GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1&quot;&gt;Valentin Khrulkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03690">
<title>On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups. (arXiv:1802.03690v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03690</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks have been extremely successful in the image
recognition domain because they ensure equivariance to translations. There have
been many recent attempts to generalize this framework to other domains,
including graphs and data lying on manifolds. In this paper we give a rigorous,
theoretical treatment of convolution and equivariance in neural networks with
respect to not just translations, but the action of any compact group. Our main
result is to prove that (given some natural constraints) convolutional
structure is not just a sufficient, but also a necessary condition for
equivariance to the action of a compact group. Our exposition makes use of
concepts from representation theory and noncommutative harmonic analysis and
derives new generalized convolution formulae.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kondor_R/0/1/0/all/0/1&quot;&gt;Risi Kondor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trivedi_S/0/1/0/all/0/1&quot;&gt;Shubhendu Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06978">
<title>Improving Transferability of Adversarial Examples with Input Diversity. (arXiv:1803.06978v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06978</link>
<description rdf:parseType="Literal">&lt;p&gt;Though convolutional neural networks have achieved state-of-the-art
performance on various vision tasks, they are extremely vulnerable to
adversarial examples, which are obtained by adding human-imperceptible
perturbations to the original images. Adversarial examples can thus be used as
an useful tool to evaluate and select the most robust models in safety-critical
applications. However, most of the existing adversarial attacks only achieve
relatively low success rates under the challenging black-box setting, where the
attackers have no knowledge of the model structure and parameters. To this end,
we propose to improve the transferability of adversarial examples by creating
diverse input patterns. Instead of only using the original images to generate
adversarial examples, our method applies random transformations to the input
images at each iteration. Extensive experiments on ImageNet show that the
proposed attack method can generate adversarial examples that transfer much
better to different networks than existing baselines. To further improve the
transferability, we (1) integrate the recently proposed momentum method into
the attack process; and (2) attack an ensemble of networks simultaneously. By
evaluating our method against top defense submissions and official baselines
from NIPS 2017 adversarial competition, this enhanced attack reaches an average
success rate of 73.0%, which outperforms the top 1 attack submission in the
NIPS competition by a large margin of 6.6%. We hope that our proposed attack
strategy can serve as a benchmark for evaluating the robustness of networks to
adversaries and the effectiveness of different defense methods in future. The
code is public available at https://github.com/cihangxie/DI-2-FGSM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhou Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10140">
<title>Securing Distributed Machine Learning in High Dimensions. (arXiv:1804.10140v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10140</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard distributed machine learning frameworks require collecting the
training data from data providers and storing it in a datacenter. To ease
privacy concerns, alternative distributed machine learning frameworks (such as
{\em Federated Learning}) have been proposed, wherein the training data is kept
confidential by its providers from the learner, and the learner learns the
model by communicating with data providers. However, such frameworks suffer
from serious security risks, as data providers are vulnerable to adversarial
attacks and the learner lacks of enough administrative power. We assume in each
communication round, up to $q$ out of the $m$ data providers/workers suffer
Byzantine faults. Each worker keeps a local sample of size $n$ and the total
sample size is $N=nm$. Of particular interest is the high-dimensional regime,
where the local sample size $n$ is much smaller than the model dimension $d$.
&lt;/p&gt;
&lt;p&gt;We propose a secured variant of the gradient descent method and show that it
tolerates up to a constant fraction of Byzantine workers. Moreover, we show the
statistical estimation error of the iterates converges in $O(\log N)$ rounds to
$O(\sqrt{q/N} + \sqrt{d/N})$, which is larger than the minimax-optimal error
rate $O(\sqrt{d/N})$ in the failure-free setting by at most an additive term
$O(\sqrt{q/N})$. As long as $q=O(d)$, our proposed algorithm achieves the
optimal error rate $O(\sqrt{d/N})$. The core of our method is a robust gradient
aggregator based on the iterative filtering algorithm proposed by Steinhardt et
al. We establish a {\em uniform} concentration of the sample covariance matrix
of gradients, and show that the aggregated gradient, as a function of model
parameter, converges uniformly to the true gradient function. As a by-product,
we develop a new concentration inequality for sample covariance matrices, which
might be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Lili Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaming Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09039">
<title>Amortized Context Vector Inference for Sequence-to-Sequence Networks. (arXiv:1805.09039v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09039</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural attention (NA) is an effective mechanism for inferring complex
structural data dependencies that span long temporal horizons. As a
consequence, it has become a key component of sequence-to-sequence models that
yield state-of-the-art performance in as hard tasks as abstractive document
summarization (ADS), machine translation (MT), and video captioning (VC). NA
mechanisms perform inference of context vectors; these constitute weighted sums
of deterministic input sequence encodings, adaptively sourced over long
temporal horizons. However, recent work in the field of amortized variational
inference (AVI) has shown that it is often useful to treat the representations
generated by deep networks as latent random variables. This allows for the
models to better explore the space of possible representations. Based on this
motivation, in this work we introduce a novel regard towards a popular NA
mechanism, namely soft-attention (SA). Our approach treats the context vectors
generated by SA models as latent variables, the posteriors of which are
inferred by employing AVI. Both the means and the covariance matrices of the
inferred posteriors are parameterized via deep network mechanisms similar to
those employed in the context of standard SA. To illustrate our method, we
implement it in the context of popular sequence-to-sequence model variants with
SA. We conduct an extensive experimental evaluation using challenging ADS, VC,
and MT benchmarks, and show how our approach compares to the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1&quot;&gt;Sotirios Chatzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charalampous_A/0/1/0/all/0/1&quot;&gt;Aristotelis Charalampous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolias_K/0/1/0/all/0/1&quot;&gt;Kyriacos Tolias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassou_S/0/1/0/all/0/1&quot;&gt;Sotiris A. Vassou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01466">
<title>Informative Gene Selection for Microarray Classification via Adaptive Elastic Net with Conditional Mutual Information. (arXiv:1806.01466v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01466</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the advantage of achieving a better performance under weak
regularization, elastic net has attracted wide attention in statistics, machine
learning, bioinformatics, and other fields. In particular, a variation of the
elastic net, adaptive elastic net (AEN), integrates the adaptive grouping
effect. In this paper, we aim to develop a new algorithm: Adaptive Elastic Net
with Conditional Mutual Information (AEN-CMI) that further improves AEN by
incorporating conditional mutual information into the gene selection process.
We apply this new algorithm to screen significant genes for two kinds of
cancers: colon cancer and leukemia. Compared with other algorithms including
Support Vector Machine, Classic Elastic Net and Adaptive Elastic Net, the
proposed algorithm, AEN-CMI, obtains the best classification performance using
the least number of genes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yadi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin-Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yongjin Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01660">
<title>Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization. (arXiv:1806.01660v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01660</link>
<description rdf:parseType="Literal">&lt;p&gt;Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) is
one of the most popular algorithms in distributed machine learning. However,
its convergence properties for these complicated nonconvex problems is still
largely unknown, because of the current technical limit. Therefore, in this
paper, we propose to analyze the algorithm through a simpler but nontrivial
nonconvex problem - streaming PCA, which helps us to understand Aync-MSGD
better even for more general problems. Specifically, we establish the
asymptotic rate of convergence of Async-MSGD for streaming PCA by diffusion
approximation. Our results indicate a fundamental tradeoff between asynchrony
and momentum: To ensure convergence and acceleration through asynchrony, we
have to reduce the momentum (compared with Sync-MSGD). To the best of our
knowledge, this is the first theoretical attempt on understanding Async-MSGD
for distributed nonconvex stochastic optimization. Numerical experiments on
both streaming PCA and training deep neural networks are provided to support
our findings for Async-MSGD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jianping Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enlu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item></rdf:RDF>