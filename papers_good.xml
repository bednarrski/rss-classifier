<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04491"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01890"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.04724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07903"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08250"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10122"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00310"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.04491">
<title>Slice as an Evolutionary Service: Genetic Optimization for Inter-Slice Resource Management in 5G Networks. (arXiv:1802.04491v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04491</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of Fifth Generation (5G) mobile networks, the concept of
&quot;Slice as a Service&quot; (SlaaS) promotes mobile network operators to flexibly
share infrastructures with mobile service providers and stakeholders. However,
it also challenges with an emerging demand for efficient online algorithms to
optimize the request-and-decision-based inter-slice resource management
strategy. Based on genetic algorithms, this paper presents a novel online
optimizer that efficiently approaches towards the ideal slicing strategy with
maximized long-term network utility. The proposed method encodes slicing
strategies into binary sequences to cope with the request-and-decision
mechanism. It requires no a priori knowledge about the traffic/utility models,
and therefore supports heterogeneous slices, while providing solid
effectiveness, good robustness against non-stationary service scenarios, and
high scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lianghai Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schotten_H/0/1/0/all/0/1&quot;&gt;Hans D. Schotten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01890">
<title>RMDL: Random Multimodel Deep Learning for Classification. (arXiv:1805.01890v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.01890</link>
<description rdf:parseType="Literal">&lt;p&gt;The continually increasing number of complex datasets each year necessitates
ever improving machine learning methods for robust and accurate categorization
of these data. This paper introduces Random Multimodel Deep Learning (RMDL): a
new ensemble, deep learning approach for classification. Deep learning models
have achieved state-of-the-art results across many domains. RMDL solves the
problem of finding the best deep learning structure and architecture while
simultaneously improving robustness and accuracy through ensembles of deep
learning architectures. RDML can accept as input a variety data to include
text, video, images, and symbolic. This paper describes RMDL and shows test
results for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,
and 20newsgroup. These test results show that RDML produces consistently better
performance than standard methods over a broad range of data types and
classification problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowsari_K/0/1/0/all/0/1&quot;&gt;Kamran Kowsari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidarysafa_M/0/1/0/all/0/1&quot;&gt;Mojtaba Heidarysafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Donald E. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meimandi_K/0/1/0/all/0/1&quot;&gt;Kiana Jafari Meimandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1&quot;&gt;Laura E. Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03379">
<title>Opinion Fraud Detection via Neural Autoencoder Decision Forest. (arXiv:1805.03379v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.03379</link>
<description rdf:parseType="Literal">&lt;p&gt;Online reviews play an important role in influencing buyers&apos; daily purchase
decisions. However, fake and meaningless reviews, which cannot reflect users&apos;
genuine purchase experience and opinions, widely exist on the Web and pose
great challenges for users to make right choices. Therefore,it is desirable to
build a fair model that evaluates the quality of products by distinguishing
spamming reviews. We present an end-to-end trainable unified model to leverage
the appealing properties from Autoencoder and random forest. A stochastic
decision tree model is implemented to guide the global parameter learning
process. Extensive experiments were conducted on a large Amazon review dataset.
The proposed model consistently outperforms a series of compared methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Manqing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lina Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xianzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benatallah_B/0/1/0/all/0/1&quot;&gt;Boualem Benatallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xiaodong Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10689">
<title>Decoupling Dynamics and Reward for Transfer Learning. (arXiv:1804.10689v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10689</link>
<description rdf:parseType="Literal">&lt;p&gt;Current reinforcement learning (RL) methods can successfully learn single
tasks but often generalize poorly to modest perturbations in task domain or
training procedure. In this work, we present a decoupled learning strategy for
RL that creates a shared representation space where knowledge can be robustly
transferred. We separate learning the task representation, the forward
dynamics, the inverse dynamics and the reward function of the domain, and show
that this decoupling improves performance within the task, transfers well to
changes in dynamics and reward, and can be effectively used for online
planning. Empirical results show good performance in both continuous and
discrete RL domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satija_H/0/1/0/all/0/1&quot;&gt;Harsh Satija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1&quot;&gt;Joelle Pineau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03551">
<title>A Unified Framework of Deep Neural Networks by Capsules. (arXiv:1805.03551v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.03551</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growth of deep learning, how to describe deep neural networks
unifiedly is becoming an important issue. We first formalize neural networks
mathematically with their directed graph representations, and prove a
generation theorem about the induced networks of connected directed acyclic
graphs. Then, we set up a unified framework for deep learning with capsule
networks. This capsule framework could simplify the description of existing
deep neural networks, and provide a theoretical basis of graphic designing and
programming techniques for deep learning models, thus would be of great
significance to the advancement of deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1&quot;&gt;Chuanhui Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03553">
<title>On Visual Hallmarks of Robustness to Adversarial Malware. (arXiv:1805.03553v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.03553</link>
<description rdf:parseType="Literal">&lt;p&gt;A central challenge of adversarial learning is to interpret the resulting
hardened model. In this contribution, we ask how robust generalization can be
visually discerned and whether a concise view of the interactions between a
hardened decision map and input samples is possible. We first provide a means
of visually comparing a hardened model&apos;s loss behavior with respect to the
adversarial variants generated during training versus loss behavior with
respect to adversarial variants generated from other sources. This allows us to
confirm that the association of observed flatness of a loss landscape with
generalization that is seen with naturally trained models extends to
adversarially hardened models and robust generalization. To complement these
means of interpreting model parameter robustness we also use self-organizing
maps to provide a visual means of superimposing adversarial and natural
variants on a model&apos;s decision space, thus allowing the model&apos;s global
robustness to be comprehensively examined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Alex Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Dujaili_A/0/1/0/all/0/1&quot;&gt;Abdullah Al-Dujaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1&quot;&gt;Erik Hemberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1&quot;&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03616">
<title>A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization. (arXiv:1805.03616v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.03616</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a deep learning approach to tackle the automatic
summarization tasks by incorporating topic information into the convolutional
sequence-to-sequence (ConvS2S) model and using self-critical sequence training
(SCST) for optimization. Through jointly attending to topics and word-level
alignment, our approach can improve coherence, diversity, and informativeness
of generated summaries via a biased probability generation mechanism. On the
other hand, reinforcement training, like SCST, directly optimizes the proposed
model with respect to the non-differentiable metric ROUGE, which also avoids
the exposure bias during inference. We carry out the experimental evaluation
with state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets.
The empirical results demonstrate the superiority of our proposed method in the
abstractive summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Junlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yunzhe Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1&quot;&gt;Li Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1&quot;&gt;Qiang Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.04724">
<title>On the Sample Complexity of Graphical Model Selection for Non-Stationary Processes. (arXiv:1701.04724v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1701.04724</link>
<description rdf:parseType="Literal">&lt;p&gt;We characterize the sample size required for accurate graphical model
selection from non- stationary samples. The observed data is modelled as a
vector-valued zero-mean Gaussian random process whose samples are uncorrelated
but have different covariance matrices. This model contains as special cases
the standard setting of i.i.d. samples as well as the case of samples forming a
stationary or underspread (non-stationary) processes. More generally, our model
applies to any process model for which an efficient decorrelation can be
obtained. By analyzing a particular model selection method, we derive a
sufficient condition on the required sample size for accurate graphical model
selection based on non-stationary data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nguyen Q. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07903">
<title>Ensemble Multi-task Gaussian Process Regression with Multiple Latent Processes. (arXiv:1709.07903v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07903</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task/Multi-output learning seeks to exploit correlation among tasks to
enhance performance over learning or solving each task independently. In this
paper, we investigate this problem in the context of Gaussian Processes (GPs)
and propose a new model which learns a mixture of latent processes by
decomposing the covariance matrix into a sum of structured hidden components
each of which is controlled by a latent GP over input features and a &quot;weight&quot;
over tasks. From this sum structure, we propose a parallelizable parameter
learning algorithm with a predetermined initialization for the &quot;weights&quot;. We
also notice that an ensemble parameter learning approach using mini-batches of
training data not only reduces the computation complexity of learning but also
improves the regression performance. We evaluate our model on two datasets, the
smaller Swiss Jura dataset and another relatively larger ATMS dataset from
NOAA. Substantial improvements are observed compared with established
alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Weitong Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miller_E/0/1/0/all/0/1&quot;&gt;Eric L. Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08250">
<title>SeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural Networks by Selective Network Augmentation. (arXiv:1802.08250v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08250</link>
<description rdf:parseType="Literal">&lt;p&gt;Lifelong learning aims to develop machine learning systems that can learn new
tasks while preserving the performance on previous learned tasks. In this paper
we present a method to overcome catastrophic forgetting on convolutional neural
networks, that learns new tasks and preserves the performance on old tasks
without accessing the data of the original model, by selective network
augmentation. The experiment results showed that SeNA-CNN, in some scenarios,
outperforms the state-of-art Learning without Forgetting algorithm. Results
also showed that in some situations it is better to use SeNA-CNN instead of
training a neural network using isolated learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zacarias_A/0/1/0/all/0/1&quot;&gt;Abel S. Zacarias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandre_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s A. Alexandre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10122">
<title>World Models. (arXiv:1803.10122v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10122</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore building generative neural network models of popular reinforcement
learning environments. Our world model can be trained quickly in an
unsupervised manner to learn a compressed spatial and temporal representation
of the environment. By using features extracted from the world model as inputs
to an agent, we can train a very compact and simple policy that can solve the
required task. We can even train our agent entirely inside of its own
hallucinated dream generated by its world model, and transfer this policy back
into the actual environment.
&lt;/p&gt;
&lt;p&gt;An interactive version of this paper is available at
https://worldmodels.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1&quot;&gt;David Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04212">
<title>Word2Vec applied to Recommendation: Hyperparameters Matter. (arXiv:1804.04212v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;Skip-gram with negative sampling, a popular variant of Word2vec originally
designed and tuned to create word embeddings for Natural Language Processing,
has been used to create item embeddings with successful applications in
recommendation. While these fields do not share the same type of data, neither
evaluate on the same tasks, recommendation applications tend to use the same
already tuned hyperparameters values, even if optimal hyperparameters values
are often known to be data and task dependent. We thus investigate the marginal
importance of each hyperparameter in a recommendation setting through large
hyperparameter grid searches on various datasets. Results reveal that
optimizing neglected hyperparameters, namely negative sampling distribution,
number of epochs, subsampling parameter and window-size, significantly improves
performance on a recommendation task, and can increase it by an order of
magnitude. Importantly, we find that optimal hyperparameters configurations for
Natural Language Processing tasks and Recommendation tasks are noticeably
different.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1&quot;&gt;Hugo Caselles-Dupr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lesaint_F/0/1/0/all/0/1&quot;&gt;Florian Lesaint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Royo_Letelier_J/0/1/0/all/0/1&quot;&gt;Jimena Royo-Letelier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00310">
<title>On the Limitation of MagNet Defense against $L_1$-based Adversarial Examples. (arXiv:1805.00310v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00310</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, defending adversarial perturbations to natural examples in
order to build robust machine learning models trained by deep neural networks
(DNNs) has become an emerging research field in the conjunction of deep
learning and security. In particular, MagNet consisting of an adversary
detector and a data reformer is by far one of the strongest defenses in the
black-box oblivious attack setting, where the attacker aims to craft
transferable adversarial examples from an undefended DNN model to bypass an
unknown defense module deployed on the same DNN model. Under this setting,
MagNet can successfully defend a variety of attacks in DNNs, including the
high-confidence adversarial examples generated by the Carlini and Wagner&apos;s
attack based on the $L_2$ distortion metric. However, in this paper, under the
same attack setting we show that adversarial examples crafted based on the
$L_1$ distortion metric can easily bypass MagNet and mislead the target DNN
image classifiers on MNIST and CIFAR-10. We also provide explanations on why
the considered approach can yield adversarial examples with superior attack
performance and conduct extensive experiments on variants of MagNet to verify
its lack of robustness to $L_1$ distortion based attacks. Notably, our results
substantially weaken the assumption of effective threat models on MagNet that
require knowing the deployed defense technique when attacking DNNs (i.e., the
gray-box attack setting).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pei-Hsuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kang-Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chia-Mu Yu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>