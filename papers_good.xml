<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00209"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1308.1603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00411"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1103.5034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.10651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08618"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00030"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.01604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01587"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.00209">
<title>Dual Recurrent Attention Units for Visual Question Answering. (arXiv:1802.00209v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an architecture for VQA which utilizes recurrent layers to
generate visual and textual attention. The memory characteristic of the
proposed recurrent attention units offers a rich joint embedding of visual and
textual features and enables the model to reason relations between several
parts of the image and question. Our single model outperforms the first place
winner on the VQA 1.0 dataset, performs within margin to the current
state-of-the-art ensemble model. We also experiment with replacing attention
mechanisms in other state-of-the-art models with our implementation and show
increased accuracy. In both cases, our recurrent attention mechanism improves
performance in tasks requiring sequential or relational reasoning on the VQA
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osman_A/0/1/0/all/0/1&quot;&gt;Ahmed Osman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1308.1603">
<title>A Note on Topology Preservation in Classification, and the Construction of a Universal Neuron Grid. (arXiv:1308.1603v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1308.1603</link>
<description rdf:parseType="Literal">&lt;p&gt;It will be shown that according to theorems of K. Menger, every neuron grid
if identified with a curve is able to preserve the adopted qualitative
structure of a data space. Furthermore, if this identification is made, the
neuron grid structure can always be mapped to a subset of a universal neuron
grid which is constructable in three space dimensions. Conclusions will be
drawn for established neuron grid types as well as neural fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volz_D/0/1/0/all/0/1&quot;&gt;Dietmar Volz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00048">
<title>Deceptive Games. (arXiv:1802.00048v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00048</link>
<description rdf:parseType="Literal">&lt;p&gt;Deceptive games are games where the reward structure or other aspects of the
game are designed to lead the agent away from a globally optimal policy. While
many games are already deceptive to some extent, we designed a series of games
in the Video Game Description Language (VGDL) implementing specific types of
deception, classified by the cognitive biases they exploit. VGDL games can be
run in the General Video Game Artificial Intelligence (GVGAI) Framework, making
it possible to test a variety of existing AI agents that have been submitted to
the GVGAI Competition on these deceptive games. Our results show that all
tested agents are vulnerable to several kinds of deception, but that different
agents have different weaknesses. This suggests that we can use deception to
understand the capabilities of a game-playing algorithm, and game-playing
algorithms to characterize the deception displayed by a game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Damien Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephenson_M/0/1/0/all/0/1&quot;&gt;Matthew Stephenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salge_C/0/1/0/all/0/1&quot;&gt;Christian Salge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_J/0/1/0/all/0/1&quot;&gt;John Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1&quot;&gt;Jochen Renz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00050">
<title>Recursive Feature Generation for Knowledge-based Learning. (arXiv:1802.00050v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00050</link>
<description rdf:parseType="Literal">&lt;p&gt;When humans perform inductive learning, they often enhance the process with
background knowledge. With the increasing availability of well-formed
collaborative knowledge bases, the performance of learning algorithms could be
significantly enhanced if a way were found to exploit these knowledge bases. In
this work, we present a novel algorithm for injecting external knowledge into
induction algorithms using feature generation. Given a feature, the algorithm
defines a new learning task over its set of values, and uses the knowledge base
to solve the constructed learning task. The resulting classifier is then used
as a new feature for the original problem. We have applied our algorithm to the
domain of text classification using large semantic knowledge bases. We have
shown that the generated features significantly improve the performance of
existing learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_L/0/1/0/all/0/1&quot;&gt;Lior Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1&quot;&gt;Shaul Markovitch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00386">
<title>Crowd Flow Prediction by Deep Spatio-Temporal Transfer Learning. (arXiv:1802.00386v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00386</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowd flow prediction is a fundamental urban computing problem. Recently,
deep learning has been successfully applied to solve this problem, but it
relies on rich historical data. In reality, many cities may suffer from data
scarcity issue when their targeted service or infrastructure is new. To
overcome this issue, this paper proposes a novel deep spatio-temporal transfer
learning framework, called RegionTrans, which can predict future crowd flow in
a data-scarce (target) city by transferring knowledge from a data-rich (source)
city. Leveraging social network check-ins, RegionTrans first links a region in
the target city to certain regions in the source city, expecting that these
inter-city region pairs will share similar crowd flow dynamics. Then, we
propose a deep spatio-temporal neural network structure, in which a hidden
layer is dedicated to keeping the region representation. A source city model is
then trained on its rich historical data with this network structure. Finally,
we propose a region-based cross-city transfer learning algorithm to learn the
target city model from the source city model by minimizing the hidden
representation discrepancy between the inter-city region pairs previously
linked by check-ins. With experiments on real crowd flow, RegionTrans can
outperform state-of-the-arts by reducing up to 10.7% prediction error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00411">
<title>3D Object Dense Reconstruction from a Single Depth View. (arXiv:1802.00411v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.00411</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs
the complete 3D structure of a given object from a single arbitrary depth view
using generative adversarial networks. Unlike existing work which typically
requires multiple views of the same object or class labels to recover the full
3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation
of a depth view of the object as input, and is able to generate the complete 3D
occupancy grid with a high resolution of 256^3 by recovering the
occluded/missing regions. The key idea is to combine the generative
capabilities of autoencoders and the conditional Generative Adversarial
Networks (GAN) framework, to infer accurate and fine-grained 3D structures of
objects in high-dimensional voxel space. Extensive experiments on large
synthetic datasets and real-world Kinect datasets show that the proposed
3D-RecGAN++ significantly outperforms the state of the art in single view 3D
object reconstruction, and is able to reconstruct unseen types of objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_S/0/1/0/all/0/1&quot;&gt;Stefano Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1&quot;&gt;Andrew Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1&quot;&gt;Niki Trigoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongkai Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1103.5034">
<title>On Understanding and Machine Understanding. (arXiv:1103.5034v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1103.5034</link>
<description rdf:parseType="Literal">&lt;p&gt;In the present paper, we try to propose a self-similar network theory for the
basic understanding. By extending the natural languages to a kind of so called
idealy sufficient language, we can proceed a few steps to the investigation of
the language searching and the language understanding of AI.
&lt;/p&gt;
&lt;p&gt;Image understanding, and the familiarity of the brain to the surrounding
environment are also discussed. Group effects are discussed by addressing the
essense of the power of influences, and constructing the influence network of a
society. We also give a discussion of inspirations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chern_T/0/1/0/all/0/1&quot;&gt;Tong Chern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.10651">
<title>Reliable Decision Support using Counterfactual Models. (arXiv:1703.10651v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.10651</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-makers are faced with the challenge of estimating what is likely to
happen when they take an action. For instance, if I choose not to treat this
patient, are they likely to die? Practitioners commonly use supervised learning
algorithms to fit predictive models that help decision-makers reason about
likely future outcomes, but we show that this approach is unreliable, and
sometimes even dangerous. The key issue is that supervised learning algorithms
are highly sensitive to the policy used to choose actions in the training data,
which causes the model to capture relationships that do not generalize. We
propose using a different learning objective that predicts counterfactuals
instead of predicting outcomes under an existing action policy as in supervised
learning. To support decision-making in temporal settings, we introduce the
Counterfactual Gaussian Process (CGP) to predict the counterfactual future
progression of continuous-time trajectories under sequences of future actions.
We demonstrate the benefits of the CGP on two important decision-support tasks:
risk prediction and &quot;what if?&quot; reasoning for individualized treatment planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schulam_P/0/1/0/all/0/1&quot;&gt;Peter Schulam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saria_S/0/1/0/all/0/1&quot;&gt;Suchi Saria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01968">
<title>Faster Deep Q-learning using Neural Episodic Control. (arXiv:1801.01968v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01968</link>
<description rdf:parseType="Literal">&lt;p&gt;The research on deep reinforcement learning which estimates Q-value by deep
learning has been attracted the interest of researchers recently. In deep
reinforcement learning, it is important to efficiently learn the experiences
that an agent has collected by exploring environment. In this research, we
propose NEC2DQN that improves learning speed of a poor sample efficiency
algorithm such as DQN by using good one such as NEC at the beginning of
learning. We show it is able to learn faster than Double DQN or N-step DQN in
the experiments of Pong.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishio_D/0/1/0/all/0/1&quot;&gt;Daichi Nishio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamane_S/0/1/0/all/0/1&quot;&gt;Satoshi Yamane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08618">
<title>JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services. (arXiv:1801.08618v1 [cs.DC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1801.08618</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are among the most influential architectures of deep
learning algorithms, being deployed in many mobile intelligent applications.
End-side services, such as intelligent personal assistants (IPAs), autonomous
cars, and smart home services often employ either simple local models or
complex remote models on the cloud. Mobile-only and cloud-only computations are
currently the status quo approaches. In this paper, we propose an efficient,
adaptive, and practical engine, JointDNN, for collaborative computation between
a mobile device and cloud for DNNs in both inference and training phase.
JointDNN not only provides an energy and performance efficient method of
querying DNNs for the mobile side, but also benefits the cloud server by
reducing the amount of its workload and communications compared to the
cloud-only approach. Given the DNN architecture, we investigate the efficiency
of processing some layers on the mobile device and some layers on the cloud
server. We provide optimization formulations at layer granularity for forward
and backward propagation in DNNs, which can adapt to mobile battery limitations
and cloud server load constraints and quality of service. JointDNN achieves up
to 18X and 32X reductions on the latency and mobile energy consumption of
querying DNNs, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eshratifar_A/0/1/0/all/0/1&quot;&gt;Amir Erfan Eshratifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrishami_M/0/1/0/all/0/1&quot;&gt;Mohammad Saeed Abrishami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1&quot;&gt;Massoud Pedram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00030">
<title>Fusarium Damaged Kernels Detection Using Transfer Learning on Deep Neural Network Architecture. (arXiv:1802.00030v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00030</link>
<description rdf:parseType="Literal">&lt;p&gt;The present work shows the application of transfer learning for a pre-trained
deep neural network (DNN), using a small image dataset ($\approx$ 12,000) on a
single workstation with enabled NVIDIA GPU card that takes up to 1 hour to
complete the training task and archive an overall average accuracy of $94.7\%$.
The DNN presents a $20\%$ score of misclassification for an external test
dataset. The accuracy of the proposed methodology is equivalent to ones using
HSI methodology $(81\%-91\%)$ used for the same task, but with the advantage of
being independent on special equipment to classify wheat kernel for FHB
symptoms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolau_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rcio Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rcia Barrocas Moreira Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tibola_C/0/1/0/all/0/1&quot;&gt;Casiane Salete Tibola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Mauricio Cunha Fernandes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavan_W/0/1/0/all/0/1&quot;&gt;Willingthon Pavan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00043">
<title>Incremental kernel PCA and the Nystr\&quot;om method. (arXiv:1802.00043v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.00043</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental versions of batch algorithms are often desired, for increased
time efficiency in the streaming data setting, or increased memory efficiency
in general. In this paper we present a novel algorithm for incremental kernel
PCA, based on rank one updates to the eigendecomposition of the kernel matrix,
which is more computationally efficient than comparable existing algorithms. We
extend our algorithm to incremental calculation of the Nystr\&quot;om approximation
to the kernel matrix, the first such algorithm proposed. Incremental
calculation of the Nystr\&quot;om approximation leads to further gains in memory
efficiency, and allows for empirical evaluation of when a subset of sufficient
size has been obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hallgren_F/0/1/0/all/0/1&quot;&gt;Fredrik Hallgren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Northrop_P/0/1/0/all/0/1&quot;&gt;Paul Northrop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00123">
<title>A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of Multinomials. (arXiv:1802.00123v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00123</link>
<description rdf:parseType="Literal">&lt;p&gt;Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural
networks can provide more powerful mapping capability than the traditional
feedforward neural networks (Sigma-Sigma neural networks). In the existing
literature, in order to reduce the number of the Pi nodes in the Pi layer, a
special multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with
respect to each particular variable sigma_i when the other variables are taken
as constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with
n&amp;gt;1 are not included. This choice may be somehow intuitive, but is not
necessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural
network (MSPSNN) with an adaptive approach to find a better multinomial for a
given problem. To elaborate, we start from a complete multinomial with a given
order. Then we employ a regularization technique in the learning process for
the given problem to reduce the number of monomials used in the multinomial,
and end up with a new SPSNN involving the same number of monomials (= the
number of nodes in the Pi-layer) as in P_s. Numerical experiments on some
benchmark problems show that our MSPSNN behaves better than the traditional
SPSNN with P_s.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_K/0/1/0/all/0/1&quot;&gt;Khidir Shaib Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00168">
<title>Deep Learning with Data Dependent Implicit Activation Function. (arXiv:1802.00168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00168</link>
<description rdf:parseType="Literal">&lt;p&gt;Though deep neural networks (DNNs) achieve remarkable performances in many
artificial intelligence tasks, the lack of training instances remains a
notorious challenge. As the network goes deeper, the generalization accuracy
decays rapidly in the situation of lacking massive amounts of training data. In
this paper, we propose novel deep neural network structures that can be
inherited from all existing DNNs with almost the same level of complexity, and
develop simple training algorithms. We show our paradigm successfully resolves
the lack of data issue. Tests on the CIFAR10 and CIFAR100 image recognition
datasets show that the new paradigm leads to 20$\%$ to $30\%$ relative error
rate reduction compared to their base DNNs. The intuition of our algorithms for
deep residual network stems from theories of the partial differential equation
(PDE) control problems. Code will be made available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zuoqiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1&quot;&gt;Stanley J. Osher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00324">
<title>One-class Collective Anomaly Detection based on Long Short-Term Memory Recurrent Neural Networks. (arXiv:1802.00324v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00324</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrusion detection for computer network systems has been becoming one of the
most critical tasks for network administrators today. It has an important role
for organizations, governments and our society due to the valuable resources
hosted on computer networks. Traditional misuse detection strategies are unable
to detect new and unknown intrusion types. In contrast, anomaly detection in
network security aims to distinguish between illegal or malicious events and
normal behavior of network systems. Anomaly detection can be considered as a
classification problem where it builds models of normal network behavior, of
which it uses to detect new patterns that significantly deviate from the model.
Most of the current approaches on anomaly detection is based on the learning of
normal behavior and anomalous actions. They do not include memory that is they
do not take into account previous events classify new ones. In this paper, we
propose a one class collective anomaly detection model based on neural network
learning. Normally a Long Short Term Memory Recurrent Neural Network (LSTM RNN)
is trained only on normal data, and it is capable of predicting several time
steps ahead of an input. In our approach, a LSTM RNN is trained on normal time
series data before performing a prediction for each time step. Instead of
considering each time-step separately, the observation of prediction errors
from a certain number of time-steps is now proposed as a new idea for detecting
collective anomalies. The prediction errors of a certain number of the latest
time-steps above a threshold will indicate a collective anomaly. The model is
evaluated on a time series version of the KDD 1999 dataset. The experiments
demonstrate that the proposed model is capable to detect collective anomaly
efficiently
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thi_N/0/1/0/all/0/1&quot;&gt;Nga Nguyen Thi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_V/0/1/0/all/0/1&quot;&gt;Van Loi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Khac_N/0/1/0/all/0/1&quot;&gt;Nhien-An Le-Khac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08431">
<title>Boundary-Seeking Generative Adversarial Networks. (arXiv:1702.08431v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08431</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) are a learning framework that rely on
training a discriminator to estimate a measure of difference between a target
and generated distributions. GANs, as normally formulated, rely on the
generated samples being completely differentiable w.r.t. the generative
parameters, and thus do not work for discrete data. We introduce a method for
training GANs with discrete data that uses the estimated difference measure
from the discriminator to compute importance weights for generated samples,
thus providing a policy gradient for training the generator. The importance
weights have a strong connection to the decision boundary of the discriminator,
and we call our method boundary-seeking GANs (BGANs). We demonstrate the
effectiveness of the proposed algorithm with discrete image and character-based
natural language generation. In addition, the boundary-seeking objective
extends to continuous data, which can be used to improve stability of training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jacob_A/0/1/0/all/0/1&quot;&gt;Athul Paul Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Che_T/0/1/0/all/0/1&quot;&gt;Tong Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trischler_A/0/1/0/all/0/1&quot;&gt;Adam Trischler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.01604">
<title>Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. (arXiv:1709.01604v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.01604</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms, when applied to sensitive data, pose a distinct
threat to privacy. A growing body of prior work demonstrates that models
produced by these algorithms may leak specific private information in the
training data to an attacker, either through the models&apos; structure or their
observable behavior. However, the underlying cause of this privacy risk is not
well understood beyond a handful of anecdotal accounts that suggest overfitting
and influence might play a role.
&lt;/p&gt;
&lt;p&gt;This paper examines the effect that overfitting and influence have on the
ability of an attacker to learn information about the training data from
machine learning models, either through training set membership inference or
attribute inference attacks. Using both formal and empirical analyses, we
illustrate a clear relationship between these factors and the privacy risk that
arises in several popular machine learning algorithms. We find that overfitting
is sufficient to allow an attacker to perform membership inference and, when
the target attribute meets certain conditions about its influence, attribute
inference attacks. Interestingly, our formal analysis also shows that
overfitting is not necessary for these attacks and begins to shed light on what
other factors may be in play. Finally, we explore the connection between
membership inference and attribute inference, showing that there are deep
connections between the two that lead to effective new attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_S/0/1/0/all/0/1&quot;&gt;Samuel Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1&quot;&gt;Matt Fredrikson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomelli_I/0/1/0/all/0/1&quot;&gt;Irene Giacomelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Somesh Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04875">
<title>Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting. (arXiv:1709.04875v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04875</link>
<description rdf:parseType="Literal">&lt;p&gt;Timely accurate traffic forecast is crucial for urban traffic control and
guidance. Due to the high nonlinearity and complexity of traffic flow,
traditional methods cannot satisfy the requirements of mid-and-long term
prediction tasks and often neglect spatial and temporal dependencies. In this
paper, we propose a novel deep learning framework, Spatio-Temporal Graph
Convolutional Networks (STGCN), to tackle the time series prediction problem in
traffic domain. Instead of applying regular convolutional and recurrent units,
we formulate the problem on graphs and build the model with complete
convolutional structures, which enable much faster training speed with fewer
parameters. Experiments show that our STGCN model effectively captures
comprehensive spatio-temporal correlations through modeling multi-scale traffic
networks and consistently outperforms state-of-the-art baselines on various
real-world traffic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Haoteng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhanxing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01587">
<title>SpectralNet: Spectral Clustering using Deep Neural Networks. (arXiv:1801.01587v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is a leading and popular technique in unsupervised data
analysis. Two of its major limitations are scalability and generalization of
the spectral embedding (i.e., out-of-sample-extension). In this paper we
introduce a deep learning approach to spectral clustering that overcomes the
above shortcomings. Our network, which we call SpectralNet, learns a map that
embeds input data points into the eigenspace of their associated graph
Laplacian matrix and subsequently clusters them. We train SpectralNet using a
procedure that involves constrained stochastic optimization. Stochastic
optimization allows it to scale to large datasets, while the constraints, which
are implemented using a special-purpose output layer, allow us to keep the
network output orthogonal. Moreover, the map learned by SpectralNet naturally
generalizes the spectral embedding to unseen data points. To further improve
the quality of the clustering, we replace the standard pairwise Gaussian
affinities with affinities leaned from unlabeled data using a Siamese network.
Additional improvement can be achieved by applying the network to code
representations produced, e.g., by standard autoencoders. Our end-to-end
learning procedure is fully unsupervised. In addition, we apply VC dimension
theory to derive a lower bound on the size of SpectralNet. State-of-the-art
clustering results are reported on the Reuters dataset. Our implementation is
publicly available at https://github.com/kstant0725/SpectralNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Henry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nadler_B/0/1/0/all/0/1&quot;&gt;Boaz Nadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basri_R/0/1/0/all/0/1&quot;&gt;Ronen Basri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item></rdf:RDF>