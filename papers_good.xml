<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11079"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11121"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11125"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11139"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08352"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02291"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07281"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10787"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11205"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11346"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11393"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11414"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08259"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07469"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09871"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12338"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05413"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08925"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10668"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.04604">
<title>Deep Quaternion Networks. (arXiv:1712.04604v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04604</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of deep learning has seen significant advancement in recent years.
However, much of the existing work has been focused on real-valued numbers.
Recent work has shown that a deep learning system using the complex numbers can
be deeper for a fixed parameter budget compared to its real-valued counterpart.
In this work, we explore the benefits of generalizing one step further into the
hyper-complex numbers, quaternions specifically, and provide the architecture
components needed to build deep quaternion networks. We develop the theoretical
basis by reviewing quaternion convolutions, developing a novel quaternion
weight initialization scheme, and developing novel algorithms for quaternion
batch-normalization. These pieces are tested in a classification model by
end-to-end training on the CIFAR-10 and CIFAR-100 data sets and a segmentation
model by end-to-end training on the KITTI Road Segmentation data set. These
quaternion networks show improved convergence compared to real-valued and
complex-valued networks, especially on the segmentation task, while having
fewer parameters
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaudet_C/0/1/0/all/0/1&quot;&gt;Chase Gaudet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony Maida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10965">
<title>Ontology-Grounded Topic Modeling for Climate Science Research. (arXiv:1807.10965v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.10965</link>
<description rdf:parseType="Literal">&lt;p&gt;In scientific disciplines where research findings have a strong impact on
society, reducing the amount of time it takes to understand, synthesize and
exploit the research is invaluable. Topic modeling is an effective technique
for summarizing a collection of documents to find the main themes among them
and to classify other documents that have a similar mixture of co-occurring
words. We show how grounding a topic model with an ontology, extracted from a
glossary of important domain phrases, improves the topics generated and makes
them easier to understand. We apply and evaluate this method to the climate
science domain. The result improves the topics generated and supports faster
research understanding, discovery of social networks among researchers, and
automatic ontology generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sleeman_J/0/1/0/all/0/1&quot;&gt;Jennifer Sleeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finin_T/0/1/0/all/0/1&quot;&gt;Tim Finin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11079">
<title>ReenactGAN: Learning to Reenact Faces via Boundary Transfer. (arXiv:1807.11079v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11079</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel learning-based framework for face reenactment. The
proposed method, known as ReenactGAN, is capable of transferring facial
movements and expressions from monocular video input of an arbitrary person to
a target person. Instead of performing a direct transfer in the pixel space,
which could result in structural artifacts, we first map the source face onto a
boundary latent space. A transformer is subsequently used to adapt the boundary
of source face to the boundary of target face. Finally, a target-specific
decoder is used to generate the reenacted target face. Thanks to the effective
and reliable boundary-based transfer, our method can perform photo-realistic
face reenactment. In addition, ReenactGAN is appealing in that the whole
reenactment process is purely feed-forward, and thus the reenactment process
can run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model will be
publicly available at
https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wayne Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11121">
<title>Neural Mesh: Introducing a Notion of Space and Conservation of Energy to Neural Networks. (arXiv:1807.11121v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11121</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are based on a simplified model of the brain. In this
project, we wanted to relax the simplifying assumptions of a traditional neural
network by making a model that more closely emulates the low level interactions
of neurons. Like in an RNN, our model has a state that persists between time
steps, so that the energies of neurons persist. However, unlike an RNN, our
state consists of a 2 dimensional matrix, rather than a 1 dimensional vector,
thereby introducing a concept of distance to other neurons within the state. In
our model, neurons can only fire to adjacent neurons, as in the brain. Like in
the brain, we only allow neurons to fire in a time step if they contain enough
energy, or excitement. We also enforce a notion of conservation of energy, so
that a neuron cannot excite its neighbors more than the excitement it already
contained at that time step. Taken together, these two features allow signals
in the form of activations to flow around in our network over time, making our
neural mesh more closely model signals traveling through the brain the brain.
Although our main goal is to design an architecture to more closely emulate the
brain in the hope of having a correct internal representation of information by
the time we know how to properly train a general intelligence, we did benchmark
our neural mash on a specific task. We found that by increasing the runtime of
the mesh, we were able to increase its accuracy without increasing the number
of parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beck_J/0/1/0/all/0/1&quot;&gt;Jacob Beck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1&quot;&gt;Zoe Papakipos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11125">
<title>Microsoft Dialogue Challenge: Building End-to-End Task-Completion Dialogue Systems. (arXiv:1807.11125v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.11125</link>
<description rdf:parseType="Literal">&lt;p&gt;This proposal introduces a Dialogue Challenge for building end-to-end
task-completion dialogue systems, with the goal of encouraging the dialogue
research community to collaborate and benchmark on standard datasets and
unified experimental environment. In this special session, we will release
human-annotated conversational data in three domains (movie-ticket booking,
restaurant reservation, and taxi booking), as well as an experiment platform
with built-in simulators in each domain, for training and evaluation purposes.
The final submitted systems will be evaluated both in simulated setting and by
human judges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1&quot;&gt;Sarah Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11139">
<title>Causal Modeling with Probabilistic Simulation Models. (arXiv:1807.11139v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11139</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent authors have proposed analyzing conditional reasoning through a notion
of intervention on a simulation program, and have found a sound and complete
axiomatization of the logic of conditionals in this setting. Here we extend
this setting to the case of probabilistic simulation models. We give a natural
definition of probability on formulas of the conditional language, allowing for
the expression of counterfactuals, and prove foundational results about this
definition. We also find an axiomatization for reasoning about linear
inequalities involving probabilities in this setting. We prove soundness,
completeness, and NP-completeness of the satisfiability problem for this logic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibeling_D/0/1/0/all/0/1&quot;&gt;Duligur Ibeling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11161">
<title>Lead Sheet Generation and Arrangement by Conditional Generative Adversarial Network. (arXiv:1807.11161v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1807.11161</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on automatic music generation has seen great progress due to the
development of deep neural networks. However, the generation of
multi-instrument music of arbitrary genres still remains a challenge. Existing
research either works on lead sheets or multi-track piano-rolls found in MIDIs,
but both musical notations have their limits. In this work, we propose a new
task called lead sheet arrangement to avoid such limits. A new recurrent
convolutional generative model for the task is proposed, along with three new
symbolic-domain harmonic features to facilitate learning from unpaired lead
sheets and MIDIs. Our model can generate lead sheets and their arrangements of
eight-bar long. Audio samples of the generated result can be found at
https://drive.google.com/open?id=1c0FfODTpudmLvuKBbc23VBCgQizY6-Rk
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao-Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11429">
<title>Kalman Filter-based Heuristic Ensemble: A New Perspective on Ensemble Classification Using Kalman Filters. (arXiv:1807.11429v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11429</link>
<description rdf:parseType="Literal">&lt;p&gt;A classifier ensemble is a combination of multiple diverse classifier models
whose outputs are aggregated into a single prediction. Ensembles have been
repeatedly shown to perform better than single classifier models, therefore
ensembles has been always a subject of research. The objective of this paper is
to introduce a new perspective on ensemble classification by considering the
training of the ensemble as a state estimation problem. The state is estimated
using noisy measurements, and these measurements are then combined using a
Kalman filter, within which heuristics are used. An implementation of this
perspective, Kalman Filter based Heuristic Ensemble (KFHE), is also presented
in this paper. Experiments performed on several datasets, indicate the
effectiveness and the potential of KFHE when compared with boosting and
bagging. Moreover, KFHE was found to perform comparatively better than bagging
and boosting in the case of datasets with noisy class label assignments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pakrashi_A/0/1/0/all/0/1&quot;&gt;Arjun Pakrashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08352">
<title>Learning to Make Predictions on Graphs with Autoencoders. (arXiv:1802.08352v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08352</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine two fundamental tasks associated with graph representation
learning: link prediction and semi-supervised node classification. We present a
novel autoencoder architecture capable of learning a joint representation of
both local graph structure and available node features for the multi-task
learning of link prediction and node classification. Our autoencoder
architecture is efficiently trained end-to-end in a single learning stage to
simultaneously perform link prediction and node classification, whereas
previous related methods require multiple training steps that are difficult to
optimize. We provide a comprehensive empirical evaluation of our models on nine
benchmark graph-structured datasets and demonstrate significant improvement
over related methods for graph representation learning. Reference code and data
are available at https://github.com/vuptran/graph-representation-learning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1&quot;&gt;Phi Vu Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02291">
<title>Synthesizing Neural Network Controllers with Probabilistic Model based Reinforcement Learning. (arXiv:1803.02291v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02291</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an algorithm for rapidly learning controllers for robotics
systems. The algorithm follows the model-based reinforcement learning paradigm,
and improves upon existing algorithms; namely Probabilistic learning in Control
(PILCO) and a sample-based version of PILCO with neural network dynamics
(Deep-PILCO). We propose training a neural network dynamics model using
variational dropout with truncated Log-Normal noise. This allows us to obtain a
dynamics model with calibrated uncertainty, which can be used to simulate
controller executions via rollouts. We also describe set of techniques,
inspired by viewing PILCO as a recurrent neural network model, that are crucial
to improve the convergence of the method. We test our method on a variety of
benchmark tasks, demonstrating data-efficiency that is competitive with PILCO,
while being able to optimize complex neural network controllers. Finally, we
assess the performance of the algorithm for learning motor controllers for a
six legged autonomous underwater vehicle. This demonstrates the potential of
the algorithm for scaling up the dimensionality and dataset sizes, in more
complex control tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Higuera_J/0/1/0/all/0/1&quot;&gt;Juan Camilo Gamboa Higuera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1&quot;&gt;David Meger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1&quot;&gt;Gregory Dudek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05049">
<title>Fractal AI: A fragile theory of intelligence. (arXiv:1803.05049v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05049</link>
<description rdf:parseType="Literal">&lt;p&gt;Fractal AI is a theory for general artificial intelligence. It allows
deriving new mathematical tools that constitute the foundations for a new kind
of stochastic calculus, by modelling information using cellular automaton-like
structures instead of smooth functions.
&lt;/p&gt;
&lt;p&gt;In the repository included we are presenting a new Agent, derived from the
first principles of the theory, which is capable of solving Atari games several
orders of magnitude more efficiently than other similar techniques, like Monte
Carlo Tree Search.
&lt;/p&gt;
&lt;p&gt;The code provided shows how it is now possible to beat some of the current
State of The Art benchmarks on Atari games, without previous learning and using
less than 1000 samples to calculate each one of the actions when standard MCTS
uses 3 Million samples. Among other things, Fractal AI makes it possible to
generate a huge database of top performing examples with a very little amount
of computation required, transforming Reinforcement Learning into a supervised
problem.
&lt;/p&gt;
&lt;p&gt;The algorithm presented is capable of solving the exploration vs exploitation
dilemma on both the discrete and continuous cases, while maintaining control
over any aspect of the behaviour of the Agent. From a general approach, new
techniques presented here have direct applications to other areas such as
Non-equilibrium thermodynamics, chemistry, quantum physics, economics,
information theory, and non-linear control theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerezo_S/0/1/0/all/0/1&quot;&gt;Sergio Hernandez Cerezo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballester_G/0/1/0/all/0/1&quot;&gt;Guillem Duran Ballester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07281">
<title>ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech. (arXiv:1807.07281v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07281</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an alternative solution for parallel wave generation
by WaveNet. In contrast to parallel WaveNet (Oord et al., 2018), we distill a
Gaussian inverse autoregressive flow from the autoregressive WaveNet by
minimizing a novel regularized KL divergence between their highly-peaked output
distributions. Our method computes the KL divergence in closed-form, which
simplifies the training algorithm and provides very efficient distillation. In
addition, we propose the first text-to-wave neural architecture for speech
synthesis, which is fully convolutional and enables fast end-to-end training
from scratch. It significantly outperforms the previous pipeline that connects
a text-to-spectrogram model to a separately trained WaveNet (Ping et al.,
2018). We also successfully distill a parallel waveform synthesizer conditioned
on the hidden representation in this end-to-end model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kainan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jitong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10755">
<title>A writer-independent approach for offline signature verification using deep convolutional neural networks features. (arXiv:1807.10755v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.10755</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of features extracted using a deep convolutional neural network (CNN)
combined with a writer-dependent (WD) SVM classifier resulted in significant
improvement in performance of handwritten signature verification (HSV) when
compared to the previous state-of-the-art methods. In this work it is
investigated whether the use of these CNN features provide good results in a
writer-independent (WI) HSV context, based on the dichotomy transformation
combined with the use of an SVM writer-independent classifier. The experiments
performed in the Brazilian and GPDS datasets show that (i) the proposed
approach outperformed other WI-HSV methods from the literature, (ii) in the
global threshold scenario, the proposed approach was able to outperform the
writer-dependent method with CNN features in the Brazilian dataset, (iii) in an
user threshold scenario, the results are similar to those obtained by the
writer-dependent method with CNN features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_V/0/1/0/all/0/1&quot;&gt;Victor L. F. Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1&quot;&gt;Adriano L. I. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10787">
<title>One-Shot Optimal Topology Generation through Theory-Driven Machine Learning. (arXiv:1807.10787v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10787</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a theory-driven mechanism for learning a neural network model
that performs generative topology design in one shot given a problem setting,
circumventing the conventional iterative procedure that computational design
tasks usually entail. The proposed mechanism can lead to machines that quickly
response to new design requirements based on its knowledge accumulated through
past experiences of design generation. Achieving such a mechanism through
supervised learning would require an impractically large amount of
problem-solution pairs for training, due to the known limitation of deep neural
networks in knowledge generalization. To this end, we introduce an interaction
between a student (the neural network) and a teacher (the optimality conditions
underlying topology optimization): The student learns from existing data and is
tested on unseen problems. Deviation of the student&apos;s solutions from the
optimality conditions is quantified, and used to choose new data points for the
student to learn from. We show through a compliance minimization problem that
the proposed learning mechanism is significantly more data efficient than using
a static dataset under the same computational budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cang_R/0/1/0/all/0/1&quot;&gt;Ruijin Cang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hope Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yi Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10875">
<title>TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. (arXiv:1807.10875v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.10875</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are notoriously difficult to interpret and debug.
This is particularly true of neural networks. In this work, we introduce
automated software testing techniques for neural networks that are well-suited
to discovering errors which occur only for rare inputs. Specifically, we
develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF,
random mutations of inputs to a neural network are guided by a coverage metric
toward the goal of satisfying user-specified constraints. We describe how fast
approximate nearest neighbor algorithms can provide this coverage metric. We
then discuss the application of CGF to the following goals: finding numerical
errors in trained neural networks, generating disagreements between neural
networks and quantized versions of those networks, and surfacing undesirable
behavior in character level language models. Finally, we release an open source
library called TensorFuzz that implements the described techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Odena_A/0/1/0/all/0/1&quot;&gt;Augustus Odena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11074">
<title>Visual Analogies between Atari Games for Studying Transfer Learning in RL. (arXiv:1807.11074v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11074</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we ask the following question: Can visual analogies, learned in
an unsupervised way, be used in order to transfer knowledge between pairs of
games and even play one game using an agent trained for another game? We
attempt to answer this research question by creating visual analogies between a
pair of games: a source game and a target game. For example, given a video
frame in the target game, we map it to an analogous state in the source game
and then attempt to play using a trained policy learned for the source game. We
demonstrate convincing visual mapping between four pairs of games (eight
mappings), which are used to evaluate three transfer learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sobol_D/0/1/0/all/0/1&quot;&gt;Doron Sobol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taigman_Y/0/1/0/all/0/1&quot;&gt;Yaniv Taigman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11156">
<title>Transformationally Identical and Invariant Convolutional Neural Networks by Combining Symmetric Operations or Input Vectors. (arXiv:1807.11156v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11156</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformationally invariant processors constructed by transformed input
vectors or operators have been suggested and applied to many applications. In
this study, transformationally identical processing based on combining results
of all sub-processes with corresponding transformations either at the final
processing step or at the beginning step were found to be equivalent through a
special algebraical operation property. This technique can be applied to most
convolutional neural network (CNN) systems. Specifically, a transformationally
identical CNN system can be constructed by running internally symmetric
operations in parallel with the same transformation family followed by a
flatten layer with weights sharing among their corresponding transformation
elements. Such a CNN can output the same result with any transformation version
of the original input vector. Interestingly, we found that this type of
transformationally identical CNN system by combining symmetric operations at
the flatten layer is mathematically equivalent to an ordinary CNN but combining
all transformation versions of the input vector at the input layer. Since the
former is computationally demanding, its equivalent with greatly simplified
implementation is suggested
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1&quot;&gt;ShihChung B. Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1&quot;&gt;Matthew T. Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_S/0/1/0/all/0/1&quot;&gt;Seong K. Mun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11158">
<title>Robust Student Network Learning. (arXiv:1807.11158v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11158</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks bring in impressive accuracy in various applications,
but the success often relies on the heavy network architecture. Taking
well-trained heavy networks as teachers, classical teacher-student learning
paradigm aims to learn a student network that is lightweight yet accurate. In
this way, a portable student network with significantly fewer parameters can
achieve a considerable accuracy which is comparable to that of teacher network.
However, beyond accuracy, robustness of the learned student network against
perturbation is also essential for practical uses. Existing teacher-student
learning frameworks mainly focus on accuracy and compression ratios, but ignore
the robustness. In this paper, we make the student network produce more
confident predictions with the help of the teacher network, and analyze the
lower bound of the perturbation that will destroy the confidence of the student
network. Two important objectives regarding prediction scores and gradients of
examples are developed to maximize this lower bound, so as to enhance the
robustness of the student network without sacrificing the performance.
Experiments on benchmark datasets demonstrate the efficiency of the proposed
approach to learn robust student networks which have satisfying accuracy and
compact sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shiyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11205">
<title>Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes. (arXiv:1807.11205v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11205</link>
<description rdf:parseType="Literal">&lt;p&gt;Synchronized stochastic gradient descent (SGD) optimizers with data
parallelism are widely used in training large-scale deep neural networks.
Although using larger mini-batch sizes can improve the system scalability by
reducing the communication-to-computation ratio, it may hurt the generalization
ability of the models. To this end, we build a highly scalable deep learning
training system for dense GPU clusters with three main contributions: (1) We
propose a mixed-precision training method that significantly improves the
training throughput of a single GPU without losing accuracy. (2) We propose an
optimization approach for extremely large mini-batch size (up to 64k) that can
train CNN models on the ImageNet dataset without losing accuracy. (3) We
propose highly optimized all-reduce algorithms that achieve up to 3x and 11x
speedup on AlexNet and ResNet-50 respectively than NCCL-based training on a
cluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the
state-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes
and achieved 74.9\% top-1 test accuracy, and another KNL-based system with 2048
Intel KNLs spent 20 minutes and achieved 75.4\% accuracy. Our training system
can achieve 75.8\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40
GPUs. When training AlexNet with 95 epochs, our system can achieve 58.7\% top-1
test accuracy within 4 minutes, which also outperforms all other existing
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xianyan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shutao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yangzihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_H/0/1/0/all/0/1&quot;&gt;Haidong Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Feihu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Liqiang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuanzhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Liwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tiegang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1&quot;&gt;Guangxiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shaohuai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiaowen Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11346">
<title>Dropout-GAN: Learning from a Dynamic Ensemble of Discriminators. (arXiv:1807.11346v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11346</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to incorporate adversarial dropout in generative multi-adversarial
networks, by omitting or dropping out, the feedback of each discriminator in
the framework with some probability at the end of each batch. Our approach
forces the single generator not to constrain its output to satisfy a single
discriminator, but, instead, to satisfy a dynamic ensemble of discriminators.
We show that this leads to a more generalized generator, promoting variety in
the generated samples and avoiding the common mode collapse problem commonly
experienced with generative adversarial networks (GANs). We further provide
evidence that the proposed framework, named Dropout-GAN, promotes sample
diversity both within and across epochs, eliminating mode collapse and
stabilizing training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordido_G/0/1/0/all/0/1&quot;&gt;Gon&amp;#xe7;alo Mordido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haojin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1&quot;&gt;Christoph Meinel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11374">
<title>Weakly-Supervised Deep Learning of Heat Transport via Physics Informed Loss. (arXiv:1807.11374v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.11374</link>
<description rdf:parseType="Literal">&lt;p&gt;In typical machine learning tasks and applications, it is necessary to obtain
or create large labeled datasets in order to to achieve high performance.
Unfortunately, large labeled datasets are not always available and can be
expensive to source, creating a bottleneck towards more widely applicable
machine learning. The paradigm of weak supervision offers an alternative that
allows for integration of domain-specific knowledge by enforcing constraints
that a correct solution to the learning problem will obey over the output
space. In this work, we explore the application of this paradigm to 2-D
physical systems governed by non-linear differential equations. We demonstrate
that knowledge of the partial differential equations governing a system can be
encoded into the loss function of a neural network via an appropriately chosen
convolutional kernel. We demonstrate this by showing that the steady-state
solution to the 2-D heat equation can be learned directly from initial
conditions by a convolutional neural network, in the absence of labeled
training data. We also extend recent work in the progressive growing of fully
convolutional networks to achieve high accuracy (&amp;lt; 1.5% error) at multiple
scales of the heat-flow problem, including at the very large scale (1024x1024).
Finally, we demonstrate that this method can be used to speed up exact
calculation of the solution to the differential equations via finite
difference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;RIshi Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Farimani_A/0/1/0/all/0/1&quot;&gt;Amir Barati Farimani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gomes_J/0/1/0/all/0/1&quot;&gt;Joe Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eastman_P/0/1/0/all/0/1&quot;&gt;Peter Eastman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11393">
<title>Making Classifier Chains Resilient to Class Imbalance. (arXiv:1807.11393v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11393</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance is an intrinsic characteristic of multi-label data. Most of
the labels in multi-label data sets are associated with a small number of
training examples, much smaller compared to the size of the data set. Class
imbalance poses a key challenge that plagues most multi-label learning methods.
Ensemble of Classifier Chains (ECC), one of the most prominent multi-label
learning methods, is no exception to this rule, as each of the binary models it
builds is trained from all positive and negative examples of a label. To make
ECC resilient to class imbalance, we first couple it with random undersampling.
We then present two extensions of this basic approach, where we build a varying
number of binary models per label and construct chains of different sizes, in
order to improve the exploitation of majority examples with approximately the
same computational budget. Experimental results on 16 multi-label datasets
demonstrate the effectiveness of the proposed approaches in a variety of
evaluation metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1&quot;&gt;Grigorios Tsoumakas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11414">
<title>Faster Convergence &amp; Generalization in DNNs. (arXiv:1807.11414v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11414</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have gained tremendous popularity in last few years.
They have been applied for the task of classification in almost every domain.
Despite the success, deep networks can be incredibly slow to train for even
moderate sized models on sufficiently large datasets. Additionally, these
networks require large amounts of data to be able to generalize. The importance
of speeding up convergence, and generalization in deep networks can not be
overstated. In this work, we develop an optimization algorithm based on
generalized-optimal updates derived from minibatches that lead to faster
convergence. Towards the end, we demonstrate on two benchmark datasets that the
proposed method achieves two orders of magnitude speed up over traditional
back-propagation, and is more robust to noise/over-fitting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gaurav Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1&quot;&gt;John Shawe-Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11459">
<title>Improving Transferability of Deep Neural Networks. (arXiv:1807.11459v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11459</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from small amounts of labeled data is a challenge in the area of
deep learning. This is currently addressed by Transfer Learning where one
learns the small data set as a transfer task from a larger source dataset.
Transfer Learning can deliver higher accuracy if the hyperparameters and source
dataset are chosen well. One of the important parameters is the learning rate
for the layers of the neural network. We show through experiments on the
ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range
of 127% can be obtained by proper choice of learning rates. We also show that
the images/label parameter for a dataset can potentially be used to determine
optimal learning rates for the layers to get the best overall accuracy. We
additionally validate this method on a sample of real-world image
classification tasks from a public visual recognition API.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dube_P/0/1/0/all/0/1&quot;&gt;Parijat Dube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_B/0/1/0/all/0/1&quot;&gt;Bishwaranjan Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petit_Bois_E/0/1/0/all/0/1&quot;&gt;Elisabeth Petit-Bois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_M/0/1/0/all/0/1&quot;&gt;Matthew Hill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08259">
<title>Fast and Accurate Inference with Adaptive Ensemble Prediction for Deep Neural Networks. (arXiv:1702.08259v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08259</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensembling multiple predictions is a widely-used technique to improve the
accuracy of various machine learning tasks. One obvious drawback of the
ensembling is its higher execution cost during inference. In this paper, we
first describe our insights on relationship between the probability of the
prediction and the effect of ensembling with current deep neural networks;
ensembling does not help mispredictions for inputs predicted with a high
probability even when there is a non-negligible number of mispredicted inputs.
This finding motivates us to develop a new technique called adaptive ensemble
prediction, which achieves the benefits of ensembling with much smaller
additional execution costs. If the prediction for an input reaches a high
enough probability on the basis of the confidence level, we stop ensembling for
this input to avoid wasting computation power. We evaluated the adaptive
ensembling by using various datasets and showed that it reduces the computation
cost significantly while achieving similar accuracy to the naive ensembling. We
also showed that our statistically rigorous confidence-level-based termination
condition reduces the burden of the task-dependent parameter tuning compared to
the naive termination based on the pre-defined threshold in addition to
yielding a better accuracy with the same cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inoue_H/0/1/0/all/0/1&quot;&gt;Hiroshi Inoue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07469">
<title>DGM: A deep learning algorithm for solving partial differential equations. (arXiv:1708.07469v4 [q-fin.MF] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07469</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional PDEs have been a longstanding computational challenge. We
propose to solve high-dimensional PDEs by approximating the solution with a
deep neural network which is trained to satisfy the differential operator,
initial condition, and boundary conditions. Our algorithm is meshfree, which is
key since meshes become infeasible in higher dimensions. Instead of forming a
mesh, the neural network is trained on batches of randomly sampled time and
space points. The algorithm is tested on a class of high-dimensional free
boundary PDEs, which we are able to accurately solve in up to $200$ dimensions.
The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE
and Burgers&apos; equation. The deep learning algorithm approximates the general
solution to the Burgers&apos; equation for a continuum of different boundary
conditions and physical conditions (which can be viewed as a high-dimensional
space). We call the algorithm a &quot;Deep Galerkin Method (DGM)&quot; since it is
similar in spirit to Galerkin methods, with the solution approximated by a
neural network instead of a linear combination of basis functions. In addition,
we prove a theorem regarding the approximation power of neural networks for a
class of quasilinear parabolic PDEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sirignano_J/0/1/0/all/0/1&quot;&gt;Justin Sirignano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Spiliopoulos_K/0/1/0/all/0/1&quot;&gt;Konstantinos Spiliopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04300">
<title>Neural Conditional Gradients. (arXiv:1803.04300v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04300</link>
<description rdf:parseType="Literal">&lt;p&gt;The move from hand-designed to learned optimizers in machine learning has
been quite successful for gradient-based and -free optimizers. When facing a
constrained problem, however, maintaining feasibility typically requires a
projection step, which might be computationally expensive and not
differentiable. We show how the design of projection-free convex optimization
algorithms can be cast as a learning problem based on Frank-Wolfe Networks:
recurrent networks implementing the Frank-Wolfe algorithm aka. conditional
gradients. This allows them to learn to exploit structure when, e.g.,
optimizing over rank-1 matrices. Our LSTM-learned optimizers outperform
hand-designed as well learned but unconstrained ones. We demonstrate this for
training support vector machines and softmax classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1&quot;&gt;Christian Bauckhage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09871">
<title>Confidence region of singular vectors for high-dimensional and low-rank matrix regression. (arXiv:1805.09871v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09871</link>
<description rdf:parseType="Literal">&lt;p&gt;Let ${\bf M}\in\mathbb{R}^{m_1\times m_2}$ be an unknown matrix with $r={\rm
rank}({\bf M})\ll \min(m_1,m_2)$ whose thin singular value decomposition is
denoted by ${\bf M}={\bf U}{\bf \Lambda}{\bf V}^{\top}$ where ${\bf
\Lambda}={\rm diag}(\lambda_1,\cdots,\lambda_r)$ contains its non-increasing
singular values. Low rank matrix regression refers to instances of estimating
${\bf M}$ from $n$ i.i.d. copies of random pair $\{({\bf X}, y)\}$ where ${\bf
X}\in\mathbb{R}^{m_1\times m_2}$ is a random measurement matrix and
$y\in\mathbb{R}$ is a noisy output satisfying $y={\rm tr}({\bf M}^{\top}{\bf
X})+\xi$ with $\xi$ being stochastic error independent of ${\bf X}$. The goal
of this paper is to construct efficient estimator (denoted by $\hat{\bf U}$ and
$\hat{\bf V}$) and confidence region of ${\bf U}$ and ${\bf V}$. In particular,
we characterize the distribution of $$ {\rm dist}^2\big[(\hat{\bf U},\hat{\bf
V}), ({\bf U},{\bf V})\big]=\|\hat{\bf U}\hat{\bf U}^{\top}-{\bf U}{\bf
U}^{\top}\|_{\rm F}^2+\|\hat{\bf V}\hat{\bf V}^{\top}-{\bf V}{\bf
V}^{\top}\|_{\rm F}^2. $$ We prove the asymptotic normality of properly
centered and normalized ${\rm dist}^2\big[(\hat{\bf U},\hat{\bf V}), ({\bf
U},{\bf V})\big]$ with data-dependent centering and normalization when
$r^{5/2}(m_1+m_2)^{3/2}=o(n/\log n)$, based on which confidence region of ${\bf
U}$ and ${\bf V}$ is constructed achieving any pre-determined confidence level
asymptotically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xia_D/0/1/0/all/0/1&quot;&gt;Dong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12338">
<title>Hallucinating robots: Inferring Obstacle Distances from Partial Laser Measurements. (arXiv:1805.12338v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12338</link>
<description rdf:parseType="Literal">&lt;p&gt;Many mobile robots rely on 2D laser scanners for localization, mapping, and
navigation. However, those sensors are unable to correctly provide distance to
obstacles such as glass panels and tables whose actual occupancy is invisible
at the height the sensor is measuring. In this work, instead of estimating the
distance to obstacles from richer sensor readings such as 3D lasers or RGBD
sensors, we present a method to estimate the distance directly from raw 2D
laser data. To learn a mapping from raw 2D laser distances to obstacle
distances we frame the problem as a learning task and train a neural network
formed as an autoencoder. A novel configuration of network hyperparameters is
proposed for the task at hand and is quantitatively validated on a test set.
Finally, we qualitatively demonstrate in real time on a Care-O-bot 4 that the
trained network can successfully infer obstacle distances from partial 2D laser
readings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lundell_J/0/1/0/all/0/1&quot;&gt;Jens Lundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verdoja_F/0/1/0/all/0/1&quot;&gt;Francesco Verdoja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1&quot;&gt;Ville Kyrki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05413">
<title>Learning Dynamics of Linear Denoising Autoencoders. (arXiv:1806.05413v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05413</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising autoencoders (DAEs) have proven useful for unsupervised
representation learning, but a thorough theoretical understanding is still
lacking of how the input noise influences learning. Here we develop theory for
how noise influences learning in DAEs. By focusing on linear DAEs, we are able
to derive analytic expressions that exactly describe their learning dynamics.
We verify our theoretical predictions with simulations as well as experiments
on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise
allows DAEs to ignore low variance directions in the inputs while learning to
reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs
to standard regularised autoencoders, we show that noise has a similar
regularisation effect to weight decay, but with faster training dynamics. We
also show that our theoretical predictions approximate learning dynamics on
real-world data and qualitatively match observed dynamics in nonlinear DAEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kroon_S/0/1/0/all/0/1&quot;&gt;Steve Kroon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kamper_H/0/1/0/all/0/1&quot;&gt;Herman Kamper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08925">
<title>Anomaly detection in static networks using egonets. (arXiv:1807.08925v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08925</link>
<description rdf:parseType="Literal">&lt;p&gt;Network data has rapidly emerged as an important and active area of
statistical methodology. In this paper we consider the problem of anomaly
detection in networks. Given a large background network, we seek to detect
whether there is a small anomalous subgraph present in the network, and if such
a subgraph is present, which nodes constitute the subgraph. We propose an
inferential tool based on egonets to answer this question. The proposed method
is computationally efficient and naturally amenable to parallel computing, and
easily extends to a wide variety of network models. We demonstrate through
simulation studies that the egonet method works well under a wide variety of
network models. We obtain some fascinating empirical results by applying the
egonet method on several well-studied benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Srijan Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10668">
<title>On the overfly algorithm in deep learning of neural networks. (arXiv:1807.10668v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10668</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we investigate the supervised backpropagation training of
multilayer neural networks from a dynamical systems point of view. We discuss
some links with the qualitative theory of differential equations and introduce
the overfly algorithm to tackle the local minima problem. Our approach is based
on the existence of first integrals of the generalised gradient system with
build-in dissipation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsygvintsev_A/0/1/0/all/0/1&quot;&gt;Alexei Tsygvintsev&lt;/a&gt;</dc:creator>
</item></rdf:RDF>