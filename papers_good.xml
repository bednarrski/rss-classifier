<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04683"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04708"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04797"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05053"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04682"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04705"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04720"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04828"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04913"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05014"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.08098"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01132"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02727"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.04849">
<title>The unreasonable effectiveness of the forget gate. (arXiv:1804.04849v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04849</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the success of the gated recurrent unit, a natural question is whether
all the gates of the long short-term memory (LSTM) network are necessary.
Previous research has shown that the forget gate is one of the most important
gates in the LSTM. Here we show that a forget-gate-only version of the LSTM
with chrono-initialized biases, not only provides computational savings but
outperforms the standard LSTM on multiple benchmark datasets and competes with
some of the best contemporary models. Our proposed network, the JANET, achieves
accuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the
standard LSTM which yields accuracies of 98.5% and 91%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westhuizen_J/0/1/0/all/0/1&quot;&gt;Jos van der Westhuizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1&quot;&gt;Joan Lasenby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04673">
<title>A Unified Batch Online Learning Framework for Click Prediction. (arXiv:1809.04673v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04673</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a unified framework for Batch Online Learning (OL) for Click
Prediction in Search Advertisement. Machine Learning models once deployed, show
non-trivial accuracy and calibration degradation over time due to model
staleness. It is therefore necessary to regularly update models, and do so
automatically. This paper presents two paradigms of Batch Online Learning, one
which incrementally updates the model parameters via an early stopping
mechanism, and another which does so through a proximal regularization. We
argue how both these schemes naturally trade-off between old and new data. We
then theoretically and empirically show that these two seemingly different
schemes are closely related. Through extensive experiments, we demonstrate the
utility of of our OL framework; how the two OL schemes relate to each other and
how they trade-off between the new and historical data. We then compare batch
OL to full model retrains, and show how online learning is more robust to data
issues. We also demonstrate the long term impact of Online Learning, the role
of the initial Models in OL, the impact of delays in the update, and finally
conclude with some implementation details and challenges in deploying a real
world online learning system in production. While this paper mostly focuses on
application of click prediction for search advertisement, we hope that the
lessons learned here can be carried over to other problem domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1&quot;&gt;Rishabh Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_N/0/1/0/all/0/1&quot;&gt;Nimit Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bompada_T/0/1/0/all/0/1&quot;&gt;Tanuja Bompada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_D/0/1/0/all/0/1&quot;&gt;Denis Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1&quot;&gt;Eren Manavoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04683">
<title>SAFE: A Neural Survival Analysis Model for Fraud Early Detection. (arXiv:1809.04683v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04683</link>
<description rdf:parseType="Literal">&lt;p&gt;Many online platforms have deployed anti-fraud systems to detect and prevent
fraudster activities. However, there is usually a gap between the time that a
user commits a fraudulent action and the time that the user is suspended by the
platform. How to detect fraudsters in time is a challenging problem. Most of
the existing approaches adopt classifiers to predict fraudsters given their
activity sequences along time. The main drawback of classification models is
that the prediction results between consecutive timestamps are often
inconsistent. In this paper, we propose a survival analysis based fraud early
detection model, SAFE, that maps dynamic user activities to survival
probabilities that are guaranteed to be monotonically decreasing along time.
SAFE adopts recurrent neural network (RNN) to handle user activity sequences
and directly outputs hazard values at each timestamp, and then, survival
probability derived from hazard values is deployed to achieve consistent
predictions. Because we only observe in the training data the user suspended
time instead of the fraudulent activity time, we revise the loss function of
the regular survival model to achieve fraud early detection. Experimental
results on two real world datasets demonstrate that SAFE outperforms both the
survival analysis model and recurrent neural network model alone as well as
state-of-the-art fraud early detection approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1&quot;&gt;Panpan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Shuhan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04684">
<title>Fair lending needs explainable models for responsible recommendation. (arXiv:1809.04684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04684</link>
<description rdf:parseType="Literal">&lt;p&gt;The financial services industry has unique explainability and fairness
challenges arising from compliance and ethical considerations in credit
decisioning. These challenges complicate the use of model machine learning and
artificial intelligence methods in business decision processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiahao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04686">
<title>Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation. (arXiv:1809.04686v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.04686</link>
<description rdf:parseType="Literal">&lt;p&gt;Transferring representations from large supervised tasks to downstream tasks
has shown promising results in AI fields such as Computer Vision and Natural
Language Processing (NLP). In parallel, the recent progress in Machine
Translation (MT) has enabled one to train multilingual Neural MT (NMT) systems
that can translate between multiple languages and are also capable of
performing zero-shot translation. However, little attention has been paid to
leveraging representations learned by a multilingual NMT system to enable
zero-shot multilinguality in other NLP tasks. In this paper, we demonstrate a
simple framework, a multilingual Encoder-Classifier, for cross-lingual transfer
learning by reusing the encoder from a multilingual NMT system and stitching it
with a task-specific classifier component. Our proposed model achieves
significant improvements in the English setup on three benchmark tasks - Amazon
Reviews, SST and SNLI. Further, our system can perform classification in a new
language for which no classification data was seen during training, showing
that zero-shot classification is possible and remarkably competitive. In order
to understand the underlying factors contributing to this finding, we conducted
a series of analyses on the effect of the shared vocabulary, the training data
type for NMT, classifier complexity, encoder representation power, and model
generalization on zero-shot performance. Our results provide strong evidence
that the representations learned from multilingual NMT systems are widely
applicable across languages and tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eriguchi_A/0/1/0/all/0/1&quot;&gt;Akiko Eriguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Melvin Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazawa_H/0/1/0/all/0/1&quot;&gt;Hideto Kazawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1&quot;&gt;Wolfgang Macherey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04708">
<title>Semantically Enhanced Models for Commonsense Knowledge Acquisition. (arXiv:1809.04708v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.04708</link>
<description rdf:parseType="Literal">&lt;p&gt;Commonsense knowledge is paramount to enable intelligent systems. Typically,
it is characterized as being implicit and ambiguous, hindering thereby the
automation of its acquisition. To address these challenges, this paper presents
semantically enhanced models to enable reasoning through resolving part of
commonsense ambiguity. The proposed models enhance in a knowledge graph
embedding (KGE) framework for knowledge base completion. Experimental results
show the effectiveness of the new semantic models in commonsense reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alhussien_I/0/1/0/all/0/1&quot;&gt;Ikhlas Alhussien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+NengSheng_Z/0/1/0/all/0/1&quot;&gt;Zhang NengSheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04771">
<title>Towards Coinductive Theory Exploration in Horn Clause Logic: Position Paper. (arXiv:1809.04771v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1809.04771</link>
<description rdf:parseType="Literal">&lt;p&gt;Coinduction occurs in two guises in Horn clause logic: in proofs of
self-referencing properties and relations, and in proofs involving construction
of (possibly irregular) infinite data. Both instances of coinductive reasoning
appeared in the literature before, but a systematic analysis of these two kinds
of proofs and of their relation was lacking. We propose a general
proof-theoretic framework for handling both kinds of coinduction arising in
Horn clause logic. To this aim, we propose a coinductive extension of Miller et
al&apos;s framework of uniform proofs and prove its soundness relative to
coinductive models of Horn clause logic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dr_E/0/1/0/all/0/1&quot;&gt;Ekaterina Komendantskaya Dr&lt;/a&gt; (Heriot-Watt University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yue Li&lt;/a&gt; (Heriot-Watt University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04797">
<title>Focus Group on Artificial Intelligence for Health. (arXiv:1809.04797v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.04797</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) - the phenomenon of machines being able to solve
problems that require human intelligence - has in the past decade seen an
enormous rise of interest due to significant advances in effectiveness and use.
The health sector, one of the most important sectors for societies and
economies worldwide, is particularly interesting for AI applications, given the
ongoing digitalisation of all types of health information. The potential for AI
assistance in the health domain is immense, because AI can support medical
decision making at reduced costs, everywhere. However, due to the complexity of
AI algorithms, it is difficult to distinguish good from bad AI-based solutions
and to understand their strengths and weaknesses, which is crucial for
clarifying responsibilities and for building trust. For this reason, the
International Telecommunication Union (ITU) has established a new Focus Group
on &quot;Artificial Intelligence for Health&quot; (FG-AI4H) in partnership with the World
Health Organization (WHO). Health and care services are usually the
responsibility of a government - even when provided through private insurance
systems - and thus under the responsibility of WHO/ITU member states. FG-AI4H
will identify opportunities for international standardization, which will
foster the application of AI to health issues on a global scale. In particular,
it will establish a standardized assessment framework with open benchmarks for
the evaluation of AI-based methods for health, such as AI-based diagnosis,
triage or treatment decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salathe_M/0/1/0/all/0/1&quot;&gt;Marcel Salath&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiegand_T/0/1/0/all/0/1&quot;&gt;Thomas Wiegand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenzel_M/0/1/0/all/0/1&quot;&gt;Markus Wenzel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04983">
<title>Part-based Graph Convolutional Network for Action Recognition. (arXiv:1809.04983v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.04983</link>
<description rdf:parseType="Literal">&lt;p&gt;Human actions comprise of joint motion of articulated body parts or
`gestures&apos;. Human skeleton is intuitively represented as a sparse graph with
joints as nodes and natural connections between them as edges. Graph
convolutional networks have been used to recognize actions from skeletal
videos. We introduce a part-based graph convolutional network (PB-GCN) for this
task, inspired by Deformable Part-based Models (DPMs). We divide the skeleton
graph into four subgraphs with joints shared across them and learn a
recognition model using a part-based graph convolutional network. We show that
such a model improves performance of recognition, compared to a model using
entire skeleton graph. Instead of using 3D joint coordinates as node features,
we show that using relative coordinates and temporal displacements boosts
performance. Our model achieves state-of-the-art performance on two challenging
benchmark datasets NTURGB+D and HDM05, for skeletal action recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakkar_K/0/1/0/all/0/1&quot;&gt;Kalpit Thakkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1&quot;&gt;P J Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05053">
<title>XNLI: Evaluating Cross-lingual Sentence Representations. (arXiv:1809.05053v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.05053</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art natural language processing systems rely on supervision in
the form of annotated data to learn competent models. These models are
generally trained on data in a single language (usually English), and cannot be
directly used beyond that language. Since collecting data in every language is
not realistic, there has been a growing interest in cross-lingual language
understanding (XLU) and low-resource cross-language transfer. In this work, we
construct an evaluation set for XLU by extending the development and test sets
of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15
languages, including low-resource languages such as Swahili and Urdu. We hope
that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence
understanding by providing an informative standard evaluation task. In
addition, we provide several baselines for multilingual sentence understanding,
including two based on machine translation systems, and two that use parallel
data to train aligned multilingual bag-of-words and LSTM encoders. We find that
XNLI represents a practical and challenging evaluation suite, and that directly
translating the test data yields the best performance among available
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1&quot;&gt;Alexis Conneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lample_G/0/1/0/all/0/1&quot;&gt;Guillaume Lample&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinott_R/0/1/0/all/0/1&quot;&gt;Ruty Rinott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Adina Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1&quot;&gt;Holger Schwenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1&quot;&gt;Veselin Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05068">
<title>Learning Shape Priors for Single-View 3D Completion and Reconstruction. (arXiv:1809.05068v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.05068</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of single-view 3D shape completion or reconstruction is
challenging, because among the many possible shapes that explain an
observation, most are implausible and do not correspond to natural objects.
Recent research in the field has tackled this problem by exploiting the
expressiveness of deep convolutional networks. In fact, there is another level
of ambiguity that is often overlooked: among plausible shapes, there are still
multiple shapes that fit the 2D image equally well; i.e., the ground truth
shape is non-deterministic given a single-view input. Existing fully supervised
approaches fail to address this issue, and often produce blurry mean shapes
with smooth surfaces but no fine details.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose ShapeHD, pushing the limit of single-view shape
completion and reconstruction by integrating deep generative models with
adversarially learned shape priors. The learned priors serve as a regularizer,
penalizing the model only if its output is unrealistic, not if it deviates from
the ground truth. Our design thus overcomes both levels of ambiguity
aforementioned. Experiments demonstrate that ShapeHD outperforms state of the
art by a large margin in both shape completion and shape reconstruction on
multiple real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengkai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1&quot;&gt;William T. Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00955">
<title>Toward Controlled Generation of Text. (arXiv:1703.00955v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00955</link>
<description rdf:parseType="Literal">&lt;p&gt;Generic generation and manipulation of text is challenging and has limited
success compared to recent deep generative modeling in visual domain. This
paper aims at generating plausible natural language sentences, whose attributes
are dynamically controlled by learning disentangled latent representations with
designated semantics. We propose a new neural generative model which combines
variational auto-encoders and holistic attribute discriminators for effective
imposition of semantic structures. With differentiable approximation to
discrete text samples, explicit constraints on independent attribute controls,
and efficient collaborative learning of generator and discriminators, our model
learns highly interpretable representations from even only word annotations,
and produces realistic sentences with desired attributes. Quantitative
evaluation validates the accuracy of sentence and attribute generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02896">
<title>Recurrent Deterministic Policy Gradient Method for Bipedal Locomotion on Rough Terrain Challenge. (arXiv:1710.02896v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02896</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep learning framework that is capable of solving
partially observable locomotion tasks based on our novel interpretation of
Recurrent Deterministic Policy Gradient (RDPG). We study on bias of sampled
error measure and its variance induced by the partial observability of
environment and subtrajectory sampling, respectively. Three major improvements
are introduced in our RDPG based learning framework: tail-step bootstrap of
interpolated temporal difference, initialisation of hidden state using past
trajectory scanning, and injection of external experiences learned by other
agents. The proposed learning framework was implemented to solve the
Bipedal-Walker challenge in OpenAI&apos;s gym simulation environment where only
partial state information is available. Our simulation study shows that the
autonomous behaviors generated by the RDPG agent are highly adaptive to a
variety of obstacles and enables the agent to effectively traverse rugged
terrains for long distance with higher success rate than leading contenders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Doo Re Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chuanyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGreavy_C/0/1/0/all/0/1&quot;&gt;Christopher McGreavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04623">
<title>Three Factors Influencing Minima in SGD. (arXiv:1711.04623v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04623</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the dynamical and convergent properties of stochastic gradient
descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the
relation between learning rate, batch size and the properties of the final
minima, such as width or generalization, remains an open question. In order to
tackle this problem we investigate the previously proposed approximation of SGD
by a stochastic differential equation (SDE). We theoretically argue that three
factors - learning rate, batch size and gradient covariance - influence the
minima found by SGD. In particular we find that the ratio of learning rate to
batch size is a key determinant of SGD dynamics and of the width of the final
minima, and that higher values of the ratio lead to wider minima and often
better generalization. We confirm these findings experimentally. Further, we
include experiments which show that learning rate schedules can be replaced
with batch size schedules and that the ratio of learning rate to batch size is
an important factor influencing the memorization process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1&quot;&gt;Stanis&amp;#x142;aw Jastrz&amp;#x119;bski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenton_Z/0/1/0/all/0/1&quot;&gt;Zachary Kenton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1&quot;&gt;Devansh Arpit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1&quot;&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1&quot;&gt;Amos Storkey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04682">
<title>Automatic Program Synthesis of Long Programs with a Learned Garbage Collector. (arXiv:1809.04682v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04682</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of generating automatic code given sample
input-output pairs. We train a neural network to map from the current state and
the outputs to the program&apos;s next statement. The neural network optimizes
multiple tasks concurrently: the next operation out of a set of high level
commands, the operands of the next statement, and which variables can be
dropped from memory. Using our method we are able to create programs that are
more than twice as long as existing state-of-the-art solutions, while improving
the success rate for comparable lengths, and cutting the run-time by two orders
of magnitude. Our code is publicly available at
https://github.com/amitz25/PCCoder
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zohar_A/0/1/0/all/0/1&quot;&gt;Amit Zohar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04705">
<title>Distilled Wasserstein Learning for Word Embedding and Topic Modeling. (arXiv:1809.04705v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04705</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel Wasserstein method with a distillation mechanism, yielding
joint learning of word embeddings and topics. The proposed method is based on
the fact that the Euclidean distance between word embeddings may be employed as
the underlying distance in the Wasserstein topic model. The word distributions
of topics, their optimal transports to the word distributions of documents, and
the embeddings of words are learned in a unified framework. When learning the
topic model, we leverage a distilled underlying distance matrix to update the
topic distributions and smoothly calculate the corresponding optimal
transports. Such a strategy provides the updating of word embeddings with
robust guidance, improving the algorithmic convergence. As an application, we
focus on patient admission records, in which the proposed method embeds the
codes of diseases and procedures and learns the topics of admissions, obtaining
superior performance on clinically-meaningful disease network construction,
mortality prediction as a function of admission codes, and procedure
recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongteng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04720">
<title>Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics. (arXiv:1809.04720v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04720</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning robot tasks or controllers using deep reinforcement learning has
been proven effective in simulations. Learning in simulation has several
advantages. For example, one can fully control the simulated environment,
including halting motions while performing computations. Another advantage when
robots are involved, is that the amount of time a robot is occupied learning a
task---rather than being productive---can be reduced by transferring the
learned task to the real robot. Transfer learning requires some amount of
fine-tuning on the real robot. For tasks which involve complex (non-linear)
dynamics, the fine-tuning itself may take a substantial amount of time. In
order to reduce the amount of fine-tuning we propose to learn robustified
controllers in simulation. Robustified controllers are learned by exploiting
the ability to change simulation parameters (both appearance and dynamics) for
successive training episodes. An additional benefit for this approach is that
it alleviates the precise determination of physics parameters for the
simulator, which is a non-trivial task. We demonstrate our proposed approach on
a real setup in which a robot aims to solve a maze puzzle, which involves
complex dynamics due to static friction and potentially large accelerations. We
show that the amount of fine-tuning in transfer learning for a robustified
controller is substantially reduced compared to a non-robustified controller.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1&quot;&gt;Jeroen van Baar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_A/0/1/0/all/0/1&quot;&gt;Alan Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordorel_R/0/1/0/all/0/1&quot;&gt;Radu Cordorel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Devesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romeres_D/0/1/0/all/0/1&quot;&gt;Diego Romeres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikovski_D/0/1/0/all/0/1&quot;&gt;Daniel Nikovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04828">
<title>Bayesian Structure Learning by Recursive Bootstrap. (arXiv:1809.04828v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.04828</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of Bayesian structure learning for domains with
hundreds of variables by employing non-parametric bootstrap, recursively. We
propose a method that covers both model averaging and model selection in the
same framework. The proposed method deals with the main weakness of
constraint-based learning---sensitivity to errors in the independence
tests---by a novel way of combining bootstrap with constraint-based learning.
Essentially, we provide an algorithm for learning a tree, in which each node
represents a scored CPDAG for a subset of variables and the level of the node
corresponds to the maximal order of conditional independencies that are encoded
in the graph. As higher order independencies are tested in deeper recursive
calls, they benefit from more bootstrap samples, and therefore more resistant
to the curse-of-dimensionality. Moreover, the re-use of stable low order
independencies allows greater computational efficiency. We also provide an
algorithm for sampling CPDAGs efficiently from their posterior given the
learned tree. We empirically demonstrate that the proposed algorithm scales
well to hundreds of variables, and learns better MAP models and more reliable
causal relationships between variables, than other state-of-the-art-methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rohekar_R/0/1/0/all/0/1&quot;&gt;Raanan Y. Rohekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gurwicz_Y/0/1/0/all/0/1&quot;&gt;Yaniv Gurwicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nisimov_S/0/1/0/all/0/1&quot;&gt;Shami Nisimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koren_G/0/1/0/all/0/1&quot;&gt;Guy Koren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Novik_G/0/1/0/all/0/1&quot;&gt;Gal Novik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04913">
<title>Query-Efficient Black-Box Attack by Active Learning. (arXiv:1809.04913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04913</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network (DNN) as a popular machine learning model is found to be
vulnerable to adversarial attack. This attack constructs adversarial examples
by adding small perturbations to the raw input, while appearing unmodified to
human eyes but will be misclassified by a well-trained classifier. In this
paper, we focus on the black-box attack setting where attackers have almost no
access to the underlying models. To conduct black-box attack, a popular
approach aims to train a substitute model based on the information queried from
the target DNN. The substitute model can then be attacked using existing
white-box attack approaches, and the generated adversarial examples will be
used to attack the target DNN. Despite its encouraging results, this approach
suffers from poor query efficiency, i.e., attackers usually needs to query a
huge amount of times to collect enough information for training an accurate
substitute model. To this end, we first utilize state-of-the-art white-box
attack methods to generate samples for querying, and then introduce an active
learning strategy to significantly reduce the number of queries needed.
Besides, we also propose a diversity criterion to avoid the sampling bias. Our
extensive experimental results on MNIST and CIFAR-10 show that the proposed
method can reduce more than $90\%$ of queries while preserve attacking success
rates and obtain an accurate substitute model which is more than $85\%$ similar
with the target oracle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pengcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05014">
<title>Minimax Learning of Ergodic Markov Chains. (arXiv:1809.05014v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.05014</link>
<description rdf:parseType="Literal">&lt;p&gt;We compute the finite-sample minimax (modulo logarithmic factors) sample
complexity of learning the parameters of a finite Markov chain from a single
long sequence of states. Our error metric is a natural variant of total
variation. The sample complexity necessarily depends on the spectral gap and
minimal stationary probability of the unknown chain - for which, at least in
the reversible case, there are known finite-sample estimators with fully
empirical confidence intervals. To our knowledge, this is the first PAC-type
result with nearly matching (up to logs) upper and lower bounds for learning,
in any metric in the context of Markov chains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wolfer_G/0/1/0/all/0/1&quot;&gt;Geoffrey Wolfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kontorovich_A/0/1/0/all/0/1&quot;&gt;Aryeh Kontorovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.08098">
<title>Local Uncertainty Sampling for Large-Scale Multi-Class Logistic Regression. (arXiv:1604.08098v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1604.08098</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge for building statistical models in the big data era is that
the available data volume far exceeds the computational capability. A common
approach for solving this problem is to employ a subsampled dataset that can be
handled by available computational resources. In this paper, we propose a
general subsampling scheme for large-scale multi-class logistic regression and
examine the variance of the resulting estimator. We show that asymptotically,
the proposed method always achieves a smaller variance than that of the uniform
random sampling. Moreover, when the classes are conditionally imbalanced,
significant improvement over uniform sampling can be achieved. Empirical
performance of the proposed method is compared to other methods on both
simulated and real-world datasets, and these results match and confirm our
theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kean Ming Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Ting Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06978">
<title>Deep Interest Network for Click-Through Rate Prediction. (arXiv:1706.06978v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06978</link>
<description rdf:parseType="Literal">&lt;p&gt;Click-through rate prediction is an essential task in industrial
applications, such as online advertising. Recently deep learning based models
have been proposed, which follow a similar Embedding\&amp;amp;MLP paradigm. In these
methods large scale sparse input features are first mapped into low dimensional
embedding vectors, and then transformed into fixed-length vectors in a
group-wise manner, finally concatenated together to fed into a multilayer
perceptron (MLP) to learn the nonlinear relations among features. In this way,
user features are compressed into a fixed-length representation vector, in
regardless of what candidate ads are. The use of fixed-length vector will be a
bottleneck, which brings difficulty for Embedding\&amp;amp;MLP methods to capture
user&apos;s diverse interests effectively from rich historical behaviors. In this
paper, we propose a novel model: Deep Interest Network (DIN) which tackles this
challenge by designing a local activation unit to adaptively learn the
representation of user interests from historical behaviors with respect to a
certain ad. This representation vector varies over different ads, improving the
expressive ability of model greatly. Besides, we develop two techniques:
mini-batch aware regularization and data adaptive activation function which can
help training industrial deep networks with hundreds of millions of parameters.
Experiments on two public datasets as well as an Alibaba real production
dataset with over 2 billion samples demonstrate the effectiveness of proposed
approaches, which achieve superior performance compared with state-of-the-art
methods. DIN now has been successfully deployed in the online display
advertising system in Alibaba, serving the main traffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guorui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chengru Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaoqiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Ying Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Han Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yanghui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Junqi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08374">
<title>Learning through deterministic assignment of hidden parameters. (arXiv:1803.08374v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08374</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning frequently boils down to determining hidden and bright
parameters in a parameterized hypothesis space based on finite input-output
samples. The hidden parameters determine the attributions of hidden predictors
or the nonlinear mechanism of an estimator, while the bright parameters
characterize how hidden predictors are linearly combined or the linear
mechanism. In traditional learning paradigm, hidden and bright parameters are
not distinguished and trained simultaneously in one learning process. Such an
one-stage learning (OSL) brings a benefit of theoretical analysis but suffers
from the high computational burden. To overcome this difficulty, a two-stage
learning (TSL) scheme, featured by learning through deterministic assignment of
hidden parameters (LtDaHP) was proposed, which suggests to deterministically
generate the hidden parameters by using minimal Riesz energy points on a sphere
and equally spaced points in an interval. We theoretically show that with such
deterministic assignment of hidden parameters, LtDaHP with a neural network
realization almost shares the same generalization performance with that of OSL.
We also present a series of simulations and application examples to support the
outperformance of LtDaHP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jian Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shaobo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zongben Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01132">
<title>Generalized Spectral Mixture Kernels for Multi-Task Gaussian Processes. (arXiv:1808.01132v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01132</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Task Gaussian processes (MTGPs) have shown a significant progress both
in expressiveness and interpretation of the relatedness between different
tasks: from linear combinations of independent single-output Gaussian processes
(GPs), through the direct modeling of the cross-covariances such as spectral
mixture kernels with phase shift, to the design of multivariate covariance
functions based on spectral mixture kernels which model delays among tasks in
addition to phase differences, and which provide a parametric interpretation of
the relatedness across tasks. In this paper we further extend expressiveness
and interpretability of MTGPs models and introduce a new family of kernels
capable to model nonlinear correlations between tasks as well as dependencies
between spectral mixtures, including time and phase delay. Specifically, we use
generalized convolution spectral mixture kernels for modeling dependencies at
spectral mixture level, and coupling coregionalization for discovering task
level correlations. The proposed kernels for MTGP are validated on artificial
data and compared with existing MTGPs methods on three real-world experiments.
Results indicate the benefits of our more expressive representation with
respect to performance and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_P/0/1/0/all/0/1&quot;&gt;Perry Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02727">
<title>Decentralized Differentially Private Without-Replacement Stochastic Gradient Descent. (arXiv:1809.02727v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02727</link>
<description rdf:parseType="Literal">&lt;p&gt;While machine learning has achieved remarkable results in a wide variety of
domains, the training of models often requires large datasets that may need to
be collected from different individuals. As sensitive information may be
contained in the individual&apos;s dataset, sharing training data may lead to severe
privacy concerns. Therefore, there is a compelling need to develop
privacy-aware machine learning methods, for which one effective approach is to
leverage the generic framework of differential privacy. Considering that
stochastic gradient descent (SGD) is one of the mostly adopted methods for
large-scale machine learning problems, two decentralized differentially private
SGD algorithms are proposed in this work. Particularly, we focus on SGD without
replacement due to its favorable structure for practical implementation. In
addition, both privacy and convergence analysis are provided for the proposed
algorithms. Finally, extensive experiments are performed to verify the
theoretical results and demonstrate the effectiveness of the proposed
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Richeng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaofan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Huaiyu Dai&lt;/a&gt;</dc:creator>
</item></rdf:RDF>