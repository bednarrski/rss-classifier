<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08517"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08777"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1506.00074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.00864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08282"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08441"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08531"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08609"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08720"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08866"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07645"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08493"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08618"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08766"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08782"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08914"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01860"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10253"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05559"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08149"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.08517">
<title>An Incremental Construction of Deep Neuro Fuzzy System for Continual Learning of Non-stationary Data Streams. (arXiv:1808.08517v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08517</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing fuzzy neural networks (FNNs) are mostly developed under a shallow
network configuration having lower generalization power than those of deep
structures. This paper proposes a novel self-organizing deep fuzzy neural
network, namely deep evolving fuzzy neural networks (DEVFNN). Fuzzy rules can
be automatically extracted from data streams or removed if they play little
role during their lifespan. The structure of the network can be deepened on
demand by stacking additional layers using a drift detection method which not
only detects the covariate drift, variations of input space, but also
accurately identifies the real drift, dynamic changes of both feature space and
target space. DEVFNN is developed under the stacked generalization principle
via the feature augmentation concept where a recently developed algorithm,
namely Generic Classifier (gClass), drives the hidden layer. It is equipped by
an automatic feature selection method which controls activation and
deactivation of input attributes to induce varying subsets of input features. A
deep network simplification procedure is put forward using the concept of
hidden layer merging to prevent uncontrollable growth of input space dimension
due to the nature of feature augmentation approach in building a deep network
structure. DEVFNN works in the sample-wise fashion and is compatible for data
stream applications. The efficacy of DEVFNN has been thoroughly evaluated using
six datasets with non-stationary properties under the prequential
test-then-train protocol. It has been compared with four state-of the art data
stream methods and its shallow counterpart where DEVFNN demonstrates
improvement of classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1&quot;&gt;Witold Pedrycz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08777">
<title>Adaptive Structural Learning of Deep Belief Network for Medical Examination Data and Its Knowledge Extraction by using C4.5. (arXiv:1808.08777v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.08777</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning has a hierarchical network architecture to represent the
complicated feature of input patterns. The adaptive structural learning method
of Deep Belief Network (DBN) has been developed. The method can discover an
optimal number of hidden neurons for given input data in a Restricted Boltzmann
Machine (RBM) by neuron generation-annihilation algorithm, and generate a new
hidden layer in DBN by the extension of the algorithm. In this paper, the
proposed adaptive structural learning of DBN was applied to the comprehensive
medical examination data for the cancer prediction. The prediction system shows
higher classification accuracy (99.8% for training and 95.5% for test) than the
traditional DBN. Moreover, the explicit knowledge with respect to the relation
between input and output patterns was extracted from the trained DBN network by
C4.5. Some characteristics extracted in the form of IF-THEN rules to find an
initial cancer at the early stage were reported in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Toshihide Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1506.00074">
<title>Recognition of convolutional neural network based on CUDA Technology. (arXiv:1506.00074v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1506.00074</link>
<description rdf:parseType="Literal">&lt;p&gt;For the problem whether Graphic Processing Unit(GPU),the stream processor
with high performance of floating-point computing is applicable to neural
networks, this paper proposes the parallel recognition algorithm of
Convolutional Neural Networks(CNNs).It adopts Compute Unified Device
Architecture(CUDA)technology, definite the parallel data structures, and
describes the mapping mechanism for computing tasks on CUDA. It compares the
parallel recognition algorithm achieved on GPU of GTX200 hardware architecture
with the serial algorithm on CPU. It improves speed by nearly 60 times. Result
shows that GPU based the stream processor architecture ate more applicable to
some related applications about neural networks than CPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi-bin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Min Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu-jia Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.00864">
<title>Spatio-temporal Dynamics of Intrinsic Networks in Functional Magnetic Imaging Data Using Recurrent Neural Networks. (arXiv:1611.00864v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1611.00864</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel recurrent neural network (RNN) approach to account for
temporal dynamics and dependencies in brain networks observed via functional
magnetic resonance imaging (fMRI). Our approach directly parameterizes temporal
dynamics through recurrent connections, which can be used to formulate blind
source separation with a conditional (rather than marginal) independence
assumption, which we call RNN-ICA. This formulation enables us to visualize the
temporal dynamics of both first order (activity) and second order (directed
connectivity) information in brain networks that are widely studied in a static
sense, but not well-characterized dynamically. RNN-ICA predicts dynamics
directly from the recurrent states of the RNN in both task and resting state
fMRI. Our results show both task-related and group-differentiating directed
connectivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damaraju_E/0/1/0/all/0/1&quot;&gt;Eswar Damaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laufs_H/0/1/0/all/0/1&quot;&gt;Helmut Laufs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1&quot;&gt;Sergey M. Plis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1&quot;&gt;Vince Calhoun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03703">
<title>LemmaTag: Jointly Tagging and Lemmatizing for Morphologically-Rich Languages with BRNNs. (arXiv:1808.03703v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03703</link>
<description rdf:parseType="Literal">&lt;p&gt;We present LemmaTag, a featureless neural network architecture that jointly
generates part-of-speech tags and lemmas for sentences by using bidirectional
RNNs with character-level and word-level embeddings. We demonstrate that both
tasks benefit from sharing the encoding part of the network, predicting tag
subcategories, and using the tagger output as an input to the lemmatizer. We
evaluate our model across several languages with complex morphology, which
surpasses state-of-the-art accuracy in both part-of-speech tagging and
lemmatization in Czech, German, and Arabic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1&quot;&gt;Daniel Kondratyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavenciak_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Gaven&amp;#x10d;iak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1&quot;&gt;Milan Straka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajic_J/0/1/0/all/0/1&quot;&gt;Jan Haji&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08282">
<title>Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection. (arXiv:1808.08282v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08282</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) allowed improving the state-of-the-art
for many vision applications. However, naive CNNs suffer from two serious
issues: vulnerability to adversarial examples and making incorrect but
confident predictions for out-distribution samples. In this paper, we draw a
connection between these two issues of CNNs through over-generalization. We
reveal an augmented CNN (an extra output class added) as a simple yet effective
end-to-end approach has the capacity for controlling over-generalization. We
demonstrate training an augmented CNN on only a properly selected natural
out-distribution dataset and interpolated samples empowers it to classify a
wide range of unseen out-distribution samples as dustbin. Meanwhile, its
misclassification rates on a broad spectrum of well-known black-box adversaries
drop drastically as it classifies a portion of adversaries as dustbin class
(rejection option) while correctly classifies some of the remaining. However,
such an augmented CNN is never trained with any types of adversaries. Finally,
generation of white-box adversarial attacks using augmented CNNs can be harder
as the attack algorithms have to avoid dustbin regions for generating actual
adversaries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_M/0/1/0/all/0/1&quot;&gt;Mahdieh Abbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_A/0/1/0/all/0/1&quot;&gt;Arezoo Rajabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozafari_A/0/1/0/all/0/1&quot;&gt;Azadeh Sadat Mozafari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobba_R/0/1/0/all/0/1&quot;&gt;Rakesh B. Bobba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Christian Gagne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08433">
<title>A Tutorial on Modular Ontology Modeling with Ontology Design Patterns: The Cooking Recipes Ontology. (arXiv:1808.08433v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08433</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a detailed example for modular ontology modeling based on ontology
design patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krisnadhi_A/0/1/0/all/0/1&quot;&gt;Adila Krisnadhi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08441">
<title>Inductive Learning of Answer Set Programs from Noisy Examples. (arXiv:1808.08441v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08441</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, non-monotonic Inductive Logic Programming has received
growing interest. Specifically, several new learning frameworks and algorithms
have been introduced for learning under the answer set semantics, allowing the
learning of common-sense knowledge involving defaults and exceptions, which are
essential aspects of human reasoning. In this paper, we present a
noise-tolerant generalisation of the learning from answer sets framework. We
evaluate our ILASP3 system, both on synthetic and on real datasets, represented
in the new framework. In particular, we show that on many of the datasets
ILASP3 achieves a higher accuracy than other ILP systems that have previously
been applied to the datasets, including a recently proposed differentiable
learning framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1&quot;&gt;Mark Law&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1&quot;&gt;Alessandra Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broda_K/0/1/0/all/0/1&quot;&gt;Krysia Broda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08447">
<title>Deep Emotion: A Computational Model of Emotion Using Deep Neural Networks. (arXiv:1808.08447v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08447</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotions are very important for human intelligence. For example, emotions are
closely related to the appraisal of the internal bodily state and external
stimuli. This helps us to respond quickly to the environment. Another important
perspective in human intelligence is the role of emotions in decision-making.
Moreover, the social aspect of emotions is also very important. Therefore, if
the mechanism of emotions were elucidated, we could advance toward the
essential understanding of our natural intelligence. In this study, a model of
emotions is proposed to elucidate the mechanism of emotions through the
computational model. Furthermore, from the viewpoint of partner robots, the
model of emotions may help us to build robots that can have empathy for humans.
To understand and sympathize with people&apos;s feelings, the robots need to have
their own emotions. This may allow robots to be accepted in human society. The
proposed model is implemented using deep neural networks consisting of three
modules, which interact with each other. Simulation results reveal that the
proposed model exhibits reasonable behavior as the basic mechanism of emotion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hieida_C/0/1/0/all/0/1&quot;&gt;Chie Hieida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horii_T/0/1/0/all/0/1&quot;&gt;Takato Horii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagai_T/0/1/0/all/0/1&quot;&gt;Takayuki Nagai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08497">
<title>FinBrain: When Finance Meets AI 2.0. (arXiv:1808.08497v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08497</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) is the core technology of technological
revolution and industrial transformation. As one of the new intelligent needs
in the AI 2.0 era, financial intelligence has elicited much attention from the
academia and industry. In our current dynamic capital market, financial
intelligence demonstrates a fast and accurate machine learning capability to
handle complex data and has gradually acquired the potential to become a
&quot;financial brain&quot;. In this work, we survey existing studies on financial
intelligence. First, we describe the concept of financial intelligence and
elaborate on its position in the financial technology field. Second, we
introduce the development of financial intelligence and review state-of-the-art
techniques in wealth management, risk management, financial security, financial
consulting, and blockchain. Finally, we propose a research framework called
FinBrain and summarize four open issues, namely, explainable financial agents
and causality, perception and prediction under uncertainty, risk-sensitive and
robust decision making, and multi-agent game and mechanism design. We believe
that these research directions can lay the foundation for the development of AI
2.0 in the finance field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaolin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mengying Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qibing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yanchao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08531">
<title>DeepTracker: Visualizing the Training Process of Convolutional Neural Networks. (arXiv:1808.08531v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08531</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (CNNs) have achieved remarkable success in
various fields. However, training an excellent CNN is practically a
trial-and-error process that consumes a tremendous amount of time and computer
resources. To accelerate the training process and reduce the number of trials,
experts need to understand what has occurred in the training process and why
the resulting CNN behaves as such. However, current popular training platforms,
such as TensorFlow, only provide very little and general information, such as
training/validation errors, which is far from enough to serve this purpose. To
bridge this gap and help domain experts with their training tasks in a
practical environment, we propose a visual analytics system, DeepTracker, to
facilitate the exploration of the rich dynamics of CNN training processes and
to identify the unusual patterns that are hidden behind the huge amount of
training log. Specifically,we combine a hierarchical index mechanism and a set
of hierarchical small multiples to help experts explore the entire training log
from different levels of detail. We also introduce a novel cube-style
visualization to reveal the complex correlations among multiple types of
heterogeneous training data including neuron weights, validation images, and
training iterations. Three case studies are conducted to demonstrate how
DeepTracker provides its users with valuable knowledge in an industry-level CNN
training process, namely in our case, training ResNet-50 on the ImageNet
dataset. We show that our method can be easily applied to other
state-of-the-art &quot;very deep&quot; CNN models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1&quot;&gt;Weiwei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;Kai Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Huamin Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08609">
<title>Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge. (arXiv:1808.08609v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08609</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples are inputs to machine learning models designed to cause
the model to make a mistake. They are useful for understanding the shortcomings
of machine learning models, interpreting their results, and for regularisation.
In NLP, however, most example generation strategies produce input text by using
known, pre-specified semantic transformations, requiring significant manual
effort and in-depth understanding of the problem and domain. In this paper, we
investigate the problem of automatically generating adversarial examples that
violate a set of given First-Order Logic constraints in Natural Language
Inference (NLI). We reduce the problem of identifying such adversarial examples
to a combinatorial optimisation problem, by maximising a quantity measuring the
degree of violation of such constraints and by using a language model for
generating linguistically-plausible examples. Furthermore, we propose a method
for adversarially regularising neural NLI models for incorporating background
knowledge. Our results show that, while the proposed method does not always
improve results on the SNLI and MultiNLI datasets, it significantly and
consistently increases the predictive accuracy on adversarially-crafted
datasets -- up to a 79.6% relative improvement -- while drastically reducing
the number of background knowledge violations. Furthermore, we show that
adversarial examples transfer among model architectures, and that the proposed
adversarial training procedure improves the robustness of NLI models to
adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1&quot;&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08703">
<title>Generating Text through Adversarial Training using Skip-Thought Vectors. (arXiv:1808.08703v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.08703</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past few years, various advancements have been made in generative
models owing to the formulation of Generative Adversarial Networks (GANs). GANs
have been shown to perform exceedingly well on a wide variety of tasks
pertaining to image generation and style transfer. In the field of Natural
Language Processing, word embeddings such as word2vec and GLoVe are
state-of-the-art methods for applying neural network models on textual data.
Attempts have been made for utilizing GANs with word embeddings for text
generation. This work presents an approach to text generation using
Skip-Thought sentence embeddings in conjunction with GANs based on gradient
penalty functions and f-measures. The results of using sentence embeddings with
GANs for generating text conditioned on input information are comparable to the
approaches where word embeddings are used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahamad_A/0/1/0/all/0/1&quot;&gt;Afroz Ahamad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08720">
<title>Predefined Sparseness in Recurrent Sequence Models. (arXiv:1808.08720v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08720</link>
<description rdf:parseType="Literal">&lt;p&gt;Inducing sparseness while training neural networks has been shown to yield
models with a lower memory footprint but similar effectiveness to dense models.
However, sparseness is typically induced starting from a dense model, and thus
this advantage does not hold during training. We propose techniques to enforce
sparseness upfront in recurrent sequence models for NLP applications, to also
benefit training. First, in language modeling, we show how to increase hidden
state sizes in recurrent layers without increasing the number of parameters,
leading to more expressive models. Second, for sequence labeling, we show that
word embeddings with predefined sparseness lead to similar performance as dense
embeddings, at a fraction of the number of trainable parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1&quot;&gt;Thomas Demeester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1&quot;&gt;Johannes Deleu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;deric Godin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1&quot;&gt;Chris Develder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08773">
<title>Learning Multilingual Word Embeddings in a Latent Metric Space: A Geometric Approach. (arXiv:1808.08773v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08773</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel geometric approach for learning bilingual mappings given
monolingual embeddings and a bilingual dictionary. Our approach decouples
learning the transformation from the source language to the target language
into (a) learning rotations for language-specific embeddings to align them to a
common space, and (b) learning a similarity metric in the common space to model
similarities between the embeddings. We model the bilingual mapping problem as
an optimization problem on smooth Riemannian manifolds. We show that our
approach outperforms previous approaches on the bilingual lexicon induction and
cross-lingual word similarity tasks. Since we represent the rotated embeddings
in a common latent space, our approach can easily represent multiple languages
in a common space. We also show that these multilingual embeddings can be
learned jointly given bilingual dictionaries for multiple language pairs. We
demonstrate the effectiveness of the multilingual embeddings in one zero-shot
word translation setting: word translation using these multilingual embeddings
is better than word translation using a pivot language when no source-target
bilingual dictionary is available, but source-pivot and pivot-target bilingual
dictionaries are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1&quot;&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balgovind_A/0/1/0/all/0/1&quot;&gt;Arjun Balgovind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1&quot;&gt;Anoop Kunchukuttan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08866">
<title>A Study of Reinforcement Learning for Neural Machine Translation. (arXiv:1808.08866v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08866</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have shown that reinforcement learning (RL) is an effective
approach for improving the performance of neural machine translation (NMT)
system. However, due to its instability, successfully RL training is
challenging, especially in real-world systems where deep models and large
datasets are leveraged. In this paper, taking several large-scale translation
tasks as testbeds, we conduct a systematic study on how to train better NMT
models using reinforcement learning. We provide a comprehensive comparison of
several important factors (e.g., baseline reward, reward shaping) in RL
training. Furthermore, to fill in the gap that it remains unclear whether RL is
still beneficial when monolingual data is used, we propose a new method to
leverage RL to further boost the performance of NMT systems trained with
source/target monolingual data. By integrating all our findings, we obtain
competitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17
Chinese-English translation tasks, especially setting a state-of-the-art
performance on WMT17 Chinese-English translation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1&quot;&gt;Fei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jianhuang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08888">
<title>Realizing quantum linear regression with auxiliary qumodes. (arXiv:1808.08888v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1808.08888</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to exploit quantum advantages, quantum algorithms are indispensable
for operating machine learning with quantum computers. We here propose an
intriguing hybrid approach of quantum information processing for quantum linear
regression, which utilizes both discrete and continuous quantum variables, in
contrast to existing wisdoms based solely upon discrete qubits. In our
framework, data information is encoded in a qubit system, while information
processing is tackled using auxiliary continuous qumodes via qubit-qumode
interactions. Moreover, it is also elaborated that finite squeezing is quite
helpful for efficiently running the quantum algorithms in realistic setup.
Comparing with an all-qubit approach, the present hybrid approach is more
efficient and feasible for implementing quantum algorithms, still retaining
exponential quantum speed-up.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan-Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zheng-Yuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shi-Liang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Z. D. Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03968">
<title>The Complexity of Learning Acyclic Conditional Preference Networks. (arXiv:1801.03968v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03968</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning of user preferences, as represented by, for example, Conditional
Preference Networks (CP-nets), has become a core issue in AI research. Recent
studies investigate learning of CP-nets from randomly chosen examples or from
membership and equivalence queries. To assess the optimality of learning
algorithms as well as to better understand the combinatorial structure of
classes of CP-nets, it is helpful to calculate certain learning-theoretic
information complexity parameters. This article focuses on the frequently
studied case of learning from so-called swap examples, which express
preferences among objects that differ in only one attribute. It presents bounds
on or exact values of some well-studied information complexity parameters,
namely the VC dimension, the teaching dimension, and the recursive teaching
dimension, for classes of acyclic CP-nets. We further provide algorithms that
learn tree-structured and general acyclic CP-nets from membership queries.
Using our results on complexity parameters, we assess the optimality of our
algorithms as well as that of another query learning algorithm for acyclic
CP-nets presented in the literature. Our algorithms are near-optimal, and can,
under certain assumptions, be adapted to the case when the membership oracle is
faulty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alanazi_E/0/1/0/all/0/1&quot;&gt;Eisa Alanazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouhoub_M/0/1/0/all/0/1&quot;&gt;Malek Mouhoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilles_S/0/1/0/all/0/1&quot;&gt;Sandra Zilles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06244">
<title>XL-NBT: A Cross-lingual Neural Belief Tracking Framework. (arXiv:1808.06244v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06244</link>
<description rdf:parseType="Literal">&lt;p&gt;Task-oriented dialog systems are becoming pervasive, and many companies
heavily rely on them to complement human agents for customer service in call
centers. With globalization, the need for providing cross-lingual customer
support becomes more urgent than ever. However, cross-lingual support poses
great challenges---it requires a large amount of additional annotated data from
native speakers. In order to bypass the expensive human annotation and achieve
the first step towards the ultimate goal of building a universal dialog system,
we set out to build a cross-lingual state tracking framework. Specifically, we
assume that there exists a source language with dialog belief tracking
annotations while the target languages have no annotated dialog data of any
form. Then, we pre-train a state tracker for the source language as a teacher,
which is able to exploit easy-to-access parallel data. We then distill and
transfer its own knowledge to the student state tracker in target languages. We
specifically discuss two types of common parallel resources: bilingual corpus
and bilingual dictionary, and design different transfer learning strategies
accordingly. Experimentally, we successfully use English state tracker as the
teacher to transfer its knowledge to both Italian and German trackers and
achieve promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianshu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xifeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06474">
<title>A study on speech enhancement using exponent-only floating point quantized neural network (EOFP-QNN). (arXiv:1808.06474v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06474</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous studies have investigated the effectiveness of neural network
quantization on pattern classification tasks. The present study, for the first
time, investigated the performance of speech enhancement (a regression task in
speech processing) using a novel exponent-only floating-point quantized neural
network (EOFP-QNN). The proposed EOFP-QNN consists of two stages:
mantissa-quantization and exponent-quantization. In the mantissa-quantization
stage, EOFP-QNN learns how to quantize the mantissa bits of the model
parameters while preserving the regression accuracy using the least mantissa
precision. In the exponent-quantization stage, the exponent part of the
parameters is further quantized without causing any additional performance
degradation. We evaluated the proposed EOFP quantization technique on two types
of neural networks, namely, bidirectional long short-term memory (BLSTM) and
fully convolutional neural network (FCN), on a speech enhancement task.
Experimental results showed that the model sizes can be significantly reduced
(the model sizes of the quantized BLSTM and FCN models were only 18.75% and
21.89%, respectively, compared to those of the original models) while
maintaining satisfactory speech-enhancement performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yi-Te Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Chen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuo_T/0/1/0/all/0/1&quot;&gt;Tei-Wei Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07374">
<title>Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation. (arXiv:1808.07374v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07374</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the Neural Machine Translation (NMT) models are based on the
sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped
with the attention mechanism. However, the conventional attention mechanism
treats the decoding at each time step equally with the same matrix, which is
problematic since the softness of the attention for different types of words
(e.g. content words and function words) should differ. Therefore, we propose a
new model with a mechanism called Self-Adaptive Control of Temperature (SACT)
to control the softness of attention by means of an attention temperature.
Experimental results on the Chinese-English translation and English-Vietnamese
translation demonstrate that our model outperforms the baseline models, and the
analysis and the case study show that our model can attend to the most relevant
elements in the source-side contexts and generate the translation of high
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Muyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qi Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07645">
<title>Playing 20 Question Game with Policy-Based Reinforcement Learning. (arXiv:1808.07645v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07645</link>
<description rdf:parseType="Literal">&lt;p&gt;The 20 Questions (Q20) game is a well known game which encourages deductive
reasoning and creativity. In the game, the answerer first thinks of an object
such as a famous person or a kind of animal. Then the questioner tries to guess
the object by asking 20 questions. In a Q20 game system, the user is considered
as the answerer while the system itself acts as the questioner which requires a
good strategy of question selection to figure out the correct object and win
the game. However, the optimal policy of question selection is hard to be
derived due to the complexity and volatility of the game environment. In this
paper, we propose a novel policy-based Reinforcement Learning (RL) method,
which enables the questioner agent to learn the optimal policy of question
selection through continuous interactions with users. To facilitate training,
we also propose to use a reward network to estimate the more informative
reward. Compared to previous methods, our RL method is robust to noisy answers
and does not rely on the Knowledge Base of objects. Experimental results show
that our RL method clearly outperforms an entropy-based engineering system and
has competitive performance in a noisy-free simulation environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Huang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xianchao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bingfeng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1&quot;&gt;Chongyang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Can Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08272">
<title>Probabilistic Model of Object Detection Based on Convolutional Neural Network. (arXiv:1808.08272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.08272</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of a CNN detector and a search framework forms the basis for
local object/pattern detection. To handle the waste of regional information and
the defective compromise between efficiency and accuracy, this paper proposes a
probabilistic model with a powerful search framework. By mapping an image into
a probabilistic distribution of objects, this new model gives more informative
outputs with less computation. The setting and analytic traits are elaborated
in this paper, followed by a series of experiments carried out on FDDB, which
show that the proposed model is sound, efficient and analytic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fang-Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xu-Die Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hao-Nan Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08294">
<title>Unknown Examples &amp; Machine Learning Model Generalization. (arXiv:1808.08294v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08294</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decades, researchers and ML practitioners have come up with
better and better ways to build, understand and improve the quality of ML
models, but mostly under the key assumption that the training data is
distributed identically to the testing data. In many real-world applications,
however, some potential training examples are unknown to the modeler, due to
sample selection bias or, more generally, covariate shift, i.e., a distribution
shift between the training and deployment stage. The resulting discrepancy
between training and testing distributions leads to poor generalization
performance of the ML model and hence biased predictions. We provide novel
algorithms that estimate the number and properties of these unknown training
examples---unknown unknowns. This information can then be used to correct the
training set, prior to seeing any test data. The key idea is to combine
species-estimation techniques with data-driven methods for estimating the
feature values for the unknown unknowns. Experiments on a variety of ML models
and datasets indicate that taking the unknown examples into account can yield a
more robust ML model that generalizes better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1&quot;&gt;Yeounoh Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_P/0/1/0/all/0/1&quot;&gt;Peter J. Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upfal_E/0/1/0/all/0/1&quot;&gt;Eli Upfal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1&quot;&gt;Tim Kraska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08316">
<title>A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.08316</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring entity relatedness is a fundamental task for many natural language
processing and information retrieval applications. Prior work often studies
entity relatedness in static settings and an unsupervised manner. However,
entities in real-world are often involved in many different relationships,
consequently entity-relations are very dynamic over time. In this work, we
propose a neural networkbased approach for dynamic entity relatedness,
leveraging the collective attention as supervision. Our model is capable of
learning rich and different entity representations in a joint framework.
Through extensive experiments on large-scale datasets, we demonstrate that our
method achieves better results than competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tuan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1&quot;&gt;Wolfgang Nejdl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08366">
<title>Relaxing the Identically Distributed Assumption in Gaussian Co-Clustering for High Dimensional Data. (arXiv:1808.08366v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.08366</link>
<description rdf:parseType="Literal">&lt;p&gt;A co-clustering model for continuous data that relaxes the identically
distributed assumption within blocks of traditional co-clustering is presented.
The proposed model, although allowing more flexibility, still maintains the
very high degree of parsimony achieved by traditional co-clustering. A
stochastic EM algorithm along with a Gibbs sampler is used for parameter
estimation and an ICL criterion is used for model selection. Simulated and real
datasets are used for illustration and comparison with traditional
co-clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gallaugher_M/0/1/0/all/0/1&quot;&gt;M.P.B. Gallaugher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Biernacki_C/0/1/0/all/0/1&quot;&gt;C. Biernacki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McNicholas_P/0/1/0/all/0/1&quot;&gt;P.D. McNicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08493">
<title>Contextual Parameter Generation for Universal Neural Machine Translation. (arXiv:1808.08493v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.08493</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple modification to existing neural machine translation (NMT)
models that enables using a single universal model to translate between
multiple languages while allowing for language specific parameterization, and
that can also be used for domain adaptation. Our approach requires no changes
to the model architecture of a standard NMT system, but instead introduces a
new component, the contextual parameter generator (CPG), that generates the
parameters of the system (e.g., weights in a neural network). This parameter
generator accepts source and target language embeddings as input, and generates
the parameters for the encoder and the decoder, respectively. The rest of the
model remains unchanged and is shared across all languages. We show how this
simple modification enables the system to use monolingual data for training and
also perform zero-shot translation. We further show it is able to surpass
state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and
that the learned language embeddings are able to uncover interesting
relationships between languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platanios_E/0/1/0/all/0/1&quot;&gt;Emmanouil Antonios Platanios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1&quot;&gt;Tom Mitchell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08618">
<title>Deep Learning: Computational Aspects. (arXiv:1808.08618v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08618</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we review computational aspects of Deep Learning (DL). Deep
learning uses network architectures consisting of hierarchical layers of latent
variables to construct predictors for high-dimensional input-output models.
Training a deep learning architecture is computationally intensive, and
efficient linear algebra libraries is the key for training and inference.
Stochastic gradient descent (SGD) optimization and batch sampling are used to
learn from massive data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polson_N/0/1/0/all/0/1&quot;&gt;Nicholas Polson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolov_V/0/1/0/all/0/1&quot;&gt;Vadim Sokolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08627">
<title>Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation. (arXiv:1808.08627v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1808.08627</link>
<description rdf:parseType="Literal">&lt;p&gt;As opposed to manual feature engineering which is tedious and difficult to
scale, network representation learning has attracted a surge of research
interests as it automates the process of feature learning on graphs. The
learned low-dimensional node vector representation is generalizable and eases
the knowledge discovery process on graphs by enabling various off-the-shelf
machine learning tools to be directly applied. Recent research has shown that
the past decade of network embedding approaches either explicitly factorize a
carefully designed matrix to obtain the low-dimensional node vector
representation or are closely related to implicit matrix factorization, with
the fundamental assumption that the factorized node connectivity matrix is
low-rank. Nonetheless, the global low-rank assumption does not necessarily hold
especially when the factorized matrix encodes complex node interactions, and
the resultant single low-rank embedding matrix is insufficient to capture all
the observed connectivity patterns. In this regard, we propose a novel
multi-level network embedding framework BoostNE, which can learn multiple
network embedding representations of different granularity from coarse to fine
without imposing the prevalent global low-rank assumption. The proposed BoostNE
method is also in line with the successful gradient boosting method in ensemble
learning as multiple weak embeddings lead to a stronger and more effective one.
We assess the effectiveness of the proposed BoostNE framework by comparing it
with existing state-of-the-art network embedding methods on various datasets,
and the experimental results corroborate the superiority of the proposed
BoostNE network embedding framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jundong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08640">
<title>Detecting Outliers in Data with Correlated Measures. (arXiv:1808.08640v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08640</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in sensor technology have enabled the collection of large-scale
datasets. Such datasets can be extremely noisy and often contain a significant
amount of outliers that result from sensor malfunction or human operation
faults. In order to utilize such data for real-world applications, it is
critical to detect outliers so that models built from these datasets will not
be skewed by outliers.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a new outlier detection method that utilizes the
correlations in the data (e.g., taxi trip distance vs. trip time). Different
from existing outlier detection methods, we build a robust regression model
that explicitly models the outliers and detects outliers simultaneously with
the model fitting.
&lt;/p&gt;
&lt;p&gt;We validate our approach on real-world datasets against methods specifically
designed for each dataset as well as the state of the art outlier detectors.
Our outlier detection method achieves better performances, demonstrating the
robustness and generality of our method. Last, we report interesting case
studies on some outliers that result from atypical events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsuan Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1&quot;&gt;Daniel Kifer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08755">
<title>Learning from Positive and Unlabeled Data under the Selected At Random Assumption. (arXiv:1808.08755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08755</link>
<description rdf:parseType="Literal">&lt;p&gt;For many interesting tasks, such as medical diagnosis and web page
classification, a learner only has access to some positively labeled examples
and many unlabeled examples. Learning from this type of data requires making
assumptions about the true distribution of the classes and/or the mechanism
that was used to select the positive examples to be labeled. The commonly made
assumptions, separability of the classes and positive examples being selected
completely at random, are very strong. This paper proposes a weaker assumption
that assumes the positive examples to be selected at random, conditioned on
some of the attributes. To learn under this assumption, an EM method is
proposed. Experiments show that our method is not only very capable of learning
under this assumption, but it also outperforms the state of the art for
learning under the selected completely at random assumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekker_J/0/1/0/all/0/1&quot;&gt;Jessa Bekker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jesse Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08766">
<title>Learning behavioral context recognition with multi-stream temporal convolutional networks. (arXiv:1808.08766v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08766</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart devices of everyday use (such as smartphones and wearables) are
increasingly integrated with sensors that provide immense amounts of
information about a person&apos;s daily life such as behavior and context. The
automatic and unobtrusive sensing of behavioral context can help develop
solutions for assisted living, fitness tracking, sleep monitoring, and several
other fields. Towards addressing this issue, we raise the question: can a
machine learn to recognize a diverse set of contexts and activities in a
real-life through joint learning from raw multi-modal signals (e.g.
accelerometer, gyroscope and audio etc.)? In this paper, we propose a
multi-stream temporal convolutional network to address the problem of
multi-label behavioral context recognition. A four-stream network architecture
handles learning from each modality with a contextualization module which
incorporates extracted representations to infer a user&apos;s context. Our empirical
evaluation suggests that a deep convolutional network trained end-to-end
achieves an optimal recognition rate. Furthermore, the presented architecture
can be extended to include similar sensors for performance improvements and
handles missing modalities through multi-task learning without any manual
feature engineering on highly imbalanced and sparsely labeled dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1&quot;&gt;Aaqib Saeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcelebi_T/0/1/0/all/0/1&quot;&gt;Tanir Ozcelebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trajanovski_S/0/1/0/all/0/1&quot;&gt;Stojan Trajanovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukkien_J/0/1/0/all/0/1&quot;&gt;Johan Lukkien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08782">
<title>Amobee at IEST 2018: Transfer Learning from Language Models. (arXiv:1808.08782v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the system developed at Amobee for the WASSA 2018
implicit emotions shared task (IEST). The goal of this task was to predict the
emotion expressed by missing words in tweets without an explicit mention of
those words. We developed an ensemble system consisting of language models
together with LSTM-based networks containing a CNN attention mechanism. Our
approach represents a novel use of language models (specifically trained on a
large Twitter dataset) to predict and classify emotions. Our system reached 1st
place with a macro $\text{F}_1$ score of 0.7145.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozental_A/0/1/0/all/0/1&quot;&gt;Alon Rozental&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fleischer_D/0/1/0/all/0/1&quot;&gt;Daniel Fleischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelrich_Z/0/1/0/all/0/1&quot;&gt;Zohar Kelrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08914">
<title>Deep Learning for Stress Field Prediction Using Convolutional Neural Networks. (arXiv:1808.08914v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08914</link>
<description rdf:parseType="Literal">&lt;p&gt;This research presents a deep learning based approach to predict stress
fields in the solid material elastic deformation using convolutional neural
networks (CNN). Two different architectures are proposed to solve the problem.
One is Feature Representation embedded Convolutional Neural Network (FR-CNN)
with a single input channel, and the other is Squeeze-and-Excitation Residual
network modules embedded Fully Convolutional Neural network (SE-Res-FCN) with
multiple input channels. Both the tow architectures are stable and converged
reliably in training and testing on GPUs. Accuracy analysis shows that
SE-Res-FCN has a significantly smaller mean squared error (MSE) and mean
absolute error (MAE) than FR-CNN. Mean relative error (MRE) of the SE-Res-FCN
model is about 0.25% with respect to the average ground truth. The validation
results indicate that the SE-Res-FCN model can accurately predict the stress
field. For stress field prediction, the hierarchical architecture becomes
deeper within certain limits, and then its prediction becomes more accurate.
Fully trained deep learning models have higher computational efficiency over
conventional FEM models, so they have great foreground and potential in
structural design and topology optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haoliang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kara_L/0/1/0/all/0/1&quot;&gt;Levent Burak Kara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01860">
<title>Attributed Network Embedding for Learning in a Dynamic Environment. (arXiv:1706.01860v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01860</link>
<description rdf:parseType="Literal">&lt;p&gt;Network embedding leverages the node proximity manifested to learn a
low-dimensional node vector representation for each node in the network. The
learned embeddings could advance various learning tasks such as node
classification, network clustering, and link prediction. Most, if not all, of
the existing works, are overwhelmingly performed in the context of plain and
static networks. Nonetheless, in reality, network structure often evolves over
time with addition/deletion of links and nodes. Also, a vast majority of
real-world networks are associated with a rich set of node attributes, and
their attribute values are also naturally changing, with the emerging of new
content patterns and the fading of old content patterns. These changing
characteristics motivate us to seek an effective embedding representation to
capture network and attribute evolving patterns, which is of fundamental
importance for learning in a dynamic environment. To our best knowledge, we are
the first to tackle this problem with the following two challenges: (1) the
inherently correlated network and node attributes could be noisy and
incomplete, it necessitates a robust consensus representation to capture their
individual properties and correlations; (2) the embedding learning needs to be
performed in an online fashion to adapt to the changes accordingly. In this
paper, we tackle this problem by proposing a novel dynamic attributed network
embedding framework - DANE. In particular, DANE first provides an offline
method for a consensus embedding and then leverages matrix perturbation theory
to maintain the freshness of the end embedding results in an online manner. We
perform extensive experiments on both synthetic and real attributed networks to
corroborate the effectiveness and efficiency of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jundong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dani_H/0/1/0/all/0/1&quot;&gt;Harsh Dani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03467">
<title>RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. (arXiv:1803.03467v4 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03467</link>
<description rdf:parseType="Literal">&lt;p&gt;To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user&apos;s
potential interests along links in the knowledge graph. The multiple &quot;ripples&quot;
activated by a user&apos;s historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Miao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10253">
<title>From Principal Subspaces to Principal Components with Linear Autoencoders. (arXiv:1804.10253v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10253</link>
<description rdf:parseType="Literal">&lt;p&gt;The autoencoder is an effective unsupervised learning model which is widely
used in deep learning. It is well known that an autoencoder with a single
fully-connected hidden layer, a linear activation function and a squared error
cost function trains weights that span the same subspace as the one spanned by
the principal component loading vectors, but that they are not identical to the
loading vectors. In this paper, we show how to recover the loading vectors from
the autoencoder weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Plaut_E/0/1/0/all/0/1&quot;&gt;Elad Plaut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08090">
<title>Graph Capsule Convolutional Neural Networks. (arXiv:1805.08090v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08090</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Convolutional Neural Networks (GCNNs) are the most recent exciting
advancement in deep learning field and their applications are quickly spreading
in multi-cross-domains including bioinformatics, chemoinformatics, social
networks, natural language processing and computer vision. In this paper, we
expose and tackle some of the basic weaknesses of a GCNN model with a capsule
idea presented in \cite{hinton2011transforming} and propose our Graph Capsule
Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve
especially graph classification problem which current GCNN models find
challenging. Through extensive experiments, we show that our proposed Graph
Capsule Network can significantly outperforms both the existing state-of-art
deep learning methods and graph kernels on graph classification benchmark
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Saurabh Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhi-Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05559">
<title>Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation. (arXiv:1806.05559v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05559</link>
<description rdf:parseType="Literal">&lt;p&gt;Parallel sentence extraction is a task addressing the data sparsity problem
found in multilingual natural language processing applications. We propose a
bidirectional recurrent neural network based approach to extract parallel
sentences from collections of multilingual texts. Our experiments with noisy
parallel corpora show that we can achieve promising results against a
competitive baseline by removing the need of specific feature engineering or
additional external resources. To justify the utility of our approach, we
extract sentence pairs from Wikipedia articles to train machine translation
systems and show significant improvements in translation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregoire_F/0/1/0/all/0/1&quot;&gt;Francis Gr&amp;#xe9;goire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1&quot;&gt;Philippe Langlais&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05784">
<title>Multiview Boosting by Controlling the Diversity and the Accuracy of View-specific Voters. (arXiv:1808.05784v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05784</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a boosting based multiview learning algorithm,
referred to as PB-MVBoost, which iteratively learns i) weights over
view-specific voters capturing view-specific information; and ii) weights over
views by optimizing a PAC-Bayes multiview C-Bound that takes into account the
accuracy of view-specific classifiers and the diversity between the views. We
derive a generalization bound for this strategy following the PAC-Bayes theory
which is a suitable tool to deal with models expressed as weighted combination
over a set of voters. Different experiments on three publicly available
datasets show the efficiency of the proposed approach with respect to
state-of-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anil Goyal&lt;/a&gt; (AMA, LHC), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morvant_E/0/1/0/all/0/1&quot;&gt;Emilie Morvant&lt;/a&gt; (LHC), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Germain_P/0/1/0/all/0/1&quot;&gt;Pascal Germain&lt;/a&gt; (MODAL), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amini_M/0/1/0/all/0/1&quot;&gt;Massih-Reza Amini&lt;/a&gt; (AMA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08149">
<title>From Random to Supervised: A Novel Dropout Mechanism Integrated with Global Information. (arXiv:1808.08149v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08149</link>
<description rdf:parseType="Literal">&lt;p&gt;Dropout is used to avoid overfitting by randomly dropping units from the
neural networks during training. Inspired by dropout, this paper presents
GI-Dropout, a novel dropout method integrating with global information to
improve neural networks for text classification. Unlike the traditional dropout
method in which the units are dropped randomly according to the same
probability, we aim to use explicit instructions based on global information of
the dataset to guide the training process. With GI-Dropout, the model is
supposed to pay more attention to inapparent features or patterns. Experiments
demonstrate the effectiveness of the dropout with global information on seven
text classification tasks, including sentiment analysis and topic
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hengru Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Renfen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Si Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Sheng Gao&lt;/a&gt;</dc:creator>
</item></rdf:RDF>