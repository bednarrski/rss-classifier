<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01653"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01660"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01575"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01947"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01002"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.01653">
<title>Review of Deep Learning. (arXiv:1804.01653v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01653</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, China, the United States and other countries, Google and
other high-tech companies have increased investment in artificial intelligence.
Deep learning is one of the current artificial intelligence research&apos;s key
areas. This paper analyzes and summarizes the latest progress and future
research directions of deep learning. Firstly, three basic models of deep
learning are outlined, including multilayer perceptrons, convolutional neural
networks, and recurrent neural networks. On this basis, we further analyze the
emerging new models of convolution neural networks and recurrent neural
networks. This paper then summarizes deep learning&apos;s applications in many areas
of artificial intelligence, including voice, computer vision, natural language
processing and so on. Finally, this paper discusses the existing problems of
deep learning and gives the corresponding possible solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1&quot;&gt;Tong Mo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01660">
<title>The structure of evolved representations across different substrates for artificial intelligence. (arXiv:1804.01660v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.01660</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial neural networks (ANNs), while exceptionally useful for
classification, are vulnerable to misdirection. Small amounts of noise can
significantly affect their ability to correctly complete a task. Instead of
generalizing concepts, ANNs seem to focus on surface statistical regularities
in a given task. Here we compare how recurrent artificial neural networks, long
short-term memory units, and Markov Brains sense and remember their
environments. We show that information in Markov Brains is localized and
sparsely distributed, while the other neural network substrates &quot;smear&quot;
information about the environment across all nodes, which makes them vulnerable
to noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintze_A/0/1/0/all/0/1&quot;&gt;Arend Hintze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirkpatrick_D/0/1/0/all/0/1&quot;&gt;Douglas Kirkpatrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adami_C/0/1/0/all/0/1&quot;&gt;Christoph Adami&lt;/a&gt; (Michigan State University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01694">
<title>Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for Optimizing Protein Functions. (arXiv:1804.01694v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/1804.01694</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) represent an attractive and novel
approach to generate realistic data, such as genes, proteins, or drugs, in
synthetic biology. Here, we apply GANs to generate synthetic DNA sequences
encoding for proteins of variable length. We propose a novel feedback-loop
architecture, called Feedback GAN (FBGAN), to optimize the synthetic gene
sequences for desired properties using an external function analyzer. The
proposed architecture also has the advantage that the analyzer need not be
differentiable. We apply the feedback-loop mechanism to two examples: 1)
generating synthetic genes coding for antimicrobial peptides, and 2) optimizing
synthetic genes for the secondary structure of their resulting peptides. A
suite of metrics demonstrate that the GAN generated proteins have desirable
biophysical properties. The FBGAN architecture can also be used to optimize
GAN-generated datapoints for useful properties in domains beyond genomics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Anvita Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01503">
<title>Abstractive Tabular Dataset Summarization via Knowledge Base Semantic Embeddings. (arXiv:1804.01503v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01503</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes an abstractive summarization method for tabular data
which employs a knowledge base semantic embedding to generate the summary.
Assuming the dataset contains descriptive text in headers, columns and/or some
augmenting metadata, the system employs the embedding to recommend a
subject/type for each text segment. Recommendations are aggregated into a small
collection of super types considered to be descriptive of the dataset by
exploiting the hierarchy of types in a pre-specified ontology. Using February
2015 Wikipedia as the knowledge base, and a corresponding DBpedia ontology as
types, we present experimental results on open data taken from several
sources--OpenML, CKAN and data.world--to illustrate the effectiveness of the
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azunre_P/0/1/0/all/0/1&quot;&gt;Paul Azunre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_C/0/1/0/all/0/1&quot;&gt;Craig Corcoran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_D/0/1/0/all/0/1&quot;&gt;David Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honke_G/0/1/0/all/0/1&quot;&gt;Garrett Honke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruppel_R/0/1/0/all/0/1&quot;&gt;Rebecca Ruppel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Sandeep Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_J/0/1/0/all/0/1&quot;&gt;Jonathon Morgan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08010">
<title>Unsupervised Adaptation with Domain Separation Networks for Robust Speech Recognition. (arXiv:1711.08010v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1711.08010</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation of speech signal aims at adapting a
well-trained source-domain acoustic model to the unlabeled data from target
domain. This can be achieved by adversarial training of deep neural network
(DNN) acoustic models to learn an intermediate deep representation that is both
senone-discriminative and domain-invariant. Specifically, the DNN is trained to
jointly optimize the primary task of senone classification and the secondary
task of domain classification with adversarial objective functions. In this
work, instead of only focusing on learning a domain-invariant feature (i.e. the
shared component between domains), we also characterize the difference between
the source and target domain distributions by explicitly modeling the private
component of each domain through a private component extractor DNN. The private
component is trained to be orthogonal with the shared component and thus
implicitly increases the degree of domain-invariance of the shared component. A
reconstructor DNN is used to reconstruct the original speech feature from the
private and shared components as a regularization. This domain separation
framework is applied to the unsupervised environment adaptation task and
achieved 11.08% relative WER reduction from the gradient reversal layer
training, a representative adversarial training method, for automatic speech
recognition on CHiME-3 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazalov_V/0/1/0/all/0/1&quot;&gt;Vadim Mazalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yifan Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01527">
<title>Boosting Handwriting Text Recognition in Small Databases with Transfer Learning. (arXiv:1804.01527v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.01527</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we deal with the offline handwriting text recognition (HTR)
problem with reduced training datasets. Recent HTR solutions based on
artificial neural networks exhibit remarkable solutions in referenced
databases. These deep learning neural networks are composed of both
convolutional (CNN) and long short-term memory recurrent units (LSTM). In
addition, connectionist temporal classification (CTC) is the key to avoid
segmentation at character level, greatly facilitating the labeling task. One of
the main drawbacks of the CNNLSTM-CTC (CLC) solutions is that they need a
considerable part of the text to be transcribed for every type of calligraphy,
typically in the order of a few thousands of lines. Furthermore, in some
scenarios the text to transcribe is not that long, e.g. in the Washington
database. The CLC typically overfits for this reduced number of training
samples. Our proposal is based on the transfer learning (TL) from the
parameters learned with a bigger database. We first investigate, for a reduced
and fixed number of training samples, 350 lines, how the learning from a large
database, the IAM, can be transferred to the learning of the CLC of a reduced
database, Washington. We focus on which layers of the network could be not
re-trained. We conclude that the best solution is to re-train the whole CLC
parameters initialized to the values obtained after the training of the CLC
from the larger database. We also investigate results when the training size is
further reduced. The differences in the CER are more remarkable when training
with just 350 lines, a CER of 3.3% is achieved with TL while we have a CER of
18.2% when training from scratch. As a byproduct, the learning times are quite
reduced. Similar good results are obtained from the Parzival database when
trained with this reduced number of lines and this new approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aradillas_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Carlos Aradillas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murillo_Fuentes_J/0/1/0/all/0/1&quot;&gt;Juan Jos&amp;#xe9; Murillo-Fuentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olmos_P/0/1/0/all/0/1&quot;&gt;Pablo M. Olmos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01575">
<title>Probabilistic Formulations of Regression with Mixed Guidance. (arXiv:1804.01575v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01575</link>
<description rdf:parseType="Literal">&lt;p&gt;Regression problems assume every instance is annotated (labeled) with a real
value, a form of annotation we call \emph{strong guidance}. In order for these
annotations to be accurate, they must be the result of a precise experiment or
measurement. However, in some cases additional \emph{weak guidance} might be
given by imprecise measurements, a domain expert or even crowd sourcing.
Current formulations of regression are unable to use both types of guidance. We
propose a regression framework that can also incorporate weak guidance based on
relative orderings, bounds, neighboring and similarity relations. Consider
learning to predict ages from portrait images, these new types of guidance
allow weaker forms of guidance such as stating a person is in their 20s or two
people are similar in age. These types of annotations can be easier to generate
than strong guidance. We introduce a probabilistic formulation for these forms
of weak guidance and show that the resulting optimization problems are convex.
Our experimental results show the benefits of these formulations on several
data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gress_A/0/1/0/all/0/1&quot;&gt;Aubrey Gress&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_I/0/1/0/all/0/1&quot;&gt;Ian Davidson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01675">
<title>Semi-Supervised Classification for oil reservoir. (arXiv:1804.01675v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01675</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the general problem of accurate identification of oil
reservoirs. Recent improvements in well or borehole logging technology have
resulted in an explosive amount of data available for processing. The
traditional methods of analysis of the logs characteristics by experts require
significant amount of time and money and is no longer practicable. In this
paper, we use the semi-supervised learning to solve the problem of
ever-increasing amount of unlabelled data available for interpretation. The
experts are needed to label only a small amount of the log data. The neural
network classifier is first trained with the initial labelled data. Next,
batches of unlabelled data are being classified and the samples with the very
high class probabilities are being used in the next training session,
bootstrapping the classifier. The process of training, classifying, enhancing
the labelled data is repeated iteratively until the stopping criteria are met,
that is, no more high probability samples are found. We make an empirical study
on the well data from Jianghan oil field and test the performance of the neural
network semi-supervised classifier. We compare this method with other
classifiers. The comparison results show that our neural network
semi-supervised classifier is superior to other classification methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Haixiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paplinski_A/0/1/0/all/0/1&quot;&gt;Andrew P Paplinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01849">
<title>A Large-Scale Study of Language Models for Chord Prediction. (arXiv:1804.01849v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01849</link>
<description rdf:parseType="Literal">&lt;p&gt;We conduct a large-scale study of language models for chord prediction.
Specifically, we compare N-gram models to various flavours of recurrent neural
networks on a comprehensive dataset comprising all publicly available datasets
of annotated chords known to us. This large amount of data allows us to
systematically explore hyper-parameter settings for the recurrent neural
networks---a crucial step in achieving good results with this model class. Our
results show not only a quantitative difference between the models, but also a
qualitative one: in contrast to static N-gram models, certain RNN
configurations adapt to the songs at test time. This finding constitutes a
further step towards the development of chord recognition systems that are more
aware of local musical context than what was previously possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korzeniowski_F/0/1/0/all/0/1&quot;&gt;Filip Korzeniowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sears_D/0/1/0/all/0/1&quot;&gt;David R. W. Sears&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1&quot;&gt;Gerhard Widmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01852">
<title>GoSGD: Distributed Optimization for Deep Learning with Gossip Exchange. (arXiv:1804.01852v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01852</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the issue of speeding up the training of convolutional neural
networks by studying a distributed method adapted to stochastic gradient
descent. Our parallel optimization setup uses several threads, each applying
individual gradient descents on a local variable. We propose a new way of
sharing information between different threads based on gossip algorithms that
show good consensus convergence properties. Our method called GoSGD has the
advantage to be fully asynchronous and decentralized.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blot_M/0/1/0/all/0/1&quot;&gt;Michael Blot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1&quot;&gt;David Picard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01947">
<title>Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative Model. (arXiv:1804.01947v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we study generative modeling via autoencoders while using the
elegant geometric properties of the optimal transport (OT) problem and the
Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE),
which are generative models that enable one to shape the distribution of the
latent space into any samplable probability distribution without the need for
training an adversarial network or defining a closed-form for the distribution.
In short, we regularize the autoencoder loss with the sliced-Wasserstein
distance between the distribution of the encoded training samples and a
predefined samplable distribution. We show that the proposed formulation has an
efficient numerical solution that provides similar capabilities to Wasserstein
Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an
embarrassingly simple implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1&quot;&gt;Soheil Kolouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1&quot;&gt;Charles E. Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1&quot;&gt;Gustavo K. Rohde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01952">
<title>Generating Neural Networks with Neural Networks. (arXiv:1801.01952v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01952</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypernetworks are neural networks that generate weights for another neural
network. We formulate the hypernetwork training objective as a compromise
between accuracy and diversity, where the diversity takes into account trivial
symmetry transformations of the target network. We explain how this simple
formulation generalizes variational inference. We use multi-layered perceptrons
to form the mapping from the low dimensional input random vector to the high
dimensional weight space, and demonstrate how to reduce the number of
parameters in this mapping by weight sharing. We perform experiments and show
that the generated weights are diverse and lie on a non-trivial manifold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deutsch_L/0/1/0/all/0/1&quot;&gt;Lior Deutsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01002">
<title>Improving Massive MIMO Belief Propagation Detector with Deep Neural Network. (arXiv:1804.01002v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01002</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, deep neural network (DNN) is utilized to improve the belief
propagation (BP) detection for massive multiple-input multiple-output (MIMO)
systems. A neural network architecture suitable for detection task is firstly
introduced by unfolding BP algorithms. DNN MIMO detectors are then proposed
based on two modified BP detectors, damped BP and max-sum BP. The correction
factors in these algorithms are optimized through deep learning techniques,
aiming at improved detection performance. Numerical results are presented to
demonstrate the performance of the DNN detectors in comparison with various BP
modifications. The neural network is trained once and can be used for multiple
online detections. The results show that, compared to other state-of-the-art
detectors, the DNN detectors can achieve lower bit error rate (BER) with
improved robustness against various antenna configurations and channel
conditions at the same level of complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xiaosi Tan&lt;/a&gt; (1 and 2 and 3), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weihong Xu&lt;/a&gt; (1 and 2 and 3), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beery_Y/0/1/0/all/0/1&quot;&gt;Yair Be&amp;#x27;ery&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zaichen Zhang&lt;/a&gt; (2 and 3), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+You_X/0/1/0/all/0/1&quot;&gt;Xiaohu You&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuan Zhang&lt;/a&gt; (1 and 2 and 3) ((1) Lab of Efficient Architectures for Digital-communication and Signal-processing (LEADS), (2) National Mobile Communications Research Laboratory, (3) Quantum Information Center, Southeast University, China, (4) School of Electrical Engineering, Tel-Aviv University, Tel-Aviv, Israel)</dc:creator>
</item></rdf:RDF>