<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01610"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07569"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07624"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07645"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07804"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07593"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07713"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05599"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.07530">
<title>An Overview of Datatype Quantization Techniques for Convolutional Neural Networks. (arXiv:1808.07530v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.07530</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) are becoming increasingly popular due to
their superior performance in the domain of computer vision, in applications
such as objection detection and recognition. However, they demand complex,
power-consuming hardware which makes them unsuitable for implementation on
low-power mobile and embedded devices. In this paper, a description and
comparison of various techniques is presented which aim to mitigate this
problem. This is primarily achieved by quantizing the floating-point weights
and activations to reduce the hardware requirements, and adapting the training
and inference algorithms to maintain the network&apos;s performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athar_A/0/1/0/all/0/1&quot;&gt;Ali Athar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07692">
<title>A Directionally Selective Neural Network with Separated ON and OFF Pathways for Translational Motion Perception in a Visually Cluttered Environment. (arXiv:1808.07692v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.07692</link>
<description rdf:parseType="Literal">&lt;p&gt;With respect to biological findings underlying fly&apos;s physiology in the past
decade, we present a directionally selective neural network, with a
feed-forward structure and entirely low-level visual processing, so as to
implement direction selective neurons in the fly&apos;s visual system, which are
mainly sensitive to wide-field translational movements in four cardinal
directions. In this research, we highlight the functionality of ON and OFF
pathways, separating motion information for parallel computation corresponding
to light-on and light-off selectivity. Through this modeling study, we
demonstrate several achievements compared with former bio-plausible
translational motion detectors, like the elementary motion detectors. First, we
thoroughly mimic the fly&apos;s preliminary motion-detecting pathways with newly
revealed fly&apos;s physiology. Second, we improve the speed response to moving
dark/light features via the design of ensembles of same polarity cells in the
dual-pathways. Moreover, we alleviate the impact of irrelevant motion in a
visually cluttered environment like the shifting of background and windblown
vegetation, via the modeling of spatiotemporal dynamics. We systematically
tested the DSNN against stimuli ranging from synthetic and real-world scenes,
to notably a visual modality of a ground micro robot. The results demonstrated
that the DSNN outperforms former bio-plausible translational motion detectors.
Importantly, we verified its computational simplicity and effectiveness
benefiting the building of neuromorphic vision sensor for robots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qinbing Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellotto_N/0/1/0/all/0/1&quot;&gt;Nicola Bellotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1&quot;&gt;Shigang Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01610">
<title>Training Generative Reversible Networks. (arXiv:1806.01610v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01610</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models with an encoding component such as autoencoders currently
receive great interest. However, training of autoencoders is typically
complicated by the need to train a separate encoder and decoder model that have
to be enforced to be reciprocal to each other. To overcome this problem,
by-design reversible neural networks (RevNets) had been previously used as
generative models either directly optimizing the likelihood of the data under
the model or using an adversarial approach on the generated data. Here, we
instead investigate their performance using an adversary on the latent space in
the adversarial autoencoder framework. We investigate the generative
performance of RevNets on the CelebA dataset, showing that generative RevNets
can generate coherent faces with similar quality as Variational Autoencoders.
This first attempt to use RevNets inside the adversarial autoencoder framework
slightly underperformed relative to recent advanced generative models using an
autoencoder component on CelebA, but this gap may diminish with further
optimization of the training setup of generative RevNets. In addition to the
experiments on CelebA, we show a proof-of-principle experiment on the MNIST
dataset suggesting that adversary-free trained RevNets can discover meaningful
latent dimensions without pre-specifying the number of dimensions of the latent
sampling distribution. In summary, this study shows that RevNets can be
employed in different generative training settings.
&lt;/p&gt;
&lt;p&gt;Source code for this study is at
https://github.com/robintibor/generative-reversible
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrabaszcz_P/0/1/0/all/0/1&quot;&gt;Patryk Chrab&amp;#x105;szcz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05385">
<title>On the Decision Boundary of Deep Neural Networks. (arXiv:1808.05385v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05385</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning models and techniques have achieved great empirical
success, our understanding of the source of success in many aspects remains
very limited. In an attempt to bridge the gap, we investigate the decision
boundary of a production deep learning architecture with weak assumptions on
both the training data and the model. We demonstrate, both theoretically and
empirically, that the last weight layer of a neural network converges to a
linear SVM trained on the output of the last hidden layer, for both the binary
case and the multi-class case with the commonly used cross-entropy loss.
Furthermore, we show empirically that training a neural network as a whole,
instead of only fine-tuning the last weight layer, may result in better bias
constant for the last weight layer, which is important for generalization. In
addition to facilitating the understanding of deep learning, our result can be
helpful for solving a broad range of practical problems of deep learning, such
as catastrophic forgetting and adversarial attacking. The experiment codes are
available at https://github.com/lykaust15/NN_decision_boundary
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Lizhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07561">
<title>Training Deeper Neural Machine Translation Models with Transparent Attention. (arXiv:1808.07561v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07561</link>
<description rdf:parseType="Literal">&lt;p&gt;While current state-of-the-art NMT models, such as RNN seq2seq and
Transformers, possess a large number of parameters, they are still shallow in
comparison to convolutional models used for both text and vision applications.
In this work we attempt to train significantly (2-3x) deeper Transformer and
Bi-RNN encoders for machine translation. We propose a simple modification to
the attention mechanism that eases the optimization of deeper models, and
results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT&apos;14
English-German and WMT&apos;15 Czech-English tasks for both architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1&quot;&gt;Ankur Bapna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mia Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07569">
<title>Robust Counterfactual Inferences using Feature Learning and their Applications. (arXiv:1808.07569v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07569</link>
<description rdf:parseType="Literal">&lt;p&gt;In a wide variety of applications, including personalization, we want to
measure the difference in outcome due to an intervention and thus have to deal
with counterfactual inference. The feedback from a customer in any of these
situations is only &apos;bandit feedback&apos; - that is, a partial feedback based on
whether we chose to intervene or not. Typically randomized experiments are
carried out to understand whether an intervention is overall better than no
intervention. Here we present a feature learning algorithm to learn from a
randomized experiment where the intervention in consideration is most effective
and where it is least effective rather than only focusing on the overall
impact, thus adding a context to our learning mechanism and extract more
information. From the randomized experiment, we learn the feature
representations which divide the population into subpopulations where we
observe statistically significant difference in average customer feedback
between those who were subjected to the intervention and those who were not,
with a level of significance l, where l is a configurable parameter in our
model. We use this information to derive the value of the intervention in
consideration for each instance in the population. With experiments, we show
that using this additional learning, in future interventions, the context for
each instance could be leveraged to decide whether to intervene or not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1&quot;&gt;Abhimanyu Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achan_K/0/1/0/all/0/1&quot;&gt;Kannan Achan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sushant Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07582">
<title>TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial Networks. (arXiv:1808.07582v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07582</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have shown great capacity on image
generation, in which a discriminative model guides the training of a generative
model to construct images that resemble real images. Recently, GANs have been
extended from generating images to generating sequences (e.g., poems, music and
codes). Existing GANs on sequence generation mainly focus on general sequences,
which are grammar-free. In many real-world applications, however, we need to
generate sequences in a formal language with the constraint of its
corresponding grammar. For example, to test the performance of a database, one
may want to generate a collection of SQL queries, which are not only similar to
the queries of real users, but also follow the SQL syntax of the target
database. Generating such sequences is highly challenging because both the
generator and discriminator of GANs need to consider the structure of the
sequences and the given grammar in the formal language. To address these
issues, we study the problem of syntax-aware sequence generation with GANs, in
which a collection of real sequences and a set of pre-defined grammatical rules
are given to both discriminator and generator. We propose a novel GAN
framework, namely TreeGAN, to incorporate a given Context-Free Grammar (CFG)
into the sequence generation process. In TreeGAN, the generator employs a
recurrent neural network (RNN) to construct a parse tree. Each generated parse
tree can then be translated to a valid sequence of the given grammar. The
discriminator uses a tree-structured RNN to distinguish the generated trees
from real trees. We show that TreeGAN can generate sequences for any CFG and
its generation fully conforms with the given syntax. Experiments on synthetic
and real data sets demonstrated that TreeGAN significantly improves the quality
of the sequence generation in context-free languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangnan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_K/0/1/0/all/0/1&quot;&gt;Kuorong Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07624">
<title>Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model. (arXiv:1808.07624v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07624</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a
sequential LSTM, to extract word order features while neglecting other valuable
syntactic information such as dependency graph or constituent trees. In this
paper, we first propose to use the \textit{syntactic graph} to represent three
types of syntactic information, i.e., word order, dependency and constituency
features. We further employ a graph-to-sequence model to encode the syntactic
graph and decode a logical form. Experimental results on benchmark datasets
show that our model is comparable to the state-of-the-art on Jobs640, ATIS and
Geo880. Experimental results on adversarial examples demonstrate the robustness
of the model is also improved by encoding more syntactic information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lingfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheinin_V/0/1/0/all/0/1&quot;&gt;Vadim Sheinin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07645">
<title>Playing 20 Question Game with Policy-Based Reinforcement Learning. (arXiv:1808.07645v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1808.07645</link>
<description rdf:parseType="Literal">&lt;p&gt;The 20 Questions (Q20) game is a well known game which encourages deductive
reasoning and creativity. In the game, the answerer first thinks of an object
such as a famous person or a kind of animal. Then the questioner tries to guess
the object by asking 20 questions. In a Q20 game system, the user is considered
as the answerer while the system itself acts as the questioner which requires a
good strategy of question selection to figure out the correct object and win
the game. However, the optimal policy of question selection is hard to be
derived due to the complexity and volatility of the game environment. In this
paper, we propose a novel policy-based Reinforcement Learning (RL) method,
which enables the questioner agent to learn the optimal policy of question
selection through continuous interactions with users. To facilitate training,
we also propose to use a reward network to estimate the more informative
reward. Compared to previous methods, our RL method is robust to noisy answers
and does not rely on the Knowledge Base of objects. Experimental results show
that our RL method clearly outperforms an entropy-based engineering system and
has competitive performance in a noisy-free simulation environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Huang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xianchao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bingfeng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1&quot;&gt;Chongyang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Can Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07658">
<title>Exploring Shared Structures and Hierarchies for Multiple NLP Tasks. (arXiv:1808.07658v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07658</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing shared neural architecture plays an important role in multi-task
learning. The challenge is that finding an optimal sharing scheme heavily
relies on the expert knowledge and is not scalable to a large number of diverse
tasks. Inspired by the promising work of neural architecture search (NAS), we
apply reinforcement learning to automatically find possible shared architecture
for multi-task learning. Specifically, we use a controller to select from a set
of shareable modules and assemble a task-specific architecture, and repeat the
same procedure for other tasks. The controller is trained with reinforcement
learning to maximize the expected accuracies for all tasks. We conduct
extensive experiments on two types of tasks, text classification and sequence
labeling, which demonstrate the benefits of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junkun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaiyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinchi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07724">
<title>Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs. (arXiv:1808.07724v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07724</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of mapping natural language text to
knowledge base entities. The mapping process is approached as a composition of
a phrase or a sentence into a point in a multi-dimensional entity space
obtained from a knowledge graph. The compositional model is an LSTM equipped
with a dynamic disambiguation mechanism on the input word embeddings (a
Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base
space is prepared by collecting random walks from a graph enhanced with textual
features, which act as a set of semantic bridges between text and knowledge
base entities. The ideas of this work are demonstrated on large-scale
text-to-entity mapping and entity classification tasks, with state of the art
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kartsaklis_D/0/1/0/all/0/1&quot;&gt;Dimitri Kartsaklis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1&quot;&gt;Mohammad Taher Pilehvar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1&quot;&gt;Nigel Collier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07804">
<title>Transfer Learning for Estimating Causal Effects using Neural Networks. (arXiv:1808.07804v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07804</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop new algorithms for estimating heterogeneous treatment effects,
combining recent developments in transfer learning for neural networks with
insights from the causal inference literature. By taking advantage of transfer
learning, we are able to efficiently use different data sources that are
related to the same underlying causal mechanisms. We compare our algorithms
with those in the extant literature using extensive simulation studies based on
large-scale voter persuasion experiments and the MNIST database. Our methods
can perform an order of magnitude better than existing benchmarks while using a
fraction of the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kunzel_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren R. K&amp;#xfc;nzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stadie_B/0/1/0/all/0/1&quot;&gt;Bradly C. Stadie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vemuri_N/0/1/0/all/0/1&quot;&gt;Nikita Vemuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramakrishnan_V/0/1/0/all/0/1&quot;&gt;Varsha Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sekhon_J/0/1/0/all/0/1&quot;&gt;Jasjeet S. Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06474">
<title>A study on speech enhancement using exponent-only floating point quantized neural network (EOFP-QNN). (arXiv:1808.06474v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06474</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous studies have investigated the effectiveness of neural network
quantization on pattern classification tasks. The present study, for the first
time, investigated the performance of speech enhancement (a regression task in
speech processing) using a novel exponent-only floating-point quantized neural
network (EOFP-QNN). The proposed EOFP-QNN consists of two stages:
mantissa-quantization and exponent-quantization. In the mantissa-quantization
stage, EOFP-QNN learns how to quantize the mantissa bits of the model
parameters while preserving the regression accuracy in the least mantissa
precision. In the exponent-quantization stage, the exponent part of the
parameters is further quantized without any additional performance degradation.
We evaluated the proposed EOFP quantization technique on two types of neural
networks, namely, bidirectional long short-term memory (BLSTM) and fully
convolutional neural network (FCN), on a speech enhancement task. Experimental
results showed that the model sizes can be significantly reduced (the model
sizes of the quantized BLSTM and FCN models were only 18.75% and 21.89%,
respectively, compared to those of the original models) while maintaining a
satisfactory speech-enhancement performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yi-Te Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Chen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuo_T/0/1/0/all/0/1&quot;&gt;Tei-Wei Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04295">
<title>Understanding training and generalization in deep learning by Fourier analysis. (arXiv:1808.04295v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.04295</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: It is still an open research area to theoretically understand why
Deep Neural Networks (DNNs)---equipped with many more parameters than training
data and trained by (stochastic) gradient-based methods---often achieve
remarkably low generalization error. Contribution: We study DNN training by
Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic)
gradient-based methods endows low-frequency components of the target function
with a higher priority during the training; ii) Small initialization leads to
good generalization ability of DNN while preserving the DNN&apos;s ability of
fitting any function. These results are further confirmed by experiments of
DNNs fitting the following datasets, i.e., natural images, one-dimensional
functions and MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiqin John Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07475">
<title>Capsule Networks for Protein Structure Classification and Prediction. (arXiv:1808.07475v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1808.07475</link>
<description rdf:parseType="Literal">&lt;p&gt;Capsule Networks have great potential to tackle problems in structural
biology because of their attention to hierarchical relationships. This paper
describes the implementation and application of a Capsule Network architecture
to the classification of RAS protein family structures on GPU-based
computational resources. The proposed Capsule Network trained on 2D and 3D
structural encodings can successfully classify HRAS and KRAS structures. The
Capsule Network can also classify a protein-based dataset derived from a
PSI-BLAST search on sequences of KRAS and HRAS mutations. Our results show an
accuracy improvement compared to traditional convolutional networks, while
improving interpretability through visualization of activation vectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jesus_D/0/1/0/all/0/1&quot;&gt;Dan Rosa de Jesus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cuevas_J/0/1/0/all/0/1&quot;&gt;Julian Cuevas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rivera_W/0/1/0/all/0/1&quot;&gt;Wilson Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Crivelli_S/0/1/0/all/0/1&quot;&gt;Silvia Crivelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07593">
<title>Pathologies in information bottleneck for deterministic supervised learning. (arXiv:1808.07593v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07593</link>
<description rdf:parseType="Literal">&lt;p&gt;Information bottleneck (IB) is a method for extracting information from one
random variable $X$ that is relevant for predicting another random variable
$Y$. To do so, IB identifies an intermediate &quot;bottleneck&quot; variable $T$ that has
low mutual information $I(X;T)$ and high mutual information $I(Y;T)$. The &quot;IB
curve&quot; characterizes the set of bottleneck variables that achieve maximal
$I(Y;T)$ for a given $I(X;T)$, and is typically explored by optimizing the &quot;IB
Lagrangian&quot;, $I(Y;T) - \beta I(X;T)$. Recently, there has been interest in
applying IB to supervised learning, particularly for classification problems
that use neural networks. In most classification problems, the output class $Y$
is a deterministic function of the input $X$, which we refer to as
&quot;deterministic supervised learning&quot;. We demonstrate three pathologies that
arise when IB is used in any scenario where $Y$ is a deterministic function of
$X$: (1) the IB curve cannot be recovered by optimizing the IB Lagrangian for
different values of $\beta$; (2) there are &quot;uninteresting&quot; solutions at all
points of the IB curve; and (3) for classifiers that achieve low error rates,
the activity of different hidden layers will not exhibit a strict trade-off
between compression and prediction, contrary to a recent proposal. To address
problem (1), we propose a functional that, unlike the IB Lagrangian, can
recover the IB curve in all cases. We finish by demonstrating these issues on
the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kolchinsky_A/0/1/0/all/0/1&quot;&gt;Artemy Kolchinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tracey_B/0/1/0/all/0/1&quot;&gt;Brendan D. Tracey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuyk_S/0/1/0/all/0/1&quot;&gt;Steven Van Kuyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07713">
<title>Adversarial Attacks on Deep-Learning Based Radio Signal Classification. (arXiv:1808.07713v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1808.07713</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL), despite its enormous success in many computer vision and
language processing applications, is exceedingly vulnerable to adversarial
attacks. We consider the use of DL for radio signal (modulation) classification
tasks, and present practical methods for the crafting of white-box and
universal black-box adversarial attacks in that application. We show that these
attacks can considerably reduce the classification performance, with extremely
small perturbations of the input. In particular, these attacks are
significantly more powerful than classical jamming attacks, which raises
significant security and robustness concerns in the use of DL-based algorithms
for the wireless physical layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1&quot;&gt;Meysam Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1&quot;&gt;Erik G. Larsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07769">
<title>Topology and Prediction Focused Research on Graph Convolutional Neural Networks. (arXiv:1808.07769v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07769</link>
<description rdf:parseType="Literal">&lt;p&gt;Important advances have been made using convolutional neural network (CNN)
approaches to solve complicated problems in areas that rely on grid structured
data such as image processing and object classification. Recently, research on
graph convolutional neural networks (GCNN) has increased dramatically as
researchers try to replicate the success of CNN for graph structured data.
Unfortunately, traditional CNN methods are not readily transferable to GCNN,
given the irregularity and geometric complexity of graphs. The emerging field
of GCNN is further complicated by research papers that differ greatly in their
scope, detail, and level of academic sophistication needed by the reader.
&lt;/p&gt;
&lt;p&gt;The present paper provides a review of some basic properties of GCNN. As a
guide to the interested reader, recent examples of GCNN research are then
grouped according to techniques that attempt to uncover the underlying topology
of the graph model and those that seek to generalize traditional CNN methods on
graph data to improve prediction of class membership. Discrete Signal
Processing on Graphs (DSPg) is used as a theoretical framework to better
understand some of the performance gains and limitations of these recent GCNN
approaches. A brief discussion of Topology Adaptive Graph Convolutional
Networks (TAGCN) is presented as an approach motivated by DSPg and future
research directions using this approach are briefly discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baron_M/0/1/0/all/0/1&quot;&gt;Matthew Baron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09952">
<title>Optimal sequential treatment allocation. (arXiv:1705.09952v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09952</link>
<description rdf:parseType="Literal">&lt;p&gt;In treatment allocation problems the individuals to be treated often arrive
sequentially. We study a problem in which the policy maker is not only
interested in the expected cumulative welfare but is also concerned about the
uncertainty/risk of the treatment outcomes. At the outset, the total number of
treatment assignments to be made may even be unknown. A sequential treatment
policy which attains the minimax optimal regret is proposed. We also
demonstrate that the expected number of suboptimal treatments only grows slowly
in the number of treatments. Finally, we study a setting where outcomes are
only observed with delay.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kock_A/0/1/0/all/0/1&quot;&gt;Anders Bredahl Kock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thyrsgaard_M/0/1/0/all/0/1&quot;&gt;Martin Thyrsgaard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07181">
<title>Efficient sparse Hessian based algorithms for the clustered lasso problem. (arXiv:1808.07181v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07181</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on solving the clustered lasso problem, which is a least squares
problem with the $\ell_1$-type penalties imposed on both the coefficients and
their pairwise differences to learn the group structure of the regression
parameters. Here we first reformulate the clustered lasso regularizer as a
weighted ordered-lasso regularizer, which is essential in reducing the
computational cost from $O(n^2)$ to $O(n\log (n))$. We then propose an inexact
semismooth Newton augmented Lagrangian (SSNAL) algorithm to solve the clustered
lasso problem or its dual via this equivalent formulation, depending on whether
the sample size is larger than the dimension of the features. An essential
component of the SSNAL algorithm is the computation of the generalized Jacobian
of the proximal mapping of the clustered lasso regularizer. Based on the new
formulation, we derive an efficient procedure for its computation.
Comprehensive results on the global convergence and local linear convergence of
the SSNAL algorithm are established. For the purpose of exposition and
comparison, we also summarize/design several first-order methods that can be
used to solve the problem under consideration, but with the key improvement
from the new formulation of the clustered lasso regularizer. As a demonstration
of the applicability of our algorithms, numerical experiments on the clustered
lasso problem are performed. The experiments show that the SSNAL algorithm
substantially outperforms the best alternative algorithm for the clustered
lasso problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Meixia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Defeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Toh_K/0/1/0/all/0/1&quot;&gt;Kim-Chuan Toh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05599">
<title>Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation. (arXiv:1808.05599v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.05599</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequence generative adversarial networks (SeqGAN) have been used to improve
conditional sequence generation tasks, for example, chit-chat dialogue
generation. To stabilize the training of SeqGAN, Monte Carlo tree search (MCTS)
or reward at every generation step (REGS) is used to evaluate the goodness of a
generated subsequence. MCTS is computationally intensive, but the performance
of REGS is worse than MCTS. In this paper, we propose stepwise GAN (StepGAN),
in which the discriminator is modified to automatically assign scores
quantifying the goodness of each subsequence at every generation step. StepGAN
has significantly less computational costs than MCTS. We demonstrate that
StepGAN outperforms previous GAN-based methods on both synthetic experiment and
chit-chat dialogue generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1&quot;&gt;Yi-Lin Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-Yi Lee&lt;/a&gt;</dc:creator>
</item></rdf:RDF>