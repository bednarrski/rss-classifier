<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04264"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06739"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04247"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04253"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04493"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04424"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.09347"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.04264">
<title>State Gradients for RNN Memory Analysis. (arXiv:1805.04264v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.04264</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for analyzing what the state in RNNs remembers from
its input embeddings. Our approach is inspired by backpropagation, in the sense
that we compute the gradients of the states with respect to the input
embeddings. The gradient matrix is decomposed with Singular Value Decomposition
to analyze which directions in the embedding space are best transferred to the
hidden state space, characterized by the largest singular values. We apply our
approach to LSTM language models and investigate to what extent and for how
long certain classes of words are remembered on average for a certain corpus.
Additionally, the extent to which a specific property or relationship is
remembered by the RNN can be tracked by comparing a vector characterizing that
property with the direction(s) in embedding space that are best preserved in
hidden state space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verwimp_L/0/1/0/all/0/1&quot;&gt;Lyan Verwimp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1&quot;&gt;Hugo Van hamme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1&quot;&gt;Vincent Renkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wambacq_P/0/1/0/all/0/1&quot;&gt;Patrick Wambacq&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04849">
<title>The unreasonable effectiveness of the forget gate. (arXiv:1804.04849v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04849</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the success of the gated recurrent unit, a natural question is whether
all the gates of the long short-term memory (LSTM) network are necessary.
Previous research has shown that the forget gate is one of the most important
gates in the LSTM. Here we show that a forget-gate-only version of the LSTM
with chrono-initialized biases, not only provides computational savings but
outperforms the standard LSTM on multiple benchmark datasets and competes with
some of the best contemporary models. Our proposed network, the JANET, achieves
accuracies of 99% and 92.5% on the MNIST and pMNIST datasets, outperforming the
standard LSTM which yields accuracies of 98.5% and 91%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westhuizen_J/0/1/0/all/0/1&quot;&gt;Jos van der Westhuizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1&quot;&gt;Joan Lasenby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06739">
<title>Are ResNets Provably Better than Linear Predictors?. (arXiv:1804.06739v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06739</link>
<description rdf:parseType="Literal">&lt;p&gt;A residual network (or ResNet) is a standard deep neural net architecture,
with state-of-the-art performance across numerous applications. The main
premise of ResNets is that they allow the training of each layer to focus on
fitting just the residual of the previous layer&apos;s output and the target output.
Thus, we should expect that the trained network is no worse than what we can
obtain if we remove the residual layers and train a shallower network instead.
However, due to the non-convexity of the optimization problem, it is not at all
clear that ResNets indeed achieve this behavior, rather than getting stuck at
some arbitrarily poor local minimum. In this paper, we rigorously prove that
arbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the
sense that the optimization landscape contains no local minima with value above
what can be obtained with a linear predictor (namely a 1-layer network).
Notably, we show this under minimal or no assumptions on the precise network
architecture, data distribution, or loss function used. We also provide a
quantitative analysis of approximate stationary points for this problem.
Finally, we show that with a certain tweak to the architecture, training the
network with standard stochastic gradient descent achieves an objective value
close or better than any linear predictor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1&quot;&gt;Ohad Shamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04247">
<title>Reciprocal Attention Fusion for Visual Question Answering. (arXiv:1805.04247v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing attention mechanisms either attend to local image grid or object
level features for Visual Question Answering (VQA). Motivated by the
observation that questions can relate to both object instances and their parts,
we propose a novel attention mechanism that jointly considers reciprocal
relationships between the two levels of visual details. The bottom-up attention
thus generated is further coalesced with the top-down information to only focus
on the scene elements that are most relevant to a given question. Our design
hierarchically fuses multi-modal information i.e., language, object- and
gird-level features, through an efficient tensor decomposition scheme. The
proposed model improves the state-of-the-art single model performances from
67.9% to 68.2% on VQAv1 and from 65.3% to 67.4% on VQAv2, demonstrating a
significant boost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farazi_M/0/1/0/all/0/1&quot;&gt;Moshiur R Farazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04253">
<title>Argument Harvesting Using Chatbots. (arXiv:1805.04253v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04253</link>
<description rdf:parseType="Literal">&lt;p&gt;Much research in computational argumentation assumes that arguments and
counterarguments can be obtained in some way. Yet, to improve and apply models
of argument, we need methods for acquiring them. Current approaches include
argument mining from text, hand coding of arguments by researchers, or
generating arguments from knowledge bases. In this paper, we propose a new
approach, which we call argument harvesting, that uses a chatbot to enter into
a dialogue with a participant to get arguments and counterarguments from him or
her. Because it is automated, the chatbot can be used repeatedly in many
dialogues, and thereby it can generate a large corpus. We describe the
architecture of the chatbot, provide methods for managing a corpus of arguments
and counterarguments, and an evaluation of our approach in a case study
concerning attitudes of women to participation in sport.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalaguine_L/0/1/0/all/0/1&quot;&gt;Lisa A. Chalaguine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_A/0/1/0/all/0/1&quot;&gt;Anthony Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_H/0/1/0/all/0/1&quot;&gt;Henry W. W. Potts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_F/0/1/0/all/0/1&quot;&gt;Fiona L. Hamilton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04493">
<title>Interactive Reinforcement Learning with Dynamic Reuse of Prior Knowledge from Human/Agent&apos;s Demonstration. (arXiv:1805.04493v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04493</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning has enjoyed multiple successes in recent years.
However, these successes typically require very large amounts of data before an
agent achieves acceptable performance. This paper introduces a novel way of
combating such requirements by leveraging existing (human or agent) knowledge.
In particular, this paper uses demonstrations from agents and humans, allowing
an untrained agent to quickly achieve high performance. We empirically compare
with, and highlight the weakness of, HAT and CHAT, methods of transferring
knowledge from a source agent/human to a target agent. This paper introduces an
effective transfer approach, DRoP, combining the offline knowledge
(demonstrations recorded before learning) with online confidence-based
performance analysis. DRoP dynamically involves the demonstrator&apos;s knowledge,
integrating it into the reinforcement learning agent&apos;s online learning loop to
achieve efficient and robust learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaodong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02632">
<title>Extracting Action Sequences from Texts Based on Deep Reinforcement Learning. (arXiv:1803.02632v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02632</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting action sequences from natural language texts is challenging, as it
requires commonsense inferences based on world knowledge. Although there has
been work on extracting action scripts, instructions, navigation actions, etc.,
they require that either the set of candidate actions be provided in advance,
or that action descriptions are restricted to a specific form, e.g.,
description templates. In this paper, we aim to extract action sequences from
texts in free natural language, i.e., without any restricted templates,
provided the candidate set of actions is unknown. We propose to extract action
sequences from texts based on the deep reinforcement learning framework.
Specifically, we view &quot;selecting&quot; or &quot;eliminating&quot; words from texts as
&quot;actions&quot;, and the texts associated with actions as &quot;states&quot;. We then build
Q-networks to learn the policy of extracting actions and extract plans from the
labeled texts. We demonstrate the effectiveness of our approach on several
datasets with comparison to state-of-the-art approaches, including online
experiments interacting with humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wenfeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1&quot;&gt;Hankz Hankui Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00150">
<title>Memory-augmented Dialogue Management for Task-oriented Dialogue Systems. (arXiv:1805.00150v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.00150</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialogue management (DM) decides the next action of a dialogue system
according to the current dialogue state, and thus plays a central role in
task-oriented dialogue systems. Since dialogue management requires to have
access to not only local utterances, but also the global semantics of the
entire dialogue session, modeling the long-range history information is a
critical issue. To this end, we propose a novel Memory-Augmented Dialogue
management model (MAD) which employs a memory controller and two additional
memory structures, i.e., a slot-value memory and an external memory. The
slot-value memory tracks the dialogue state by memorizing and updating the
values of semantic slots (for instance, cuisine, price, and location), and the
external memory augments the representation of hidden states of traditional
recurrent neural networks through storing more context information. To update
the dialogue state efficiently, we also propose slot-level attention on user
utterances to extract specific semantic information for each slot. Experiments
show that our model can obtain state-of-the-art performance and outperforms
existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Minlie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongzhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1&quot;&gt;Feng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haiqing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04234">
<title>Distributed Deep Forest and its Application to Automatic Detection of Cash-out Fraud. (arXiv:1805.04234v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04234</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet companies are facing the need of handling large scale machine
learning applications in a daily basis, and distributed system which can handle
extra-large scale tasks is needed. Deep forest is a recently proposed deep
learning framework which uses tree ensembles as its building blocks and it has
achieved highly competitive results on various domains of tasks. However, it
has not been tested on extremely large scale tasks. In this work, based on our
parameter server system and platform of artificial intelligence, we developed
the distributed version of deep forest with an easy-to-use GUI. To the best of
our knowledge, this is the first implementation of distributed deep forest. To
meet the need of real-world tasks, many improvements are introduced to the
original deep forest model. We tested the deep forest model on an extra-large
scale task, i.e., automatic detection of cash-out fraud, with more than 100
millions of training samples. Experimental results showed that the deep forest
model has the best performance according to the evaluation metrics from
different perspectives even with very little effort for parameter tuning. This
model can block fraud transactions in a large amount of money \footnote{detail
is business confidential} each day. Even compared with the best deployed model,
deep forest model can additionally bring into a significant decrease of
economic loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya-Lin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Ji Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi-Hua Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04272">
<title>An $O(N)$ Sorting Algorithm: Machine Learning Sorting. (arXiv:1805.04272v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04272</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an $O(N)$ sorting algorithm based on Machine Learning method,
which shows a huge potential for sorting big data. This sorting algorithm can
be applied to parallel sorting and is suitable for GPU or TPU acceleration.
Furthermore, we apply this algorithm to sparse hash table.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hanqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yuehan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04276">
<title>Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis. (arXiv:1805.04276v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04276</link>
<description rdf:parseType="Literal">&lt;p&gt;Program synthesis is the task of automatically generating a program
consistent with a specification. Recent years have seen proposal of a number of
neural approaches for program synthesis, many of which adopt a sequence
generation paradigm similar to neural machine translation, in which
sequence-to-sequence models are trained to maximize the likelihood of known
reference programs. While achieving impressive results, this strategy has two
key limitations. First, it ignores Program Aliasing: the fact that many
different programs may satisfy a given specification (especially with
incomplete specifications such as a few input-output examples). By maximizing
the likelihood of only a single reference program, it penalizes many
semantically correct programs, which can adversely affect the synthesizer
performance. Second, this strategy overlooks the fact that programs have a
strict syntax that can be efficiently checked. To address the first limitation,
we perform reinforcement learning on top of a supervised model with an
objective that explicitly maximizes the likelihood of generating semantically
correct programs. For addressing the second limitation, we introduce a training
procedure that directly maximizes the probability of generating syntactically
correct programs that fulfill the specification. We show that our contributions
lead to improved accuracy of the models, especially in cases where the training
data is limited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunel_R/0/1/0/all/0/1&quot;&gt;Rudy Bunel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausknecht_M/0/1/0/all/0/1&quot;&gt;Matthew Hausknecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1&quot;&gt;Jacob Devlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04424">
<title>Novel Deep Learning Model for Traffic Sign Detection Using Capsule Networks. (arXiv:1805.04424v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.04424</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks are the most widely used deep learning
algorithms for traffic signal classification till date but they fail to capture
pose, view, orientation of the images because of the intrinsic inability of max
pooling layer.This paper proposes a novel method for Traffic sign detection
using deep learning architecture called capsule networks that achieves
outstanding performance on the German traffic sign dataset.Capsule network
consists of capsules which are a group of neurons representing the
instantiating parameters of an object like the pose and orientation by using
the dynamic routing and route by agreement algorithms.unlike the previous
approaches of manual feature extraction,multiple deep neural networks with many
parameters,our method eliminates the manual effort and provides resistance to
the spatial variances.CNNs can be fooled easily using various adversary attacks
and capsule networks can overcome such attacks from the intruders and can offer
more reliability in traffic sign detection for autonomous vehicles.Capsule
network have achieved the state-of-the-art accuracy of 97.6% on German Traffic
Sign Recognition Benchmark dataset (GTSRB).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Amara Dinesh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.09347">
<title>Quantum Machine Learning. (arXiv:1611.09347v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1611.09347</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuelled by increasing computer power and algorithmic advances, machine
learning techniques have become powerful tools for finding patterns in data.
Since quantum systems produce counter-intuitive patterns believed not to be
efficiently produced by classical systems, it is reasonable to postulate that
quantum computers may outperform classical computers on machine learning tasks.
The field of quantum machine learning explores how to devise and implement
concrete quantum software that offers such advantages. Recent work has made
clear that the hardware and software challenges are still considerable but has
also opened paths towards solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Biamonte_J/0/1/0/all/0/1&quot;&gt;Jacob Biamonte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wittek_P/0/1/0/all/0/1&quot;&gt;Peter Wittek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pancotti_N/0/1/0/all/0/1&quot;&gt;Nicola Pancotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rebentrost_P/0/1/0/all/0/1&quot;&gt;Patrick Rebentrost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wiebe_N/0/1/0/all/0/1&quot;&gt;Nathan Wiebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lloyd_S/0/1/0/all/0/1&quot;&gt;Seth Lloyd&lt;/a&gt;</dc:creator>
</item></rdf:RDF>