<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00209"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1103.5034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08094"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00002"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00030"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10467"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.00180">
<title>Machine learning and evolutionary techniques in interplanetary trajectory design. (arXiv:1802.00180v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.00180</link>
<description rdf:parseType="Literal">&lt;p&gt;After providing a brief historical overview on the synergies between
artificial intelligence research, in the areas of evolutionary computations and
machine learning, and the optimal design of interplanetary trajectories, we
propose and study the use of deep artificial neural networks to represent,
on-board, the optimal guidance profile of an interplanetary mission. The
results, limited to the chosen test case of an Earth-Mars orbital transfer,
extend the findings made previously for landing scenarios and quadcopter
dynamics, opening a new research area in interplanetary trajectory planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izzo_D/0/1/0/all/0/1&quot;&gt;Dario Izzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprague_C/0/1/0/all/0/1&quot;&gt;Christopher Sprague&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tailor_D/0/1/0/all/0/1&quot;&gt;Dharmesh Tailor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00209">
<title>Dual Recurrent Attention Units for Visual Question Answering. (arXiv:1802.00209v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an architecture for VQA which utilizes recurrent layers to
generate visual and textual attention. The memory characteristic of the
proposed recurrent attention units offers a rich joint embedding of visual and
textual features and enables the model to reason relations between several
parts of the image and question. Our single model outperforms the first place
winner on the VQA 1.0 dataset, performs within margin to the current
state-of-the-art ensemble model. We also experiment with replacing attention
mechanisms in other state-of-the-art models with our implementation and show
increased accuracy. In both cases, our recurrent attention mechanism improves
performance in tasks requiring sequential or relational reasoning on the VQA
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osman_A/0/1/0/all/0/1&quot;&gt;Ahmed Osman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00048">
<title>Deceptive Games. (arXiv:1802.00048v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00048</link>
<description rdf:parseType="Literal">&lt;p&gt;Deceptive games are games where the reward structure or other aspects of the
game are designed to lead the agent away from a globally optimal policy. While
many games are already deceptive to some extent, we designed a series of games
in the Video Game Description Language (VGDL) implementing specific types of
deception, classified by the cognitive biases they exploit. VGDL games can be
run in the General Video Game Artificial Intelligence (GVGAI) Framework, making
it possible to test a variety of existing AI agents that have been submitted to
the GVGAI Competition on these deceptive games. Our results show that all
tested agents are vulnerable to several kinds of deception, but that different
agents have different weaknesses. This suggests that we can use deception to
understand the capabilities of a game-playing algorithm, and game-playing
algorithms to characterize the deception displayed by a game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Damien Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephenson_M/0/1/0/all/0/1&quot;&gt;Matthew Stephenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salge_C/0/1/0/all/0/1&quot;&gt;Christian Salge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_J/0/1/0/all/0/1&quot;&gt;John Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1&quot;&gt;Jochen Renz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00050">
<title>Recursive Feature Generation for Knowledge-based Learning. (arXiv:1802.00050v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00050</link>
<description rdf:parseType="Literal">&lt;p&gt;When humans perform inductive learning, they often enhance the process with
background knowledge. With the increasing availability of well-formed
collaborative knowledge bases, the performance of learning algorithms could be
significantly enhanced if a way were found to exploit these knowledge bases. In
this work, we present a novel algorithm for injecting external knowledge into
induction algorithms using feature generation. Given a feature, the algorithm
defines a new learning task over its set of values, and uses the knowledge base
to solve the constructed learning task. The resulting classifier is then used
as a new feature for the original problem. We have applied our algorithm to the
domain of text classification using large semantic knowledge bases. We have
shown that the generated features significantly improve the performance of
existing learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_L/0/1/0/all/0/1&quot;&gt;Lior Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1&quot;&gt;Shaul Markovitch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00382">
<title>Classifying medical notes into standard disease codes using Machine Learning. (arXiv:1802.00382v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.00382</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the automatic classification of patient discharge notes into
standard disease labels. We find that Convolutional Neural Networks with
Attention outperform previous algorithms used in this task, and suggest further
areas for improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmakar_A/0/1/0/all/0/1&quot;&gt;Amitabha Karmakar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00420">
<title>Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. (arXiv:1802.00420v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00420</link>
<description rdf:parseType="Literal">&lt;p&gt;We identify obfuscated gradients as a phenomenon that leads to a false sense
of security in defenses against adversarial examples. While defenses that cause
obfuscated gradients appear to defeat optimization-based attacks, we find
defenses relying on this effect can be circumvented.
&lt;/p&gt;
&lt;p&gt;For each of the three types of obfuscated gradients we discover, we describe
indicators of defenses exhibiting this effect and develop attack techniques to
overcome it. In a case study, examining all defenses accepted to ICLR 2018, we
find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying
on obfuscated gradients. Using our new attack techniques, we successfully
circumvent all 7 of them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_A/0/1/0/all/0/1&quot;&gt;Anish Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1103.5034">
<title>On Understanding and Machine Understanding. (arXiv:1103.5034v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1103.5034</link>
<description rdf:parseType="Literal">&lt;p&gt;In the present paper, we try to propose a self-similar network theory for the
basic understanding. By extending the natural languages to a kind of so called
idealy sufficient language, we can proceed a few steps to the investigation of
the language searching and the language understanding of AI.
&lt;/p&gt;
&lt;p&gt;Image understanding, and the familiarity of the brain to the surrounding
environment are also discussed. Group effects are discussed by addressing the
essense of the power of influences, and constructing the influence network of a
society. We also give a discussion of inspirations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chern_T/0/1/0/all/0/1&quot;&gt;Tong Chern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01968">
<title>Faster Deep Q-learning using Neural Episodic Control. (arXiv:1801.01968v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01968</link>
<description rdf:parseType="Literal">&lt;p&gt;The research on deep reinforcement learning which estimates Q-value by deep
learning has been attracted the interest of researchers recently. In deep
reinforcement learning, it is important to efficiently learn the experiences
that an agent has collected by exploring environment. In this research, we
propose NEC2DQN that improves learning speed of a poor sample efficiency
algorithm such as DQN by using good one such as NEC at the beginning of
learning. We show it is able to learn faster than Double DQN or N-step DQN in
the experiments of Pong.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishio_D/0/1/0/all/0/1&quot;&gt;Daichi Nishio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamane_S/0/1/0/all/0/1&quot;&gt;Satoshi Yamane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08094">
<title>PRNN: Recurrent Neural Network with Persistent Memory. (arXiv:1801.08094v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08094</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Recurrent Neural Network (RNN) has been a powerful tool for modeling
sequential data, its performance is inadequate when processing sequences with
multiple patterns. In this paper, we address this challenge by introducing an
external memory and constructing a novel persistent memory augmented RNN
(termed as PRNN). The PRNN captures the principle patterns in training
sequences and stores them in an external memory. By leveraging the persistent
memory, the proposed method can adaptively update states according to the
similarities between encoded inputs and memory slots, leading to a stronger
capacity in assimilating sequences with multiple patterns. Content-based
addressing is suggested in memory accessing, and gradient descent is utilized
for implicitly updating the memory. Our approach can be further extended by
combining the prior knowledge of data. Experiments on several datasets
demonstrate the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuechuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shenghuo Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00002">
<title>DxNAT - Deep Neural Networks for Explaining Non-Recurring Traffic Congestion. (arXiv:1802.00002v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00002</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-recurring traffic congestion is caused by temporary disruptions, such as
accidents, sports games, adverse weather, etc. We use data related to real-time
traffic speed, jam factors (a traffic congestion indicator), and events
collected over a year from Nashville, TN to train a multi-layered deep neural
network. The traffic dataset contains over 900 million data records. The
network is thereafter used to classify the real-time data and identify
anomalous operations. Compared with traditional approaches of using statistical
or machine learning techniques, our model reaches an accuracy of 98.73 percent
when identifying traffic congestion caused by football games. Our approach
first encodes the traffic across a region as a scaled image. After that the
image data from different timestamps is fused with event- and time-related
data. Then a crossover operator is used as a data augmentation method to
generate training datasets with more balanced classes. Finally, we use the
receiver operating characteristic (ROC) analysis to tune the sensitivity of the
classifier. We present the analysis of the training time and the inference time
separately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+sun_F/0/1/0/all/0/1&quot;&gt;Fangzhou sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1&quot;&gt;Jules White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00030">
<title>Fusarium Damaged Kernels Detection Using Transfer Learning on Deep Neural Network Architecture. (arXiv:1802.00030v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00030</link>
<description rdf:parseType="Literal">&lt;p&gt;The present work shows the application of transfer learning for a pre-trained
deep neural network (DNN), using a small image dataset ($\approx$ 12,000) on a
single workstation with enabled NVIDIA GPU card that takes up to 1 hour to
complete the training task and archive an overall average accuracy of $94.7\%$.
The DNN presents a $20\%$ score of misclassification for an external test
dataset. The accuracy of the proposed methodology is equivalent to ones using
HSI methodology $(81\%-91\%)$ used for the same task, but with the advantage of
being independent on special equipment to classify wheat kernel for FHB
symptoms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolau_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rcio Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rcia Barrocas Moreira Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tibola_C/0/1/0/all/0/1&quot;&gt;Casiane Salete Tibola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Mauricio Cunha Fernandes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavan_W/0/1/0/all/0/1&quot;&gt;Willingthon Pavan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00150">
<title>Alternating Multi-bit Quantization for Recurrent Neural Networks. (arXiv:1802.00150v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00150</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks have achieved excellent performance in many
applications. However, on portable devices with limited resources, the models
are often too large to deploy. For applications on the server with large scale
concurrent requests, the latency during inference can also be very critical for
costly computing resources. In this work, we address these problems by
quantizing the network, both weights and activations, into multiple binary
codes {-1,+1}. We formulate the quantization as an optimization problem. Under
the key observation that once the quantization coefficients are fixed the
binary codes can be derived efficiently by binary search tree, alternating
minimization is then applied. We test the quantization for two well-known RNNs,
i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the
language models. Compared with the full-precision counter part, by 2-bit
quantization we can achieve ~16x memory saving and ~6x real inference
acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit
quantization, we can achieve almost no loss in the accuracy or even surpass the
original model, with ~10.5x memory saving and ~3x real inference acceleration.
Both results beat the exiting quantization works with large margins. We extend
our alternating quantization to image classification tasks. In both RNNs and
feedforward neural networks, the method also achieves excellent performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jianqiang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhouchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1&quot;&gt;Wenwu Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuanbin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhirong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongbin Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00324">
<title>One-class Collective Anomaly Detection based on Long Short-Term Memory Recurrent Neural Networks. (arXiv:1802.00324v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.00324</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrusion detection for computer network systems has been becoming one of the
most critical tasks for network administrators today. It has an important role
for organizations, governments and our society due to the valuable resources
hosted on computer networks. Traditional misuse detection strategies are unable
to detect new and unknown intrusion types. In contrast, anomaly detection in
network security aims to distinguish between illegal or malicious events and
normal behavior of network systems. Anomaly detection can be considered as a
classification problem where it builds models of normal network behavior, of
which it uses to detect new patterns that significantly deviate from the model.
Most of the current approaches on anomaly detection is based on the learning of
normal behavior and anomalous actions. They do not include memory that is they
do not take into account previous events classify new ones. In this paper, we
propose a one class collective anomaly detection model based on neural network
learning. Normally a Long Short Term Memory Recurrent Neural Network (LSTM RNN)
is trained only on normal data, and it is capable of predicting several time
steps ahead of an input. In our approach, a LSTM RNN is trained on normal time
series data before performing a prediction for each time step. Instead of
considering each time-step separately, the observation of prediction errors
from a certain number of time-steps is now proposed as a new idea for detecting
collective anomalies. The prediction errors of a certain number of the latest
time-steps above a threshold will indicate a collective anomaly. The model is
evaluated on a time series version of the KDD 1999 dataset. The experiments
demonstrate that the proposed model is capable to detect collective anomaly
efficiently
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thi_N/0/1/0/all/0/1&quot;&gt;Nga Nguyen Thi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_V/0/1/0/all/0/1&quot;&gt;Van Loi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Khac_N/0/1/0/all/0/1&quot;&gt;Nhien-An Le-Khac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02444">
<title>Global optimality conditions for deep neural networks. (arXiv:1707.02444v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02444</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the error landscape of deep linear and nonlinear neural networks
with the squared error loss. Minimizing the loss of a deep linear neural
network is a nonconvex problem, and despite recent progress, our understanding
of this loss surface is still incomplete. For deep linear networks, we present
necessary and sufficient conditions for a critical point of the risk function
to be a global minimum. Our conditions provide an efficiently checkable test
for global optimality, which is remarkable because such tests are typically
intractable in nonconvex optimization. We further extend these results to deep
nonlinear neural networks and prove similar sufficient conditions for global
optimality, albeit in a more limited function space setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1&quot;&gt;Chulhee Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1&quot;&gt;Suvrit Sra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04875">
<title>Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting. (arXiv:1709.04875v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04875</link>
<description rdf:parseType="Literal">&lt;p&gt;Timely accurate traffic forecast is crucial for urban traffic control and
guidance. Due to the high nonlinearity and complexity of traffic flow,
traditional methods cannot satisfy the requirements of mid-and-long term
prediction tasks and often neglect spatial and temporal dependencies. In this
paper, we propose a novel deep learning framework, Spatio-Temporal Graph
Convolutional Networks (STGCN), to tackle the time series prediction problem in
traffic domain. Instead of applying regular convolutional and recurrent units,
we formulate the problem on graphs and build the model with complete
convolutional structures, which enable much faster training speed with fewer
parameters. Experiments show that our STGCN model effectively captures
comprehensive spatio-temporal correlations through modeling multi-scale traffic
networks and consistently outperforms state-of-the-art baselines on various
real-world traffic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Haoteng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhanxing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10467">
<title>Generalized End-to-End Loss for Speaker Verification. (arXiv:1710.10467v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10467</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new loss function called generalized end-to-end
(GE2E) loss, which makes the training of speaker verification models more
efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike
TE2E, the GE2E loss function updates the network in a way that emphasizes
examples that are difficult to verify at each step of the training process.
Additionally, the GE2E loss does not require an initial stage of example
selection. With these properties, our model with the new loss function
decreases speaker verification EER by more than 10%, while reducing the
training time by 60% at the same time. We also introduce the MultiReader
technique, which allows us to do domain adaptation - training a more accurate
model that supports multiple keywords (i.e. &quot;OK Google&quot; and &quot;Hey Google&quot;) as
well as multiple dialects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Li Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Papir_A/0/1/0/all/0/1&quot;&gt;Alan Papir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1&quot;&gt;Ignacio Lopez Moreno&lt;/a&gt;</dc:creator>
</item></rdf:RDF>