<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10273"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10402"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10597"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00449"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.10546">
<title>Multi-Layer Competitive-Cooperative Framework for Performance Enhancement of Differential Evolution. (arXiv:1801.10546v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.10546</link>
<description rdf:parseType="Literal">&lt;p&gt;Differential Evolution (DE) is one of the most powerful optimizers in the
evolutionary algorithm (EA) family. In recent years, many DE variants have been
proposed to enhance performance. However, when compared with each other,
significant differences in performances are seldomly observed. To meet this
challenge of a more significant improvement, this paper proposes a multi-layer
competitive-cooperative (MLCC) framework to combine the advantages of multiple
DEs. Existing multi-method strategies commonly use a multi-population based
structure, which classifies the entire population into several subpopulations
and evolve individuals only in their corresponding subgroups. MLCC proposes to
implement a parallel structure with the entire population simultaneously
monitored by multiple DEs assigned in multiple layers. Each individual can
store, utilize and update its evolution information in different layers by
using a novel individual preference based layer selecting (IPLS) mechanism and
a computational resource allocation bias (RAB) mechanism. In IPLS, individuals
only connect to one favorite layer. While in RAB, high quality solutions are
evolved by considering all the layers. In this way, the multiple layers work in
a competitive and cooperative manner. The proposed MLCC framework has been
implemented on several highly competitive DEs. Experimental studies show that
MLCC variants significantly outperform the baseline DEs as well as several
state-of-the-art and up-to-date DEs on the CEC benchmark functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Li Ming Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kit Sang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shao Yong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Wing Shing Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10467">
<title>Deep Reinforcement Learning for Programming Language Correction. (arXiv:1801.10467v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10467</link>
<description rdf:parseType="Literal">&lt;p&gt;Novice programmers often struggle with the formal syntax of programming
languages. To assist them, we design a novel programming language correction
framework amenable to reinforcement learning. The framework allows an agent to
mimic human actions for text navigation and editing. We demonstrate that the
agent can be trained through self-exploration directly from the raw input, that
is, program text itself, without any knowledge of the formal syntax of the
programming language. We leverage expert demonstrations for one tenth of the
training data to accelerate training. The proposed technique is evaluated on
6975 erroneous C programs with typographic errors, written by students during
an introductory programming course. Our technique fixes 14% more programs and
29% more compiler error messages relative to those fixed by a state-of-the-art
tool, DeepFix, which uses a fully supervised neural machine translation
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Rahul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1&quot;&gt;Aditya Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1&quot;&gt;Shirish Shevade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00066">
<title>Fraternal Dropout. (arXiv:1711.00066v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00066</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are important class of architectures among
neural networks useful for language modeling and sequential prediction.
However, optimizing RNNs is known to be harder compared to feed-forward neural
networks. A number of techniques have been proposed in literature to address
this problem. In this paper we propose a simple technique called fraternal
dropout that takes advantage of dropout to achieve this goal. Specifically, we
propose to train two identical copies of an RNN (that share parameters) with
different dropout masks while minimizing the difference between their
(pre-softmax) predictions. In this way our regularization encourages the
representations of RNNs to be invariant to dropout mask, thus being robust. We
show that our regularization term is upper bounded by the expectation-linear
dropout objective which has been shown to address the gap due to the difference
between the train and inference phases of dropout. We evaluate our model and
achieve state-of-the-art results in sequence modeling tasks on two benchmark
datasets - Penn Treebank and Wikitext-2. We also show that our approach leads
to performance improvement by a significant margin in image captioning
(Microsoft COCO) and semi-supervised (CIFAR-10) tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zolna_K/0/1/0/all/0/1&quot;&gt;Konrad Zolna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arpit_D/0/1/0/all/0/1&quot;&gt;Devansh Arpit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suhubdy_D/0/1/0/all/0/1&quot;&gt;Dendi Suhubdy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10158">
<title>Learning to Classify from Impure Samples. (arXiv:1801.10158v1 [hep-ph])</title>
<link>http://arxiv.org/abs/1801.10158</link>
<description rdf:parseType="Literal">&lt;p&gt;A persistent challenge in practical classification tasks is that labelled
training sets are not always available. In particle physics, this challenge is
surmounted by the use of simulations. These simulations accurately reproduce
most features of data, but cannot be trusted to capture all of the complex
correlations exploitable by modern machine learning methods. Recent work in
weakly supervised learning has shown that simple, low-dimensional classifiers
can be trained using only the impure mixtures present in data. Here, we
demonstrate that complex, high-dimensional classifiers can also be trained on
impure mixtures using weak supervision techniques, with performance comparable
to what could be achieved with pure samples. Using weak supervision will
therefore allow us to avoid relying exclusively on simulations for
high-dimensional classification. This work opens the door to a new regime
whereby complex models are trained directly on data, providing direct access to
probe the underlying physics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Komiske_P/0/1/0/all/0/1&quot;&gt;Patrick T. Komiske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Metodiev_E/0/1/0/all/0/1&quot;&gt;Eric M. Metodiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Nachman_B/0/1/0/all/0/1&quot;&gt;Benjamin Nachman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Schwartz_M/0/1/0/all/0/1&quot;&gt;Matthew D. Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10273">
<title>Kernel Distillation for Gaussian Processes. (arXiv:1801.10273v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10273</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian processes (GPs) are flexible models that can capture complex
structure in large-scale dataset due to their non-parametric nature. However,
the usage of GPs in real-world application is limited due to their high
computational cost at inference time. In this paper, we introduce a new
framework, \textit{kernel distillation}, for kernel matrix approximation. The
idea adopts from knowledge distillation in deep learning community, where we
approximate a fully trained teacher kernel matrix of size $n\times n$ with a
student kernel matrix. We combine inducing points method with sparse low-rank
approximation in the distillation procedure. The distilled student kernel
matrix only cost $\mathcal{O}(m^2)$ storage where $m$ is the number of inducing
points and $m \ll n$. We also show that one application of kernel distillation
is for fast GP prediction, where we demonstrate empirically that our
approximation provide better balance between the prediction time and the
predictive performance compared to the alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Congzheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiming Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10402">
<title>Deep Multi-view Learning to Rank. (arXiv:1801.10402v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.10402</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning to rank from multiple sources. Though
multi-view learning and learning to rank have been studied extensively leading
to a wide range of applications, multi-view learning to rank as a synergy of
both topics has received little attention. The aim of the paper is to propose a
composite ranking method while keeping a close correlation with the individual
rankings simultaneously. We propose a multi-objective solution to ranking by
capturing the information of the feature mapping from both within each view as
well as across views using autoencoder-like networks. Moreover, a novel
end-to-end solution is introduced to enhance the joint ranking with minimum
view-specific ranking loss, so that we can achieve the maximum global view
agreements within a single optimization process. The proposed method is
validated on a wide variety of ranking problems, including university ranking,
multi-view lingual text ranking and image data ranking, providing superior
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1&quot;&gt;Guanqun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1&quot;&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1&quot;&gt;Vijay Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottumukkala_R/0/1/0/all/0/1&quot;&gt;Raju Gottumukkala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10597">
<title>Model compression for faster structural separation of macromolecules captured by Cellular Electron Cryo-Tomography. (arXiv:1801.10597v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1801.10597</link>
<description rdf:parseType="Literal">&lt;p&gt;Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule
structure inside single cells. Macromolecule classification approaches based on
convolutional neural networks (CNN) were developed to separate millions of
macromolecules captured from ECT systematically. However, given the fast
accumulation of ECT data, it will soon become necessary to use CNN models to
efficiently and accurately separate substantially more macromolecules at the
prediction stage, which requires additional computational costs. To speed up
the prediction, we compress classification models into compact neural networks
with little in accuracy for deployment. Specifically, we propose to perform
model compression through knowledge distillation. Firstly, a complex teacher
network is trained to generate soft labels with better classification
feasibility followed by training of customized student networks with simple
architectures using the soft label to compress model complexity. Our tests
demonstrate that our compressed models significantly reduce the number of
parameters and time cost while maintaining similar classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jialiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Freyberg_Z/0/1/0/all/0/1&quot;&gt;Zachary Freyberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00449">
<title>Attacking Binarized Neural Networks. (arXiv:1711.00449v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00449</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks with low-precision weights and activations offer compelling
efficiency advantages over their full-precision equivalents. The two most
frequently discussed benefits of quantization are reduced memory consumption,
and a faster forward pass when implemented with efficient bitwise operations.
We propose a third benefit of very low-precision neural networks: improved
robustness against some adversarial attacks, and in the worst case, performance
that is on par with full-precision models. We focus on the very low-precision
case where weights and activations are both quantized to $\pm$1, and note that
stochastically quantizing weights in just one layer can sharply reduce the
impact of iterative attacks. We observe that non-scaled binary neural networks
exhibit a similar effect to the original defensive distillation procedure that
led to gradient masking, and a false notion of security. We address this by
conducting both black-box and white-box experiments with binary models that do
not artificially mask gradients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galloway_A/0/1/0/all/0/1&quot;&gt;Angus Galloway&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moussa_M/0/1/0/all/0/1&quot;&gt;Medhat Moussa&lt;/a&gt;</dc:creator>
</item></rdf:RDF>