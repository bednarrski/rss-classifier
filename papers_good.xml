<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03523"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.07285"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03526"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03625"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.06460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03749"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00282"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02642"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.03523">
<title>Generative Models for Stochastic Processes Using Convolutional Neural Networks. (arXiv:1801.03523v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.03523</link>
<description rdf:parseType="Literal">&lt;p&gt;The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neto_F/0/1/0/all/0/1&quot;&gt;Fernando Fernandes Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.07285">
<title>A guide to convolution arithmetic for deep learning. (arXiv:1603.07285v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1603.07285</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a guide to help deep learning practitioners understand and
manipulate convolutional neural network architectures. The guide clarifies the
relationship between various properties (input shape, kernel shape, zero
padding, strides and output shape) of convolutional, pooling and transposed
convolutional layers, as well as the relationship between convolutional and
transposed convolutional layers. Relationships are derived for various cases,
and are illustrated in order to make them intuitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dumoulin_V/0/1/0/all/0/1&quot;&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Visin_F/0/1/0/all/0/1&quot;&gt;Francesco Visin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03526">
<title>Neural Program Synthesis with Priority Queue Training. (arXiv:1801.03526v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03526</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of program synthesis in the presence of a reward
function over the output of programs, where the goal is to find programs with
maximal rewards. We employ an iterative optimization scheme, where we train an
RNN on a dataset of K best programs from a priority queue of the generated
programs so far. Then, we synthesize new programs and add them to the priority
queue by sampling from the RNN. We benchmark our algorithm, called priority
queue training (or PQT), against genetic algorithm and reinforcement learning
baselines on a simple but expressive Turing complete programming language
called BF. Our experimental results show that our simple PQT algorithm
significantly outperforms the baselines. By adding a program length penalty to
the reward function, we are able to synthesize short, human readable programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abolafia_D/0/1/0/all/0/1&quot;&gt;Daniel A. Abolafia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03604">
<title>Conversational AI: The Science Behind the Alexa Prize. (arXiv:1801.03604v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03604</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational agents are exploding in popularity. However, much work remains
in the area of social conversation as well as free-form conversation over a
broad range of domains and topics. To advance the state of the art in
conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar
university competition where sixteen selected university teams were challenged
to build conversational agents, known as socialbots, to converse coherently and
engagingly with humans on popular topics such as Sports, Politics,
Entertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers
the academic community a unique opportunity to perform research with a live
system used by millions of users. The competition provided university teams
with real user conversational data at scale, along with the user-provided
ratings and feedback augmented with annotations by the Alexa team. This enabled
teams to effectively iterate and make improvements throughout the competition
while being evaluated in real-time through live user interactions. To build
their socialbots, university teams combined state-of-the-art techniques with
novel strategies in the areas of Natural Language Understanding, Context
Modeling, Dialog Management, Response Generation, and Knowledge Acquisition. To
support the efforts of participating teams, the Alexa Prize team made
significant scientific and engineering investments to build and improve
Conversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice
User Experience, and tools for traffic management and scalability. This paper
outlines the advances created by the university teams as well as the Alexa
Prize team to achieve the common goal of solving the problem of Conversational
AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_A/0/1/0/all/0/1&quot;&gt;Ashwin Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1&quot;&gt;Rohit Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_C/0/1/0/all/0/1&quot;&gt;Chandra Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_A/0/1/0/all/0/1&quot;&gt;Anu Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_R/0/1/0/all/0/1&quot;&gt;Raefer Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunn_J/0/1/0/all/0/1&quot;&gt;Jeff Nunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1&quot;&gt;Behnam Hedayatnia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagar_A/0/1/0/all/0/1&quot;&gt;Ashish Nagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_E/0/1/0/all/0/1&quot;&gt;Eric King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bland_K/0/1/0/all/0/1&quot;&gt;Kate Bland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wartick_A/0/1/0/all/0/1&quot;&gt;Amanda Wartick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Han Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayadevan_S/0/1/0/all/0/1&quot;&gt;Sk Jayadevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1&quot;&gt;Gene Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pettigrue_A/0/1/0/all/0/1&quot;&gt;Art Pettigrue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03622">
<title>Topic-based Evaluation for Conversational Bots. (arXiv:1801.03622v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.03622</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialog evaluation is a challenging problem, especially for non task-oriented
dialogs where conversational success is not well-defined. We propose to
evaluate dialog quality using topic-based metrics that describe the ability of
a conversational bot to sustain coherent and engaging conversations on a topic,
and the diversity of topics that a bot can handle. To detect conversation
topics per utterance, we adopt Deep Average Networks (DAN) and train a topic
classifier on a variety of question and query data categorized into multiple
topics. We propose a novel extension to DAN by adding a topic-word attention
table that allows the system to jointly capture topic keywords in an utterance
and perform topic classification. We compare our proposed topic based metrics
with the ratings provided by users and show that our metrics both correlate
with and complement human judgment. Our analysis is performed on tens of
thousands of real human-bot dialogs from the Alexa Prize competition and
highlights user expectations for conversational bots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fenfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metallinou_A/0/1/0/all/0/1&quot;&gt;Angeliki Metallinou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_C/0/1/0/all/0/1&quot;&gt;Chandra Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1&quot;&gt;Anirudh Raju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_A/0/1/0/all/0/1&quot;&gt;Anu Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_A/0/1/0/all/0/1&quot;&gt;Ashwin Ram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03625">
<title>On Evaluating and Comparing Conversational Agents. (arXiv:1801.03625v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.03625</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational agents are exploding in popularity. However, much work remains
in the area of non goal-oriented conversations, despite significant growth in
research interest over recent years. To advance the state of the art in
conversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar
university competition where sixteen selected university teams built
conversational agents to deliver the best social conversational experience.
Alexa Prize provided the academic community with the unique opportunity to
perform research with a live system used by millions of users. The subjectivity
associated with evaluating conversations is key element underlying the
challenge of building non-goal oriented dialogue systems. In this paper, we
propose a comprehensive evaluation strategy with multiple metrics designed to
reduce subjectivity by selecting metrics which correlate well with human
judgement. The proposed metrics provide granular analysis of the conversational
agents, which is not captured in human ratings. We show that these metrics can
be used as a reasonable proxy for human judgment. We provide a mechanism to
unify the metrics for selecting the top performing agents, which has also been
applied throughout the Alexa Prize competition. To our knowledge, to date it is
the largest setting for evaluating agents with millions of conversations and
hundreds of thousands of ratings from users. We believe that this work is a
step towards an automatic evaluation process for conversational AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_A/0/1/0/all/0/1&quot;&gt;Anu Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_C/0/1/0/all/0/1&quot;&gt;Chandra Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_A/0/1/0/all/0/1&quot;&gt;Ashwin Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fenfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_R/0/1/0/all/0/1&quot;&gt;Raefer Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagar_A/0/1/0/all/0/1&quot;&gt;Ashish Nagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1&quot;&gt;Rohit Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1&quot;&gt;Behnam Hedayatnia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metallinou_A/0/1/0/all/0/1&quot;&gt;Angeliki Metallinou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1&quot;&gt;Rahul Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shaohua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1&quot;&gt;Anirudh Raju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03737">
<title>Counterfactual equivalence for POMDPs, and underlying deterministic environments. (arXiv:1801.03737v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03737</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially Observable Markov Decision Processes (POMDPs) are rich environments
often used in machine learning. But the issue of information and causal
structures in POMDPs has been relatively little studied. This paper presents
the concepts of equivalent and counterfactually equivalent POMDPs, where agents
cannot distinguish which environment they are in though any observations and
actions. It shows that any POMDP is counterfactually equivalent, for any finite
number of turns, to a deterministic POMDP with all uncertainty concentrated
into the initial state. This allows a better understanding of POMDP
uncertainty, information, and learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Stuart Armstrong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00984">
<title>Sentence Object Notation: Multilingual sentence notation based on Wordnet. (arXiv:1801.00984v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00984</link>
<description rdf:parseType="Literal">&lt;p&gt;The representation of sentences is a very important task. It can be used as a
way to exchange data inter-applications. One main characteristic, that a
notation must have, is a minimal size and a representative form. This can
reduce the transfer time, and hopefully the processing time as well.
&lt;/p&gt;
&lt;p&gt;Usually, sentence representation is associated to the processed language. The
grammar of this language affects how we represent the sentence. To avoid
language-dependent notations, we have to come up with a new representation
which don&apos;t use words, but their meanings. This can be done using a lexicon
like wordnet, instead of words we use their synsets. As for syntactic
relations, they have to be universal as much as possible.
&lt;/p&gt;
&lt;p&gt;Our new notation is called STON &quot;SenTences Object Notation&quot;, which somehow
has similarities to JSON. It is meant to be minimal, representative and
language-independent syntactic representation. Also, we want it to be readable
and easy to be created. This simplifies developing simple automatic generators
and creating test banks manually. Its benefit is to be used as a medium between
different parts of applications like: text summarization, language translation,
etc. The notation is based on 4 languages: Arabic, English, Franch and
Japanese; and there are some cases where these languages don&apos;t agree on one
representation. Also, given the diversity of grammatical structure of different
world languages, this annotation may fail for some languages which allows more
future improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aries_A/0/1/0/all/0/1&quot;&gt;Abdelkrime Aries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zegour_D/0/1/0/all/0/1&quot;&gt;Djamel Eddine Zegour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hidouci_W/0/1/0/all/0/1&quot;&gt;Walid Khaled Hidouci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.06460">
<title>Evolving Ensemble Fuzzy Classifier. (arXiv:1705.06460v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1705.06460</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of ensemble learning offers a promising avenue in learning from
data streams under complex environments because it addresses the bias and
variance dilemma better than its single model counterpart and features a
reconfigurable structure, which is well suited to the given context. While
various extensions of ensemble learning for mining non-stationary data streams
can be found in the literature, most of them are crafted under a static base
classifier and revisits preceding samples in the sliding window for a
retraining step. This feature causes computationally prohibitive complexity and
is not flexible enough to cope with rapidly changing environments. Their
complexities are often demanding because it involves a large collection of
offline classifiers due to the absence of structural complexities reduction
mechanisms and lack of an online feature selection mechanism. A novel evolving
ensemble classifier, namely Parsimonious Ensemble pENsemble, is proposed in
this paper. pENsemble differs from existing architectures in the fact that it
is built upon an evolving classifier from data streams, termed Parsimonious
Classifier pClass. pENsemble is equipped by an ensemble pruning mechanism,
which estimates a localized generalization error of a base classifier. A
dynamic online feature selection scenario is integrated into the pENsemble.
This method allows for dynamic selection and deselection of input features on
the fly. pENsemble adopts a dynamic ensemble structure to output a final
classification decision where it features a novel drift detection scenario to
grow the ensemble structure. The efficacy of the pENsemble has been numerically
demonstrated through rigorous numerical studies with dynamic and evolving data
streams where it delivers the most encouraging performance in attaining a
tradeoff between accuracy and complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1&quot;&gt;Witold Pedrycz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1&quot;&gt;Edwin Lughofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03744">
<title>Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?. (arXiv:1801.03744v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.03744</link>
<description rdf:parseType="Literal">&lt;p&gt;We give a rigorous analysis of the statistical behavior of gradients in
randomly initialized feed-forward networks with ReLU activations. Our results
show that a fully connected depth $d$ ReLU net with hidden layer widths $n_j$
will have exploding and vanishing gradients if and only if $\sum_{j=1}^{d-1}
1/n_j$ is large. The point of view of this article is that whether a given
neural net will have exploding/vanishing gradients is a function mainly of the
architecture of the net, and hence can be tested at initialization. Our results
imply that a fully connected network that produces manageable gradients at
initialization must have many hidden layers that are about as wide as the
network is deep. This work is related to the mean field theory approach to
random neural nets. From this point of view, we give a rigorous computation of
the $1/n_j$ corrections to the propagation of gradients at the so-called edge
of chaos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03749">
<title>Improved asynchronous parallel optimization analysis for stochastic incremental methods. (arXiv:1801.03749v1 [math.OC])</title>
<link>http://arxiv.org/abs/1801.03749</link>
<description rdf:parseType="Literal">&lt;p&gt;As datasets continue to increase in size and multi-core computer
architectures are developed, asynchronous parallel optimization algorithms
become more and more essential to the field of Machine Learning. Unfortunately,
conducting the theoretical analysis asynchronous methods is difficult, notably
due to the introduction of delay and inconsistency in inherently sequential
algorithms. Handling these issues often requires resorting to simplifying but
unrealistic assumptions. Through a novel perspective, we revisit and clarify a
subtle but important technical issue present in a large fraction of the recent
convergence rate proofs for asynchronous parallel optimization algorithms, and
propose a simplification of the recently introduced &quot;perturbed iterate&quot;
framework that resolves it. We demonstrate the usefulness of our new framework
by analyzing three distinct asynchronous parallel incremental optimization
algorithms: Hogwild (asynchronous SGD), KROMAGNON (asynchronous SVRG) and
ASAGA, a novel asynchronous parallel version of the incremental gradient
algorithm SAGA that enjoys fast linear convergence rates. We are able to both
remove problematic assumptions and obtain better theoretical results. Notably,
we prove that ASAGA and KROMAGNON can obtain a theoretical linear speedup on
multi-core systems even without sparsity assumptions. We present results of an
implementation on a 40-core architecture illustrating the practical speedups as
well as the hardware overhead. Finally, we investigate the overlap constant, an
ill-understood but central quantity for the theoretical analysis of
asynchronous parallel algorithms. We find that it encompasses much more
complexity than suggested in previous work, and often is order-of-magnitude
bigger than traditionally thought.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Leblond_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Leblond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pederegosa_F/0/1/0/all/0/1&quot;&gt;Fabian Pederegosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lacoste_Julien_S/0/1/0/all/0/1&quot;&gt;Simon Lacoste-Julien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00282">
<title>Using Deep Neural Network Approximate Bayesian Network. (arXiv:1801.00282v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00282</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new method to approximate posterior probabilities of Bayesian
Network using Deep Neural Network. Experiment results on several public
Bayesian Network datasets shows that Deep Neural Network is capable of learning
joint probability distri- bution of Bayesian Network by learning from a few
observation and posterior probability distribution pairs with high accuracy.
Compared with traditional approximate method likelihood weighting sampling
algorithm, our method is much faster and gains higher accuracy in medium sized
Bayesian Network. Another advantage of our method is that our method can be
parallelled much easier in GPU without extra effort. We also ex- plored the
connection between the accuracy of our model and the number of training
examples. The result shows that our model saturate as the number of training
examples grow and we don&apos;t need many training examples to get reasonably good
result. Another contribution of our work is that we have shown discriminative
model like Deep Neural Network can approximate generative model like Bayesian
Network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jie Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honggang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunchun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02642">
<title>Boundary Optimizing Network (BON). (arXiv:1801.02642v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02642</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite all the success that deep neural networks have seen in classifying
certain datasets, the challenge of finding optimal solutions that generalize
well still remains. In this paper, we propose the Boundary Optimizing Network
(BON), a new approach to generalization for deep neural networks when used for
supervised learning. Given a classification network, we propose to use a
collaborative generative network that produces new synthetic data points in the
form of perturbations of original data points. In this way, we create a data
support around each original data point which prevents decision boundaries to
pass too close to the original data points, i.e. prevents overfitting. To
prevent catastrophic forgetting during training, we propose to use a variation
of Memory Aware Synapses to optimize the generative networks. On the Iris
dataset, we show that the BON algorithm creates better decision boundaries when
compared to a network regularized by the popular dropout scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Marco Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1&quot;&gt;Akshay Pai&lt;/a&gt;</dc:creator>
</item></rdf:RDF>