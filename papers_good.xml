<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01653"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09384"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09442"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07855"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07036"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07187"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07645"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08994"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09372"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00209"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06901"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07018"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.09058">
<title>Quantum enhanced cross-validation for near-optimal neural networks architecture selection. (arXiv:1808.09058v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1808.09058</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a quantum-classical algorithm to evaluate and select
classical artificial neural networks architectures. The proposed algorithm is
based on a probabilistic quantum memory and the possibility to train artificial
neural networks in superposition. We obtain an exponential quantum speedup in
the evaluation of neural networks. We also verify experimentally through a
reduced experimental analysis that the proposed algorithm can be used to select
near-optimal neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Santos_P/0/1/0/all/0/1&quot;&gt;Priscila G. M. dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sousa_R/0/1/0/all/0/1&quot;&gt;Rodrigo S. Sousa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Araujo_I/0/1/0/all/0/1&quot;&gt;Ismael C. S. Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Adenilton J. da Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10615">
<title>SqueezeNext: Hardware-Aware Neural Network Design. (arXiv:1803.10615v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10615</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main barriers for deploying neural networks on embedded systems
has been large memory and power consumption of existing neural networks. In
this work, we introduce SqueezeNext, a new family of neural network
architectures whose design was guided by considering previous architectures
such as SqueezeNet, as well as by simulation results on a neural network
accelerator. This new network is able to match AlexNet&apos;s accuracy on the
ImageNet benchmark with $112\times$ fewer parameters, and one of its deeper
variants is able to achieve VGG-19 accuracy with only 4.4 Million parameters,
($31\times$ smaller than VGG-19). SqueezeNext also achieves better top-5
classification accuracy with $1.3\times$ fewer parameters as compared to
MobileNet, but avoids using depthwise-separable convolutions that are
inefficient on some mobile processor platforms. This wide range of accuracy
gives the user the ability to make speed-accuracy tradeoffs, depending on the
available resources on the target hardware. Using hardware simulation results
for power and inference speed on an embedded system has guided us to design
variations of the baseline model that are $2.59\times$/$8.26\times$ faster and
$2.25\times$/$7.5\times$ more energy efficient as compared to
SqueezeNet/AlexNet without any accuracy degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1&quot;&gt;Amir Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_K/0/1/0/all/0/1&quot;&gt;Kiseok Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Z/0/1/0/all/0/1&quot;&gt;Zizheng Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peter Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01653">
<title>Review of Deep Learning. (arXiv:1804.01653v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01653</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, China, the United States and other countries, Google and
other high-tech companies have increased investment in artificial intelligence.
Deep learning is one of the current artificial intelligence research&apos;s key
areas. This paper analyzes and summarizes the latest progress and future
research directions of deep learning. Firstly, three basic models of deep
learning are outlined, including multilayer perceptrons, convolutional neural
networks, and recurrent neural networks. On this basis, we further analyze the
emerging new models of convolution neural networks and recurrent neural
networks. This paper then summarizes deep learning&apos;s applications in many areas
of artificial intelligence, including speech processing, computer vision,
natural language processing and so on. Finally, this paper discusses the
existing problems of deep learning and gives the corresponding possible
solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1&quot;&gt;Tong Mo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08568">
<title>Continuous Learning in Single-Incremental-Task Scenarios. (arXiv:1806.08568v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.08568</link>
<description rdf:parseType="Literal">&lt;p&gt;It was recently shown that architectural, regularization and rehearsal
strategies can be used to train deep models sequentially on a number of
disjoint tasks without forgetting previously acquired knowledge. However, these
strategies are still unsatisfactory if the tasks are not disjoint but
constitute a single incremental task (e.g., class-incremental learning). In
this paper we point out the differences between multi-task and
single-incremental-task scenarios and show that well-known approaches such as
LWF, EWC and SI are not ideal for incremental task scenarios. A new approach,
denoted as AR1, combining architectural and regularization strategies is then
specifically proposed. AR1 overhead (in term of memory and computation) is very
small thus making it suitable for online learning. When tested on CORe50 and
iCIFAR-100, AR1 outperformed existing regularization strategies by a good
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1&quot;&gt;Davide Maltoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1&quot;&gt;Vincenzo Lomonaco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09016">
<title>Review Helpfulness Assessment based on Convolutional Neural Network. (arXiv:1808.09016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.09016</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we describe the implementation of a convolutional neural
network (CNN) used to assess online review helpfulness. To our knowledge, this
is the first use of this architecture to address this problem. We explore the
impact of two related factors impacting CNN performance: different word
embedding initializations and different input review lengths. We also propose
an approach to combining rating star information with review text to further
improve prediction accuracy. We demonstrate that this can improve the overall
accuracy by 2%. Finally, we evaluate the method on a benchmark dataset and show
an improvement in accuracy relative to published results for traditional
methods of 2.5% for a model trained using only review text and 4.24% for a
model trained on a combination of rating star information and review text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xianshan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rose_J/0/1/0/all/0/1&quot;&gt;John R. Rose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09293">
<title>A Summary Description of the A2RD Project. (arXiv:1808.09293v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.09293</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper brieviy describes the Autonomous Architecture Over Restricted
Domains project (A2RD). In It begins with the description of the context upon
which the project is focused, and in the sequence describes the A2RD abstract
and implementation models. Finish by presenting the environment conceptual
model named Structure for Knowledge Acquisition, Use and Collaboration Inter
A2RD Agents (SKAU), showing where stand the components, inputs and facilities
required to interact among the intelligent agents of the various
implementations in their respective and restricted routing domains (Autonomous
Systems) that build the Internet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09333">
<title>Bridging Knowledge Gaps in Neural Entailment via Symbolic Models. (arXiv:1808.09333v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09333</link>
<description rdf:parseType="Literal">&lt;p&gt;Most textual entailment models focus on lexical gaps between the premise text
and the hypothesis, but rarely on knowledge gaps. We focus on filling these
knowledge gaps in the Science Entailment task, by leveraging an external
structured knowledge base (KB) of science facts. Our new architecture combines
standard neural entailment models with a knowledge lookup module. To facilitate
this lookup, we propose a fact-level decomposition of the hypothesis, and
verifying the resulting sub-facts against both the textual premise and the
structured KB. Our model, NSnet, learns to aggregate predictions from these
heterogeneous data formats. On the SciTail dataset, NSnet outperforms a simpler
combination of the two predictions by 3% and the base entailment model by 5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dongyeop Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1&quot;&gt;Tushar Khot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1&quot;&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09384">
<title>What Makes Reading Comprehension Questions Easier?. (arXiv:1808.09384v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09384</link>
<description rdf:parseType="Literal">&lt;p&gt;A challenge in creating a dataset for machine reading comprehension (MRC) is
to collect questions that require a sophisticated understanding of language to
answer beyond using superficial cues. In this work, we investigate what makes
questions easier across recent 12 MRC datasets with three question styles
(answer extraction, description, and multiple choice). We propose to employ
simple heuristics to split each dataset into easy and hard subsets and examine
the performance of two baseline models for each of the subsets. We then
manually annotate questions sampled from each subset with both validity and
requisite reasoning skills to investigate which skills explain the difference
between easy and hard questions. From this study, we observed that (i) the
baseline performances for the hard subsets remarkably degrade compared to those
of entire datasets, (ii) hard questions require knowledge inference and
multiple-sentence reasoning in comparison with easy questions, and (iii)
multiple-choice questions tend to require a broader range of reasoning skills
than answer extraction and description questions. These results suggest that
one might overestimate recent advances in MRC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1&quot;&gt;Saku Sugawara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1&quot;&gt;Kentaro Inui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekine_S/0/1/0/all/0/1&quot;&gt;Satoshi Sekine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1&quot;&gt;Akiko Aizawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09442">
<title>Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning. (arXiv:1808.09442v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09442</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving
the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed
framework that extends the Dyna-Q algorithm to integrate planning for
task-completion dialogue policy learning. To obviate DDQ&apos;s high dependency on
the quality of simulated experiences, we incorporate an RNN-based discriminator
in D3Q to differentiate simulated experience from real user experience in order
to control the quality of training data. Experiments show that D3Q
significantly outperforms DDQ by controlling the quality of simulated
experience used for planning. The effectiveness and robustness of D3Q is
further demonstrated in a domain extension setting, where the agent&apos;s
capability of adapting to a changing environment is tested.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shang-Yu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun-Nung Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03225">
<title>Large-scale Cloze Test Dataset Created by Teachers. (arXiv:1711.03225v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03225</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloze tests are widely adopted in language exams to evaluate students&apos;
language proficiency. In this paper, we propose the first large-scale
human-created cloze test dataset CLOTH, containing questions used in
middle-school and high-school language exams. With missing blanks carefully
created by teachers and candidate choices purposely designed to be nuanced,
CLOTH requires a deeper language understanding and a wider attention span than
previously automatically-generated cloze datasets. We test the performance of
dedicatedly designed baseline models including a language model trained on the
One Billion Word Corpus and show humans outperform them by a significant
margin. We investigate the source of the performance gap, trace model
deficiencies to some distinct properties of CLOTH, and identify the limited
ability of comprehending the long-term context to be the key bottleneck.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qizhe Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_G/0/1/0/all/0/1&quot;&gt;Guokun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zihang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1&quot;&gt;Eduard Hovy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07855">
<title>Subgoal Discovery for Hierarchical Dialogue Policy Learning. (arXiv:1804.07855v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07855</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing agents to engage in complex goal-oriented dialogues is challenging
partly because the main learning signals are very sparse in long conversations.
In this paper, we propose a divide-and-conquer approach that discovers and
exploits the hidden structure of the task to enable efficient policy learning.
First, given successful example dialogues, we propose the Subgoal Discovery
Network (SDN) to divide a complex goal-oriented task into a set of simpler
subgoals in an unsupervised fashion. We then use these subgoals to learn a
multi-level policy by hierarchical reinforcement learning. We demonstrate our
method by building a dialogue agent for the composite task of travel planning.
Experiments with simulated and real users show that our approach performs
competitively against a state-of-the-art method that requires human-defined
subgoals. Moreover, we show that the learned subgoals are often human
comprehensible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Da Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lihong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jebara_T/0/1/0/all/0/1&quot;&gt;Tony Jebara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07036">
<title>QuAC : Question Answering in Context. (arXiv:1808.07036v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07036</link>
<description rdf:parseType="Literal">&lt;p&gt;We present QuAC, a dataset for Question Answering in Context that contains
14K information-seeking QA dialogs (100K questions in total). The dialogs
involve two crowd workers: (1) a student who poses a sequence of freeform
questions to learn as much as possible about a hidden Wikipedia text, and (2) a
teacher who answers the questions by providing short excerpts from the text.
QuAC introduces challenges not found in existing machine comprehension
datasets: its questions are often more open-ended, unanswerable, or only
meaningful within the dialog context, as we show in a detailed qualitative
evaluation. We also report results for a number of reference models, including
a recently state-of-the-art reading comprehension architecture extended to
model dialog context. Our best model underperforms humans by 20 F1, suggesting
that there is significant room for future work on this data. Dataset, baseline,
and leaderboard available at &lt;a href=&quot;http://quac.ai.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Eunsol Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1&quot;&gt;Mohit Iyyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1&quot;&gt;Mark Yatskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1&quot;&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07187">
<title>Neural Latent Extractive Document Summarization. (arXiv:1808.07187v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07187</link>
<description rdf:parseType="Literal">&lt;p&gt;Extractive summarization models require sentence-level labels, which are
usually created heuristically (e.g., with rule-based methods) given that most
summarization datasets only have document-summary pairs. Since these labels
might be suboptimal, we propose a latent variable extractive model where
sentences are viewed as latent variables and sentences with activated variables
are used to infer gold summaries. During training the loss comes
\emph{directly} from gold summaries. Experiments on the CNN/Dailymail dataset
show that our model improves over a strong extractive baseline trained on
heuristically approximated labels and also performs competitively to several
recent models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1&quot;&gt;Mirella Lapata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08003">
<title>Approximate Distribution Matching for Sequence-to-Sequence Learning. (arXiv:1808.08003v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08003</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequence-to-Sequence models were introduced to tackle many real-life problems
like machine translation, summarization, image captioning, etc. The standard
optimization algorithms are mainly based on example-to-example matching like
maximum likelihood estimation, which is known to suffer from data sparsity
problem. Here we present an alternate view to explain sequence-to-sequence
learning as a distribution matching problem, where each source or target
example is viewed to represent a local latent distribution in the source or
target domain. Then, we interpret sequence-to-sequence learning as learning a
transductive model to transform the source local latent distributions to match
their corresponding target distributions. In our framework, we approximate both
the source and target latent distributions with recurrent neural networks
(augmenter). During training, the parallel augmenters learn to better
approximate the local latent distributions, while the sequence prediction model
learns to minimize the KL-divergence of the transformed source distributions
and the approximated target distributions. This algorithm can alleviate the
data sparsity issues in sequence learning by locally augmenting more unseen
data pairs and increasing the model&apos;s robustness. Experiments conducted on
machine translation and image captioning consistently demonstrate the
superiority of our proposed algorithm over the other competing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shujie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08773">
<title>Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach. (arXiv:1808.08773v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08773</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel geometric approach for learning bilingual mappings given
monolingual embeddings and a bilingual dictionary. Our approach decouples
learning the transformation from the source language to the target language
into (a) learning rotations for language-specific embeddings to align them to a
common space, and (b) learning a similarity metric in the common space to model
similarities between the embeddings. We model the bilingual mapping problem as
an optimization problem on smooth Riemannian manifolds. We show that our
approach outperforms previous approaches on the bilingual lexicon induction and
cross-lingual word similarity tasks. We also generalize our framework to
represent multiple languages in a common latent space. In particular, the
latent space representations for several languages are learned jointly, given
bilingual dictionaries for multiple language pairs. We illustrate the
effectiveness of joint learning for multiple languages in zero-shot word
translation setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1&quot;&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balgovind_A/0/1/0/all/0/1&quot;&gt;Arjun Balgovind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1&quot;&gt;Anoop Kunchukuttan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07645">
<title>Playing 20 Question Game with Policy-Based Reinforcement Learning. (arXiv:1808.07645v2 [cs.HC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.07645</link>
<description rdf:parseType="Literal">&lt;p&gt;The 20 Questions (Q20) game is a well known game which encourages deductive
reasoning and creativity. In the game, the answerer first thinks of an object
such as a famous person or a kind of animal. Then the questioner tries to guess
the object by asking 20 questions. In a Q20 game system, the user is considered
as the answerer while the system itself acts as the questioner which requires a
good strategy of question selection to figure out the correct object and win
the game. However, the optimal policy of question selection is hard to be
derived due to the complexity and volatility of the game environment. In this
paper, we propose a novel policy-based Reinforcement Learning (RL) method,
which enables the questioner agent to learn the optimal policy of question
selection through continuous interactions with users. To facilitate training,
we also propose to use a reward network to estimate the more informative
reward. Compared to previous methods, our RL method is robust to noisy answers
and does not rely on the Knowledge Base of objects. Experimental results show
that our RL method clearly outperforms an entropy-based engineering system and
has competitive performance in a noisy-free simulation environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Huang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xianchao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bingfeng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1&quot;&gt;Chongyang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Can Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08994">
<title>Data Poisoning Attacks against Online Learning. (arXiv:1808.08994v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08994</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider data poisoning attacks, a class of adversarial attacks on machine
learning where an adversary has the power to alter a small fraction of the
training data in order to make the trained classifier satisfy certain
objectives. While there has been much prior work on data poisoning, most of it
is in the offline setting, and attacks for online learning, where training data
arrives in a streaming manner, are not well understood.
&lt;/p&gt;
&lt;p&gt;In this work, we initiate a systematic investigation of data poisoning
attacks for online learning. We formalize the problem into two settings, and we
propose a general attack strategy, formulated as an optimization problem, that
applies to both with some modifications. We propose three solution strategies,
and perform extensive experimental evaluation. Finally, we discuss the
implications of our findings for building successful defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1&quot;&gt;Kamalika Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09222">
<title>Making \emph{ordinary least squares} linear classfiers more robust. (arXiv:1808.09222v1 [physics.data-an])</title>
<link>http://arxiv.org/abs/1808.09222</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of statistics and machine learning, the sums-of-squares,
commonly referred to as \emph{ordinary least squares}, can be used as a
convenient choice of cost function because of its many nice analytical
properties, though not always the best choice. However, it has been long known
that \emph{ordinary least squares} is not robust to outliers. Several attempts
to resolve this problem led to the creation of alternative methods that, either
did not fully resolved the \emph{outlier problem} or were computationally
difficult. In this paper, we provide a very simple solution that can make
\emph{ordinary least squares} less sensitive to outliers in data
classification, by \emph{scaling the augmented input vector by its length}. We
show some mathematical expositions of the \emph{outlier problem} using some
approximations and geometrical techniques. We present numerical results to
support the efficacy of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ayeni_B/0/1/0/all/0/1&quot;&gt;Babatunde M. Ayeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09372">
<title>Mean Field Analysis of Neural Networks: A Central Limit Theorem. (arXiv:1808.09372v1 [math.PR])</title>
<link>http://arxiv.org/abs/1808.09372</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning has revolutionized fields such as image, text, and speech
recognition. There&apos;s also growing interest in applying machine and deep
learning methods in science, engineering, medicine, and finance. Despite their
immense success in practice, there is limited mathematical understanding of
neural networks. We mathematically study neural networks in the asymptotic
regime of simultaneously (A) large network sizes and (B) large numbers of
stochastic gradient descent training iterations. We rigorously prove that the
neural network satisfies a central limit theorem. Our result describes the
neural network&apos;s fluctuations around its mean-field limit. The fluctuations
have a Gaussian distribution and satisfy a stochastic partial differential
equation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sirignano_J/0/1/0/all/0/1&quot;&gt;Justin Sirignano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Spiliopoulos_K/0/1/0/all/0/1&quot;&gt;Konstantinos Spiliopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00209">
<title>Online Natural Gradient as a Kalman Filter. (arXiv:1703.00209v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;We cast Amari&apos;s natural gradient in statistical learning as a specific case
of Kalman filtering. Namely, applying an extended Kalman filter to estimate a
fixed unknown parameter of a probabilistic model from a series of observations,
is rigorously equivalent to estimating this parameter via an online stochastic
natural gradient descent on the log-likelihood of the observations.
&lt;/p&gt;
&lt;p&gt;In the i.i.d. case, this relation is a consequence of the &quot;information
filter&quot; phrasing of the extended Kalman filter. In the recurrent (state space,
non-i.i.d.) case, we prove that the joint Kalman filter over states and
parameters is a natural gradient on top of real-time recurrent learning (RTRL),
a classical algorithm to train recurrent models.
&lt;/p&gt;
&lt;p&gt;This exact algebraic correspondence provides relevant interpretations for
natural gradient hyperparameters such as learning rates or initialization and
regularization of the Fisher information matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ollivier_Y/0/1/0/all/0/1&quot;&gt;Yann Ollivier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06901">
<title>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement. (arXiv:1802.06901v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06901</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a conditional non-autoregressive neural sequence model based on
iterative refinement. The proposed model is designed based on the principles of
latent variable models and denoising autoencoders, and is generally applicable
to any sequence generation task. We extensively evaluate the proposed model on
machine translation (En-De and En-Ro) and image caption generation, and observe
that it significantly speeds up decoding while maintaining the generation
quality comparable to the autoregressive counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1&quot;&gt;Elman Mansimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06561">
<title>A Mean Field View of the Landscape of Two-Layers Neural Networks. (arXiv:1804.06561v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06561</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-layer neural networks are among the most powerful models in machine
learning, yet the fundamental reasons for this success defy mathematical
understanding. Learning a neural network requires to optimize a non-convex
high-dimensional objective (risk function), a problem which is usually attacked
using stochastic gradient descent (SGD). Does SGD converge to a global optimum
of the risk or only to a local optimum? In the first case, does this happen
because local minima are absent, or because SGD somehow avoids them? In the
second, why do local minima reached by SGD have good generalization properties?
&lt;/p&gt;
&lt;p&gt;In this paper we consider a simple case, namely two-layers neural networks,
and prove that -in a suitable scaling limit- SGD dynamics is captured by a
certain non-linear partial differential equation (PDE) that we call
distributional dynamics (DD). We then consider several specific examples, and
show how DD can be used to prove convergence of SGD to networks with nearly
ideal generalization error. This description allows to &apos;average-out&apos; some of
the complexities of the landscape of neural networks, and can be used to prove
a general convergence result for noisy SGD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Song Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Andrea Montanari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phan-Minh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05697">
<title>Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study. (arXiv:1808.05697v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05697</link>
<description rdf:parseType="Literal">&lt;p&gt;Several recent papers investigate Active Learning (AL) for mitigating the
data dependence of deep learning for natural language processing. However, the
applicability of AL to real-world problems remains an open question. While in
supervised learning, practitioners can try many different methods, evaluating
each against a validation set before selecting a model, AL affords no such
luxury. Over the course of one AL run, an agent annotates its dataset
exhausting its labeling budget. Thus, given a new task, an active learner has
no opportunity to compare models and acquisition functions. This paper provides
a large scale empirical study of deep active learning, addressing multiple
tasks and, for each, multiple datasets, multiple models, and a full suite of
acquisition functions. We find that across all settings, Bayesian active
learning by disagreement, using uncertainty estimates provided either by
Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and
usually outperforms classic uncertainty sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1&quot;&gt;Aditya Siddhant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07018">
<title>Hypernetwork Knowledge Graph Embeddings. (arXiv:1808.07018v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07018</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs are large graph-structured databases of facts, which
typically suffer from incompleteness. Link prediction is the task of inferring
missing relations (links) between entities (nodes) in a knowledge graph. We
propose to solve this task by using a hypernetwork architecture to generate
convolutional layer filters specific to each relation and apply those filters
to the subject entity embeddings. This architecture enables a trade-off between
non-linear expressiveness and the number of parameters to learn. Our model
simplifies the entity and relation embedding interactions introduced by the
predecessor convolutional model, while outperforming all previous approaches to
link prediction across all standard link prediction datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balazevic_I/0/1/0/all/0/1&quot;&gt;Ivana Balazevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy M. Hospedales&lt;/a&gt;</dc:creator>
</item></rdf:RDF>