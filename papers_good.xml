<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04488"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05812"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04765"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.04379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11595"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01191"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06167"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03764"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.04439">
<title>From Nodes to Networks: Evolving Recurrent Neural Networks. (arXiv:1803.04439v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.04439</link>
<description rdf:parseType="Literal">&lt;p&gt;Gated recurrent networks such as those composed of Long Short-Term Memory
(LSTM) nodes have recently been used to improve state of the art in many
sequential processing tasks such as speech recognition and machine translation.
However, the basic structure of the LSTM node is essentially the same as when
it was first conceived 25 years ago. Recently, evolutionary and reinforcement
learning mechanisms have been employed to create new variations of this
structure. This paper proposes a new method, evolution of a tree-based encoding
of the gated memory nodes, and shows that it makes it possible to explore new
variations more effectively than other methods. The method discovers nodes with
multiple recurrent paths and multiple memory cells, which lead to significant
improvement in the standard language modeling benchmark task. The paper also
shows how the search process can be speeded up by training an LSTM network to
estimate performance of candidate structures, and by encouraging exploration of
novel solutions. Thus, evolutionary design of complex neural network structures
promises to improve performance of deep learning architectures beyond human
ability to do so.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_A/0/1/0/all/0/1&quot;&gt;Aditya Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04884">
<title>IDEL: In-Database Entity Linking with Neural Embeddings. (arXiv:1803.04884v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1803.04884</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel architecture, In-Database Entity Linking (IDEL), in which
we integrate the analytics-optimized RDBMS MonetDB with neural text mining
abilities. Our system design abstracts core tasks of most neural entity linking
systems for MonetDB. To the best of our knowledge, this is the first defacto
implemented system integrating entity-linking in a database. We leverage the
ability of MonetDB to support in-database-analytics with user defined functions
(UDFs) implemented in Python. These functions call machine learning libraries
for neural text mining, such as TensorFlow. The system achieves zero cost for
data shipping and transformation by utilizing MonetDB&apos;s ability to embed Python
processes in the database kernel and exchange data in NumPy arrays. IDEL
represents text and relational data in a joint vector space with neural
embeddings and can compensate errors with ambiguous entity representations. For
detecting matching entities, we propose a novel similarity function based on
joint neural embeddings which are learned via minimizing pairwise contrastive
ranking loss. This function utilizes a high dimensional index structures for
fast retrieval of matching entities. Our first implementation and experiments
using the WebNLG corpus show the effectiveness and the potentials of IDEL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilias_T/0/1/0/all/0/1&quot;&gt;Torsten Kilias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1&quot;&gt;Alexander L&amp;#xf6;ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gers_F/0/1/0/all/0/1&quot;&gt;Felix A. Gers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koopmanschap_R/0/1/0/all/0/1&quot;&gt;Richard Koopmanschap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersten_M/0/1/0/all/0/1&quot;&gt;Martin Kersten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04488">
<title>Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts. (arXiv:1803.04488v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.04488</link>
<description rdf:parseType="Literal">&lt;p&gt;Although there is an emerging trend towards generating embeddings for
primarily unstructured data, and recently for structured data, there is not yet
any systematic suite for measuring the quality of embeddings. This deficiency
is further sensed with respect to embeddings generated for structured data
because there are no concrete evaluation metrics measuring the quality of
encoded structure as well as semantic patterns in the embedding space. In this
paper, we introduce a framework containing three distinct tasks concerned with
the individual aspects of ontological concepts: (i) the categorization aspect,
(ii) the hierarchical aspect, and (iii) the relational aspect. Then, in the
scope of each task, a number of intrinsic metrics are proposed for evaluating
the quality of the embeddings. Furthermore, w.r.t. this framework multiple
experimental studies were run to compare the quality of the available embedding
models. Employing this framework in future research can reduce misjudgment and
provide greater insight about quality comparisons of embeddings for ontological
concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshargi_F/0/1/0/all/0/1&quot;&gt;Faisal Alshargi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1&quot;&gt;Saeedeh Shekarpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soru_T/0/1/0/all/0/1&quot;&gt;Tommaso Soru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quasthoff_U/0/1/0/all/0/1&quot;&gt;Uwe Quasthoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04596">
<title>Automatic Detection of Online Jihadist Hate Speech. (arXiv:1803.04596v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.04596</link>
<description rdf:parseType="Literal">&lt;p&gt;We have developed a system that automatically detects online jihadist hate
speech with over 80% accuracy, by using techniques from Natural Language
Processing and Machine Learning. The system is trained on a corpus of 45,000
subversive Twitter messages collected from October 2014 to December 2016. We
present a qualitative and quantitative analysis of the jihadist rhetoric in the
corpus, examine the network of Twitter users, outline the technical procedure
used to train the system, and discuss examples of use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smedt_T/0/1/0/all/0/1&quot;&gt;Tom De Smedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauw_G/0/1/0/all/0/1&quot;&gt;Guy De Pauw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostaeyen_P/0/1/0/all/0/1&quot;&gt;Pieter Van Ostaeyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05541">
<title>Good and safe uses of AI Oracles. (arXiv:1711.05541v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05541</link>
<description rdf:parseType="Literal">&lt;p&gt;An Oracle is a design for potentially high power artificial intelligences
(AIs), where the AI is made safe by restricting it to only answer questions.
Unfortunately most designs cause the Oracle to be motivated to manipulate
humans with the contents of their answers, and Oracles of potentially high
intelligence might be very successful at this. Solving that problem, without
compromising the accuracy of the answer, is tricky. This paper reduces the
issue to a cryptographic-style problem of Alice ensuring that her Oracle
answers her questions while not providing key information to an eavesdropping
Eve. Two Oracle designs solve this problem, one counterfactual (the Oracle
answers as if it expected its answer to never be read) and one on-policy, but
limited by the quantity of information it can transmit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Stuart Armstrong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05812">
<title>Impossibility of deducing preferences and rationality from human policy. (arXiv:1712.05812v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.05812</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning (IRL) attempts to infer human rewards or
preferences from observed behavior. Since human planning systematically
deviates from rationality, several approaches have been tried to account for
specific human shortcomings. However, there has been little analysis of the
general problem of inferring the reward of a human of unknown rationality. The
observed behavior can, in principle, be decomposed into two components: a
reward function and a planning algorithm, both of which have to be inferred
from behavior. This paper presents a No Free Lunch theorem, showing that,
without making `normative&apos; assumptions beyond the data, nothing about the human
reward function can be deduced from human behavior. Unlike most No Free Lunch
theorems, this cannot be alleviated by regularising with simplicity
assumptions. We show that the simplest hypotheses which explain the data are
generally degenerate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Stuart Armstrong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07740">
<title>Machine Theory of Mind. (arXiv:1802.07740v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07740</link>
<description rdf:parseType="Literal">&lt;p&gt;Theory of mind (ToM; Premack &amp;amp; Woodruff, 1978) broadly refers to humans&apos;
ability to represent the mental states of others, including their desires,
beliefs, and intentions. We propose to train a machine to build such models
too. We design a Theory of Mind neural network -- a ToMnet -- which uses
meta-learning to build models of the agents it encounters, from observations of
their behaviour alone. Through this process, it acquires a strong prior model
for agents&apos; behaviour, as well as the ability to bootstrap to richer
predictions about agents&apos; characteristics and mental states using only a small
number of behavioural observations. We apply the ToMnet to agents behaving in
simple gridworld environments, showing that it learns to model random,
algorithmic, and deep reinforcement learning agents from varied populations,
and that it passes classic ToM tasks such as the &quot;Sally-Anne&quot; test (Wimmer &amp;amp;
Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold
false beliefs about the world. We argue that this system -- which autonomously
learns how to model other agents in its world -- is an important step forward
for developing multi-agent AI systems, for building intermediating technology
for machine-human interaction, and for advancing the progress on interpretable
AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabinowitz_N/0/1/0/all/0/1&quot;&gt;Neil C. Rabinowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perbet_F/0/1/0/all/0/1&quot;&gt;Frank Perbet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;H. Francis Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1&quot;&gt;S.M. Ali Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1&quot;&gt;Matthew Botvinick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04489">
<title>Probabilistic and Regularized Graph Convolutional Networks. (arXiv:1803.04489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04489</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the recently proposed Graph Convolutional Network
architecture proposed in (Kipf &amp;amp; Welling, 2016) The key points of their work is
summarized and their results are reproduced. Graph regularization and
alternative graph convolution approaches are explored. I find that explicit
graph regularization was correctly rejected by (Kipf &amp;amp; Welling, 2016). I
attempt to improve the performance of GCN by approximating a k-step transition
matrix in place of the normalized graph laplacian, but I fail to find positive
results. Nonetheless, the performance of several configurations of this GCN
variation is shown for the Cora, Citeseer, and Pubmed datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Billings_S/0/1/0/all/0/1&quot;&gt;Sean Billings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04765">
<title>Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning. (arXiv:1803.04765v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04765</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) enable innovative applications of machine
learning like image recognition, machine translation, or malware detection.
However, deep learning is often criticized for its lack of robustness in
adversarial settings (e.g., vulnerability to adversarial inputs) and general
inability to rationalize its predictions. In this work, we exploit the
structure of deep learning to enable new learning-based inference and decision
strategies that achieve desirable properties such as robustness and
interpretability. We take a first step in this direction and introduce the Deep
k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest
neighbors algorithm with representations of the data learned by each layer of
the DNN: a test input is compared to its neighboring training points according
to the distance that separates them in the representations. We show the labels
of these neighboring points afford confidence estimates for inputs outside the
model&apos;s training manifold, including on malicious inputs like adversarial
examples--and therein provides protections against inputs that are outside the
models understanding. This is because the nearest neighbors can be used to
estimate the nonconformity of, i.e., the lack of support for, a prediction in
the training data. The neighbors also constitute human-interpretable
explanations of predictions. We evaluate the DkNN algorithm on several
datasets, and show the confidence estimates accurately identify inputs outside
the model, and that the explanations provided by nearest neighbors are
intuitive and useful in understanding model failures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1&quot;&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDaniel_P/0/1/0/all/0/1&quot;&gt;Patrick McDaniel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.04379">
<title>The Network Nullspace Property for Compressed Sensing of Big Data over Networks. (arXiv:1705.04379v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.04379</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel condition, which we term the net- work nullspace property,
which ensures accurate recovery of graph signals representing massive
network-structured datasets from few signal values. The network nullspace
property couples the cluster structure of the underlying network-structure with
the geometry of the sampling set. Our results can be used to design efficient
sampling strategies based on the network topology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hulsebos_M/0/1/0/all/0/1&quot;&gt;Madelon Hulsebos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11595">
<title>Small Moving Window Calibration Models for Soft Sensing Processes with Limited History. (arXiv:1710.11595v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11595</link>
<description rdf:parseType="Literal">&lt;p&gt;Five simple soft sensor methodologies with two update conditions were
compared on two experimentally-obtained datasets and one simulated dataset. The
soft sensors investigated were moving window partial least squares regression
(and a recursive variant), moving window random forest regression, the mean
moving window of $y$, and a novel random forest partial least squares
regression ensemble (RF-PLS), all of which can be used with small sample sizes
so that they can be rapidly placed online. It was found that, on two of the
datasets studied, small window sizes led to the lowest prediction errors for
all of the moving window methods studied. On the majority of datasets studied,
the RF-PLS calibration method offered the lowest one-step-ahead prediction
errors compared to those of the other methods, and it demonstrated greater
predictive stability at larger time delays than moving window PLS alone. It was
found that both the random forest and RF-PLS methods most adequately modeled
the datasets that did not feature purely monotonic increases in property
values, but that both methods performed more poorly than moving window PLS
models on one dataset with purely monotonic property values. Other data
dependent findings are presented and discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kneale_C/0/1/0/all/0/1&quot;&gt;Casey Kneale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_S/0/1/0/all/0/1&quot;&gt;Steven D. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01191">
<title>Learning flexible representations of stochastic processes on graphs. (arXiv:1711.01191v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01191</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks adapt the architecture of convolutional neural
networks to learn rich representations of data supported on arbitrary graphs by
replacing the convolution operations of convolutional neural networks with
graph-dependent linear operations. However, these graph-dependent linear
operations are developed for scalar functions supported on undirected graphs.
We propose a class of linear operations for stochastic (time-varying) processes
on directed (or undirected) graphs to be used in graph convolutional networks.
We propose a parameterization of such linear operations using functional
calculus to achieve arbitrarily low learning complexity. The proposed approach
is shown to model richer behaviors and display greater flexibility in learning
representations than product graph methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bohannon_A/0/1/0/all/0/1&quot;&gt;Addison Bohannon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sadler_B/0/1/0/all/0/1&quot;&gt;Brian Sadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balan_R/0/1/0/all/0/1&quot;&gt;Radu Balan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06167">
<title>CapsuleGAN: Generative Adversarial Capsule Network. (arXiv:1802.06167v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06167</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Generative Adversarial Capsule Network (CapsuleGAN), a framework
that uses capsule networks (CapsNets) instead of the standard convolutional
neural networks (CNNs) as discriminators within the generative adversarial
network (GAN) setting, while modeling image data. We provide guidelines for
designing CapsNet discriminators and the updated GAN objective function, which
incorporates the CapsNet margin loss, for training CapsuleGAN models. We show
that CapsuleGAN outperforms convolutional-GAN at modeling image data
distribution on MNIST and CIFAR-10 datasets, evaluated on the generative
adversarial metric and at semi-supervised image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaiswal_A/0/1/0/all/0/1&quot;&gt;Ayush Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+AbdAlmageed_W/0/1/0/all/0/1&quot;&gt;Wael AbdAlmageed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Natarajan_P/0/1/0/all/0/1&quot;&gt;Premkumar Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06640">
<title>Finding Influential Training Samples for Gradient Boosted Decision Trees. (arXiv:1802.06640v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06640</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of finding influential training samples for a
particular case of tree ensemble-based models, e.g., Random Forest (RF) or
Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this
problem is studying how the model&apos;s predictions change upon leave-one-out
retraining, leaving out each individual training sample. Recent work has shown
that, for parametric models, this analysis can be conducted in a
computationally efficient way. We propose several ways of extending this
framework to non-parametric GBDT ensembles under the assumption that tree
structures remain fixed. Furthermore, we introduce a general scheme of
obtaining further approximations to our method that balance the trade-off
between performance and computational complexity. We evaluate our approaches on
various experimental setups and use-case scenarios and demonstrate both the
quality of our approach to finding influential training samples in comparison
to the baselines and its computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharchilev_B/0/1/0/all/0/1&quot;&gt;Boris Sharchilev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustinovsky_Y/0/1/0/all/0/1&quot;&gt;Yury Ustinovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serdyukov_P/0/1/0/all/0/1&quot;&gt;Pavel Serdyukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1&quot;&gt;Maarten de Rijke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03764">
<title>Variance Networks: When Expectation Does Not Meet Your Expectations. (arXiv:1803.03764v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03764</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose variance networks, a new model that stores the
learned information in the variances of the network weights. Surprisingly, no
information gets stored in the expectations of the weights, therefore if we
replace these weights with their expectations, we would obtain a random guess
quality prediction. We provide a numerical criterion that uses the loss
curvature to determine which random variables can be replaced with their
expected values, and find that only a small fraction of weights is needed for
ensembling. Variance networks represent a diverse ensemble that is more robust
to adversarial attacks than conventional low-variance ensembles. The success of
this model raises several counter-intuitive implications for the training and
application of Deep Learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neklyudov_K/0/1/0/all/0/1&quot;&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1&quot;&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;</dc:creator>
</item></rdf:RDF>