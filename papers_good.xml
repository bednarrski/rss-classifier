<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06105"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05853"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05931"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05894"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05922"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01769"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.06105">
<title>Overcoming the vanishing gradient problem in plain recurrent networks. (arXiv:1801.06105v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.06105</link>
<description rdf:parseType="Literal">&lt;p&gt;Plain recurrent networks greatly suffer from the vanishing gradient problem
while Gated Neural Networks (GNNs) such as Long-short Term Memory (LSTM) and
Gated Recurrent Unit (GRU) deliver promising results in many sequence learning
tasks through sophisticated network designs. This paper shows how we can
address this problem in a plain recurrent network by analyzing the gating
mechanisms in GNNs. We propose a novel network called the Recurrent Identity
Network (RIN) which allows a plain recurrent network to overcome the vanishing
gradient problem while training very deep models without the use of gates. We
compare this model with IRNNs and LSTMs on multiple sequence modeling
benchmarks. The RINs demonstrate competitive performance and converge faster in
all tasks. Notably, small RIN models produce 12%--67% higher accuracy on the
Sequential and Permuted MNIST datasets and reach state-of-the-art performance
on the bAbI question answering dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuhuang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_A/0/1/0/all/0/1&quot;&gt;Adrian Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anumula_J/0/1/0/all/0/1&quot;&gt;Jithendar Anumula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shih-Chii Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06176">
<title>Integrating planning for task-completion dialogue policy learning. (arXiv:1801.06176v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.06176</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a task-completion dialogue agent with real users via reinforcement
learning (RL) could be prohibitively expensive, because it requires many
interactions with users. One alternative is to resort to a user simulator,
while the discrepancy of between simulated and real users makes the learned
policy unreliable in practice. This paper addresses these challenges by
integrating planning into the dialogue policy learning based on Dyna-Q
framework, and provides a more sample-efficient approach to learn the dialogue
polices. The proposed agent consists of a planner trained on-line with limited
real user experience that can generate large amounts of simulated experience to
supplement with limited real user experience, and a policy model trained on
these hybrid experiences. The effectiveness of our approach is validated on a
movie-booking task in both a simulation setting and a human-in-the-loop
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Baolin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kam-Fai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05853">
<title>Time Matters: Multi-scale Temporalization of Social Media Popularity. (arXiv:1801.05853v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1801.05853</link>
<description rdf:parseType="Literal">&lt;p&gt;The evolution of social media popularity exhibits rich temporality, i.e.,
popularities change over time at various levels of temporal granularity. This
is influenced by temporal variations of public attentions or user activities.
For example, popularity patterns of street snap on Flickr are observed to
depict distinctive fashion styles at specific time scales, such as season-based
periodic fluctuations for Trench Coat or one-off peak in days for Evening
Dress. However, this fact is often overlooked by existing research of
popularity modeling. We present the first study to incorporate multiple
time-scale dynamics into predicting online popularity. We propose a novel
computational framework in the paper, named Multi-scale Temporalization, for
estimating popularity based on multi-scale decomposition and structural
reconstruction in a tensor space of user, post, and time by joint low-rank
constraints. By considering the noise caused by context inconsistency, we
design a data rearrangement step based on context aggregation as preprocessing
to enhance contextual relevance of neighboring data in the tensor space. As a
result, our approach can leverage multiple levels of temporal characteristics
and reduce the noise of data decomposition to improve modeling effectiveness.
We evaluate our approach on two large-scale Flickr image datasets with over 1.8
million photos in total, for the task of popularity prediction. The results
show that our approach significantly outperforms state-of-the-art popularity
prediction techniques, with a relative improvement of 10.9%-47.5% in terms of
prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wen-Huang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1&quot;&gt;Tao Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05931">
<title>Faster Algorithms for Large-scale Machine Learning using Simple Sampling Techniques. (arXiv:1801.05931v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.05931</link>
<description rdf:parseType="Literal">&lt;p&gt;Now a days, the major challenge in machine learning is the `Big~Data&apos;
challenge. The big data problems due to large number of data points or large
number of features in each data point, or both, the training of models have
become very slow. The training time has two major components: Time to access
the data and time to process the data. In this paper, we have proposed one
possible solution to handle the big data problems in machine learning. The
focus is on reducing the training time through reducing data access time by
proposing systematic sampling and cyclic/sequential sampling to select
mini-batches from the dataset. To prove the effectiveness of proposed sampling
techniques, we have used Empirical Risk Minimization, which is commonly used
machine learning problem, for strongly convex and smooth case. The problem has
been solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each
using two step determination techniques, namely, constant step size and
backtracking line search method. Theoretical results prove the same convergence
for systematic sampling, cyclic sampling and the widely used random sampling
technique, in expectation. Experimental results with bench marked datasets
prove the efficacy of the proposed sampling techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1&quot;&gt;Vinod Kumar Chauhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1&quot;&gt;Kalpana Dahiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anuj Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06024">
<title>Natural Language Multitasking: Analyzing and Improving Syntactic Saliency of Hidden Representations. (arXiv:1801.06024v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.06024</link>
<description rdf:parseType="Literal">&lt;p&gt;We train multi-task autoencoders on linguistic tasks and analyze the learned
hidden sentence representations. The representations change significantly when
translation and part-of-speech decoders are added. The more decoders a model
employs, the better it clusters sentences according to their syntactic
similarity, as the representation space becomes less entangled. We explore the
structure of the representation space by interpolating between sentences, which
yields interesting pseudo-English sentences, many of which have recognizable
syntactic structure. Lastly, we point out an interesting property of our
models: The difference-vector between two sentences can be added to change a
third sentence with similar features in a meaningful way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunner_G/0/1/0/all/0/1&quot;&gt;Gino Brunner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weigelt_M/0/1/0/all/0/1&quot;&gt;Michael Weigelt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05894">
<title>Deep Learning: An Introduction for Applied Mathematicians. (arXiv:1801.05894v1 [math.HO])</title>
<link>http://arxiv.org/abs/1801.05894</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilayered artificial neural networks are becoming a pervasive tool in a
host of application fields. At the heart of this deep learning revolution are
familiar concepts from applied and computational mathematics; notably, in
calculus, approximation theory, optimization and linear algebra. This article
provides a very brief introduction to the basic ideas that underlie deep
learning from an applied mathematics perspective. Our target audience includes
postgraduate and final year undergraduate students in mathematics who are keen
to learn about the area. The article may also be useful for instructors in
mathematics who wish to enliven their classes with references to the
application of deep learning techniques. We focus on three fundamental
questions: what is a deep neural network? how is a network trained? what is the
stochastic gradient method? We illustrate the ideas with a short MATLAB code
that sets up and trains a network. We also show the use of state-of-the art
software on a large scale image classification problem. We finish with
references to the current literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Higham_C/0/1/0/all/0/1&quot;&gt;Catherine F. Higham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Higham_D/0/1/0/all/0/1&quot;&gt;Desmond J. Higham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05922">
<title>Graph Based Analysis for Gene Segment Organization In a Scrambled Genome. (arXiv:1801.05922v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.05922</link>
<description rdf:parseType="Literal">&lt;p&gt;DNA rearrangement processes recombine gene segments that are organized on the
chromosome in a variety of ways. The segments can overlap, interleave or one
may be a subsegment of another. We use directed graphs to represent segment
organizations on a given locus where contigs containing rearranged segments
represent vertices and the edges correspond to the segment relationships. Using
graph properties we associate a point in a higher dimensional Euclidean space
to each graph such that cluster formations and analysis can be performed with
methods from topological data analysis. The method is applied to a recently
sequenced model organism \textit{Oxytricha trifallax}, a species of ciliate
with highly scrambled genome that undergoes massive rearrangement process after
conjugation. The analysis shows some emerging star-like graph structures
indicating that segments of a single gene can interleave, or even contain all
of the segments from fifteen or more other genes in between its segments. We
also observe that as many as six genes can have their segments mutually
interleaving or overlapping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajij_M/0/1/0/all/0/1&quot;&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jonoska_N/0/1/0/all/0/1&quot;&gt;Nata&amp;#x161;a Jonoska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kukushkin_D/0/1/0/all/0/1&quot;&gt;Denys Kukushkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saito_M/0/1/0/all/0/1&quot;&gt;Masahico Saito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01769">
<title>State-of-the-art Speech Recognition With Sequence-to-Sequence Models. (arXiv:1712.01769v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01769</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention-based encoder-decoder architectures such as Listen, Attend, and
Spell (LAS), subsume the acoustic, pronunciation and language model components
of a traditional automatic speech recognition (ASR) system into a single neural
network. In our previous work, we have shown that such architectures are
comparable to state-of-the-art ASR systems on dictation tasks, but it was not
clear if such architectures would be practical for more challenging tasks such
as voice search. In this work, we explore a variety of structural and
optimization improvements to our LAS model which significantly improve
performance. On the structural side, we show that word piece models can be used
instead of graphemes. We introduce a multi-head attention architecture, which
offers improvements over the commonly-used single-head attention. On the
optimization side, we explore techniques such as synchronous training,
scheduled sampling, label smoothing, and minimum word error rate optimization,
which are all shown to improve accuracy. We present results with a
unidirectional LSTM encoder for streaming recognition. On a 12,500 hour voice
search task, we find that the proposed changes improve the WER of the LAS
system from 9.2% to 5.6%, while the best conventional system achieve 6.7% WER.
We also test both models on a dictation dataset, and our model provide 4.1% WER
while the conventional system provides 5% WER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Chung-Cheng Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1&quot;&gt;Tara N. Sainath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1&quot;&gt;Rohit Prabhavalkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Patrick Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anjuli Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1&quot;&gt;Ron J. Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonina_E/0/1/0/all/0/1&quot;&gt;Ekaterina Gonina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1&quot;&gt;Navdeep Jaitly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1&quot;&gt;Jan Chorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacchiani_M/0/1/0/all/0/1&quot;&gt;Michiel Bacchiani&lt;/a&gt;</dc:creator>
</item></rdf:RDF>