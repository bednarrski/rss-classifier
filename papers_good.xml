<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03453"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03635"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08465"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03289"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03407"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03479"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00885"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03376"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01217"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08645"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06167"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.03453">
<title>The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities. (arXiv:1803.03453v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.03453</link>
<description rdf:parseType="Literal">&lt;p&gt;Biological evolution provides a creative fount of complex and subtle
adaptations, often surprising the scientists who discover them. However,
because evolution is an algorithmic process that transcends the substrate in
which it occurs, evolution&apos;s creativity is not limited to nature. Indeed, many
researchers in the field of digital evolution have observed their evolving
algorithms and organisms subverting their intentions, exposing unrecognized
bugs in their code, producing unexpected adaptations, or exhibiting outcomes
uncannily convergent with ones in nature. Such stories routinely reveal
creativity by evolution in these digital worlds, but they rarely fit into the
standard scientific narrative. Instead they are often treated as mere obstacles
to be overcome, rather than results that warrant study in their own right. The
stories themselves are traded among researchers through oral tradition, but
that mode of information transmission is inefficient and prone to error and
outright loss. Moreover, the fact that these stories tend to be shared only
among practitioners means that many natural scientists do not realize how
interesting and lifelike digital organisms are and how natural their evolution
can be. To our knowledge, no collection of such anecdotes has been published
before. This paper is the crowd-sourced product of researchers in the fields of
artificial life and evolutionary computation who have provided first-hand
accounts of such cases. It thus serves as a written, fact-checked collection of
scientifically important and even entertaining stories. In doing so we also
present here substantial evidence that the existence and importance of
evolutionary surprises extends beyond the natural world, and may indeed be a
universal property of all complex evolving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misevic_D/0/1/0/all/0/1&quot;&gt;Dusan Misevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adami_C/0/1/0/all/0/1&quot;&gt;Christoph Adami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaulieu_J/0/1/0/all/0/1&quot;&gt;Julie Beaulieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bentley_P/0/1/0/all/0/1&quot;&gt;Peter J. Bentley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_S/0/1/0/all/0/1&quot;&gt;Samuel Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belson_G/0/1/0/all/0/1&quot;&gt;Guillaume Belson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bryson_D/0/1/0/all/0/1&quot;&gt;David M. Bryson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donciuex_S/0/1/0/all/0/1&quot;&gt;Stephane Donciuex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_F/0/1/0/all/0/1&quot;&gt;Fred C. Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellefsen_K/0/1/0/all/0/1&quot;&gt;Kai Olav Ellefsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldt_R/0/1/0/all/0/1&quot;&gt;Robert Feldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_S/0/1/0/all/0/1&quot;&gt;Stephan Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forrest_S/0/1/0/all/0/1&quot;&gt;Stephanie Forrest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frenoy_A/0/1/0/all/0/1&quot;&gt;Antoine Fr&amp;#xe9;noy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagnee_C/0/1/0/all/0/1&quot;&gt;Christian Gagne&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goff_L/0/1/0/all/0/1&quot;&gt;Leni Le Goff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabowski_L/0/1/0/all/0/1&quot;&gt;Laura M. Grabowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodjat_B/0/1/0/all/0/1&quot;&gt;Babak Hodjat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_L/0/1/0/all/0/1&quot;&gt;Laurent Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knibbe_C/0/1/0/all/0/1&quot;&gt;Carole Knibbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krcah_P/0/1/0/all/0/1&quot;&gt;Peter Krcah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenski_R/0/1/0/all/0/1&quot;&gt;Richard E. Lenski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacCurdy_R/0/1/0/all/0/1&quot;&gt;Robert MacCurdy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maestre_C/0/1/0/all/0/1&quot;&gt;Carlos Maestre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitri_S/0/1/0/all/0/1&quot;&gt;Sara Mitri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moriarty_D/0/1/0/all/0/1&quot;&gt;David E. Moriarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ofria_C/0/1/0/all/0/1&quot;&gt;Charles Ofria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parizeau_M/0/1/0/all/0/1&quot;&gt;Marc Parizeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsons_D/0/1/0/all/0/1&quot;&gt;David Parsons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennock_R/0/1/0/all/0/1&quot;&gt;Robert T. Pennock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Punch_W/0/1/0/all/0/1&quot;&gt;William F. Punch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_T/0/1/0/all/0/1&quot;&gt;Thomas S. Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenauer_M/0/1/0/all/0/1&quot;&gt;Marc Schoenauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shulte_E/0/1/0/all/0/1&quot;&gt;Eric Shulte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sims_K/0/1/0/all/0/1&quot;&gt;Karl Sims&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taddei_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Taddei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarapore_D/0/1/0/all/0/1&quot;&gt;Danesh Tarapore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thibault_S/0/1/0/all/0/1&quot;&gt;Simon Thibault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weimer_W/0/1/0/all/0/1&quot;&gt;Westley Weimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_R/0/1/0/all/0/1&quot;&gt;Richard Watson&lt;/a&gt;, et al. (1 additional author not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03635">
<title>The Lottery Ticket Hypothesis: Training Pruned Neural Networks. (arXiv:1803.03635v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03635</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on neural network pruning indicates that, at training time,
neural networks need to be significantly larger in size than is necessary to
represent the eventual functions that they learn. This paper articulates a new
hypothesis to explain this phenomenon. This conjecture, which we term the
&quot;lottery ticket hypothesis,&quot; proposes that successful training depends on lucky
random initialization of a smaller subcomponent of the network. Larger networks
have more of these &quot;lottery tickets,&quot; meaning they are more likely to luck out
with a subcomponent initialized in a configuration amenable to successful
optimization.
&lt;/p&gt;
&lt;p&gt;This paper conducts a series of experiments with XOR and MNIST that support
the lottery ticket hypothesis. In particular, we identify these
fortuitously-initialized subcomponents by pruning low-magnitude weights from
trained networks. We then demonstrate that these subcomponents can be
successfully retrained in isolation so long as the subnetworks are given the
same initializations as they had at the beginning of the training process.
Initialized as such, these small networks reliably converge successfully, often
faster than the original network at the same level of accuracy. However, when
these subcomponents are randomly reinitialized or rearranged, they perform
worse than the original network. In other words, large networks that train
successfully contain small subnetworks with initializations conducive to
optimization.
&lt;/p&gt;
&lt;p&gt;The lottery ticket hypothesis and its connection to pruning are a step toward
developing architectures, initializations, and training strategies that make it
possible to solve the same problems with much smaller networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1&quot;&gt;Jonathan Frankle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbin_M/0/1/0/all/0/1&quot;&gt;Michael Carbin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08465">
<title>AEkNN: An AutoEncoder kNN-based classifier with built-in dimensionality reduction. (arXiv:1802.08465v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08465</link>
<description rdf:parseType="Literal">&lt;p&gt;High dimensionality, i.e. data having a large number of variables, tends to
be a challenge for most machine learning tasks, including classification. A
classifier usually builds a model representing how a set of inputs explain the
outputs. The larger is the set of inputs and/or outputs, the more complex would
be that model. There is a family of classification algorithms, known as lazy
learning methods, which does not build a model. One of the best known members
of this family is the kNN algorithm. Its strategy relies on searching a set of
nearest neighbors, using the input variables as position vectors and computing
distances among them. These distances loss significance in high-dimensional
spaces. Therefore kNN, as many other classifiers, tends to worse its
performance as the number of input variables grows.
&lt;/p&gt;
&lt;p&gt;In this work AEkNN, a new kNN-based algorithm with built-in dimensionality
reduction, is presented. Aiming to obtain a new representation of the data,
having a lower dimensionality but with more informational features, AEkNN
internally uses autoencoders. From this new feature vectors the computed
distances should be more significant, thus providing a way to choose better
neighbors. A experimental evaluation of the new proposal is conducted,
analyzing several configurations and comparing them against the classical kNN
algorithm. The obtained conclusions demonstrate that AEkNN offers better
results in predictive and runtime performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pulgar_F/0/1/0/all/0/1&quot;&gt;Francisco J. Pulgar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charte_F/0/1/0/all/0/1&quot;&gt;Francisco Charte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1&quot;&gt;Antonio J. Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jesus_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a J. del Jesus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03289">
<title>Deep Neural Network Compression with Single and Multiple Level Quantization. (arXiv:1803.03289v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03289</link>
<description rdf:parseType="Literal">&lt;p&gt;Network quantization is an effective solution to compress deep neural
networks for practical usage. Existing network quantization methods cannot
sufficiently exploit the depth information to generate low-bit compressed
network. In this paper, we propose two novel network quantization approaches,
single-level network quantization (SLQ) for high-bit quantization and
multi-level network quantization (MLQ) for extremely low-bit quantization
(ternary).We are the first to consider the network quantization from both width
and depth level. In the width level, parameters are divided into two parts: one
for quantization and the other for re-training to eliminate the quantization
loss. SLQ leverages the distribution of the parameters to improve the width
level. In the depth level, we introduce incremental layer compensation to
quantize layers iteratively which decreases the quantization loss in each
iteration. The proposed approaches are validated with extensive experiments
based on the state-of-the-art neural networks including AlexNet, VGG-16,
GoogleNet and ResNet-18. Both SLQ and MLQ achieve impressive results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuhui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongzhuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aojun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weiyao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03407">
<title>Institutional Metaphors for Designing Large-Scale Distributed AI versus AI Techniques for Running Institutions. (arXiv:1803.03407v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.03407</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) started out with an ambition to reproduce the
human mind, but, as the sheer scale of that ambition became apparent, quickly
retreated into either studying specialized intelligent behaviours, or proposing
overarching architectural concepts for interfacing specialized intelligent
behaviour components, conceived of as agents in a kind of organization. This
agent-based modeling paradigm, in turn, proves to have interesting applications
in understanding, simulating, and predicting the behaviour of social and legal
structures on an aggregate level. This chapter examines a number of relevant
cross-cutting concerns, conceptualizations, modeling problems and design
challenges in large-scale distributed Artificial Intelligence, as well as in
institutional systems, and identifies potential grounds for novel advances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boer_A/0/1/0/all/0/1&quot;&gt;Alexander Boer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sileno_G/0/1/0/all/0/1&quot;&gt;Giovanni Sileno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03479">
<title>Highly Automated Learning for Improved Active Safety of Vulnerable Road Users. (arXiv:1803.03479v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.03479</link>
<description rdf:parseType="Literal">&lt;p&gt;Highly automated driving requires precise models of traffic participants.
Many state of the art models are currently based on machine learning
techniques. Among others, the required amount of labeled data is one major
challenge. An autonomous learning process addressing this problem is proposed.
The initial models are iteratively refined in three steps: (1) detection and
context identification, (2) novelty detection and active learning and (3)
online model adaption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bieshaar_M/0/1/0/all/0/1&quot;&gt;Maarten Bieshaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reitberger_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nther Reitberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1&quot;&gt;Viktor Kre&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1&quot;&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1&quot;&gt;Konrad Doll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_E/0/1/0/all/0/1&quot;&gt;Erich Fuchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00885">
<title>Essentially No Barriers in Neural Network Energy Landscape. (arXiv:1803.00885v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00885</link>
<description rdf:parseType="Literal">&lt;p&gt;Training neural networks involves finding minima of a high-dimensional
non-convex loss function. Knowledge of the structure of this energy landscape
is sparse. Relaxing from linear interpolations, we construct continuous paths
between minima of recent neural network architectures on CIFAR10 and CIFAR100.
Surprisingly, the paths are essentially flat in both the training and test
landscapes. This implies that neural networks have enough capacity for
structural changes, or that these changes are small between minima. Also, each
minimum has at least one vanishing Hessian eigenvalue in addition to those
resulting from trivial invariance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Draxler_F/0/1/0/all/0/1&quot;&gt;Felix Draxler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veschgini_K/0/1/0/all/0/1&quot;&gt;Kambis Veschgini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salmhofer_M/0/1/0/all/0/1&quot;&gt;Manfred Salmhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hamprecht_F/0/1/0/all/0/1&quot;&gt;Fred A. Hamprecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03324">
<title>Learning Deep Generative Models of Graphs. (arXiv:1803.03324v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03324</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphs are fundamental data structures which concisely capture the relational
structure in many important real-world domains, such as knowledge graphs,
physical and social interactions, language, and chemistry. Here we introduce a
powerful new approach for learning generative models over graphs, which can
capture both their structure and attributes. Our approach uses graph neural
networks to express probabilistic dependencies among a graph&apos;s nodes and edges,
and can, in principle, learn distributions over any arbitrary graph. In a
series of experiments our results show that once trained, our models can
generate good quality samples of both synthetic graphs as well as real
molecular graphs, both unconditionally and conditioned on data. Compared to
baselines that do not use graph-structured representations, our models often
perform far better. We also explore key challenges of learning generative
models of graphs, such as how to handle symmetries and ordering of elements
during the graph generation process, and offer possible solutions. Our work is
the first and most general approach for learning generative models over
arbitrary graphs, and opens new directions for moving away from restrictions of
vector- and sequence-like knowledge representations, toward more expressive and
flexible relational data structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1&quot;&gt;Chris Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1&quot;&gt;Peter Battaglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03376">
<title>Learning Approximate Inference Networks for Structured Prediction. (arXiv:1803.03376v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.03376</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured prediction energy networks (SPENs; Belanger &amp;amp; McCallum 2016) use
neural network architectures to define energy functions that can capture
arbitrary dependencies among parts of structured outputs. Prior work used
gradient descent for inference, relaxing the structured output to a set of
continuous variables and then optimizing the energy with respect to them. We
replace this use of gradient descent with a neural network trained to
approximate structured argmax inference. This &quot;inference network&quot; outputs
continuous values that we treat as the output structure. We develop
large-margin training criteria for joint training of the structured energy
function and inference network. On multi-label classification we report
speed-ups of 10-60x compared to (Belanger et al, 2017) while also improving
accuracy. For sequence labeling with simple structured energies, our approach
performs comparably to exact inference while being much faster at test time. We
then demonstrate improved accuracy by augmenting the energy with a &quot;label
language model&quot; that scores entire output label sequences, showing it can
improve handling of long-distance dependencies in part-of-speech tagging.
Finally, we show how inference networks can replace dynamic programming for
test-time inference in conditional random fields, suggestive for their general
use for fast inference in structured settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1&quot;&gt;Lifu Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1&quot;&gt;Kevin Gimpel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03467">
<title>Ripple Network: Propagating User Preferences on the Knowledge Graph for Recommender Systems. (arXiv:1803.03467v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1803.03467</link>
<description rdf:parseType="Literal">&lt;p&gt;To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user&apos;s
potential interests along links in the knowledge graph. The multiple &quot;ripples&quot;
activated by a user&apos;s historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Miao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05063">
<title>A concentration inequality for the excess risk in least-squares regression with random design and heteroscedastic noise. (arXiv:1702.05063v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05063</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove a new and general concentration inequality for the excess risk in
least-squares regression with random design and heteroscedastic noise. No
specific structure is required on the model, except the existence of a suitable
function that controls the local suprema of the empirical process. So far, only
the case of linear contrast estimation was tackled in the literature with this
level of generality on the model. We solve here the case of a quadratic
contrast, by separating the behavior of a linearized empirical process and the
empirical process driven by the squares of functions of models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Saumard_A/0/1/0/all/0/1&quot;&gt;Adrien Saumard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01217">
<title>Wasserstein Distance Guided Representation Learning for Domain Adaptation. (arXiv:1707.01217v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01217</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation aims at generalizing a high-performance learner on a target
domain via utilizing the knowledge distilled from a source domain which has a
different but related data distribution. One solution to domain adaptation is
to learn domain invariant feature representations while the learned
representations should also be discriminative in prediction. To learn such
representations, domain adaptation frameworks usually include a domain
invariant representation learning approach to measure and reduce the domain
discrepancy, as well as a discriminator for classification. Inspired by
Wasserstein GAN, in this paper we propose a novel approach to learn domain
invariant feature representations, namely Wasserstein Distance Guided
Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by
the domain critic, to estimate empirical Wasserstein distance between the
source and target samples and optimizes the feature extractor network to
minimize the estimated Wasserstein distance in an adversarial manner. The
theoretical advantages of Wasserstein distance for domain adaptation lie in its
gradient property and promising generalization bound. Empirical studies on
common sentiment and image classification adaptation datasets demonstrate that
our proposed WDGRL outperforms the state-of-the-art domain invariant
representation learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yanru Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08645">
<title>Dropout Feature Ranking for Deep Learning Models. (arXiv:1712.08645v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08645</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) achieve state-of-the-art results in a variety of
domains. Unfortunately, DNNs are notorious for their non-interpretability, and
thus limit their applicability in hypothesis-driven domains such as biology and
healthcare. Moreover, in the resource-constraint setting, it is critical to
design tests relying on fewer more informative features leading to high
accuracy performance within reasonable budget. We aim to close this gap by
proposing a new general feature ranking method for deep learning. We show that
our simple yet effective method performs on par or compares favorably to eight
strawman, classical and deep-learning feature ranking methods in two
simulations and five very different datasets on tasks ranging from
classification to regression, in both static and time series scenarios. We also
illustrate the use of our method on a drug response dataset and show that it
identifies genes relevant to the drug-response.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chun-Hao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1&quot;&gt;Ladislav Rampasek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1&quot;&gt;Anna Goldenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06167">
<title>CapsuleGAN: Generative Adversarial Capsule Network. (arXiv:1802.06167v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06167</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Generative Adversarial Capsule Network (CapsuleGAN), a framework
that uses capsule networks (CapsNets) instead of the standard convolutional
neural networks (CNNs) as discriminators within the generative adversarial
network (GAN) setting, while modeling image data. We provide guidelines for
designing CapsNet discriminators and the updated GAN objective function, which
incorporates the CapsNet margin loss, for training CapsuleGAN models. We show
that CapsuleGAN outperforms convolutional-GAN at modeling image data
distribution on MNIST and CIFAR-10 datasets, evaluated on the generative
adversarial metric and at semi-supervised image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaiswal_A/0/1/0/all/0/1&quot;&gt;Ayush Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+AbdAlmageed_W/0/1/0/all/0/1&quot;&gt;Wael AbdAlmageed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Natarajan_P/0/1/0/all/0/1&quot;&gt;Premkumar Natarajan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>