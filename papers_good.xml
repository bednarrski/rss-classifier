<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10223"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10343"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.02320"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.05502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.05249"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.00524"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10253"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10574"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.10223">
<title>Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip. (arXiv:1804.10223v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.10223</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent Neural Networks (RNNs) are powerful tools for solving
sequence-based problems, but their efficacy and execution time are dependent on
the size of the network. Following recent work in simplifying these networks
with model pruning and a novel mapping of work onto GPUs, we design an
efficient implementation for sparse RNNs. We investigate several optimizations
and tradeoffs: Lamport timestamps, wide memory loads, and a bank-aware weight
layout. With these optimizations, we achieve speedups of over 6x over the next
best algorithm for a hidden layer of size 2304, batch size of 4, and a density
of 30%. Further, our technique allows for models of over 5x the size to fit on
a GPU for a speedup of 2x, enabling larger networks to help advance the
state-of-the-art. We perform case studies on NMT and speech recognition tasks
in the appendix, accelerating their recurrent layers by up to 3x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feiwen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pool_J/0/1/0/all/0/1&quot;&gt;Jeff Pool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersch_M/0/1/0/all/0/1&quot;&gt;Michael Andersch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Appleyard_J/0/1/0/all/0/1&quot;&gt;Jeremy Appleyard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1&quot;&gt;Fung Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10316">
<title>CompNet: Neural networks growing via the compact network morphism. (arXiv:1804.10316v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.10316</link>
<description rdf:parseType="Literal">&lt;p&gt;It is often the case that the performance of a neural network can be improved
by adding layers. In real-world practices, we always train dozens of neural
network architectures in parallel which is a wasteful process. We explored
$CompNet$, in which case we morph a well-trained neural network to a deeper one
where network function can be preserved and the added layer is compact. The
work of the paper makes two contributions: a). The modified network can
converge fast and keep the same functionality so that we do not need to train
from scratch again; b). The layer size of the added layer in the neural network
is controlled by removing the redundant parameters with sparse optimization.
This differs from previous network morphism approaches which tend to add more
neurons or channels beyond the actual requirements and result in redundance of
the model. The method is illustrated using several neural network structures on
different data sets including MNIST and CIFAR10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1&quot;&gt;Boi Faltings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10343">
<title>Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation. (arXiv:1804.10343v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.10343</link>
<description rdf:parseType="Literal">&lt;p&gt;Many imaging tasks require global information about all pixels in an image.
Conventional bottom-up classification networks globalize information by
decreasing resolution; features are pooled and downsampled into a single
output. But for semantic segmentation and object detection tasks, a network
must provide higher-resolution pixel-level outputs. To globalize information
while preserving resolution, many researchers propose the inclusion of
sophisticated auxiliary blocks, but these come at the cost of a considerable
increase in network size and computational cost. This paper proposes stacked
u-nets (SUNets), which iteratively combine features from different resolution
scales while maintaining resolution. SUNets leverage the information
globalization power of u-nets in a deeper network architectures that is capable
of handling the complexity of natural images. SUNets perform extremely well on
semantic segmentation tasks using a small number of parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Sohil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1&quot;&gt;Pallabi Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1&quot;&gt;Larry S Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.02320">
<title>Adversarial Ladder Networks. (arXiv:1611.02320v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1611.02320</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of unsupervised data in addition to supervised data in training
discriminative neural networks has improved the performance of this clas-
sification scheme. However, the best results were achieved with a training
process that is divided in two parts: first an unsupervised pre-training step
is done for initializing the weights of the network and after these weights are
refined with the use of supervised data. On the other hand adversarial noise
has improved the results of clas- sical supervised learning. Recently, a new
neural network topology called Ladder Network, where the key idea is based in
some properties of hierar- chichal latent variable models, has been proposed as
a technique to train a neural network using supervised and unsupervised data at
the same time with what is called semi-supervised learning. This technique has
reached state of the art classification. In this work we add adversarial noise
to the ladder network and get state of the art classification, with several
important conclusions on how adversarial noise can help in addition with new
possible lines of investi- gation. We also propose an alternative to add
adversarial noise to unsu- pervised data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molano_J/0/1/0/all/0/1&quot;&gt;Juan Maro&amp;#xf1;as Molano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colomer_A/0/1/0/all/0/1&quot;&gt;Alberto Albiol Colomer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacios_R/0/1/0/all/0/1&quot;&gt;Roberto Paredes Palacios&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.05502">
<title>The power of deeper networks for expressing natural functions. (arXiv:1705.05502v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.05502</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well-known that neural networks are universal approximators, but that
deeper networks tend in practice to be more powerful than shallower ones. We
shed light on this by proving that the total number of neurons $m$ required to
approximate natural classes of multivariate polynomials of $n$ variables grows
only linearly with $n$ for deep neural networks, but grows exponentially when
merely a single hidden layer is allowed. We also provide evidence that when the
number of hidden layers is increased from $1$ to $k$, the neuron requirement
grows exponentially not with $n$ but with $n^{1/k}$, suggesting that the
minimum number of layers required for practical expressibility grows only
logarithmically with $n$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt; (MIT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1&quot;&gt;Max Tegmark&lt;/a&gt; (MIT)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10123">
<title>IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image Classification. (arXiv:1804.10123v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1804.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep residual networks (ResNets) made a recent breakthrough in deep learning.
The core idea of ResNets is to have shortcut connections between layers that
allow the network to be much deeper while still being easy to optimize avoiding
vanishing gradients. These shortcut connections have interesting side-effects
that make ResNets behave differently from other typical network architectures.
In this work we use these properties to design a network based on a ResNet but
with parameter sharing and with adaptive computation time. The resulting
network is much smaller than the original network and can adapt the
computational cost to the complexity of the input image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leroux_S/0/1/0/all/0/1&quot;&gt;Sam Leroux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1&quot;&gt;Pavlo Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoens_P/0/1/0/all/0/1&quot;&gt;Pieter Simoens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhoedt_B/0/1/0/all/0/1&quot;&gt;Bart Dhoedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1&quot;&gt;Thomas Breuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10202">
<title>Sounding Board: A User-Centric and Content-Driven Social Chatbot. (arXiv:1804.10202v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1804.10202</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Sounding Board, a social chatbot that won the 2017 Amazon Alexa
Prize. The system architecture consists of several components including spoken
language processing, dialogue management, language generation, and content
management, with emphasis on user-centric and content-driven design. We also
share insights gained from large-scale online logs based on 160,000
conversations with real-world users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1&quot;&gt;Maarten Sap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1&quot;&gt;Elizabeth Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1&quot;&gt;Ari Holtzman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1&quot;&gt;Mari Ostendorf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10587">
<title>An improvement of the convergence proof of the ADAM-Optimizer. (arXiv:1804.10587v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10587</link>
<description rdf:parseType="Literal">&lt;p&gt;A common way to train neural networks is the Backpropagation. This algorithm
includes a gradient descent method, which needs an adaptive step size. In the
area of neural networks, the ADAM-Optimizer is one of the most popular adaptive
step size methods. It was invented in \cite{Kingma.2015} by Kingma and Ba. The
$5865$ citations in only three years shows additionally the importance of the
given paper. We discovered that the given convergence proof of the optimizer
contains some mistakes, so that the proof will be wrong. In this paper we give
an improvement to the convergence proof of the ADAM-Optimizer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bock_S/0/1/0/all/0/1&quot;&gt;Sebastian Bock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goppold_J/0/1/0/all/0/1&quot;&gt;Josef Goppold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1&quot;&gt;Martin Wei&amp;#xdf;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.05249">
<title>CLBlast: A Tuned OpenCL BLAS Library. (arXiv:1705.05249v2 [cs.MS] UPDATED)</title>
<link>http://arxiv.org/abs/1705.05249</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces CLBlast, an open-source BLAS library providing optimized
OpenCL routines to accelerate dense linear algebra for a wide variety of
devices. It is targeted at machine learning and HPC applications and thus
provides a fast matrix-multiplication routine (GEMM) to accelerate the core of
many applications (e.g. deep learning, iterative solvers, astrophysics,
computational fluid dynamics, quantum chemistry). CLBlast has five main
advantages over other OpenCL BLAS libraries: 1) it is optimized for and tested
on a large variety of OpenCL devices including less commonly used devices such
as embedded and low-power GPUs, 2) it can be explicitly tuned for specific
problem-sizes on specific hardware platforms, 3) it can perform operations in
half-precision floating-point FP16 saving bandwidth, time and energy, 4) it has
an optional CUDA back-end, 5) and it can combine multiple operations in a
single batched routine, accelerating smaller problems significantly. This paper
describes the library and demonstrates the advantages of CLBlast experimentally
for different use-cases on a wide variety of OpenCL hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nugteren_C/0/1/0/all/0/1&quot;&gt;Cedric Nugteren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.00524">
<title>Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning. (arXiv:1707.00524v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.00524</link>
<description rdf:parseType="Literal">&lt;p&gt;In deep reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL, where we build the capability for an RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to predict future frames given a state-action pair, and a
convolutional autoencoder model to hash over the seen frames. In addition, to
utilize the counts derived from the seen frames to evaluate the frequentness
for the predicted frames, we tackle the challenge of matching the predicted
future frames and their corresponding seen frames at the latent feature level.
In this way, we derive a reliable metric for evaluating the novelty of the
future direction pointed by each action, and hence inform the agent to explore
the least frequent one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Haiyan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Sinno Jialin Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00946">
<title>Unsupervised Learning of Sequence Representations by Autoencoders. (arXiv:1804.00946v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00946</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequence data is challenging for machine learning approaches, because the
lengths of the sequences may vary between samples. In this paper, we present an
unsupervised learning model for sequence data, called the Integrated Sequence
Autoencoder (ISA), to learn a fixed-length vectorial representation by
minimizing the reconstruction error. Specifically, we propose to integrate two
classical mechanisms for sequence reconstruction which takes into account both
the global silhouette information and the local temporal dependencies.
Furthermore, we propose a stop feature that serves as a temporal stamp to guide
the reconstruction process, which results in a higher-quality representation.
The learned representation is able to effectively summarize not only the
apparent features, but also the underlying and high-level style information.
Take for example a speech sequence sample: our ISA model can not only recognize
the spoken text (apparent feature), but can also discriminate the speaker who
utters the audio (more high-level style). One promising application of the ISA
model is that it can be readily used in the semi-supervised learning scenario,
in which a large amount of unlabeled data is leveraged to extract high-quality
sequence representations and thus to improve the performance of the subsequent
supervised learning tasks on limited labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1&quot;&gt;Wenjie Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tax_D/0/1/0/all/0/1&quot;&gt;David M.J. Tax&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09849">
<title>The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation. (arXiv:1804.09849v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09849</link>
<description rdf:parseType="Literal">&lt;p&gt;The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)
modeling for Machine Translation (MT). The classic RNN-based approaches to MT
were first out-performed by the convolutional seq2seq model, which was then
out-performed by the more recent Transformer model. Each of these new
approaches consists of a fundamental architecture accompanied by a set of
modeling and training techniques that are in principle applicable to other
seq2seq architectures. In this paper, we tease apart the new architectures and
their accompanying techniques in two ways. First, we identify several key
modeling and training techniques, and apply them to the RNN architecture,
yielding a new RNMT+ model that outperforms all of the three fundamental
architectures on the benchmark WMT&apos;14 English to French and English to German
tasks. Second, we analyze the properties of each fundamental seq2seq
architecture and devise new hybrid architectures intended to combine their
strengths. Our hybrid models obtain further improvements, outperforming the
RNMT+ model on both benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mia Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1&quot;&gt;Ankur Bapna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Melvin Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1&quot;&gt;Wolfgang Macherey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_G/0/1/0/all/0/1&quot;&gt;George Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1&quot;&gt;Llion Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1&quot;&gt;Niki Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_M/0/1/0/all/0/1&quot;&gt;Mike Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Macduff Hughes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10253">
<title>From Principal Subspaces to Principal Components with Linear Autoencoders. (arXiv:1804.10253v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10253</link>
<description rdf:parseType="Literal">&lt;p&gt;The autoencoder is an effective unsupervised learning model which is widely
used in deep learning. It is well known that an autoencoder with a single
fully-connected hidden layer, a linear activation function and a squared error
cost function trains weights that span the same subspace as the one spanned by
the principal component loading vectors, but that they are not identical to the
loading vectors. In this paper, we show how to recover the loading vectors from
the autoencoder weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Plaut_E/0/1/0/all/0/1&quot;&gt;Elad Plaut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10272">
<title>Network Transplanting. (arXiv:1804.10272v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10272</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on a novel problem, i.e., transplanting a
category-and-task-specific neural network to a generic, distributed network
without strong supervision. Like playing LEGO blocks, incrementally
constructing a generic network by asynchronously merging specific neural
networks is a crucial bottleneck for deep learning. Suppose that the
pre-trained specific network contains a module $f$ to extract features of the
target category, and the generic network has a module $g$ for a target task,
which is trained using other categories except for the target category. Instead
of using numerous training samples to teach the generic network a new category,
we aim to learn a small adapter module to connect $f$ and $g$ to accomplish the
task on a target category in a weakly-supervised manner. The core challenge is
to efficiently learn feature projections between the two connected modules. We
propose a new distillation algorithm, which exhibited superior performance. Our
method without training samples even significantly outperformed the baseline
with 100 training samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10574">
<title>Decoupled Parallel Backpropagation with Convergence Guarantee. (arXiv:1804.10574v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10574</link>
<description rdf:parseType="Literal">&lt;p&gt;Backpropagation algorithm is indispensable for the training of feedforward
neural networks. It requires propagating error gradients sequentially from the
output layer all the way back to the input layer. The backward locking in
backpropagation algorithm constrains us from updating network layers in
parallel and fully leveraging the computing resources. Recently, several
algorithms have been proposed for breaking the backward locking. However, their
performances degrade seriously when networks are deep. In this paper, we
propose decoupled parallel backpropagation algorithm for deep learning
optimization with convergence guarantee. Firstly, we decouple the
backpropagation algorithm using delayed gradients, and show that the backward
locking is removed when we split the networks into multiple modules. Then, we
utilize decoupled parallel backpropagation in two stochastic methods and prove
that our method guarantees convergence to critical points for the non-convex
problem. Finally, we perform experiments for training deep convolutional neural
networks on benchmark datasets. The experimental results not only confirm our
theoretical analysis, but also demonstrate that the proposed method can achieve
significant speedup without loss of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;</dc:creator>
</item></rdf:RDF>