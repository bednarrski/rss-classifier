<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10131"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10332"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10449"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10478"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10518"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10166"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10179"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10182"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10308"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10349"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10480"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04368"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05428"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01947"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09730"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.10181">
<title>Unsupervised Learning by Competing Hidden Units. (arXiv:1806.10181v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10181</link>
<description rdf:parseType="Literal">&lt;p&gt;It is widely believed that the backpropagation algorithm is essential for
learning good feature detectors in early layers of artificial neural networks,
so that these detectors are useful for the task performed by the higher layers
of that neural network. At the same time, the traditional form of
backpropagation is biologically implausible. In the present paper we propose an
unusual learning rule, which has a degree of biological plausibility, and which
is motivated by Hebb&apos;s idea that change of the synapse strength should be local
- i.e. should depend only on the activities of the pre and post synaptic
neurons. We design a learning algorithm that utilizes global inhibition in the
hidden layer, and is capable of learning early feature detectors in a
completely unsupervised way. These learned lower layer feature detectors can be
used to train higher layer weights in a usual supervised way so that the
performance of the full network is comparable to the performance of standard
feedforward networks trained end-to-end with a backpropagation algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krotov_D/0/1/0/all/0/1&quot;&gt;Dmitry Krotov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hopfield_J/0/1/0/all/0/1&quot;&gt;John Hopfield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05978">
<title>Bayesian Convolutional Neural Networks. (arXiv:1806.05978v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05978</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Bayesian convolutional neural network built upon Bayes by
Backprop and elaborate how this known method can serve as the fundamental
construct of our novel, reliable variational inference method for convolutional
neural networks. First, we show how Bayes by Backprop can be applied to
convolutional layers where weights in filters have probability distributions
instead of point-estimates; and second, how our proposed framework leads with
various network architectures to performances comparable to convolutional
neural networks with point-estimates weights. In the past, Bayes by Backprop
has been successfully utilised in feedforward and recurrent neural networks,
but not in convolutional ones. This work symbolises the extension of the group
of Bayesian neural networks which encompasses all three aforementioned types of
network architectures now.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laumann_F/0/1/0/all/0/1&quot;&gt;Felix Laumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1&quot;&gt;Kumar Shridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maurin_A/0/1/0/all/0/1&quot;&gt;Adrian Llopart Maurin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10131">
<title>Request-and-Reverify: Hierarchical Hypothesis Testing for Concept Drift Detection with Expensive Labels. (arXiv:1806.10131v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10131</link>
<description rdf:parseType="Literal">&lt;p&gt;One important assumption underlying common classification models is the
stationarity of the data. However, in real-world streaming applications, the
data concept indicated by the joint distribution of feature and label is not
stationary but drifting over time. Concept drift detection aims to detect such
drifts and adapt the model so as to mitigate any deterioration in the model&apos;s
predictive performance. Unfortunately, most existing concept drift detection
methods rely on a strong and over-optimistic condition that the true labels are
available immediately for all already classified instances. In this paper, a
novel Hierarchical Hypothesis Testing framework with Request-and-Reverify
strategy is developed to detect concept drifts by requesting labels only when
necessary. Two methods, namely Hierarchical Hypothesis Testing with
Classification Uncertainty (HHT-CU) and Hierarchical Hypothesis Testing with
Attribute-wise &quot;Goodness-of-fit&quot; (HHT-AG), are proposed respectively under the
novel framework. In experiments with benchmark datasets, our methods
demonstrate overwhelming advantages over state-of-the-art unsupervised drift
detectors. More importantly, our methods even outperform DDM (the widely used
supervised drift detector) when we use significantly fewer labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10322">
<title>The Virtuous Machine - Old Ethics for New Technology?. (arXiv:1806.10322v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.10322</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern AI and robotic systems are characterized by a high and ever-increasing
level of autonomy. At the same time, their applications in fields such as
autonomous driving, service robotics and digital personal assistants move
closer to humans. From the combination of both developments emerges the field
of AI ethics which recognizes that the actions of autonomous machines entail
moral dimensions and tries to answer the question of how we can build moral
machines. In this paper we argue for taking inspiration from Aristotelian
virtue ethics by showing that it forms a suitable combination with modern AI
due to its focus on learning from experience. We furthermore propose that
imitation learning from moral exemplars, a central concept in virtue ethics,
can solve the value alignment problem. Finally, we show that an intelligent
system endowed with the virtues of temperance and friendship to humans would
not pose a control problem as it would not have the desire for limitless
self-improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berberich_N/0/1/0/all/0/1&quot;&gt;Nicolas Berberich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1&quot;&gt;Klaus Diepold&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10332">
<title>MONAS: Multi-Objective Neural Architecture Search using Reinforcement Learning. (arXiv:1806.10332v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10332</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on neural architecture search have shown that automatically
designed neural networks perform as good as human-designed architectures. While
most existing works on neural architecture search aim at finding architectures
that optimize for prediction accuracy. These methods may generate complex
architectures consuming excessively high energy consumption, which is not
suitable for computing environment with limited power budgets. We propose
MONAS, a Multi-Objective Neural Architecture Search with novel reward functions
that consider both prediction accuracy and power consumption when exploring
neural architectures. MONAS effectively explores the design space and searches
for architectures satisfying the given requirements. The experimental results
demonstrate that the architectures found by MONAS achieve accuracy comparable
to or better than the state-of-the-art models, while having better energy
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chi-Hung Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shu-Huan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1&quot;&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jia-Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Chieh Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10449">
<title>A Proof of the Front-Door Adjustment Formula. (arXiv:1806.10449v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.10449</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a proof of the the Front-Door adjustment formula using the
do-calculus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidian_M/0/1/0/all/0/1&quot;&gt;Mohammad Ali Javidian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valtorta_M/0/1/0/all/0/1&quot;&gt;Marco Valtorta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10478">
<title>Neural Machine Translation for Query Construction and Composition. (arXiv:1806.10478v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.10478</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on question answering with knowledge base has recently seen an
increasing use of deep architectures. In this extended abstract, we study the
application of the neural machine translation paradigm for question parsing. We
employ a sequence-to-sequence model to learn graph patterns in the SPARQL graph
query language and their compositions. Instead of inducing the programs through
question-answer pairs, we expect a semi-supervised approach, where alignments
between questions and queries are built through templates. We argue that the
coverage of language utterances can be expanded using late notable works in
natural language generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soru_T/0/1/0/all/0/1&quot;&gt;Tommaso Soru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marx_E/0/1/0/all/0/1&quot;&gt;Edgard Marx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdestilhas_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Valdestilhas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteves_D/0/1/0/all/0/1&quot;&gt;Diego Esteves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1&quot;&gt;Diego Moussallem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Publio_G/0/1/0/all/0/1&quot;&gt;Gustavo Publio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10518">
<title>Knowledge-Driven Wireless Networks with Artificial Intelligence: Design, Challenges and Opportunities. (arXiv:1806.10518v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1806.10518</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses technology challenges and opportunities to embrace
artificial intelligence (AI) era in the design of wireless networks. We aim to
provide readers with motivation and general methodology for adoption of AI in
the context of next-generation networks. First, we discuss the rise of network
intelligence and then, we introduce a brief overview of AI with machine
learning (ML) and their relationship to self-organization designs. Finally, we
discuss design of intelligent agent and it&apos;s functions to enable
knowledge-driven wireless networks with AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gacanin_H/0/1/0/all/0/1&quot;&gt;Haris Gacanin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10574">
<title>This looks like that: deep learning for interpretable image recognition. (arXiv:1806.10574v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10574</link>
<description rdf:parseType="Literal">&lt;p&gt;When we are faced with challenging image classification tasks, we often
explain our reasoning by dissecting the image, and pointing out prototypical
aspects of one class or another. The mounting evidence for each of the classes
helps us make our final decision. In this work, we introduce a deep network
architecture that reasons in a similar way: the network dissects the image by
finding prototypical parts, and combines evidence from the prototypes to make a
final classification. The algorithm thus reasons in a way that is qualitatively
similar to the way ornithologists, physicians, geologists, architects, and
others would explain to people on how to solve challenging image classification
tasks. The network uses only image-level labels for training, meaning that
there are no labels for parts of images. We demonstrate the method on the
CIFAR-10 dataset and 10 classes from the CUB-200-2011 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1&quot;&gt;Oscar Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnett_A/0/1/0/all/0/1&quot;&gt;Alina Barnett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jonathan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02965">
<title>A Multi-Objective Deep Reinforcement Learning Framework. (arXiv:1803.02965v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02965</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new multi-objective deep reinforcement learning (MODRL)
framework based on deep Q-networks. We propose the use of linear and non-linear
methods to develop the MODRL framework that includes both single-policy and
multi-policy strategies. The experimental results on two benchmark problems
including the two-objective deep sea treasure environment and the
three-objective mountain car problem indicate that the proposed framework is
able to converge to the optimal Pareto solutions effectively. The proposed
framework is generic, which allows implementation of different deep
reinforcement learning algorithms in different complex environments. This
therefore overcomes many difficulties involved with standard multi-objective
reinforcement learning (MORL) methods existing in the current literature. The
framework creates a platform as a testbed environment to develop methods for
solving various problems associated with the current MORL. Details of the
framework implementation can be referred to
&lt;a href=&quot;http://www.deakin.edu.au/~thanhthi/drl.htm.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh Thi Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10095">
<title>Research on Artificial Intelligence Ethics Based on the Evolution of Population Knowledge Base. (arXiv:1806.10095v2 [cs.OH] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10095</link>
<description rdf:parseType="Literal">&lt;p&gt;The unclear development direction of human society is a deep reason for that
it is difficult to form a uniform ethical standard for human society and
artificial intelligence. Since the 21st century, the latest advances in the
Internet, brain science and artificial intelligence have brought new
inspiration to the research on the development direction of human society.
Through the study of the Internet brain model, AI IQ evaluation, and the
evolution of the brain, this paper proposes that the evolution of population
knowledge base is the key for judging the development direction of human
society, thereby discussing the standards and norms for the construction of
artificial intelligence ethics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yong Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10166">
<title>Modular meta-learning. (arXiv:1806.10166v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10166</link>
<description rdf:parseType="Literal">&lt;p&gt;Many prediction problems, such as those that arise in the context of
robotics, have a simplifying underlying structure that could accelerate
learning. In this paper, we present a strategy for learning a set of neural
network modules that can be combined in different ways. We train different
modular structures on a set of related tasks and generalize to new tasks by
composing the learned modules in new ways. We show this improves performance in
two robotics-related problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1&quot;&gt;Ferran Alet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie P. Kaelbling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10179">
<title>Multi-Merge Budget Maintenance for Stochastic Gradient Descent SVM Training. (arXiv:1806.10179v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10179</link>
<description rdf:parseType="Literal">&lt;p&gt;Budgeted Stochastic Gradient Descent (BSGD) is a state-of-the-art technique
for training large-scale kernelized support vector machines. The budget
constraint is maintained incrementally by merging two points whenever the
pre-defined budget is exceeded. The process of finding suitable merge partners
is costly; it can account for up to 45% of the total training time. In this
paper we investigate computationally more efficient schemes that merge more
than two points at once. We obtain significant speed-ups without sacrificing
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qaadan_S/0/1/0/all/0/1&quot;&gt;Sahar Qaadan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1&quot;&gt;Tobias Glasmachers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10180">
<title>Speeding Up Budgeted Stochastic Gradient Descent SVM Training with Precomputed Golden Section Search. (arXiv:1806.10180v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10180</link>
<description rdf:parseType="Literal">&lt;p&gt;Limiting the model size of a kernel support vector machine to a pre-defined
budget is a well-established technique that allows to scale SVM learning and
prediction to large-scale data. Its core addition to simple stochastic gradient
training is budget maintenance through merging of support vectors. This
requires solving an inner optimization problem with an iterative method many
times per gradient step. In this paper we replace the iterative procedure with
a fast lookup. We manage to reduce the merging time by up to 65% and the total
training time by 44% without any loss of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1&quot;&gt;Tobias Glasmachers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qaadan_S/0/1/0/all/0/1&quot;&gt;Sahar Qaadan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10182">
<title>Dual SVM Training on a Budget. (arXiv:1806.10182v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10182</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a dual subspace ascent algorithm for support vector machine
training that respects a budget constraint limiting the number of support
vectors. Budget methods are effective for reducing the training time of kernel
SVM while retaining high accuracy. To date, budget training is available only
for primal (SGD-based) solvers. Dual subspace ascent methods like sequential
minimal optimization are attractive for their good adaptation to the problem
structure, their fast convergence rate, and their practical speed. By
incorporating a budget constraint into a dual algorithm, our method enjoys the
best of both worlds. We demonstrate considerable speed-ups over primal budget
training methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qaadan_S/0/1/0/all/0/1&quot;&gt;Sahar Qaadan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuler_M/0/1/0/all/0/1&quot;&gt;Merlin Sch&amp;#xfc;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1&quot;&gt;Tobias Glasmachers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10308">
<title>Matrix Completion from Non-Uniformly Sampled Entries. (arXiv:1806.10308v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10308</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider matrix completion from non-uniformly sampled
entries including fully observed and partially observed columns. Specifically,
we assume that a small number of columns are randomly selected and fully
observed, and each remaining column is partially observed with uniform
sampling. To recover the unknown matrix, we first recover its column space from
the fully observed columns. Then, for each partially observed column, we
recover it by finding a vector which lies in the recovered column space and
consists of the observed entries. When the unknown $m\times n$ matrix is
low-rank, we show that our algorithm can exactly recover it from merely
$\Omega(rn\ln n)$ entries, where $r$ is the rank of the matrix. Furthermore,
for a noisy low-rank matrix, our algorithm computes a low-rank approximation of
the unknown matrix and enjoys an additive error bound measured by Frobenius
norm. Experimental results on synthetic datasets verify our theoretical claims
and demonstrate the effectiveness of our proposed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yuanyu Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10317">
<title>Adversarial Distillation of Bayesian Neural Network Posteriors. (arXiv:1806.10317v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10317</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian neural networks (BNNs) allow us to reason about uncertainty in a
principled way. Stochastic Gradient Langevin Dynamics (SGLD) enables efficient
BNN learning by drawing samples from the BNN posterior using mini-batches.
However, SGLD and its extensions require storage of many copies of the model
parameters, a potentially prohibitive cost, especially for large neural
networks. We propose a framework, Adversarial Posterior Distillation, to
distill the SGLD samples using a Generative Adversarial Network (GAN). At
test-time, samples are generated by the GAN. We show that this distillation
framework incurs no loss in performance on recent BNN applications including
anomaly detection, active learning, and defense against adversarial attacks. By
construction, our framework not only distills the Bayesian predictive
distribution, but the posterior itself. This allows one to compute quantities
such as the approximate model variance, which is useful in downstream tasks. To
our knowledge, these are the first results applying MCMC-based BNNs to the
aforementioned downstream applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kuan-Chieh Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicol_P/0/1/0/all/0/1&quot;&gt;Paul Vicol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1&quot;&gt;James Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Li Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10349">
<title>Quantum-chemical insights from interpretable atomistic neural networks. (arXiv:1806.10349v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/1806.10349</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of deep neural networks for quantum chemistry applications,
there is a pressing need for architectures that, beyond delivering accurate
predictions of chemical properties, are readily interpretable by researchers.
Here, we describe interpretation techniques for atomistic neural networks on
the example of Behler-Parrinello networks as well as the end-to-end model
SchNet. Both models obtain predictions of chemical properties by aggregating
atom-wise contributions. These latent variables can serve as local explanations
of a prediction and are obtained during training without additional cost. Due
to their correspondence to well-known chemical concepts such as atomic energies
and partial charges, these atom-wise explanations enable insights not only
about the model but more importantly about the underlying quantum-chemical
regularities. We generalize from atomistic explanations to 3d space, thus
obtaining spatially resolved visualizations which further improve
interpretability. Finally, we analyze learned embeddings of chemical elements
that exhibit a partial ordering that resembles the order of the periodic table.
As the examined neural networks show excellent agreement with chemical
knowledge, the presented techniques open up new venues for data-driven research
in chemistry, physics and materials science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Schutt_K/0/1/0/all/0/1&quot;&gt;Kristof T. Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gastegger_M/0/1/0/all/0/1&quot;&gt;Michael Gastegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tkatchenko_A/0/1/0/all/0/1&quot;&gt;Alexandre Tkatchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10480">
<title>Employee Attrition Prediction. (arXiv:1806.10480v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.10480</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to predict whether an employee of a company will leave or not, using
the k-Nearest Neighbors algorithm. We use evaluation of employee performance,
average monthly hours at work and number of years spent in the company, among
others, as our features. Other approaches to this problem include the use of
ANNs, decision trees and logistic regression. The dataset was split, using 70%
for training the algorithm and 30% for testing it, achieving an accuracy of
94.32%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yedida_R/0/1/0/all/0/1&quot;&gt;Rahul Yedida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Reddy_R/0/1/0/all/0/1&quot;&gt;Rahul Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vahi_R/0/1/0/all/0/1&quot;&gt;Rakshit Vahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jana_R/0/1/0/all/0/1&quot;&gt;Rahul Jana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+GV_A/0/1/0/all/0/1&quot;&gt;Abhilash GV&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kulkarni_D/0/1/0/all/0/1&quot;&gt;Deepti Kulkarni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04368">
<title>Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples. (arXiv:1711.04368v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04368</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, researchers have discovered that the state-of-the-art object
classifiers can be fooled easily by small perturbations in the input
unnoticeable to human eyes. It is also known that an attacker can generate
strong adversarial examples if she knows the classifier parameters. Conversely,
a defender can robustify the classifier by retraining if she has access to the
adversarial examples. We explain and formulate this adversarial example problem
as a two-player continuous zero-sum game, and demonstrate the fallacy of
evaluating a defense or an attack as a static problem. To find the best
worst-case defense against whitebox attacks, we propose a continuous minimax
optimization algorithm. We demonstrate the minimax defense with two types of
attack classes -- gradient-based and neural network-based attacks. Experiments
with the MNIST and the CIFAR-10 datasets demonstrate that the defense found by
numerical minimax optimization is indeed more robust than non-minimax defenses.
We discuss directions for improving the result toward achieving robustness
against multiple types of attack classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1&quot;&gt;Jihun Hamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1&quot;&gt;Akshay Mehra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03329">
<title>Weakly Supervised One-Shot Detection with Attention Similarity Networks. (arXiv:1801.03329v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network models that are not conditioned on class identities were shown
to facilitate knowledge transfer between classes and to be well-suited for
one-shot learning tasks. Following this motivation, we further explore and
establish such models and present a novel neural network architecture for the
task of weakly supervised one-shot detection. Our model is only conditioned on
a single exemplar of an unseen class and a larger target example that may or
may not contain an instance of the same class as the exemplar. By pairing a
Siamese similarity network with an attention mechanism, we design a model that
manages to simultaneously identify and localise instances of classes unseen at
training time. In experiments with datasets from the computer vision and audio
domains, the proposed method considerably outperforms the baseline methods for
the weakly supervised one-shot detection task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keren_G/0/1/0/all/0/1&quot;&gt;Gil Keren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmitt_M/0/1/0/all/0/1&quot;&gt;Maximilian Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kehrenberg_T/0/1/0/all/0/1&quot;&gt;Thomas Kehrenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05428">
<title>A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music. (arXiv:1803.05428v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05428</link>
<description rdf:parseType="Literal">&lt;p&gt;The Variational Autoencoder (VAE) has proven to be an effective model for
producing semantically meaningful latent representations for natural data.
However, it has thus far seen limited application to sequential data, and, as
we demonstrate, existing recurrent VAE models have difficulty modeling
sequences with long-term structure. To address this issue, we propose the use
of a hierarchical decoder, which first outputs embeddings for subsequences of
the input and then uses these embeddings to generate each subsequence
independently. This structure encourages the model to utilize its latent code,
thereby avoiding the &quot;posterior collapse&quot; problem which remains an issue for
recurrent VAEs. We apply this architecture to modeling sequences of musical
notes and find that it exhibits dramatically better sampling, interpolation,
and reconstruction performance than a &quot;flat&quot; baseline model. An implementation
of our &quot;MusicVAE&quot; is available online at &lt;a href=&quot;http://g.co/magenta/musicvae-code.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1&quot;&gt;Adam Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1&quot;&gt;Jesse Engel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1&quot;&gt;Curtis Hawthorne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1&quot;&gt;Douglas Eck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01947">
<title>Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative Model. (arXiv:1804.01947v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we study generative modeling via autoencoders while using the
elegant geometric properties of the optimal transport (OT) problem and the
Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE),
which are generative models that enable one to shape the distribution of the
latent space into any samplable probability distribution without the need for
training an adversarial network or defining a closed-form for the distribution.
In short, we regularize the autoencoder loss with the sliced-Wasserstein
distance between the distribution of the encoded training samples and a
predefined samplable distribution. We show that the proposed formulation has an
efficient numerical solution that provides similar capabilities to Wasserstein
Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an
embarrassingly simple implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1&quot;&gt;Soheil Kolouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pope_P/0/1/0/all/0/1&quot;&gt;Phillip E. Pope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1&quot;&gt;Charles E. Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1&quot;&gt;Gustavo K. Rohde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05161">
<title>Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate. (arXiv:1806.05161v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05161</link>
<description rdf:parseType="Literal">&lt;p&gt;Many modern machine learning models are trained to achieve zero or near-zero
training error in order to obtain near-optimal (but non-zero) test error. This
phenomenon of strong generalization performance for &quot;overfitted&quot; / interpolated
classifiers appears to be ubiquitous in high-dimensional data, having been
observed in deep networks, kernel machines, boosting and random forests. Their
performance is robust even when the data contain large amounts of label noise.
&lt;/p&gt;
&lt;p&gt;Very little theory is available to explain these observations. The vast
majority of theoretical analyses of generalization allows for interpolation
only when there is little or no label noise. This paper takes a step toward a
theoretical foundation for interpolated classifiers by analyzing local
interpolating schemes, including geometric simplicial interpolation algorithm
and weighted $k$-nearest neighbor schemes. Consistency or near-consistency is
proved for these schemes in classification and regression problems. These
schemes have an inductive bias that benefits from higher dimension, a kind of
&quot;blessing of dimensionality&quot;. Finally, connections to kernel machines, random
forests, and adversarial examples in the interpolated regime are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Belkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;Daniel Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Partha Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09730">
<title>Analysis of Invariance and Robustness via Invertibility of ReLU-Networks. (arXiv:1806.09730v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09730</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying the invertibility of deep neural networks (DNNs) provides a
principled approach to better understand the behavior of these powerful models.
Despite being a promising diagnostic tool, a consistent theory on their
invertibility is still lacking. We derive a theoretically motivated approach to
explore the preimages of ReLU-layers and mechanisms affecting the stability of
the inverse. Using the developed theory, we numerically show how this approach
uncovers characteristic properties of the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behrmann_J/0/1/0/all/0/1&quot;&gt;Jens Behrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dittmer_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Dittmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernsel_P/0/1/0/all/0/1&quot;&gt;Pascal Fernsel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_P/0/1/0/all/0/1&quot;&gt;Peter Maa&amp;#xdf;&lt;/a&gt;</dc:creator>
</item></rdf:RDF>