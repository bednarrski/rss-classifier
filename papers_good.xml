<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05377"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00930"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05976"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05344"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07828"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10251"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05355"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05443"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01367"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05163"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.05377">
<title>Neural Architecture Search: A Survey. (arXiv:1808.05377v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05377</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning has enabled remarkable progress over the last years on a
variety of tasks, such as image recognition, speech recognition, and machine
translation. One crucial aspect for this progress are novel neural
architectures. Currently employed architectures have mostly been developed
manually by human experts, which is a time-consuming and error-prone process.
Because of this, there is growing interest in automated neural architecture
search methods. We provide an overview of existing work in this field of
research and categorize them according to three dimensions: search space,
search strategy, and performance estimation strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elsken_T/0/1/0/all/0/1&quot;&gt;Thomas Elsken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Metzen_J/0/1/0/all/0/1&quot;&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05385">
<title>On the Decision Boundary of Deep Neural Networks. (arXiv:1808.05385v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.05385</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning models and techniques have achieved great empirical
success, our understanding of the source of success in many aspects remains
very limited. In an attempt to bridge the gap, we investigate the decision
boundary of a production deep learning architecture with weak assumptions on
both the training data and the model. We demonstrate, both theoretically and
empirically, that the last weight layer of a neural network converges to a
linear SVM trained on the output of the last hidden layer, for both the binary
case and the multi-class case with the commonly used cross-entropy loss.
Furthermore, we show empirically that training a neural network as a whole,
instead of only fine-tuning the last weight layer, may result in better bias
constant for the last weight layer, which is important for generalization. In
addition to facilitating the understanding of deep learning, our result can be
helpful for solving a broad range of practical problems of deep learning, such
as catastrophic forgetting and adversarial attacking. The experiment codes are
available at https://github.com/lykaust15/NN_decision_boundary
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richtarik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Lizhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05525">
<title>Experiential Robot Learning with Accelerated Neuroevolution. (arXiv:1808.05525v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.05525</link>
<description rdf:parseType="Literal">&lt;p&gt;Derivative-based optimization techniques such as Stochastic Gradient Descent
has been wildly successful in training deep neural networks. However, it has
constraints such as end-to-end network differentiability. As an alternative, we
present the Accelerated Neuroevolution algorithm. The new algorithm is aimed
towards physical robotic learning tasks following the Experiential Robot
Learning method. We test our algorithm first on a simulated task of playing the
game Flappy Bird, then on a physical NAO robot in a static Object Centering
task. The agents successfully navigate the given tasks, in a relatively low
number of generations. Based on our results, we propose to use the algorithm in
more complex tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1&quot;&gt;Ahmed Aly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dugan_J/0/1/0/all/0/1&quot;&gt;Joanne B. Dugan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00930">
<title>Neural Random Projections for Language Modelling. (arXiv:1807.00930v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00930</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network-based language models deal with data sparsity problems by
mapping the large discrete space of words into a smaller continuous space of
real-valued vectors. By learning distributed vector representations for words,
each training sample informs the neural network model about a combinatorial
number of other patterns. We exploit the sparsity in natural language even
further by encoding each unique input word using a reduced sparse random
representation. In this paper, we propose an encoder for discrete inputs that
uses random projections to allow for the learning of language models using
significantly smaller parameter spaces when compared with similar neural
network architectures. Furthermore, random projections also eliminate the
dependency between a neural network architecture and the size of a
pre-established dictionary. We investigate the properties of our encoding
mechanism empirically, by evaluating its performance on the widely used Penn
Treebank corpus, using several configurations of baseline feedforward neural
network models. We show that guaranteeing approximately equidistant inner
products between representations of unique discrete inputs is enough to provide
the neural network model with enough information to learn useful distributed
representations for these inputs. By not requiring prior enumeration of the
lexicon, random projections allow us to face the dynamic and open character of
natural languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1&quot;&gt;Davide Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antunes_L/0/1/0/all/0/1&quot;&gt;Luis Antunes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05976">
<title>Why don&apos;t the modules dominate - Investigating the Structure of a Well-Known Modularity-Inducing Problem Domain. (arXiv:1807.05976v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05976</link>
<description rdf:parseType="Literal">&lt;p&gt;Wagner&apos;s modularity inducing problem domain is a key contribution to the
study of the evolution of modularity, including both evolutionary theory and
evolutionary computation. We study its behavior under classical genetic
algorithms. Unlike what we seem to observe in nature, the emergence of
modularity is highly conditional and dependent, for example, on the eagerness
of search. In nature, modular solutions generally dominate populations, whereas
in this domain, modularity, when it emerges, is a relatively rare variant.
Emergence of modularity depends heavily on random fluctuations in the fitness
function, with a randomly varied but unchanging fitness function, modularity
evolved far more rarely. Interestingly, high-fitness non-modular solutions
could frequently be converted into even-higher-fitness modular solutions by
manually removing all inter-module edges. Despite careful exploration, we do
not yet have a full explanation of why the genetic algorithm was unable to find
these better solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhenyue Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKay_R/0/1/0/all/0/1&quot;&gt;Robert McKay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1&quot;&gt;Tom Gedeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05344">
<title>Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model based on BLSTM. (arXiv:1808.05344v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1808.05344</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, most of the objective speech quality assessment tools (e.g.,
perceptual evaluation of speech quality (PESQ)) are based on the comparison of
the degraded/processed speech with its clean counterpart. The need of a
&quot;golden&quot; reference considerably restricts the practicality of such assessment
tools in real-world scenarios since the clean reference usually cannot be
accessed. On the other hand, human beings can readily evaluate the speech
quality without any reference (e.g., mean opinion score (MOS) tests), implying
the existence of an objective and non-intrusive (no clean reference needed)
quality assessment mechanism. In this study, we propose a novel end-to-end,
non-intrusive speech quality evaluation model, termed Quality-Net, based on
bidirectional long short-term memory. The evaluation of utterance-level quality
in Quality-Net is based on the frame-level assessment. Frame constraints and
sensible initializations of forget gate biases are applied to learn meaningful
frame-level quality assessment from the utterance-level quality label.
Experimental results show that Quality-Net can yield high correlation to PESQ
(0.9 for the noisy speech and 0.84 for the speech processed by speech
enhancement). We believe that Quality-Net has potential to be used in a wide
variety of applications of speech signal processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hsin-Te Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hsin-Min Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07828">
<title>A VEST of the Pseudoinverse Learning Algorithm. (arXiv:1805.07828v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07828</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we briefly review the basic scheme of the pseudoinverse
learning (PIL) algorithm and present some discussions on the PIL, as well as
its variants. The PIL algorithm, first presented in 1995, is a non-gradient
descent and non-iterative learning algorithm for multi-layer neural networks
and has several advantages compared with gradient descent based algorithms.
Some new viewpoints to PIL algorithm are presented, and several common pitfalls
in practical implementation of the neural network learning task are also
addressed. In addition, we show that so called extreme learning machine is a
Variant crEated by Simple name alTernation (VEST) of the PIL algorithm for
single hidden layer feedforward neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Ping Guo&lt;/a&gt; (School of Systems Science, Beijing Normal University, Beijing, China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10251">
<title>Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks. (arXiv:1807.10251v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10251</link>
<description rdf:parseType="Literal">&lt;p&gt;We establish an equivalence between information bottleneck (IB) learning and
an unconventional quantization problem, `IB quantization&apos;. Under this
equivalence, standard neural network models correspond to scalar IB quantizers.
We prove a coding theorem for IB quantization, which implies that scalar IB
quantizers are in general inferior to vector IB quantizers. This inspires us to
develop a learning framework for neural networks, AgrLearn, that corresponds to
vector IB quantizers. We experimentally verify that AgrLearn applied to some
deep network models of current art improves upon them, while requiring less
training data. With a heuristic smoothing, AgrLearn further improves its
performance, resulting in new state of the art in image classification on
Cifar10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hongyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongyi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05329">
<title>Sequential Behavioral Data Processing Using Deep Learning and the Markov Transition Field in Online Fraud Detection. (arXiv:1808.05329v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05329</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the popularity of the Internet and smart mobile devices, more and more
financial transactions and activities have been digitalized. Compared to
traditional financial fraud detection strategies using credit-related features,
customers are generating a large amount of unstructured behavioral data every
second. In this paper, we propose an Recurrent Neural Netword (RNN) based
deep-learning structure integrated with Markov Transition Field (MTF) for
predicting online fraud behaviors using customer&apos;s interactions with websites
or smart-phone apps as a series of states. In practice, we tested and proved
that the proposed network structure for processing sequential behavioral data
could significantly boost fraud predictive ability comparing with the
multilayer perceptron network and distance based classifier with Dynamic Time
Warping(DTW) as distance metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Fanglan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1&quot;&gt;Wei Min&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05355">
<title>Conceptual Domain Adaptation Using Deep Learning. (arXiv:1808.05355v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05355</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has recently been shown to be instrumental in the problem of
domain adaptation, where the goal is to learn a model on a target domain using
a similar --but not identical-- source domain. The rationale for coupling both
techniques is the possibility of extracting common concepts across domains.
Considering (strictly) local representations, traditional deep learning assumes
common concepts must be captured in the same hidden units. We contend that
jointly training a model with source and target data using a single deep
network is prone to failure when there is inherently lower-level
representational discrepancy between the two domains; such discrepancy leads to
a misalignment of corresponding concepts in separate hidden units. We introduce
a search framework to correctly align high-level representations when training
deep networks; such framework leads to the notion of conceptual --as opposed to
representational-- domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrparvar_B/0/1/0/all/0/1&quot;&gt;Behrang Mehrparvar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilalta_R/0/1/0/all/0/1&quot;&gt;Ricardo Vilalta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05443">
<title>Transfer Learning and Organic Computing for Autonomous Vehicles. (arXiv:1808.05443v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05443</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Vehicles(AV) are one of the brightest promises of the future which
would help cut down fatalities and improve travel time while working in
harmony. Autonomous vehicles will face with challenging situations and
experiences not seen before. These experiences should be converted to knowledge
and help the vehicle prepare better in the future. Online Transfer Learning
will help transferring prior knowledge to a new task and also keep the
knowledge updated as the task evolves. This paper presents the different
methods of transfer learning, online transfer learning and organic computing
that could be adapted to the domain of autonomous vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fellicious_C/0/1/0/all/0/1&quot;&gt;Christofer Fellicious&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05464">
<title>Transfer Learning for Brain-Computer Interfaces: An Euclidean Space Data Alignment Approach. (arXiv:1808.05464v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05464</link>
<description rdf:parseType="Literal">&lt;p&gt;Almost all EEG-based brain-computer interfaces (BCIs) need some labeled
subject-specific data to calibrate a new subject, as neural responses are
different across subjects to even the same stimulus. So, a major challenge in
developing high-performance and user-friendly BCIs is to cope with such
individual differences so that the calibration can be reduced or even
completely eliminated. This paper focuses on the latter. More specifically, we
consider an offline application scenario, in which we have unlabeled EEG trials
from a new subject, and would like to accurately label them by leveraging
auxiliary labeled EEG trials from other subjects in the same task. To
accommodate the individual differences, we propose a novel unsupervised
approach to align the EEG trials from different subjects in the Euclidean space
to make them more consistent. It has three desirable properties: 1) the aligned
trial lie in the Euclidean space, which can be used by any Euclidean space
signal processing and machine learning approach; 2) it can be computed very
efficiently; and, 3) it does not need any labeled trials from the new subject.
Experiments on motor imagery and event-related potentials demonstrated the
effectiveness and efficiency of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongrui Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05563">
<title>Learning Invariances using the Marginal Likelihood. (arXiv:1808.05563v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05563</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalising well in supervised learning tasks relies on correctly
extrapolating the training data to a large region of the input space. One way
to achieve this is to constrain the predictions to be invariant to
transformations on the input that are known to be irrelevant (e.g.
translation). Commonly, this is done through data augmentation, where the
training set is enlarged by applying hand-crafted transformations to the
inputs. We argue that invariances should instead be incorporated in the model
structure, and learned using the marginal likelihood, which correctly rewards
the reduced complexity of invariant models. We demonstrate this for Gaussian
process models, due to the ease with which their marginal likelihood can be
estimated. Our main contribution is a variational inference scheme for Gaussian
processes containing invariances described by a sampling procedure. We learn
the sampling procedure by back-propagating through it to maximise the marginal
likelihood.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilk_M/0/1/0/all/0/1&quot;&gt;Mark van der Wilk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1&quot;&gt;Matthias Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1&quot;&gt;ST John&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hensman_J/0/1/0/all/0/1&quot;&gt;James Hensman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05578">
<title>LARNN: Linear Attention Recurrent Neural Network. (arXiv:1808.05578v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.05578</link>
<description rdf:parseType="Literal">&lt;p&gt;The Linear Attention Recurrent Neural Network (LARNN) is a recurrent
attention module derived from the Long Short-Term Memory (LSTM) cell and ideas
from the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The
LARNN uses attention on its past cell state values for a limited window size
$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)
cell and the Transformer Network for its Multi-Head Attention Mechanism. The
Multi-Head Attention Mechanism is used inside the cell such that it can query
its own $k$ past values with the attention window. This has the effect of
augmenting the rank of the tensor with the attention mechanism, such that the
cell can perform complex queries to question its previous inner memories, which
should augment the long short-term effect of the memory. With a clever trick,
the LARNN cell with attention can be easily used inside a loop on the cell
state, just like how any other Recurrent Neural Network (RNN) cell can be
looped linearly through time series. This is due to the fact that its state,
which is looped upon throughout time steps within time series, stores the inner
states in a &quot;first in, first out&quot; queue which contains the $k$ most recent
states and on which it is easily possible to add static positional encoding
when the queue is represented as a tensor. This neural architecture yields
better results than the vanilla LSTM cells. It can obtain results of 91.92% for
the test accuracy, compared to the previously attained 91.65% using vanilla
LSTM cells. Note that this is not to compare to other research, where up to
93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3
cells as analyzed here. Finally, an interesting discovery is made, such that
adding activation within the multi-head attention mechanism&apos;s linear layers can
yield better results in the context researched hereto.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chevalier_G/0/1/0/all/0/1&quot;&gt;Guillaume Chevalier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05587">
<title>Deep Convolutional Networks as shallow Gaussian Processes. (arXiv:1808.05587v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05587</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that the output of a (residual) convolutional neural network (CNN)
with an appropriate prior over the weights and biases is a Gaussian process
(GP) in the limit of infinitely many convolutional filters, extending similar
results for dense networks. For a CNN, the equivalent kernel can be computed
exactly and, unlike &quot;deep kernels&quot;, has very few parameters: only the
hyperparameters of the original CNN. Further, we show that this kernel has two
properties that allow it to be computed efficiently; the cost of evaluating the
kernel for a pair of images is similar to a single forward pass through the
original CNN with only one filter per layer. The kernel equivalent to a
32-layer ResNet obtains 0.84% classification error on MNIST, a new record for
GPs with a comparable number of parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garriga_Alonso_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe0; Garriga-Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1&quot;&gt;Laurence Aitchison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rasmussen_C/0/1/0/all/0/1&quot;&gt;Carl Edward Rasmussen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00868">
<title>A Dynamic Model for Traffic Flow Prediction Using Improved DRN. (arXiv:1805.00868v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00868</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time traffic flow prediction can not only provide travelers with
reliable traffic information so that it can save people&apos;s time, but also assist
the traffic management agency to manage traffic system. It can greatly improve
the efficiency of the transportation system. Traditional traffic flow
prediction approaches usually need a large amount of data but still give poor
performances. With the development of deep learning, researchers begin to pay
attention to artificial neural networks (ANNs) such as RNN and LSTM. However,
these ANNs are very time-consuming. In our research, we improve the Deep
Residual Network and build a dynamic model which previous researchers hardly
use. We firstly integrate the input and output of the $i^{th}$ layer to the
input of the $i+1^{th}$ layer and prove that each layer will fit a simpler
function so that the error rate will be much smaller. Then, we use the concept
of online learning in our model to update pre-trained model during prediction.
Our result shows that our model has higher accuracy than some state-of-the-art
models. In addition, our dynamic model can perform better in practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zeren Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruimin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01020">
<title>Coopetitive Soft Gating Ensemble. (arXiv:1807.01020v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01020</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose the Coopetititve Soft Gating Ensemble or CSGE for
general machine learning tasks and interwoven systems. The goal of machine
learning is to create models that generalize well for unknown datasets. Often,
however, the problems are too complex to be solved with a single model, so
several models are combined. Similar, Autonomic Computing requires the
integration of different systems. Here, especially, the local, temporal online
evaluation and the resulting (re-)weighting scheme of the CSGE makes the
approach highly applicable for self-improving system integrations. To achieve
the best potential performance the CSGE can be optimized according to arbitrary
loss functions making it accessible for a broader range of problems. We
introduce a novel training procedure including a hyper-parameter initialisation
at its heart. We show that the CSGE approach reaches state-of-the-art
performance for both classification and regression tasks. Further on, the CSGE
provides a human-readable quantification on the influence of all base
estimators employing the three weighting aspects. Moreover, we provide a
scikit-learn compatible implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deist_S/0/1/0/all/0/1&quot;&gt;Stephan Deist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bieshaar_M/0/1/0/all/0/1&quot;&gt;Maarten Bieshaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schreiber_J/0/1/0/all/0/1&quot;&gt;Jens Schreiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gensler_A/0/1/0/all/0/1&quot;&gt;Andre Gensler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01367">
<title>EmbNum: Semantic labeling for numerical values with deep metric learning. (arXiv:1807.01367v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01367</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic labeling for numerical values is a task of assigning semantic labels
to unknown numerical attributes. The semantic labels could be numerical
properties in ontologies, instances in knowledge bases, or labeled data that
are manually annotated by domain experts. In this paper, we refer to semantic
labeling as a retrieval setting where the label of an unknown attribute is
assigned by the label of the most relevant attribute in labeled data. One of
the greatest challenges is that an unknown attribute rarely has the same set of
values with the similar one in the labeled data. To overcome the issue,
statistical interpretation of value distribution is taken into account.
However, the existing studies assume a specific form of distribution. It is not
appropriate in particular to apply open data where there is no knowledge of
data in advance. To address these problems, we propose a neural numerical
embedding model (EmbNum) to learn useful representation vectors for numerical
attributes without prior assumptions on the distribution of data. Then, the
&quot;semantic similarities&quot; between the attributes are measured on these
representation vectors by the Euclidean distance. Our empirical experiments on
City Data and Open Data show that EmbNum significantly outperforms
state-of-the-art methods for the task of numerical attribute semantic labeling
regarding effectiveness and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khai Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichise_R/0/1/0/all/0/1&quot;&gt;Ryutaro Ichise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_H/0/1/0/all/0/1&quot;&gt;Hideaki Takeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05163">
<title>A Simple but Hard-to-Beat Baseline for Session-based Recommendations. (arXiv:1808.05163v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05163</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) models have been recently introduced in
the domain of top-$N$ session-based recommendations. An ordered collection of
past items the user has interacted with in a session (or sequence) are embedded
into a 2-dimensional latent matrix, and treated as an image. The convolution
and pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we propose
a simple, but very effective generative model that is capable of learning
high-level representation from both short- and long-range dependencies. The
network architecture of the proposed model is formed of a stack of holed
convolutional layers, which can efficiently increase the receptive fields
without relying on the pooling operation. Another contribution is the effective
use of residual block structure in recommender systems, which not only reduces
the number of parameters but also eases the optimization for much deeper
networks. The proposed generative model attains state-of-the-art accuracy with
less training time in the session-based recommendation task. It accordingly can
be used as a powerful session-based recommendation baseline to beat in future,
especially when there are long sequences of user feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1&quot;&gt;Fajie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzoglou_A/0/1/0/all/0/1&quot;&gt;Alexandros Karatzoglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arapakis_I/0/1/0/all/0/1&quot;&gt;Ioannis Arapakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1&quot;&gt;Joemon M Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;</dc:creator>
</item></rdf:RDF>