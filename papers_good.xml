<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-25T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09299"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08375"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09304"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09399"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09540"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.08852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09593"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09605"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08420"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.09299">
<title>Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models. (arXiv:1804.09299v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09299</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Sequence-to-Sequence models have proven to be accurate and robust for
many sequence prediction tasks, and have become the standard approach for
automatic translation of text. The models work in a five stage blackbox process
that involves encoding a source sequence to a vector space and then decoding
out to a new target sequence. This process is now standard, but like many deep
learning methods remains quite difficult to understand or debug. In this work,
we present a visual analysis tool that allows interaction with a trained
sequence-to-sequence model through each stage of the translation process. The
aim is to identify which patterns have been learned and to detect model errors.
We demonstrate the utility of our tool through several real-world large-scale
sequence-to-sequence use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1&quot;&gt;Hendrik Strobelt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1&quot;&gt;Sebastian Gehrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behrisch_M/0/1/0/all/0/1&quot;&gt;Michael Behrisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perer_A/0/1/0/all/0/1&quot;&gt;Adam Perer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08375">
<title>Reusing Weights in Subword-aware Neural Language Models. (arXiv:1802.08375v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08375</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose several ways of reusing subword embeddings and other weights in
subword-aware neural language models. The proposed techniques do not benefit a
competitive character-aware model, but some of them improve the performance of
syllable- and morpheme-aware models while showing significant reductions in
model sizes. We discover a simple hands-on principle: in a multi-layer input
embedding model, layers should be tied consecutively bottom-up if reused at
output. Our best morpheme-aware model with properly reused weights beats the
competitive word-level model by a large margin across multiple languages and
has 20%-87% fewer parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1&quot;&gt;Zhenisbek Assylbekov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takhanov_R/0/1/0/all/0/1&quot;&gt;Rustem Takhanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09304">
<title>Real-Time Inference of User Types to Assist with More Inclusive Social Media Activism Campaigns. (arXiv:1804.09304v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1804.09304</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media provides a mechanism for people to engage with social causes
across a range of issues. It also provides a strategic tool to those looking to
advance a cause to exchange, promote or publicize their ideas. In such
instances, AI can be either an asset if used appropriately or a barrier. One of
the key issues for a workforce diversity campaign is to understand in real-time
who is participating - specifically, whether the participants are individuals
or organizations, and in case of individuals, whether they are male or female.
In this paper, we present a study to demonstrate a case for AI for social good
that develops a model to infer in real-time the different user types
participating in a cause-driven hashtag campaign on Twitter,
ILookLikeAnEngineer (ILLAE). A generic framework is devised to classify a
Twitter user into three classes: organization, male and female in a real-time
manner. The framework is tested against two datasets (ILLAE and a general
dataset) and outperforms the baseline binary classifiers for categorizing
organization/individual and male/female. The proposed model can be applied to
future social cause-driven campaigns to get real-time insights on the
macro-level social behavior of participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karbasian_H/0/1/0/all/0/1&quot;&gt;Habib Karbasian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purohit_H/0/1/0/all/0/1&quot;&gt;Hemant Purohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Handa_R/0/1/0/all/0/1&quot;&gt;Rajat Handa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1&quot;&gt;Aqdas Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johri_A/0/1/0/all/0/1&quot;&gt;Aditya Johri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09399">
<title>Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation. (arXiv:1804.09399v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09399</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been shown recently that convolutional generative adversarial networks
(GANs) are able to capture the temporal-pitch patterns in music using the
piano-roll representation, which represents music by binary-valued time-pitch
matrices. However, existing models can only generate real-valued piano-rolls
and require further post-processing (e.g. hard thresholding, Bernoulli
sampling) at test time to obtain the final binary-valued results. In this work,
we first investigate how the real-valued predictions generated by the generator
may lead to difficulties in training the discriminator. To overcome the
binarization issue, we propose to append to the generator an additional refiner
network, which uses binary neurons at the output layer. The whole network can
be trained in a two-stage training setting: the generator and the discriminator
are pretrained in the first stage; the refiner network is then trained along
with the discriminator in the second stage to refine the real-valued
piano-rolls generated by the pretrained generator to binary-valued ones. The
proposed model is able to directly generate binary-valued piano-rolls at test
time. Experimental results show improvements to the existing models in most of
the evaluation metrics. All source code, training data and audio samples can be
found at https://salu133445.github.io/bmusegan/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao-Wen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09502">
<title>Unsupervised Disentangled Representation Learning with Analogical Relations. (arXiv:1804.09502v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09502</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning the disentangled representation of interpretable generative factors
of data is one of the foundations to allow artificial intelligence to think
like people. In this paper, we propose the analogical training strategy for the
unsupervised disentangled representation learning in generative models. The
analogy is one of the typical cognitive processes, and our proposed strategy is
based on the observation that sample pairs in which one is different from the
other in one specific generative factor show the same analogical relation.
Thus, the generator is trained to generate sample pairs from which a designed
classifier can identify the underlying analogical relation. In addition, we
propose a disentanglement metric called the subspace score, which is inspired
by subspace learning methods and does not require supervised information.
Experiments show that our proposed training strategy allows the generative
models to find the disentangled factors, and that our methods can give
competitive performances as compared with the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zejian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yongchuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongxing He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09540">
<title>Named Entities troubling your Neural Methods? Build NE-Table: A neural approach for handling Named Entities. (arXiv:1804.09540v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09540</link>
<description rdf:parseType="Literal">&lt;p&gt;Many natural language processing tasks require dealing with Named Entities
(NEs) in the texts themselves and sometimes also in external knowledge sources.
While this is often easy for humans, recent neural methods that rely on learned
word embeddings for NLP tasks have difficulty with it, especially with out of
vocabulary or rare NEs. In this paper, we propose a new neural method for this
problem, and present empirical evaluations on a structured Question-Answering
task, three related Goal-Oriented dialog tasks and a
reading-comprehension-based task. They show that our proposed method can be
effective in dealing with both in-vocabulary and out of vocabulary (OOV) NEs.
We create extended versions of dialog bAbI tasks 1,2 and 4 and
Out-of-vocabulary (OOV) versions of the CBT test set which will be made
publicly available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1&quot;&gt;Janarthanan Rajendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganhotra_J/0/1/0/all/0/1&quot;&gt;Jatin Ganhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satinder Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09541">
<title>QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension. (arXiv:1804.09541v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09541</link>
<description rdf:parseType="Literal">&lt;p&gt;Current end-to-end machine reading and question answering (Q\&amp;amp;A) models are
primarily based on recurrent neural networks (RNNs) with attention. Despite
their success, these models are often slow for both training and inference due
to the sequential nature of RNNs. We propose a new Q\&amp;amp;A architecture called
QANet, which does not require recurrent networks: Its encoder consists
exclusively of convolution and self-attention, where convolution models local
interactions and self-attention models global interactions. On the SQuAD
dataset, our model is 3x to 13x faster in training and 4x to 9x faster in
inference, while achieving equivalent accuracy to recurrent models. The
speed-up gain allows us to train the model with much more data. We hence
combine our model with data generated by backtranslation from a neural machine
translation model. On the SQuAD dataset, our single model, trained with
augmented data, achieves 84.6 F1 score on the test set, which is significantly
better than the best published F1 score of 81.8.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1&quot;&gt;Adams Wei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1&quot;&gt;David Dohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1&quot;&gt;Minh-Thang Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09690">
<title>Fast View Synthesis with Deep Stereo Vision. (arXiv:1804.09690v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.09690</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel view synthesis is an important problem in computer vision and graphics.
Over the years a large number of solutions have been put forward to solve the
problem. However, the large-baseline novel view synthesis problem is far from
being &quot;solved&quot;. Recent works have attempted to use Convolutional Neural
Networks (CNNs) to solve view synthesis tasks. Due to the difficulty of
learning scene geometry and interpreting camera motion, CNNs are often unable
to generate realistic novel views. In this paper, we present a novel view
synthesis approach based on stereo-vision and CNNs that decomposes the problem
into two sub-tasks: view dependent geometry estimation and texture inpainting.
Both tasks are structured prediction problems that could be effectively learned
with CNNs. Experiments on the KITTI Odometry dataset show that our approach is
more accurate and significantly faster than the current state-of-the-art. The
code and supplementary material will be publicly available. Results could be
found here https://youtu.be/5pzS9jc-5t0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habtegebrial_T/0/1/0/all/0/1&quot;&gt;Tewodros Habtegebrial&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varanasi_K/0/1/0/all/0/1&quot;&gt;Kiran Varanasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailer_C/0/1/0/all/0/1&quot;&gt;Christian Bailer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1&quot;&gt;Didier Stricker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02225">
<title>Pose-Normalized Image Generation for Person Re-identification. (arXiv:1712.02225v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02225</link>
<description rdf:parseType="Literal">&lt;p&gt;Person Re-identification (re-id) faces two major challenges: the lack of
cross-view paired training data and learning discriminative identity-sensitive
and view-invariant features in the presence of large pose variations. In this
work, we address both problems by proposing a novel deep person image
generation model for synthesizing realistic person images conditional on the
pose. The model is based on a generative adversarial network (GAN) designed
specifically for pose normalization in re-id, thus termed pose-normalization
GAN (PN-GAN). With the synthesized images, we can learn a new type of deep
re-id feature free of the influence of pose variations. We show that this
feature is strong on its own and complementary to features learned with the
original images. Importantly, under the transfer learning setting, we show that
our model generalizes well to any new re-id dataset without the need for
collecting any training data for model fine-tuning. The model thus has the
potential to make re-id model truly scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xuelin Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jie Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07243">
<title>Personalizing Dialogue Agents: I have a dog, do you have pets too?. (arXiv:1801.07243v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07243</link>
<description rdf:parseType="Literal">&lt;p&gt;Chit-chat models are known to have several problems: they lack specificity,
do not display a consistent personality and are often not very captivating. In
this work we present the task of making chit-chat more engaging by conditioning
on profile information. We collect data and train models to (i) condition on
their given profile information; and (ii) information about the person they are
talking to, resulting in improved dialogues, as measured by next utterance
prediction. Since (ii) is initially unknown our model is trained to engage its
partner with personal topics, and we show the resulting dialogue can be used to
predict profile information about the interlocutors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Saizheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1&quot;&gt;Emily Dinan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urbanek_J/0/1/0/all/0/1&quot;&gt;Jack Urbanek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1&quot;&gt;Arthur Szlam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1&quot;&gt;Douwe Kiela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jason Weston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01433">
<title>Interactive Grounded Language Acquisition and Generalization in a 2D World. (arXiv:1802.01433v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01433</link>
<description rdf:parseType="Literal">&lt;p&gt;We build a virtual agent for learning language in a 2D maze-like world. The
agent sees images of the surrounding environment, listens to a virtual teacher,
and takes actions to receive rewards. It interactively learns the teacher&apos;s
language from scratch based on two language use cases: sentence-directed
navigation and question answering. It learns simultaneously the visual
representations of the world, the language, and the action control. By
disentangling language grounding from other computational routines and sharing
a concept detection function between language grounding and prediction, the
agent reliably interpolates and extrapolates to interpret sentences that
contain new word combinations or new words missing from training sentences. The
new words are transferred from the answers of language prediction. Such a
language ability is trained and evaluated on a population of over 1.6 million
distinct sentences consisting of 119 object words, 8 color words, 9
spatial-relation words, and 50 grammatical words. The proposed model
significantly outperforms five comparison methods for interpreting zero-shot
sentences. In addition, we demonstrate human-interpretable intermediate outputs
of the model in the appendix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haonan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haichao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.08852">
<title>Detecting and Explaining Causes From Text For a Time Series Event. (arXiv:1707.08852v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1707.08852</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining underlying causes or effects about events is a challenging but
valuable task. We define a novel problem of generating explanations of a time
series event by (1) searching cause and effect relationships of the time series
with textual data and (2) constructing a connecting chain between them to
generate an explanation. To detect causal features from text, we propose a
novel method based on the Granger causality of time series between features
extracted from text such as N-grams, topics, sentiments, and their composition.
The generation of the sequence of causal entities requires a commonsense
causative knowledge base with efficient reasoning. To ensure good
interpretability and appropriate lexical usage we combine symbolic and neural
representations, using a neural reasoning algorithm trained on commonsense
causal tuples to predict the next cause step. Our quantitative and human
analysis show empirical evidence that our method successfully extracts
meaningful causality relationships between time series with textual features
and generates appropriate explanation between them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dongyeop Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1&quot;&gt;Varun Gangal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1&quot;&gt;Ang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1&quot;&gt;Eduard Hovy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09401">
<title>Generative Temporal Models with Spatial Memory for Partially Observed Environments. (arXiv:1804.09401v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09401</link>
<description rdf:parseType="Literal">&lt;p&gt;In model-based reinforcement learning, generative and temporal models of
environments can be leveraged to boost agent performance, either by tuning the
agent&apos;s representations during training or via use as part of an explicit
planning mechanism. However, their application in practice has been limited to
simplistic environments, due to the difficulty of training such models in
larger, potentially partially-observed and 3D environments. In this work we
introduce a novel action-conditioned generative model of such challenging
environments. The model features a non-parametric spatial memory system in
which we store learned, disentangled representations of the environment.
Low-dimensional spatial updates are computed using a state-space model that
makes use of knowledge on the prior dynamics of the moving agent, and
high-dimensional visual observations are modelled with a Variational
Auto-Encoder. The result is a scalable architecture capable of performing
coherent predictions over hundreds of time steps across a range of partially
observed 2D and 3D environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fraccaro_M/0/1/0/all/0/1&quot;&gt;Marco Fraccaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rezende_D/0/1/0/all/0/1&quot;&gt;Danilo Jimenez Rezende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zwols_Y/0/1/0/all/0/1&quot;&gt;Yori Zwols&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pritzel_A/0/1/0/all/0/1&quot;&gt;Alexander Pritzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eslami_S/0/1/0/all/0/1&quot;&gt;S. M. Ali Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Viola_F/0/1/0/all/0/1&quot;&gt;Fabio Viola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09593">
<title>Speaker-independent raw waveform model for glottal excitation. (arXiv:1804.09593v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.09593</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent speech technology research has seen a growing interest in using
WaveNets as statistical vocoders, i.e., generating speech waveforms from
acoustic features. These models have been shown to improve the generated speech
quality over classical vocoders in many tasks, such as text-to-speech synthesis
and voice conversion. Furthermore, conditioning WaveNets with acoustic features
allows sharing the waveform generator model across multiple speakers without
additional speaker codes. However, multi-speaker WaveNet models require large
amounts of training data and computation to cover the entire acoustic space.
This paper proposes leveraging the source-filter model of speech production to
more effectively train a speaker-independent waveform generator with limited
resources. We present a multi-speaker &apos;GlotNet&apos; vocoder, which utilizes a
WaveNet to generate glottal excitation waveforms, which are then used to excite
the corresponding vocal tract filter to produce speech. Listening tests show
that the proposed model performs favourably to a direct WaveNet vocoder trained
with the same model architecture and data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juvela_L/0/1/0/all/0/1&quot;&gt;Lauri Juvela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsiaras_V/0/1/0/all/0/1&quot;&gt;Vassilis Tsiaras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1&quot;&gt;Bajibabu Bollepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Airaksinen_M/0/1/0/all/0/1&quot;&gt;Manu Airaksinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alku_P/0/1/0/all/0/1&quot;&gt;Paavo Alku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09604">
<title>A comparative study of feature selection methods for stress hotspot classification in materials. (arXiv:1804.09604v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09604</link>
<description rdf:parseType="Literal">&lt;p&gt;The first step in constructing a machine learning model is defining the
features of the data set that can be used for optimal learning. In this work we
discuss feature selection methods, which can be used to build better models, as
well as achieve model interpretability. We applied these methods in the context
of stress hotspot classification problem, to determine what microstructural
characteristics can cause stress to build up in certain grains during uniaxial
tensile deformation. The results show how some feature selection techniques are
biased and demonstrate a preferred technique to get feature rankings for
physical interpretations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mangal_A/0/1/0/all/0/1&quot;&gt;Ankita Mangal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holm_E/0/1/0/all/0/1&quot;&gt;Elizabeth A. Holm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09605">
<title>When is there a Representer Theorem? Nondifferentiable Regularisers and Banach spaces. (arXiv:1804.09605v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09605</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a general regularised interpolation problem for learning a
parameter vector from data. The well known representer theorem says that under
certain conditions on the regulariser there exists a solution in the linear
span of the data points. This is the core of kernel methods in machine learning
as it makes the problem computationally tractable. Necessary and sufficient
conditions for differentiable regularisers on Hilbert spaces to admit a
representer theorem have been proved. We extend those results to
nondifferentiable regularisers on uniformly convex and uniformly smooth Banach
spaces. This gives a (more) complete answer to the question when there is a
representer theorem. We then note that for regularised interpolation in fact
the solution is determined by the function space alone and independent of the
regulariser, making the extension to Banach spaces even more valuable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlegel_K/0/1/0/all/0/1&quot;&gt;Kevin Schlegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07839">
<title>Large Scale Automated Reading of Frontal and Lateral Chest X-Rays using Dual Convolutional Neural Networks. (arXiv:1804.07839v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07839</link>
<description rdf:parseType="Literal">&lt;p&gt;The MIMIC-CXR dataset is (to date) the largest released chest x-ray dataset
consisting of 473,064 chest x-rays and 206,574 radiology reports collected from
63,478 patients. We present the results of training and evaluating a collection
of deep convolutional neural networks on this dataset to recognize multiple
common thorax diseases. To the best of our knowledge, this is the first work
that trains CNNs for this task on such a large collection of chest x-ray
images, which is over four times the size of the largest previously released
chest x-ray corpus (ChestX-Ray14). We describe and evaluate individual CNN
models trained on frontal and lateral CXR view types. In addition, we present a
novel DualNet architecture that emulates routine clinical practice by
simultaneously processing both frontal and lateral CXR images obtained from a
radiological exam. Our DualNet architecture shows improved performance in
recognizing findings in CXR images when compared to applying separate baseline
frontal and lateral classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubin_J/0/1/0/all/0/1&quot;&gt;Jonathan Rubin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghavi_D/0/1/0/all/0/1&quot;&gt;Deepan Sanghavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Claire Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kathy Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadir_A/0/1/0/all/0/1&quot;&gt;Ashequl Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Wilson_M/0/1/0/all/0/1&quot;&gt;Minnan Xu-Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08420">
<title>Exploiting Partially Annotated Data for Temporal Relation Extraction. (arXiv:1804.08420v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08420</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotating temporal relations (TempRel) between events described in natural
language is known to be labor intensive, partly because the total number of
TempRels is quadratic in the number of events. As a result, only a small number
of documents are typically annotated, limiting the coverage of various
lexical/semantic phenomena. In order to improve existing approaches, one
possibility is to make use of the readily available, partially annotated data
(P as in partial) that cover more documents. However, missing annotations in P
are known to hurt, rather than help, existing systems. This work is a case
study in exploring various usages of P for TempRel extraction. Results show
that despite missing annotations, P is still a useful supervision signal for
this task within a constrained bootstrapping learning framework. The system
described in this system is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1&quot;&gt;Qiang Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhongzhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chuchu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;</dc:creator>
</item></rdf:RDF>