<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04872"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04718"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04915"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05106"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03989"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04798"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04910"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04931"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04941"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05009"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05138"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07938"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06853"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10321"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09069"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01466"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03972"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.04872">
<title>Unsupervised Adaptation with Interpretable Disentangled Representations for Distant Conversational Speech Recognition. (arXiv:1806.04872v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.04872</link>
<description rdf:parseType="Literal">&lt;p&gt;The current trend in automatic speech recognition is to leverage large
amounts of labeled data to train supervised neural network models.
Unfortunately, obtaining data for a wide range of domains to train robust
models can be costly. However, it is relatively inexpensive to collect large
amounts of unlabeled data from domains that we want the models to generalize
to. In this paper, we propose a novel unsupervised adaptation method that
learns to synthesize labeled data for the target domain from unlabeled
in-domain data and labeled out-of-domain data. We first learn without
supervision an interpretable latent representation of speech that encodes
linguistic and nuisance factors (e.g., speaker and channel) using different
latent variables. To transform a labeled out-of-domain utterance without
altering its transcript, we transform the latent nuisance variables while
maintaining the linguistic variables. To demonstrate our approach, we focus on
a channel mismatch setting, where the domain of interest is distant
conversational speech, and labels are only available for close-talking speech.
Our proposed method is evaluated on the AMI dataset, outperforming all
baselines and bridging the gap between unadapted and in-domain models by over
77% without using any parallel data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03751">
<title>State Space Representations of Deep Neural Networks. (arXiv:1806.03751v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03751</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deals with neural networks as dynamical systems governed by
differential or difference equations. It shows that the introduction of skip
connections into network architectures, such as residual networks and dense
networks, turns a system of static equations into a system of dynamical
equations with varying levels of smoothness on the layer-wise transformations.
Closed form solutions for the state space representations of general dense
networks, as well as $k^{th}$ order smooth networks, are found in general
settings. Furthermore, it is shown that imposing $k^{th}$ order smoothness on a
network architecture with $d$-many nodes per layer increases the state space
dimension by a multiple of $k$, and so the effective embedding dimension of the
data manifold is $k \cdot d$-many dimensions. It follows that network
architectures of these types reduce the number of parameters needed to maintain
the same embedding dimension by a factor of $k^2$ when compared to an
equivalent first-order, residual network, significantly motivating the
development of network architectures of these types. Numerical simulations were
run to validate parts of the developed theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauser_M/0/1/0/all/0/1&quot;&gt;Michael Hauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunn_S/0/1/0/all/0/1&quot;&gt;Sean Gunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saab_S/0/1/0/all/0/1&quot;&gt;Samer Saab Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Asok Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04718">
<title>Talakat: Bullet Hell Generation through Constrained Map-Elites. (arXiv:1806.04718v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.04718</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a search-based approach to generating new levels for bullet hell
games, which are action games characterized by and requiring avoidance of a
very large amount of projectiles. Levels are represented using a
domain-specific description language, and search in the space defined by this
language is performed by a novel variant of the Map-Elites algorithm which
incorporates a feasible- infeasible approach to constraint satisfaction.
Simulation-based evaluation is used to gauge the fitness of levels, using an
agent based on best-first search. The performance of the agent can be tuned
according to the two dimensions of strategy and dexterity, making it possible
to search for level configurations that require a specific combination of both.
As far as we know, this paper describes the first generator for this game
genre, and includes several algorithmic innovations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1&quot;&gt;Ahmed Khalifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Scott Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nealen_A/0/1/0/all/0/1&quot;&gt;Andy Nealen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julain Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04795">
<title>Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data. (arXiv:1806.04795v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.04795</link>
<description rdf:parseType="Literal">&lt;p&gt;With automobiles becoming increasingly reliant on sensors to perform various
driving tasks, it is important to encode the relevant CAN bus sensor data in a
way that captures the general state of the vehicle in a compact form. In this
paper, we develop a deep learning-based method, called Drive2Vec, for embedding
such sensor data in a low-dimensional yet actionable form. Our method is based
on stacked gated recurrent units (GRUs). It accepts a short interval of
automobile sensor data as input and computes a low-dimensional representation
of that data, which can then be used to accurately solve a range of tasks. With
this representation, we (1) predict the exact values of the sensors in the
short term (up to three seconds in the future), (2) forecast the long-term
average values of these same sensors, (3) infer additional contextual
information that is not encoded in the data, including the identity of the
driver behind the wheel, and (4) build a knowledge base that can be used to
auto-label data and identify risky states. We evaluate our approach on a
dataset collected by Audi, which equipped a fleet of test vehicles with data
loggers to store all sensor readings on 2,098 hours of driving on real roads.
We show in several experiments that our method outperforms other baselines by
up to 90%, and we further demonstrate how these embeddings of sensor data can
be used to solve a variety of real-world automotive applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallac_D/0/1/0/all/0/1&quot;&gt;David Hallac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhooshan_S/0/1/0/all/0/1&quot;&gt;Suvrat Bhooshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Michael Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abida_K/0/1/0/all/0/1&quot;&gt;Kacem Abida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sosic_R/0/1/0/all/0/1&quot;&gt;Rok Sosic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04808">
<title>Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection. (arXiv:1806.04808v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.04808</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning expressive low-dimensional representations of ultrahigh-dimensional
data, e.g., data with thousands/millions of features, has been a major way to
enable learning methods to address the curse of dimensionality. However,
existing unsupervised representation learning methods mainly focus on
preserving the data regularity information and learning the representations
independently of subsequent outlier detection methods, which can result in
suboptimal and unstable performance of detecting irregularities (i.e.,
outliers).
&lt;/p&gt;
&lt;p&gt;This paper introduces a ranking model-based framework, called RAMODO, to
address this issue. RAMODO unifies representation learning and outlier
detection to learn low-dimensional representations that are tailored for a
state-of-the-art outlier detection approach - the random distance-based
approach. This customized learning yields more optimal and stable
representations for the targeted outlier detectors. Additionally, RAMODO can
leverage little labeled data as prior knowledge to learn more expressive and
application-relevant representations. We instantiate RAMODO to an efficient
method called REPEN to demonstrate the performance of RAMODO.
&lt;/p&gt;
&lt;p&gt;Extensive empirical results on eight real-world ultrahigh dimensional data
sets show that REPEN (i) enables a random distance-based detector to obtain
significantly better AUC performance and two orders of magnitude speedup; (ii)
performs substantially better and more stably than four state-of-the-art
representation learning methods; and (iii) leverages less than 1% labeled data
to achieve up to 32% AUC improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Longbing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04899">
<title>Ensemble Pruning based on Objection Maximization with a General Distributed Framework. (arXiv:1806.04899v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.04899</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensemble pruning, selecting a subset of individual learners from an original
ensemble, alleviates the deficiencies of ensemble learning on the cost of time
and space. Accuracy and diversity serve as two crucial factors while they
usually conflict with each other. To balance both of them, we formalize the
ensemble pruning problem as an objection maximization problem based on
information entropy. Then we propose an ensemble pruning method including a
centralized version and a distributed version, in which the latter is to speed
up the former&apos;s execution. At last, we extract a general distributed framework
for ensemble pruning, which can be widely suitable for most of existing
ensemble pruning methods and achieve less time consuming without much accuracy
decline. Experimental results validate the efficiency of our framework and
methods, particularly with regard to a remarkable improvement of the execution
speed, accompanied by gratifying accuracy performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1&quot;&gt;Yijun Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yaqiang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huanhuan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04915">
<title>The IQ of Artificial Intelligence. (arXiv:1806.04915v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.04915</link>
<description rdf:parseType="Literal">&lt;p&gt;All it takes to identify the computer programs which are Artificial
Intelligence is to give them a test and award AI to those that pass the test.
Let us say that the scores they earn at the test will be called IQ. We cannot
pinpoint a minimum IQ threshold that a program has to cover in order to be AI,
however, we will choose a certain value. Thus, our definition for AI will be
any program the IQ of which is above the chosen value. While this idea has
already been implemented in [3], here we will revisit this construct in order
to introduce certain improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobrev_D/0/1/0/all/0/1&quot;&gt;Dimiter Dobrev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04952">
<title>Towards Semantically Enhanced Data Understanding. (arXiv:1806.04952v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1806.04952</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of machine learning, data understanding is the practice of
getting initial insights in unknown datasets. Such knowledge-intensive tasks
require a lot of documentation, which is necessary for data scientists to grasp
the meaning of the data. Usually, documentation is separate from the data in
various external documents, diagrams, spreadsheets and tools which causes
considerable look up overhead. Moreover, other supporting applications are not
able to consume and utilize such unstructured data. That is why we propose a
methodology that uses a single semantic model that interlinks data with its
documentation. Hence, data scientists are able to directly look up the
connected information about the data by simply following links. Equally, they
can browse the documentation which always refers to the data. Furthermore, the
model can be used by other approaches providing additional support, like
searching, comparing, integrating or visualizing data. To showcase our approach
we also demonstrate an early prototype.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Markus Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jilek_C/0/1/0/all/0/1&quot;&gt;Christian Jilek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn Hees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04968">
<title>Crowd-Powered Data Mining. (arXiv:1806.04968v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1806.04968</link>
<description rdf:parseType="Literal">&lt;p&gt;Many data mining tasks cannot be completely addressed by automated processes,
such as sentiment analysis and image classification. Crowdsourcing is an
effective way to harness the human cognitive ability to process these
machine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon
Mechanical Turk and CrowdFlower, we can easily involve hundreds of thousands of
ordi- nary workers (i.e., the crowd) to address these machine-hard tasks. In
this tutorial, we will survey and synthesize a wide spectrum of existing
studies on crowd-powered data mining. We rst give an overview of crowdsourcing,
and then summarize the fundamental techniques, including quality control, cost
control, and latency control, which must be considered in crowdsourced data
mining. Next we review crowd-powered data mining operations, including
classification, clustering, pattern mining, outlier detection, knowledge base
construction and enrichment. Finally, we provide the emerging challenges in
crowdsourced data mining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_C/0/1/0/all/0/1&quot;&gt;Chengliang Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Ju Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yudian Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05106">
<title>DRE-Bot: A Hierarchical First Person Shooter Bot Using Multiple Sarsa({\lambda}) Reinforcement Learners. (arXiv:1806.05106v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.05106</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes an architecture for controlling non-player characters
(NPC) in the First Person Shooter (FPS) game Unreal Tournament 2004.
Specifically, the DRE-Bot architecture is made up of three reinforcement
learners, Danger, Replenish and Explore, which use the tabular Sarsa({\lambda})
algorithm. This algorithm enables the NPC to learn through trial and error
building up experience over time in an approach inspired by human learning.
Experimentation is carried to measure the performance of DRE-Bot when competing
against fixed strategy bots that ship with the game. The discount parameter,
{\gamma}, and the trace parameter, {\lambda}, are also varied to see if their
values have an effect on the performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavin_F/0/1/0/all/0/1&quot;&gt;Frank G. Glavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madden_M/0/1/0/all/0/1&quot;&gt;Michael G. Madden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00144">
<title>Learning Longer-term Dependencies in RNNs with Auxiliary Losses. (arXiv:1803.00144v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00144</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent advances in training recurrent neural networks (RNNs),
capturing long-term dependencies in sequences remains a fundamental challenge.
Most approaches use backpropagation through time (BPTT), which is difficult to
scale to very long sequences. This paper proposes a simple method that improves
the ability to capture long term dependencies in RNNs by adding an unsupervised
auxiliary loss to the original objective. This auxiliary loss forces RNNs to
either reconstruct previous events or predict next events in a sequence, making
truncated backpropagation feasible for long sequences and also improving full
BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel
image classification with sequence lengths up to 16\,000, and a real document
classification benchmark. Our results highlight good performance and resource
efficiency of this approach over competitive baselines, including other
recurrent models and a comparable sized Transformer. Further analyses reveal
beneficial effects of the auxiliary loss on optimization and regularization, as
well as extreme cases where there is little to no backpropagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trinh_T/0/1/0/all/0/1&quot;&gt;Trieu H. Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1&quot;&gt;Minh-Thang Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09655">
<title>Global-Locally Self-Attentive Dialogue State Tracker. (arXiv:1805.09655v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09655</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialogue state tracking, which estimates user goals and requests given the
dialogue context, is an essential part of task-oriented dialogue systems. In
this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker
(GLAD), which learns representations of the user utterance and previous system
actions with global-local modules. Our model uses global modules to share
parameters between estimators for different types (called slots) of dialogue
states, and uses local modules to learn slot-specific features. We show that
this significantly improves tracking of rare states and achieves
state-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD
obtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ,
outperforming prior work by 3.7% and 5.5%. On DSTC2, our model obtains 74.5%
joint goal accuracy and 97.5% request accuracy, outperforming prior work by
1.1% and 1.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1&quot;&gt;Victor Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04387">
<title>Knowledge Amalgam: Generating Jokes and Quotes Together. (arXiv:1806.04387v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04387</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating humor and quotes are very challenging problems in the field of
computational linguistics and are often tackled separately. In this paper, we
present a controlled Long Short-Term Memory (LSTM) architecture which is
trained with categorical data like jokes and quotes together by passing
category as an input along with the sequence of words. The idea is that a
single neural net will learn the structure of both jokes and quotes to generate
them on demand according to input category. Importantly, we believe the neural
net has more knowledge as it&apos;s trained on different datasets and hence will
enable it to generate more creative jokes or quotes from the mixture of
information. May the network generate a funny inspirational joke!
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chippada_B/0/1/0/all/0/1&quot;&gt;Bhargav Chippada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Shubajit Saha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03989">
<title>Global Encoding for Abstractive Summarization. (arXiv:1805.03989v2 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.03989</link>
<description rdf:parseType="Literal">&lt;p&gt;In neural abstractive summarization, the conventional sequence-to-sequence
(seq2seq) model often suffers from repetition and semantic irrelevance. To
tackle the problem, we propose a global encoding framework, which controls the
information flow from the encoder to the decoder based on the global
information of the source context. It consists of a convolutional gated unit to
perform global encoding to improve the representations of the source-side
information. Evaluations on the LCSTS and the English Gigaword both demonstrate
that our model outperforms the baseline models, and the analysis shows that our
model is capable of reducing repetition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qi Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03692">
<title>Deconvolution-Based Global Decoding for Neural Machine Translation. (arXiv:1806.03692v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.03692</link>
<description rdf:parseType="Literal">&lt;p&gt;A great proportion of sequence-to-sequence (Seq2Seq) models for Neural
Machine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate
translation word by word following a sequential order. As the studies of
linguistics have proved that language is not linear word sequence but sequence
of complex structure, translation at each step should be conditioned on the
whole target-side context. To tackle the problem, we propose a new NMT model
that decodes the sequence with the guidance of its structural prediction of the
context of the target sequence. Our model generates translation based on the
structural prediction of the target-side context so that the translation can be
freed from the bind of sequential order. Experimental results demonstrate that
our model is more competitive compared with the state-of-the-art methods, and
the analysis reflects that our model is also robust to translating sentences of
different lengths and it also reduces repetition with the instruction from the
target-side context for decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jinsong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qi Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04702">
<title>Resource Allocation for a Wireless Coexistence Management System Based on Reinforcement Learning. (arXiv:1806.04702v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1806.04702</link>
<description rdf:parseType="Literal">&lt;p&gt;In industrial environments, an increasing amount of wireless devices are
used, which utilize license-free bands. As a consequence of these mutual
interferences of wireless systems might decrease the state of coexistence.
Therefore, a central coexistence management system is needed, which allocates
conflict-free resources to wireless systems. To ensure a conflict-free resource
utilization, it is useful to predict the prospective medium utilization before
resources are allocated. This paper presents a self-learning concept, which is
based on reinforcement learning. A simulative evaluation of reinforcement
learning agents based on neural networks, called deep Q-networks and double
deep Q-networks, was realized for exemplary and practically relevant
coexistence scenarios. The evaluation of the double deep Q-network showed that
a prediction accuracy of at least 98 % can be reached in all investigated
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soeffker_P/0/1/0/all/0/1&quot;&gt;Philip Soeffker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Block_D/0/1/0/all/0/1&quot;&gt;Dimitri Block&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiebusch_N/0/1/0/all/0/1&quot;&gt;Nico Wiebusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meier_U/0/1/0/all/0/1&quot;&gt;Uwe Meier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04798">
<title>Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning. (arXiv:1806.04798v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.04798</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning (AL) aims to enable training high performance classifiers
with low annotation cost by predicting which subset of unlabelled instances
would be most beneficial to label. The importance of AL has motivated extensive
research, proposing a wide variety of manually designed AL algorithms with
diverse theoretical and intuitive motivations. In contrast to this body of
research, we propose to treat active learning algorithm design as a
meta-learning problem and learn the best criterion from data. We model an
active learning algorithm as a deep neural network that inputs the base learner
state and the unlabelled point set and predicts the best point to annotate
next. Training this active query policy network with reinforcement learning,
produces the best non-myopic policy for a given dataset. The key challenge in
achieving a general solution to AL then becomes that of learner generalisation,
particularly across heterogeneous datasets. We propose a multi-task
dataset-embedding approach that allows dataset-agnostic active learners to be
trained. Our evaluation shows that AL algorithms trained in this way can
directly generalise across diverse problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_K/0/1/0/all/0/1&quot;&gt;Kunkun Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Mingzhi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04884">
<title>Weight Initialization without Local Minima in Deep Nonlinear Neural Networks. (arXiv:1806.04884v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.04884</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new weight initialization method called even
initialization for wide and deep nonlinear neural networks with the ReLU
activation function. We prove that no poor local minimum exists in the initial
loss landscape in the wide and deep nonlinear neural network initialized by the
even initialization method that we propose. Specifically, in the initial loss
landscape of such a wide and deep ReLU neural network model, the following four
statements hold true: 1) the loss function is non-convex and non-concave; 2)
every local minimum is a global minimum; 3) every critical point that is not a
global minimum is a saddle point; and 4) bad saddle points exist. We also show
that the weight values initialized by the even initialization method are
contained in those initialized by both of the (often used) standard
initialization and He initialization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nitta_T/0/1/0/all/0/1&quot;&gt;Tohru Nitta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04910">
<title>Bilevel Programming for Hyperparameter Optimization and Meta-Learning. (arXiv:1806.04910v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.04910</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a framework based on bilevel programming that unifies
gradient-based hyperparameter optimization and meta-learning. We show that an
approximate version of the bilevel problem can be solved by taking into
explicit account the optimization dynamics for the inner objective. Depending
on the specific setting, the outer variables take either the meaning of
hyperparameters in a supervised learning problem or parameters of a
meta-learner. We provide sufficient conditions under which solutions of the
approximate problem converge to those of the exact problem. We instantiate our
approach for meta-learning in the case of deep learning where representation
layers are treated as hyperparameters shared across a set of training episodes.
In experiments, we confirm our theoretical findings, present encouraging
results for few-shot learning and contrast the bilevel approach against
classical approaches for learning-to-learn.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franceschi_L/0/1/0/all/0/1&quot;&gt;Luca Franceschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frasconi_P/0/1/0/all/0/1&quot;&gt;Paolo Frasconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salzo_S/0/1/0/all/0/1&quot;&gt;Saverio Salzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimilano Pontil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04931">
<title>An image representation based convolutional network for DNA classification. (arXiv:1806.04931v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.04931</link>
<description rdf:parseType="Literal">&lt;p&gt;The folding structure of the DNA molecule combined with helper molecules,
also referred to as the chromatin, is highly relevant for the functional
properties of DNA. The chromatin structure is largely determined by the
underlying primary DNA sequence, though the interaction is not yet fully
understood. In this paper we develop a convolutional neural network that takes
an image-representation of primary DNA sequence as its input, and predicts key
determinants of chromatin structure. The method is developed such that it is
capable of detecting interactions between distal elements in the DNA sequence,
which are known to be highly relevant. Our experiments show that the method
outperforms several existing methods both in terms of prediction accuracy and
training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Bojian Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balvert_M/0/1/0/all/0/1&quot;&gt;Marleen Balvert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambrano_D/0/1/0/all/0/1&quot;&gt;Davide Zambrano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonhuth_A/0/1/0/all/0/1&quot;&gt;Alexander Sch&amp;#xf6;nhuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohte_S/0/1/0/all/0/1&quot;&gt;Sander Bohte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04941">
<title>Far-HO: A Bilevel Programming Package for Hyperparameter Optimization and Meta-Learning. (arXiv:1806.04941v1 [cs.MS])</title>
<link>http://arxiv.org/abs/1806.04941</link>
<description rdf:parseType="Literal">&lt;p&gt;In (Franceschi et al., 2018) we proposed a unified mathematical framework,
grounded on bilevel programming, that encompasses gradient-based hyperparameter
optimization and meta-learning. We formulated an approximate version of the
problem where the inner objective is solved iteratively, and gave sufficient
conditions ensuring convergence to the exact problem. In this work we show how
to optimize learning rates, automatically weight the loss of single examples
and learn hyper-representations with Far-HO, a software package based on the
popular deep learning framework TensorFlow that allows to seamlessly tackle
both HO and ML problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franceschi_L/0/1/0/all/0/1&quot;&gt;Luca Franceschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grazzi_R/0/1/0/all/0/1&quot;&gt;Riccardo Grazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimiliano Pontil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salzo_S/0/1/0/all/0/1&quot;&gt;Saverio Salzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frasconi_P/0/1/0/all/0/1&quot;&gt;Paolo Frasconi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04965">
<title>The streaming rollout of deep networks - towards fully model-parallel execution. (arXiv:1806.04965v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.04965</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks, and in particular recurrent networks, are promising
candidates to control autonomous agents that interact in real-time with the
physical world. However, this requires a seamless integration of temporal
features into the network&apos;s architecture. For the training of and inference
with recurrent neural networks, they are usually rolled out over time, and
different rollouts exist. Conventionally, during inference the layers of a
network are computed in a sequential manner resulting in sparse temporal
integration of information and long response times. In this study, we present a
theoretical framework to describe the set of all rollouts and demonstrate their
differences in solving specific tasks. We prove that certain rollouts, also
with only skip and no recurrent connections, enable earlier and more frequent
responses, and show empirically that these early responses have better
performance. The streaming rollout maximizes these properties and, in addition,
enables a fully parallel execution of the network reducing the runtime on
massively parallel devices. Additionally, we provide an open-source toolbox to
design, train, evaluate, and online-interact with streaming rollouts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fischer_V/0/1/0/all/0/1&quot;&gt;Volker Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jan K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pfeil_T/0/1/0/all/0/1&quot;&gt;Thomas Pfeil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05009">
<title>Tree Edit Distance Learning via Adaptive Symbol Embeddings. (arXiv:1806.05009v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.05009</link>
<description rdf:parseType="Literal">&lt;p&gt;Metric learning has the aim to improve classification accuracy by learning a
distance measure which brings data points from the same class closer together
and pushes data points from different classes further apart. Recent research
has demonstrated that metric learning approaches can also be applied to trees,
such as molecular structures, abstract syntax trees of computer programs, or
syntax trees of natural language, by learning the cost function of an edit
distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.
However, learning such costs directly may yield an edit distance which violates
metric axioms, is challenging to interpret, and may not generalize well.
&lt;/p&gt;
&lt;p&gt;In this contribution, we propose a novel metric learning approach for trees
which learns an edit distance indirectly by embedding the tree nodes as
vectors, such that the Euclidean distance between those vectors supports class
discrimination. We learn such embeddings by reducing the distance to
prototypical trees from the same class and increasing the distance to
prototypical trees from different classes. In our experiments, we show that our
proposed metric learning approach improves upon the state-of-the-art in metric
learning for trees on six benchmark data sets, ranging from computer science
over biomedical data to a natural-language processing data set containing over
300,000 nodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paassen_B/0/1/0/all/0/1&quot;&gt;Benjamin Paa&amp;#xdf;en&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1&quot;&gt;Claudio Gallicchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1&quot;&gt;Alessio Micheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1&quot;&gt;Barbara Hammer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05138">
<title>Generative Neural Machine Translation. (arXiv:1806.05138v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.05138</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Generative Neural Machine Translation (GNMT), a latent variable
architecture which is designed to model the semantics of the source and target
sentences. We modify an encoder-decoder translation model by adding a latent
variable as a language agnostic representation which is encouraged to learn the
meaning of the sentence. GNMT achieves competitive BLEU scores on pure
translation tasks, and is superior when there are missing words in the source
sentence. We augment the model to facilitate multilingual translation and
semi-supervised learning without adding parameters. This framework
significantly reduces overfitting when there is limited paired data available,
and is effective for translating between pairs of languages not seen during
training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1&quot;&gt;Harshil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1&quot;&gt;David Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05159">
<title>On Tighter Generalization Bound for Deep Neural Networks: CNNs, ResNets, and Beyond. (arXiv:1806.05159v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.05159</link>
<description rdf:parseType="Literal">&lt;p&gt;Our paper proposes a generalization error bound for a general family of deep
neural networks based on the spectral norm of weight matrices. Through
introducing a novel characterization of the Lipschitz properties of neural
network family, we achieve a tighter generalization error bound for ultra-deep
neural networks, whose depth is much larger than the square root of its width.
Besides the general deep neural networks, our results can be applied to derive
new bounds for several popular architectures, including convolutional neural
networks (CNNs), residual networks (ResNets), and hyperspherical networks
(SphereNets). In the regime that the depth of these architectures is
dominating, our bounds allow for the choice of much larger parameter spaces of
weight matrices, inducing potentially stronger expressive ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junwei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haupt_J/0/1/0/all/0/1&quot;&gt;Jarvis Haupt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05161">
<title>Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate. (arXiv:1806.05161v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.05161</link>
<description rdf:parseType="Literal">&lt;p&gt;Many modern machine learning models are trained to achieve zero or near-zero
training error in order to obtain near-optimal (but non-zero) test error. This
phenomenon of strong generalization performance for &quot;overfitted&quot; / interpolated
classifiers appears to be ubiquitous in high-dimensional data, having been
observed in deep networks, kernel machines, boosting and random forests. Their
performance is robust even when the data contain large amounts of label noise.
&lt;/p&gt;
&lt;p&gt;Very little theory is available to explain these observations. The vast
majority of theoretical analyses of generalization allows for interpolation
only when there is little or no label noise. This paper takes a step toward a
theoretical foundation for interpolated classifiers by analyzing local
interpolating schemes, including geometric simplicial interpolation algorithm
and weighted $k$-nearest neighbor schemes. Consistency or near-consistency is
proved for these schemes in classification and regression problems. These
schemes have an inductive bias that benefits from higher dimension, a kind of
&quot;blessing of dimensionality&quot;. Finally, connections to kernel machines, random
forests, and adversarial examples in the interpolated regime are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Belkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;Daniel Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Partha Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07938">
<title>Error Bounds for Piecewise Smooth and Switching Regression. (arXiv:1707.07938v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07938</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper deals with regression problems, in which the nonsmooth target is
assumed to switch between different operating modes. Specifically, piecewise
smooth (PWS) regression considers target functions switching deterministically
via a partition of the input space, while switching regression considers
arbitrary switching laws. The paper derives generalization error bounds in
these two settings by following the approach based on Rademacher complexities.
For PWS regression, our derivation involves a chaining argument and a
decomposition of the covering numbers of PWS classes in terms of the ones of
their component functions and the capacity of the classifier partitioning the
input space. This yields error bounds with a radical dependency on the number
of modes. For switching regression, the decomposition can be performed directly
at the level of the Rademacher complexities, which yields bounds with a linear
dependency on the number of modes. By using once more chaining and a
decomposition at the level of covering numbers, we show how to recover a
radical dependency. Examples of applications are given in particular for PWS
and swichting regression with linear and kernel-based component functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lauer_F/0/1/0/all/0/1&quot;&gt;Fabien Lauer&lt;/a&gt; (ABC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06853">
<title>Bandits with Delayed, Aggregated Anonymous Feedback. (arXiv:1709.06853v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06853</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a variant of the stochastic $K$-armed bandit problem, which we call
&quot;bandits with delayed, aggregated anonymous feedback&quot;. In this problem, when
the player pulls an arm, a reward is generated, however it is not immediately
observed. Instead, at the end of each round the player observes only the sum of
a number of previously generated rewards which happen to arrive in the given
round. The rewards are stochastically delayed and due to the aggregated nature
of the observations, the information of which arm led to a particular reward is
lost. The question is what is the cost of the information loss due to this
delayed, aggregated anonymous feedback? Previous works have studied bandits
with stochastic, non-anonymous delays and found that the regret increases only
by an additive factor relating to the expected delay. In this paper, we show
that this additive regret increase can be maintained in the harder delayed,
aggregated anonymous feedback setting when the expected delay (or a bound on
it) is known. We provide an algorithm that matches the worst case regret of the
non-anonymous problem exactly when the delays are bounded, and up to
logarithmic factors or an additive variance term for unbounded delays.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pike_Burke_C/0/1/0/all/0/1&quot;&gt;Ciara Pike-Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Shipra Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesvari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grunewalder_S/0/1/0/all/0/1&quot;&gt;Steffen Grunewalder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10321">
<title>Learning Structural Node Embeddings Via Diffusion Wavelets. (arXiv:1710.10321v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10321</link>
<description rdf:parseType="Literal">&lt;p&gt;Nodes residing in different parts of a graph can have similar structural
roles within their local network topology. The identification of such roles
provides key insight into the organization of networks and can be used for a
variety of machine learning tasks. However, learning structural representations
of nodes is a challenging problem, and it has typically involved manually
specifying and tailoring topological features for each node. In this paper, we
develop GraphWave, a method that represents each node&apos;s network neighborhood
via a low-dimensional embedding by leveraging heat wavelet diffusion patterns.
Instead of training on hand-selected features, GraphWave learns these
embeddings in an unsupervised way. We mathematically prove that nodes with
similar network neighborhoods will have similar GraphWave embeddings even
though these nodes may reside in very different parts of the network, and our
method scales linearly with the number of edges. Experiments in a variety of
different settings demonstrate GraphWave&apos;s real-world potential for capturing
structural roles in networks, and our approach outperforms existing
state-of-the-art baselines in every experiment, by as much as 137%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donnat_C/0/1/0/all/0/1&quot;&gt;Claire Donnat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallac_D/0/1/0/all/0/1&quot;&gt;David Hallac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09069">
<title>Active Learning with Logged Data. (arXiv:1802.09069v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09069</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider active learning with logged data, where labeled examples are
drawn conditioned on a predetermined logging policy, and the goal is to learn a
classifier on the entire population, not just conditioned on the logging
policy. Prior work addresses this problem either when only logged data is
available, or purely in a controlled random experimentation setting where the
logged data is ignored. In this work, we combine both approaches to provide an
algorithm that uses logged data to bootstrap and inform experimentation, thus
achieving the best of both worlds. Our work is inspired by a connection between
controlled random experimentation and active learning, and modifies existing
disagreement-based active learning algorithms to exploit logged data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Songbai Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1&quot;&gt;Kamalika Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1&quot;&gt;Tara Javidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01466">
<title>Informative Gene Selection for Microarray Classification via Adaptive Elastic Net with Conditional Mutual Information. (arXiv:1806.01466v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01466</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the advantage of achieving a better performance under weak
regularization, elastic net has attracted wide attention in statistics, machine
learning, bioinformatics, and other fields. In particular, a variation of the
elastic net, adaptive elastic net (AEN), integrates the adaptive grouping
effect. In this paper, we aim to develop a new algorithm: Adaptive Elastic Net
with Conditional Mutual Information (AEN-CMI) that further improves AEN by
incorporating conditional mutual information into the gene selection process.
We apply this new algorithm to screen significant genes for two kinds of
cancers: colon cancer and leukemia. Compared with other algorithms including
Support Vector Machine, Classic Elastic Net and Adaptive Elastic Net, the
proposed algorithm, AEN-CMI, obtains the best classification performance using
the least number of genes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin-Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yongjin Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03972">
<title>Multi-task Learning for Maritime Traffic Surveillance from AIS Data Streams. (arXiv:1806.03972v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03972</link>
<description rdf:parseType="Literal">&lt;p&gt;In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular time-sampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadaine_R/0/1/0/all/0/1&quot;&gt;Rodolphe Vadaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajduch_G/0/1/0/all/0/1&quot;&gt;Guillaume Hajduch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garello_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Garello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt;</dc:creator>
</item></rdf:RDF>