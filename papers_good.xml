<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07305"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07424"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07436"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07557"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.08566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10161"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1710.11622">
<title>Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. (arXiv:1710.11622v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11622</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to learn is a powerful paradigm for enabling models to learn from
data more effectively and efficiently. A popular approach to meta-learning is
to train a recurrent model to read in a training dataset as input and output
the parameters of a learned model, or output predictions for new test inputs.
Alternatively, a more recent approach to meta-learning aims to acquire deep
representations that can be effectively fine-tuned, via standard gradient
descent, to new tasks. In this paper, we consider the meta-learning problem
from the perspective of universality, formalizing the notion of learning
algorithm approximation and comparing the expressive power of the
aforementioned recurrent models to the more recent approaches that embed
gradient descent into the meta-learner. In particular, we seek to answer the
following question: does deep representation combined with standard gradient
descent have sufficient capacity to approximate any learning algorithm? We find
that this is indeed true, and further find, in our experiments, that
gradient-based meta-learning consistently leads to learning strategies that
generalize more widely compared to those represented by recurrent models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07296">
<title>Block-diagonal Hessian-free Optimization for Training Neural Networks. (arXiv:1712.07296v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.07296</link>
<description rdf:parseType="Literal">&lt;p&gt;Second-order methods for neural network optimization have several advantages
over methods based on first-order gradient descent, including better scaling to
large mini-batch sizes and fewer updates needed for convergence. But they are
rarely applied to deep learning in practice because of high computational cost
and the need for model-dependent algorithmic variations. We introduce a variant
of the Hessian-free method that leverages a block-diagonal approximation of the
generalized Gauss-Newton matrix. Our method computes the curvature
approximation matrix only for pairs of parameters from the same layer or block
of the neural network and performs conjugate gradient updates independently for
each block. Experiments on deep autoencoders, deep convolutional networks, and
multilayer LSTMs demonstrate better convergence and generalization compared to
the original Hessian-free approach and the Adam method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1&quot;&gt;James Bradbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07305">
<title>Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning. (arXiv:1712.07305v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07305</link>
<description rdf:parseType="Literal">&lt;p&gt;Many tasks in artificial intelligence require the collaboration of multiple
agents. We exam deep reinforcement learning for multi-agent domains. Recent
research efforts often take the form of two seemingly conflicting perspectives,
the decentralized perspective, where each agent is supposed to have its own
controller; and the centralized perspective, where one assumes there is a
larger model controlling all agents. In this regard, we revisit the idea of the
master-slave architecture by incorporating both perspectives within one
framework. Such a hierarchical structure naturally leverages advantages from
one another. The idea of combining both perspectives is intuitive and can be
well motivated from many real world systems, however, out of a variety of
possible realizations, we highlights three key ingredients, i.e. composed
action representation, learnable communication and independent reasoning. With
network designs to facilitate these explicitly, our proposal consistently
outperforms latest competing methods both in synthetic experiments and when
applied to challenging StarCraft micromanagement tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangyu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_B/0/1/0/all/0/1&quot;&gt;Bo Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fangchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07316">
<title>A Flexible Approach to Automated RNN Architecture Generation. (arXiv:1712.07316v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.07316</link>
<description rdf:parseType="Literal">&lt;p&gt;The process of designing neural architectures requires expert knowledge and
extensive trial and error. While automated architecture search may simplify
these requirements, the recurrent neural network (RNN) architectures generated
by existing methods are limited in both flexibility and components. We propose
a domain-specific language (DSL) for use in automated architecture search which
can produce novel RNNs of arbitrary depth and width. The DSL is flexible enough
to define standard architectures such as the Gated Recurrent Unit and Long
Short Term Memory and allows the introduction of non-standard RNN components
such as trigonometric curves and layer normalization. Using two different
candidate generation techniques, random search with a ranking function and
reinforcement learning, we explore the novel architectures produced by the RNN
DSL for language modeling and machine translation domains. The resulting
architectures do not follow human intuition yet perform well on their targeted
tasks, suggesting the space of usable RNN architectures is far larger than
previously assumed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrimpf_M/0/1/0/all/0/1&quot;&gt;Martin Schrimpf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merity_S/0/1/0/all/0/1&quot;&gt;Stephen Merity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1&quot;&gt;James Bradbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07203">
<title>Discovery of Shifting Patterns in Sequence Classification. (arXiv:1712.07203v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.07203</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the multi-variate sequence classification
problem from a multi-instance learning perspective. Real-world sequential data
commonly show discriminative patterns only at specific time periods. For
instance, we can identify a cropland during its growing season, but it looks
similar to a barren land after harvest or before planting. Besides, even within
the same class, the discriminative patterns can appear in different periods of
sequential data. Due to such property, these discriminative patterns are also
referred to as shifting patterns. The shifting patterns in sequential data
severely degrade the performance of traditional classification methods without
sufficient training data.
&lt;/p&gt;
&lt;p&gt;We propose a novel sequence classification method by automatically mining
shifting patterns from multi-variate sequence. The method employs a
multi-instance learning approach to detect shifting patterns while also
modeling temporal relationships within each multi-instance bag by an LSTM model
to further improve the classification performance. We extensively evaluate our
method on two real-world applications - cropland mapping and affective state
recognition. The experiments demonstrate the superiority of our proposed method
in sequence classification performance and in detecting discriminative shifting
patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1&quot;&gt;Ankush Khandelwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1&quot;&gt;Anuj Karpatne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vipin Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07240">
<title>Metadynamics for Training Neural Network Model Chemistries: a Competitive Assessment. (arXiv:1712.07240v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/1712.07240</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network (NN) model chemistries (MCs) promise to facilitate the
accurate exploration of chemical space and simulation of large reactive
systems. One important path to improving these models is to add layers of
physical detail, especially long-range forces. At short range, however, these
models are data driven and data limited. Little is systematically known about
how data should be sampled, and `test data&apos; chosen randomly from some sampling
techniques can provide poor information about generality. If the sampling
method is narrow `test error&apos; can appear encouragingly tiny while the model
fails catastrophically elsewhere. In this manuscript we competitively evaluate
two common sampling methods: molecular dynamics (MD), normal-mode sampling
(NMS) and one uncommon alternative, Metadynamics (MetaMD), for preparing
training geometries. We show that MD is an inefficient sampling method in the
sense that additional samples do not improve generality. We also show MetaMD is
easily implemented in any NNMC software package with cost that scales linearly
with the number of atoms in a sample molecule. MetaMD is a black-box way to
ensure samples always reach out to new regions of chemical space, while
remaining relevant to chemistry near $k_bT$. It is one cheap tool to address
the issue of generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Herr_J/0/1/0/all/0/1&quot;&gt;John E. Herr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yao_K/0/1/0/all/0/1&quot;&gt;Kun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+McIntyre_R/0/1/0/all/0/1&quot;&gt;Ryker McIntyre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Toth_D/0/1/0/all/0/1&quot;&gt;David Toth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Parkhill_J/0/1/0/all/0/1&quot;&gt;John Parkhill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07420">
<title>Finding Competitive Network Architectures Within a Day Using UCT. (arXiv:1712.07420v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.07420</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of neural network architectures for a new data set is a laborious
task which requires human deep learning expertise. In order to make deep
learning available for a broader audience, automated methods for finding a
neural network architecture are vital. Recently proposed methods can already
achieve human expert level performances. However, these methods have run times
of months or even years of GPU computing time, ignoring hardware constraints as
faced by many researchers and companies. We propose the use of Monte Carlo
planning in combination with two different UCT (upper confidence bound applied
to trees) derivations to search for network architectures. We adapt the UCT
algorithm to the needs of network architecture search by proposing two ways of
sharing information between different branches of the search tree. In an
empirical study we are able to demonstrate that this method is able to find
competitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day.
Extending the search time to five GPU days, we are able to outperform human
architectures and our competitors which consider the same types of layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1&quot;&gt;Martin Wistuba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07424">
<title>ADINE: An Adaptive Momentum Method for Stochastic Gradient Descent. (arXiv:1712.07424v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07424</link>
<description rdf:parseType="Literal">&lt;p&gt;Two major momentum-based techniques that have achieved tremendous success in
optimization are Polyak&apos;s heavy ball method and Nesterov&apos;s accelerated
gradient. A crucial step in all momentum-based methods is the choice of the
momentum parameter $m$ which is always suggested to be set to less than $1$.
Although the choice of $m &amp;lt; 1$ is justified only under very strong theoretical
assumptions, it works well in practice even when the assumptions do not
necessarily hold. In this paper, we propose a new momentum based method
$\textit{ADINE}$, which relaxes the constraint of $m &amp;lt; 1$ and allows the
learning algorithm to use adaptive higher momentum. We motivate our hypothesis
on $m$ by experimentally verifying that a higher momentum ($\ge 1$) can help
escape saddles much faster. Using this motivation, we propose our method
$\textit{ADINE}$ that helps weigh the previous updates more (by setting the
momentum parameter $&amp;gt; 1$), evaluate our proposed algorithm on deep neural
networks and show that $\textit{ADINE}$ helps the learning algorithm to
converge much faster without compromising on the generalization error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srinivasan_V/0/1/0/all/0/1&quot;&gt;Vishwak Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sankar_A/0/1/0/all/0/1&quot;&gt;Adepu Ravi Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07436">
<title>Incremental Adversarial Domain Adaptation. (arXiv:1712.07436v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07436</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous appearance shifts such as changes in weather and lighting
conditions can impact the performance of deployed machine learning models.
Unsupervised domain adaptation aims to address this challenge, though current
approaches do not utilise the continuity of the occurring shifts. Many robotic
applications exhibit these conditions and thus facilitate the potential to
incrementally adapt a learnt model over minor shifts which integrate to massive
differences over time. Our work presents an adversarial approach for lifelong,
incremental domain adaptation which benefits from unsupervised alignment to a
series of sub-domains which successively diverge from the labelled source
domain. We demonstrate on a drivable-path segmentation task that our
incremental approach can better handle large appearance changes, e.g. day to
night, compared with a prior single alignment step approach. Furthermore, by
approximating the marginal feature distribution for the source domain with a
generative adversarial network, the deployment module can be rendered fully
independent of retaining potentially large amounts of the related source
training data for only a minor reduction in performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wulfmeier_M/0/1/0/all/0/1&quot;&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bewley_A/0/1/0/all/0/1&quot;&gt;Alex Bewley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07557">
<title>Differentially Private Federated Learning: A Client Level Perspective. (arXiv:1712.07557v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1712.07557</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a recent advance in privacy protection. In this
context, a trusted curator aggregates parameters optimized in decentralized
fashion by multiple clients. The resulting model is then distributed back to
all clients, ultimately converging to a joint representative model without
explicitly having to share the data. However, the protocol is vulnerable to
differential attacks, which could originate from any party contributing during
federated optimization. In such an attack, a client&apos;s contribution during
training and information about their data set is revealed through analyzing the
distributed model. We tackle this problem and propose an algorithm for client
sided differential privacy preserving federated optimization. The aim is to
hide clients&apos; contributions during training, balancing the trade-off between
privacy loss and model performance. Empirical studies suggest that given a
sufficiently large number of participating clients, our proposed procedure can
maintain client-level differential privacy at only a minor cost in model
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_R/0/1/0/all/0/1&quot;&gt;Robin C. Geyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1&quot;&gt;Tassilo Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1&quot;&gt;Moin Nabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.08566">
<title>SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. (arXiv:1706.08566v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.08566</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has the potential to revolutionize quantum chemistry as it is
ideally suited to learn representations for structured data and speed up the
exploration of chemical space. While convolutional neural networks have proven
to be the first choice for images, audio and video data, the atoms in molecules
are not restricted to a grid. Instead, their precise locations contain
essential physical information, that would get lost if discretized. Thus, we
propose to use continuous-filter convolutional layers to be able to model local
correlations without requiring the data to lie on a grid. We apply those layers
in SchNet: a novel deep learning architecture modeling quantum interactions in
molecules. We obtain a joint model for the total energy and interatomic forces
that follows fundamental quantum-chemical principles. This includes
rotationally invariant energy predictions and a smooth, differentiable
potential energy surface. Our architecture achieves state-of-the-art
performance for benchmarks of equilibrium molecules and molecular dynamics
trajectories. Finally, we introduce a more challenging benchmark with chemical
and structural variations that suggests the path for further work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schutt_K/0/1/0/all/0/1&quot;&gt;Kristof T. Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kindermans_P/0/1/0/all/0/1&quot;&gt;Pieter-Jan Kindermans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sauceda_H/0/1/0/all/0/1&quot;&gt;Huziel E. Sauceda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chmiela_S/0/1/0/all/0/1&quot;&gt;Stefan Chmiela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tkatchenko_A/0/1/0/all/0/1&quot;&gt;Alexandre Tkatchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10161">
<title>Development and analysis of a Bayesian water balance model for large lake systems. (arXiv:1710.10161v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10161</link>
<description rdf:parseType="Literal">&lt;p&gt;Water balance models are often employed to improve understanding of drivers
of change in regional hydrologic cycles. Most of these models, however, are
physically-based, and few employ state-of-the-art statistical methods to
reconcile measurement uncertainty and bias. Here, we introduce a framework for
developing, analyzing, and selecting among alternative formulations of a
statistical water balance model for large lake systems that addresses this
research gap. We demonstrate our new analytical framework using a model
customized for Lakes Superior and Michigan-Huron, the two largest lakes on
Earth by surface area. The selected model (from among 26 alternatives) closed
the water balance across both lakes with an order of magnitude less computation
time than prototype versions of the same model. We expect our new framework
will be used to improve computational efficiency and skill of water balance
models for other lakes around the world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Joeseph P. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gronewold_A/0/1/0/all/0/1&quot;&gt;Andrew D. Gronewold&lt;/a&gt;</dc:creator>
</item></rdf:RDF>