<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05976"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05597"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05691"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05154"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05247"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05292"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05344"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05490"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05650"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05726"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05849"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05936"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05960"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10885"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05009"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04162"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.05976">
<title>Why don&apos;t the modules dominate - Investigating the Structure of a Well-Known Modularity-Inducing Problem Domain. (arXiv:1807.05976v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.05976</link>
<description rdf:parseType="Literal">&lt;p&gt;Wagner&apos;s modularity inducing problem domain is a key contribution to the
study of the evolution of modularity, including both evolutionary theory and
evolutionary computation. We study its behavior under classical genetic
algorithms. Unlike what we seem to observe in nature, the emergence of
modularity is highly conditional and dependent, for example, on the eagerness
of search. In nature, modular solutions generally dominate populations, whereas
in this domain, modularity, when it emerges, is a relatively rare variant.
Emergence of modularity depends heavily on random fluctuations in the fitness
function, with a randomly varied but unchanging fitness function, modularity
evolved far more rarely. Interestingly, high-fitness non-modular solutions
could frequently be converted into even-higher-fitness modular solutions by
manually removing all inter-module edges. Despite careful exploration, we do
not yet have a full explanation of why the genetic algorithm was unable to find
these better solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhenyue Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKay_R/0/1/0/all/0/1&quot;&gt;Robert McKay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1&quot;&gt;Tom Gedeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05597">
<title>Deep Learning for Semantic Segmentation on Minimal Hardware. (arXiv:1807.05597v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05597</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has revolutionised many fields, but it is still challenging to
transfer its success to small mobile robots with minimal hardware.
Specifically, some work has been done to this effect in the RoboCup humanoid
football domain, but results that are performant and efficient and still
generally applicable outside of this domain are lacking. We propose an approach
conceptually different from those taken previously. It is based on semantic
segmentation and does achieve these desired properties. In detail, it is being
able to process full VGA images in real-time on a low-power mobile processor.
It can further handle multiple image dimensions without retraining, it does not
require specific domain knowledge for achieving a high frame rate and it is
applicable on a minimal mobile hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dijk_S/0/1/0/all/0/1&quot;&gt;Sander G. van Dijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheunemann_M/0/1/0/all/0/1&quot;&gt;Marcus M. Scheunemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05691">
<title>Teaching machines to understand data science code by semantic enrichment of dataflow graphs. (arXiv:1807.05691v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.05691</link>
<description rdf:parseType="Literal">&lt;p&gt;Your computer is continuously executing programs, but does it really
understand them? Not in any meaningful sense. That burden falls upon human
knowledge workers, who are increasingly asked to write and understand code.
They would benefit greatly from intelligent tools that reveal the connections
between their code and its subject matter. Towards this prospect, we develop an
AI system that forms semantic representations of computer programs, using
techniques from knowledge representation and program analysis. We focus on code
written for data science, although our method is more generally applicable. The
semantic representations are created through a novel algorithm for the semantic
enrichment of dataflow graphs. This algorithm is undergirded by a new ontology
language for modeling computer programs and a new ontology about data science,
written in this language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patterson_E/0/1/0/all/0/1&quot;&gt;Evan Patterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1&quot;&gt;Ioana Baldini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mojsilovic_A/0/1/0/all/0/1&quot;&gt;Aleksandra Mojsilovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1&quot;&gt;Kush R. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05924">
<title>Bipedal Walking Robot using Deep Deterministic Policy Gradient. (arXiv:1807.05924v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.05924</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms have found several applications in the field of
robotics and control systems. The control systems community has started to show
interest towards several machine learning algorithms from the sub-domains such
as supervised learning, imitation learning and reinforcement learning to
achieve autonomous control and intelligent decision making. Amongst many
complex control problems, stable bipedal walking has been the most challenging
problem. In this paper, we present an architecture to design and simulate a
planar bipedal walking robot(BWR) using a realistic robotics simulator, Gazebo.
The robot demonstrates successful walking behaviour by learning through several
of its trial and errors, without any prior knowledge of itself or the world
dynamics. The autonomous walking of the BWR is achieved using reinforcement
learning algorithm called Deep Deterministic Policy Gradient(DDPG). DDPG is one
of the algorithms for learning controls in continuous action spaces. After
training the model in simulation, it was observed that, with a proper shaped
reward function, the robot achieved faster walking or even rendered a running
gait with an average speed of 0.83 m/s. The gait pattern of the bipedal walker
was compared with the actual human walking pattern. The results show that the
bipedal walking pattern had similar characteristics to that of a human walking
pattern.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Arun Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_N/0/1/0/all/0/1&quot;&gt;Navneet Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omkar_S/0/1/0/all/0/1&quot;&gt;S N Omkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05154">
<title>Deep Enhanced Representation for Implicit Discourse Relation Recognition. (arXiv:1807.05154v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.05154</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit discourse relation recognition is a challenging task as the relation
prediction without explicit connectives in discourse parsing needs
understanding of text spans and cannot be easily derived from surface features
from the input sentence pairs. Thus, properly representing the text is very
crucial to this task. In this paper, we propose a model augmented with
different grained text representations, including character, subword, word,
sentence, and sentence pair levels. The proposed deeper model is evaluated on
the benchmark treebank and achieves state-of-the-art accuracy with greater than
48% in 11-way and $F_1$ score greater than 50% in 4-way classifications for the
first time according to our best knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Hongxiao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hai Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05247">
<title>Channel Charting: Locating Users within the Radio Environment using Channel State Information. (arXiv:1807.05247v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1807.05247</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose channel charting (CC), a novel framework in which a multi-antenna
network element learns a chart of the radio geometry in its surrounding area.
The channel chart captures the local spatial geometry of the area so that
points that are close in space will also be close in the channel chart and vice
versa. CC works in a fully unsupervised manner, i.e., learning is only based on
channel state information (CSI) that is passively collected at a single point
in space, but from multiple transmit locations in the area over time. The
method then extracts channel features that characterize large-scale fading
properties of the wireless channel. Finally, the channel charts are generated
with tools from dimensionality reduction, manifold learning, and deep neural
networks. The network element performing CC may be, for example, a
multi-antenna base-station in a cellular system, and the charted area the
served cell. Logical relationships related to the position and movement of a
transmitter, e.g., a user equipment (UE), in the cell can then be directly
deduced from comparing measured radio channel characteristics to the channel
chart. The unsupervised nature of CC enables a range of new applications in UE
localization, network planning, user scheduling, multipoint connectivity,
hand-over, cell search, user grouping, and other cognitive tasks that rely on
CSI and UE movement relative to the base-station, without the need of
information from global navigation satellite systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Studer_C/0/1/0/all/0/1&quot;&gt;Christoph Studer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medjkouh_S/0/1/0/all/0/1&quot;&gt;Sa&amp;#xef;d Medjkouh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonultas_E/0/1/0/all/0/1&quot;&gt;Emre G&amp;#xf6;n&amp;#xfc;lta&amp;#x15f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirkkonen_O/0/1/0/all/0/1&quot;&gt;Olav Tirkkonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05292">
<title>Neural Networks Regularization Through Representation Learning. (arXiv:1807.05292v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05292</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network models and deep models are one of the leading and state of the
art models in machine learning. Most successful deep neural models are the ones
with many layers which highly increases their number of parameters. Training
such models requires a large number of training samples which is not always
available. One of the fundamental issues in neural networks is overfitting
which is the issue tackled in this thesis. Such problem often occurs when the
training of large models is performed using few training samples. Many
approaches have been proposed to prevent the network from overfitting and
improve its generalization performance such as data augmentation, early
stopping, parameters sharing, unsupervised learning, dropout, batch
normalization, etc.
&lt;/p&gt;
&lt;p&gt;In this thesis, we tackle the neural network overfitting issue from a
representation learning perspective by considering the situation where few
training samples are available which is the case of many real world
applications. We propose three contributions. The first one presented in
chapter 2 is dedicated to dealing with structured output problems to perform
multivariate regression when the output variable y contains structural
dependencies between its components. The second contribution described in
chapter 3 deals with the classification task where we propose to exploit prior
knowledge about the internal representation of the hidden layers in neural
networks. Our last contribution presented in chapter 4 showed the interest of
transfer learning in applications where only few samples are available. In this
contribution, we provide an automatic system based on such learning scheme with
an application to medical domain. In this application, the task consists in
localizing the third lumbar vertebra in a 3D CT scan. This work has been done
in collaboration with the clinic Rouen Henri Becquerel Center who provided us
with data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1&quot;&gt;Soufiane Belharbi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05306">
<title>Generative Adversarial Privacy. (arXiv:1807.05306v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05306</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a data-driven framework called generative adversarial privacy
(GAP). Inspired by recent advancements in generative adversarial networks
(GANs), GAP allows the data holder to learn the privatization mechanism
directly from the data. Under GAP, finding the optimal privacy mechanism is
formulated as a constrained minimax game between a privatizer and an adversary.
We show that for appropriately chosen adversarial loss functions, GAP provides
privacy guarantees against strong information-theoretic adversaries. We also
evaluate the performance of GAP on multi-dimensional Gaussian mixture models
and the GENKI face database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1&quot;&gt;Peter Kairouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1&quot;&gt;Lalitha Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1&quot;&gt;Ram Rajagopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05307">
<title>How Do Classifiers Induce Agents To Invest Effort Strategically?. (arXiv:1807.05307v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05307</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is often used to produce decision-making rules that classify
or evaluate individuals. When these individuals have incentives to be
classified a certain way, they may behave strategically to influence their
outcomes. We develop a model for how strategic agents can invest effort in
order to change the outcomes they receive, and we give a tight characterization
of when such agents can be incentivized to invest specified forms of effort
into improving their outcomes as opposed to &quot;gaming&quot; the classifier. We show
that whenever any &quot;reasonable&quot; mechanism can do so, a simple linear mechanism
suffices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_M/0/1/0/all/0/1&quot;&gt;Manish Raghavan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05317">
<title>LeFlow: Enabling Flexible FPGA High-Level Synthesis of Tensorflow Deep Neural Networks. (arXiv:1807.05317v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05317</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has shown that Field-Programmable Gate Arrays (FPGAs) play an
important role in the acceleration of Machine Learning applications. Initial
specification of machine learning applications are often done using a
high-level Python-oriented framework such as Tensorflow, followed by a manual
translation to either C or RTL for synthesis using vendor tools. This manual
translation step is time-consuming and requires expertise that limit the
applicability of FPGAs in this important domain. In this paper, we present an
open-source tool-flow that maps numerical computation models written in
Tensorflow to synthesizable hardware. Unlike other tools, which are often
constrained by a small number of inflexible templates, our flow uses Google&apos;s
XLA compiler which emits LLVM code directly from a Tensorflow specification.
This LLVM code can then be used with a high-level synthesis tool to
automatically generate hardware. We show that our flow allows users to generate
Deep Neural Networks with very few lines of Python code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noronha_D/0/1/0/all/0/1&quot;&gt;Daniel H. Noronha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehpour_B/0/1/0/all/0/1&quot;&gt;Bahar Salehpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilton_S/0/1/0/all/0/1&quot;&gt;Steven J.E. Wilton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05344">
<title>Adversarially Learned Mixture Model. (arXiv:1807.05344v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.05344</link>
<description rdf:parseType="Literal">&lt;p&gt;The Adversarially Learned Mixture Model (AMM) is a generative model for
unsupervised or semi-supervised data clustering. The AMM is the first
adversarially optimized method to model the conditional dependence between
inferred continuous and categorical latent variables. Experiments on the MNIST
and SVHN datasets show that the AMM allows for semantic separation of complex
data when little or no labeled data is available. The AMM achieves a
state-of-the-art unsupervised clustering error rate of 2.86% on the MNIST
dataset. A semi-supervised extension of the AMM yields competitive results on
the SVHN dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jesson_A/0/1/0/all/0/1&quot;&gt;Andrew Jesson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Low_Kam_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;cile Low-Kam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soudan_F/0/1/0/all/0/1&quot;&gt;Florian Soudan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chapados_N/0/1/0/all/0/1&quot;&gt;Nicolas Chapados&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05490">
<title>Semi-Supervised Feature Learning for Off-Line Writer Identifications. (arXiv:1807.05490v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05490</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional approaches used supervised learning to estimate off-line writer
identifications. In this study, we improved the off-line writer identifica-
tions by semi-supervised feature learning pipeline, which trained the extra
unla- beled data and the original labeled data simultaneously. In specific, we
proposed a weighted label smoothing regularization (WLSR) method, which
assigned the weighted uniform label distribution to the extra unlabeled data.
We regularized the convolutional neural network (CNN) baseline, which allows
learning more discriminative features to represent the properties of different
writing styles. Based on experiments on ICDAR2013, CVL and IAM benchmark
datasets, our results showed that semi-supervised feature learning improved the
baseline meas- urement and achieved better performance compared with existing
writer identifications approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zehong Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05650">
<title>Time Series Deinterleaving of DNS Traffic. (arXiv:1807.05650v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05650</link>
<description rdf:parseType="Literal">&lt;p&gt;Stream deinterleaving is an important problem with various applications in
the cybersecurity domain. In this paper, we consider the specific problem of
deinterleaving DNS data streams using machine-learning techniques, with the
objective of automating the extraction of malware domain sequences. We first
develop a generative model for user request generation and DNS stream
interleaving. Based on these we evaluate various inference strategies for
deinterleaving including augmented HMMs and LSTMs on synthetic datasets. Our
results demonstrate that state-of-the-art LSTMs outperform more traditional
augmented HMMs in this application domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asiaee_A/0/1/0/all/0/1&quot;&gt;Amir Asiaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_H/0/1/0/all/0/1&quot;&gt;Hardik Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shalini Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yegneswaran_V/0/1/0/all/0/1&quot;&gt;Vinod Yegneswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Arindam Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05726">
<title>Backward Reduction of CNN Models with Information Flow Analysis. (arXiv:1807.05726v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05726</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes backward reduction, an algorithm that explores the
compact CNN design from the information flow perspective. This algorithm can
remove substantial non-zero weighting parameters (redundant neural channels) by
considering the network dynamic behavior, which the traditional model
compaction techniques cannot achieve, to reduce the size of a model. With the
aid of our proposed algorithm, we achieve significant model reduction results
of ResNet-34 in ImageNet scale (32.3% reduction), which is 3X better than the
state-of-the-art result (10.8%). Even for highly optimized models like
SqueezeNet and MobileNet, we still achieve additional 10.81% and 37.56%
reduction, respectively, with negligible performance degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1&quot;&gt;Chun-Nan Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Edward Y. Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05748">
<title>Learning Stochastic Differential Equations With Gaussian Processes Without Gradient Matching. (arXiv:1807.05748v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.05748</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel paradigm for learning non-parametric drift and diffusion
functions for stochastic differential equation (SDE) that are learnt to
simulate trajectory distributions that match observations of arbitrary
spacings. This is in contrast to existing gradient matching or other
approximations that do not optimize simulated responses. We demonstrate that
our general stochastic distribution optimisation leads to robust and efficient
learning of SDE systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yildiz_C/0/1/0/all/0/1&quot;&gt;Cagatay Yildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Intosalmi_J/0/1/0/all/0/1&quot;&gt;Jukka Intosalmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mannerstrom_H/0/1/0/all/0/1&quot;&gt;Henrik Mannerstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lahdesmaki_H/0/1/0/all/0/1&quot;&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05849">
<title>Neural Chinese Word Segmentation with Dictionary Knowledge. (arXiv:1807.05849v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.05849</link>
<description rdf:parseType="Literal">&lt;p&gt;Chinese word segmentation (CWS) is an important task for Chinese NLP.
Recently, many neural network based methods have been proposed for CWS.
However, these methods require a large number of labeled sentences for model
training, and usually cannot utilize the useful information in Chinese
dictionary. In this paper, we propose two methods to exploit the dictionary
information for CWS. The first one is based on pseudo labeled data generation,
and the second one is based on multi-task learning. The experimental results on
two benchmark datasets validate that our approach can effectively improve the
performance of Chinese word segmentation, especially when training data is
insufficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chuhan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05852">
<title>Machine Learning with Membership Privacy using Adversarial Regularization. (arXiv:1807.05852v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.05852</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models leak information about the datasets on which they are
trained. An adversary can build an algorithm to trace the individual members of
a model&apos;s training dataset. As a fundamental inference attack, he aims to
distinguish between data points that were part of the model&apos;s training set and
any other data points from the same distribution. This is known as the tracing
(and also membership inference) attack. In this paper, we focus on such attacks
against black-box models, where the adversary can only observe the output of
the model, but not its parameters. This is the current setting of machine
learning as a service in the Internet.
&lt;/p&gt;
&lt;p&gt;We introduce a privacy mechanism to train machine learning models that
provably achieve membership privacy: the model&apos;s predictions on its training
data are indistinguishable from its predictions on other data points from the
same distribution. We design a strategic mechanism where the privacy mechanism
anticipates the membership inference attacks. The objective is to train a model
such that not only does it have the minimum prediction error (high utility),
but also it is the most robust model against its corresponding strongest
inference attack (high privacy). We formalize this as a min-max game
optimization problem, and design an adversarial training algorithm that
minimizes the classification loss of the model as well as the maximum gain of
the membership inference attack against it. This strategy, which guarantees
membership privacy (as prediction indistinguishability), acts also as a strong
regularizer and significantly generalizes the model.
&lt;/p&gt;
&lt;p&gt;We evaluate our privacy mechanism on deep neural networks using different
benchmark datasets. We show that our min-max strategy can mitigate the risk of
membership inference attacks (close to the random guess) with a negligible cost
in terms of the classification error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nasr_M/0/1/0/all/0/1&quot;&gt;Milad Nasr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shokri_R/0/1/0/all/0/1&quot;&gt;Reza Shokri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Houmansadr_A/0/1/0/all/0/1&quot;&gt;Amir Houmansadr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05887">
<title>Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees. (arXiv:1807.05887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05887</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Reinforcement Learning (DRL) has achieved impressive success in many
applications. A key component of many DRL models is a neural network
representing a Q function, to estimate the expected cumulative reward following
a state-action pair. The Q function neural network contains a lot of implicit
knowledge about the RL problems, but often remains unexamined and
uninterpreted. To our knowledge, this work develops the first mimic learning
framework for Q functions in DRL. We introduce Linear Model U-trees (LMUTs) to
approximate neural network predictions. An LMUT is learned using a novel
on-line algorithm that is well-suited for an active play setting, where the
mimic learner observes an ongoing interaction between the neural net and the
environment. Empirical evaluation shows that an LMUT mimics a Q function
substantially better than five baseline methods. The transparent tree structure
of an LMUT facilitates understanding the network&apos;s learned knowledge by
analyzing feature influence, extracting rules, and highlighting the
super-pixels in image inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guiliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1&quot;&gt;Oliver Schulte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingcan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05936">
<title>Variational Inference: A Unified Framework of Generative Models and Some Revelations. (arXiv:1807.05936v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05936</link>
<description rdf:parseType="Literal">&lt;p&gt;We reinterpreting the variational inference in a new perspective. Via this
way, we can easily prove that EM algorithm, VAE, GAN, AAE, ALI(BiGAN) are all
special cases of variational inference. The proof also reveals the loss of
standard GAN is incomplete and it explains why we need to train GAN cautiously.
From that, we find out a regularization term to improve stability of GAN
training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jianlin Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05960">
<title>Meta-Learning with Latent Embedding Optimization. (arXiv:1807.05960v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.05960</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient-based meta-learning techniques are both widely applicable and
proficient at solving challenging few-shot learning and fast adaptation
problems. However, they have the practical difficulties of operating in
high-dimensional parameter spaces in extreme low-data regimes. We show that it
is possible to bypass these limitations by learning a low-dimensional latent
generative representation of model parameters and performing gradient-based
meta-learning in this space with latent embedding optimization (LEO),
effectively decoupling the gradient-based adaptation procedure from the
underlying high-dimensional space of model parameters. Our evaluation shows
that LEO can achieve state-of-the-art performance on the competitive 5-way
1-shot miniImageNet classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1&quot;&gt;Andrei A. Rusu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1&quot;&gt;Dushyant Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1&quot;&gt;Jakub Sygnowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1&quot;&gt;Simon Osindero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1&quot;&gt;Raia Hadsell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05979">
<title>Lesion Analysis and Diagnosis with Mask-RCNN. (arXiv:1807.05979v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1807.05979</link>
<description rdf:parseType="Literal">&lt;p&gt;This project applies Mask R-CNN method to ISIC 2018 challenge tasks: lesion
boundary segmentation (task1), lesion attributes detection (task 2), lesion
diagnosis (task 3), a solution to the latter is using a trained model for task
1 and a simple voting procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sorokin_A/0/1/0/all/0/1&quot;&gt;Andrey Sorokin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10885">
<title>Dense Adaptive Cascade Forest: A Self Adaptive Deep Ensemble for Classification Problems. (arXiv:1804.10885v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10885</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has shown that deep ensemble for forest can achieve a huge
increase in classification accuracy compared with the general ensemble learning
method. Especially when there are only few training data. In this paper, we
decide to take full advantage of this observation and introduce the Dense
Adaptive Cascade Forest (daForest), which has better performance than the
original one named Cascade Forest. And it is particularly noteworthy that
daForest has a powerful ability to handle high-dimensional sparse data without
any preprocessing on raw data like PCA or any other dimensional reduction
methods. Our model is distinguished by three major features: the first feature
is the combination of the SAMME.R boosting algorithm in the model, boosting
gives the model the ability to continuously improve as the number of layer
increases, which is not possible in stacking model or plain cascade forest. The
second feature is our model connects each layer to its subsequent layers in a
feed-forward fashion, to some extent this structure enhances the ability of the
model to resist degeneration. When number of layers goes up, accuracy of model
goes up a little in the first few layers then drop down quickly, we call this
phenomenon degeneration in training stacking model. The third feature is that
we add a hyper-parameter optimization layer before the first classification
layer in the proposed deep model, which can search for the optimal
hyper-parameter and set up the model in a brief period and nearly halve the
training time without having too much impact on the final performance.
Experimental results show that daForest performs particularly well on both
high-dimensional low-order features and low-dimensional high-order features,
and in some cases, even better than neural networks and achieves
state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05009">
<title>Tree Edit Distance Learning via Adaptive Symbol Embeddings. (arXiv:1806.05009v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05009</link>
<description rdf:parseType="Literal">&lt;p&gt;Metric learning has the aim to improve classification accuracy by learning a
distance measure which brings data points from the same class closer together
and pushes data points from different classes further apart. Recent research
has demonstrated that metric learning approaches can also be applied to trees,
such as molecular structures, abstract syntax trees of computer programs, or
syntax trees of natural language, by learning the cost function of an edit
distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.
However, learning such costs directly may yield an edit distance which violates
metric axioms, is challenging to interpret, and may not generalize well. In
this contribution, we propose a novel metric learning approach for trees which
we call embedding edit distance learning (BEDL) and which learns an edit
distance indirectly by embedding the tree nodes as vectors, such that the
Euclidean distance between those vectors supports class discrimination. We
learn such embeddings by reducing the distance to prototypical trees from the
same class and increasing the distance to prototypical trees from different
classes. In our experiments, we show that BEDL improves upon the
state-of-the-art in metric learning for trees on six benchmark data sets,
ranging from computer science over biomedical data to a natural-language
processing data set containing over 300,000 nodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paassen_B/0/1/0/all/0/1&quot;&gt;Benjamin Paa&amp;#xdf;en&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1&quot;&gt;Claudio Gallicchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1&quot;&gt;Alessio Micheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1&quot;&gt;Barbara Hammer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04162">
<title>TherML: Thermodynamics of Machine Learning. (arXiv:1807.04162v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we offer a framework for reasoning about a wide class of
existing objectives in machine learning. We develop a formal correspondence
between this work and thermodynamics and discuss its implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1&quot;&gt;Alexander A. Alemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1&quot;&gt;Ian Fischer&lt;/a&gt;</dc:creator>
</item></rdf:RDF>