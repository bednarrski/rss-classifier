<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10179"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10321"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.04080"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.10158">
<title>Non-linear motor control by local learning in spiking neural networks. (arXiv:1712.10158v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1712.10158</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning weights in a spiking neural network with hidden neurons, using
local, stable and online rules, to control non-linear body dynamics is an open
problem. Here, we employ a supervised scheme, Feedback-based Online Local
Learning Of Weights (FOLLOW), to train a network of heterogeneous spiking
neurons with hidden layers, to control a two-link arm so as to reproduce a
desired state trajectory. The network first learns an inverse model of the
non-linear dynamics, i.e. from state trajectory as input to the network, it
learns to infer the continuous-time command that produced the trajectory.
Connection weights are adjusted via a local plasticity rule that involves
pre-synaptic firing and post-synaptic feedback of the error in the inferred
command. We choose a network architecture, termed differential feedforward,
that gives the lowest test error from different feedforward and recurrent
architectures. The learned inverse model is then used to generate a
continuous-time motor command to control the arm, given a desired trajectory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gilra_A/0/1/0/all/0/1&quot;&gt;Aditya Gilra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gerstner_W/0/1/0/all/0/1&quot;&gt;Wulfram Gerstner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10050">
<title>Kernel Robust Bias-Aware Prediction under Covariate Shift. (arXiv:1712.10050v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.10050</link>
<description rdf:parseType="Literal">&lt;p&gt;Under covariate shift, training (source) data and testing (target) data
differ in input space distribution, but share the same conditional label
distribution. This poses a challenging machine learning task. Robust Bias-Aware
(RBA) prediction provides the conditional label distribution that is robust to
the worstcase logarithmic loss for the target distribution while matching
feature expectation constraints from the source distribution. However,
employing RBA with insufficient feature constraints may result in high
certainty predictions for much of the source data, while leaving too much
uncertainty for target data predictions. To overcome this issue, we extend the
representer theorem to the RBA setting, enabling minimization of regularized
expected target risk by a reweighted kernel expectation under the source
distribution. By applying kernel methods, we establish consistency guarantees
and demonstrate better performance of the RBA classifier than competing methods
on synthetically biased UCI datasets as well as datasets that have natural
covariate shift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fathony_R/0/1/0/all/0/1&quot;&gt;Rizal Fathony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziebart_B/0/1/0/all/0/1&quot;&gt;Brian D. Ziebart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10179">
<title>RedDwarfData: a simplified dataset of StarCraft matches. (arXiv:1712.10179v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.10179</link>
<description rdf:parseType="Literal">&lt;p&gt;The game Starcraft is one of the most interesting arenas to test new machine
learning and computational intelligence techniques; however, StarCraft matches
take a long time and creating a good dataset for training can be hard. Besides,
analyzing match logs to extract the main characteristics can also be done in
many different ways to the point that extracting and processing data itself can
take an inordinate amount of time and of course, depending on what you choose,
can bias learning algorithms. In this paper we present a simplified dataset
extracted from the set of matches published by Robinson and Watson, which we
have called RedDwarfData, containing several thousand matches processed to
frames, so that temporal studies can also be undertaken. This dataset is
available from GitHub under a free license. An initial analysis and appraisal
of these matches is also made.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merelo_Guervos_J/0/1/0/all/0/1&quot;&gt;Juan J. Merelo-Guerv&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Ares_A/0/1/0/all/0/1&quot;&gt;Antonio Fern&amp;#xe1;ndez-Ares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caballero_A/0/1/0/all/0/1&quot;&gt;Antonio &amp;#xc1;lvarez Caballero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Sanchez_P/0/1/0/all/0/1&quot;&gt;Pablo Garc&amp;#xed;a-S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivas_V/0/1/0/all/0/1&quot;&gt;Victor Rivas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07170">
<title>Parameter Reference Loss for Unsupervised Domain Adaptation. (arXiv:1711.07170v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1711.07170</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of deep learning in computer vision is mainly attributed to an
abundance of data. However, collecting large-scale data is not always possible,
especially for the supervised labels. Unsupervised domain adaptation (UDA) aims
to utilize labeled data from a source domain to learn a model that generalizes
to a target domain of unlabeled data. A large amount of existing work uses
Siamese network-based models, where two streams of neural networks process the
source and the target domain data respectively. Nevertheless, most of these
approaches focus on minimizing the domain discrepancy, overlooking the
importance of preserving the discriminative ability for target domain features.
Another important problem in UDA research is how to evaluate the methods
properly. Common evaluation procedures require target domain labels for
hyper-parameter tuning and model selection, contradicting the definition of the
UDA task. Hence we propose a more reasonable evaluation principle that avoids
this contradiction by simply adopting the latest snapshot of a model for
evaluation. This adds an extra requirement for UDA methods besides the main
performance criteria: the stability during training. We design a novel method
that connects the target domain stream to the source domain stream with a
Parameter Reference Loss (PRL) to solve these problems simultaneously.
Experiments on various datasets show that the proposed PRL not only improves
the performance on the target domain, but also stabilizes the training
procedure. As a result, PRL based models do not need the contradictory model
selection, and thus are more suitable for practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jiren Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calland_R/0/1/0/all/0/1&quot;&gt;Richard G. Calland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miyato_T/0/1/0/all/0/1&quot;&gt;Takeru Miyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogel_B/0/1/0/all/0/1&quot;&gt;Brian K. Vogel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1&quot;&gt;Hideki Nakayama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10082">
<title>Application of Convolutional Neural Network to Predict Airfoil Lift Coefficient. (arXiv:1712.10082v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.10082</link>
<description rdf:parseType="Literal">&lt;p&gt;The adaptability of the convolutional neural network (CNN) technique for
aerodynamic meta-modeling tasks is probed in this work. The primary objective
is to develop suitable CNN architecture for variable flow conditions and object
geometry, in addition to identifying a sufficient data preparation process.
Multiple CNN structures were trained to learn the lift coefficients of the
airfoils with a variety of shapes in multiple flow Mach numbers, Reynolds
numbers, and diverse angles of attack. This is conducted to illustrate the
concept of the technique. A multi-layered perceptron (MLP) is also used for the
training sets. The MLP results are compared with that of the CNN results. The
newly proposed meta-modeling concept has been found to be comparable with the
MLP in learning capability; and more importantly, our CNN model exhibits a
competitive prediction accuracy with minimal constraints in a geometric
representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sung_W/0/1/0/all/0/1&quot;&gt;Woong-Je Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mavris_D/0/1/0/all/0/1&quot;&gt;Dimitri Mavris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10107">
<title>Objective evaluation metrics for automatic classification of EEG events. (arXiv:1712.10107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.10107</link>
<description rdf:parseType="Literal">&lt;p&gt;The evaluation of machine learning algorithms in biomedical fields for
applications involving sequential data lacks standardization. Common
quantitative scalar evaluation metrics such as sensitivity and specificity can
often be misleading depending on the requirements of the application.
Evaluation metrics must ultimately reflect the needs of users yet be
sufficiently sensitive to guide algorithm development. Feedback from critical
care clinicians who use automated event detection software in clinical
applications has been overwhelmingly emphatic that a low false alarm rate,
typically measured in units of the number of errors per 24 hours, is the single
most important criterion for user acceptance. Though using a single metric is
not often as insightful as examining performance over a range of operating
conditions, there is a need for a single scalar figure of merit. In this paper,
we discuss the deficiencies of existing metrics for a seizure detection task
and propose several new metrics that offer a more balanced view of performance.
We demonstrate these metrics on a seizure detection task based on the TUH EEG
Corpus. We show that two promising metrics are a measure based on a concept
borrowed from the spoken term detection literature, Actual Term-Weighted Value,
and a new metric, Time-Aligned Event Scoring (TAES), that accounts for the
temporal alignment of the hypothesis to the reference annotation. We also
demonstrate that state of the art technology based on deep learning, though
impressive in its performance, still needs significant improvement before it
will meet very strict user acceptance guidelines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziyabari_S/0/1/0/all/0/1&quot;&gt;Saeedeh Ziyabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Vinit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golmohammadi_M/0/1/0/all/0/1&quot;&gt;Meysam Golmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obeid_I/0/1/0/all/0/1&quot;&gt;Iyad Obeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picone_J/0/1/0/all/0/1&quot;&gt;Joseph Picone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10321">
<title>CaloGAN: Simulating 3D High Energy Particle Showers in Multi-Layer Electromagnetic Calorimeters with Generative Adversarial Networks. (arXiv:1712.10321v1 [hep-ex])</title>
<link>http://arxiv.org/abs/1712.10321</link>
<description rdf:parseType="Literal">&lt;p&gt;The precise modeling of subatomic particle interactions and propagation
through matter is paramount for the advancement of nuclear and particle physics
searches and precision measurements. The most computationally expensive step in
the simulation pipeline of a typical experiment at the Large Hadron Collider
(LHC) is the detailed modeling of the full complexity of physics processes that
govern the motion and evolution of particle showers inside calorimeters. We
introduce \textsc{CaloGAN}, a new fast simulation technique based on generative
adversarial networks (GANs). We apply these neural networks to the modeling of
electromagnetic showers in a longitudinally segmented calorimeter, and achieve
speedup factors comparable to or better than existing full simulation
techniques on CPU ($100\times$-$1000\times$) and even faster on GPU (up to
$\sim10^5\times$). There are still challenges for achieving precision across
the entire phase space, but our solution can reproduce a variety of geometric
shower shape properties of photons, positrons and charged pions. This
represents a significant stepping stone toward a full neural network-based
detector simulation that could save significant computing time and enable many
analyses now and in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Paganini_M/0/1/0/all/0/1&quot;&gt;Michela Paganini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Oliveira_L/0/1/0/all/0/1&quot;&gt;Luke de Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Nachman_B/0/1/0/all/0/1&quot;&gt;Benjamin Nachman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.04080">
<title>Matching Networks for One Shot Learning. (arXiv:1606.04080v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1606.04080</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from a few examples remains a key challenge in machine learning.
Despite recent advances in important domains such as vision and language, the
standard supervised deep learning paradigm does not offer a satisfactory
solution for learning new concepts rapidly from little data. In this work, we
employ ideas from metric learning based on deep neural features and from recent
advances that augment neural networks with external memories. Our framework
learns a network that maps a small labelled support set and an unlabelled
example to its label, obviating the need for fine-tuning to adapt to new class
types. We then define one-shot learning problems on vision (using Omniglot,
ImageNet) and language tasks. Our algorithm improves one-shot accuracy on
ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to
competing approaches. We also demonstrate the usefulness of the same model on
language modeling by introducing a one-shot task on the Penn Treebank.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1&quot;&gt;Timothy Lillicrap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kavukcuoglu_K/0/1/0/all/0/1&quot;&gt;Koray Kavukcuoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierstra_D/0/1/0/all/0/1&quot;&gt;Daan Wierstra&lt;/a&gt;</dc:creator>
</item></rdf:RDF>