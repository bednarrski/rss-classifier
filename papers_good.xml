<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02738"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04263"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.04582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04676"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03880"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04042"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04223"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04025"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11278"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00828"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.03688">
<title>Bit-Tactical: Exploiting Ineffectual Computations in Convolutional Neural Networks: Which, Why, and How. (arXiv:1803.03688v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.03688</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that, during inference with Convolutional Neural Networks (CNNs),
more than 2x to $8x ineffectual work can be exposed if instead of targeting
those weights and activations that are zero, we target different combinations
of value stream properties. We demonstrate a practical application with
Bit-Tactical (TCL), a hardware accelerator which exploits weight sparsity, per
layer precision variability and dynamic fine-grain precision reduction for
activations, and optionally the naturally occurring sparse effectual bit
content of activations to improve performance and energy efficiency. TCL
benefits both sparse and dense CNNs, natively supports both convolutional and
fully-connected layers, and exploits properties of all activations to reduce
storage, communication, and computation demands. While TCL does not require
changes to the CNN to deliver benefits, it does reward any technique that would
amplify any of the aforementioned weight and activation value properties.
Compared to an equivalent data-parallel accelerator for dense CNNs, TCLp, a
variant of TCL improves performance by 5.05x and is 2.98x more energy efficient
while requiring 22% more area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delmas_A/0/1/0/all/0/1&quot;&gt;Alberto Delmas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Judd_P/0/1/0/all/0/1&quot;&gt;Patrick Judd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuart_D/0/1/0/all/0/1&quot;&gt;Dylan Malone Stuart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poulos_Z/0/1/0/all/0/1&quot;&gt;Zissis Poulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoud_M/0/1/0/all/0/1&quot;&gt;Mostafa Mahmoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharify_S/0/1/0/all/0/1&quot;&gt;Sayeh Sharify&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolic_M/0/1/0/all/0/1&quot;&gt;Milos Nikolic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moshovos_A/0/1/0/all/0/1&quot;&gt;Andreas Moshovos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03745">
<title>Evolutionary Architecture Search For Deep Multitask Networks. (arXiv:1803.03745v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.03745</link>
<description rdf:parseType="Literal">&lt;p&gt;Multitask learning, i.e. learning several tasks at once with the same neural
network, can improve performance in each of the tasks. Designing deep neural
network architectures for multitask learning is a challenge: There are many
ways to tie the tasks together, and the design choices matter. The size and
complexity of this problem exceeds human design ability, making it a compelling
domain for evolutionary optimization. Using the existing state of the art soft
ordering architecture as the starting point, methods for evolving the modules
of this architecture and for evolving the overall topology or routing between
modules are evaluated in this paper. A synergetic approach of evolving custom
routings with evolved, shared modules for each task is found to be very
powerful, significantly improving the state of the art in the Omniglot
multitask, multialphabet character recognition domain. This result demonstrates
how evolution can be instrumental in advancing deep neural network and complex
system design in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jason Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyerson_E/0/1/0/all/0/1&quot;&gt;Elliot Meyerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05639">
<title>Deep Stochastic Configuration Networks with Universal Approximation Property. (arXiv:1702.05639v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05639</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops a randomized approach for incrementally building deep
neural networks, where a supervisory mechanism is proposed to constrain the
random assignment of the weights and biases, and all the hidden layers have
direct links to the output layer. A fundamental result on the universal
approximation property is established for such a class of randomized leaner
models, namely deep stochastic configuration networks (DeepSCNs). A learning
algorithm is presented to implement DeepSCNs with either specific architecture
or self-organization. The read-out weights attached with all direct links from
each hidden layer to the output layer are evaluated by the least squares
method. Given a set of training examples, DeepSCNs can speedily produce a
learning representation, that is, a collection of random basis functions with
the cascaded inputs together with the read-out weights. An empirical study on a
function approximation is carried out to demonstrate some properties of the
proposed deep learner model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dianhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02738">
<title>Neural network feedback controller for inertial platform. (arXiv:1803.02738v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02738</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper describes an algorithm for the synthesis of neural networks to
control gyro stabilizer. The neural network performs the role of observer for
state vector. The role of an observer in a feedback of gyro stabilizer is
illustrated. Paper detail a problem specific features stage of classics
algorithm: choosing of network architecture, learning of neural network and
verification of result feedback control. In the article presented optimal
configuration of the neural network like a memory depth, the number of layers
and neuron in these layers and activation functions in layers. Using the
information of dynamic system for improving learning of neural network is
provided. A scheme creation of an optimal training sample is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anisimov_Y/0/1/0/all/0/1&quot;&gt;Yan Anisimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lysov_A/0/1/0/all/0/1&quot;&gt;Alexandr Lysov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsai_D/0/1/0/all/0/1&quot;&gt;Dmitry Katsai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03664">
<title>Automating Reading Comprehension by Generating Question and Answer Pairs. (arXiv:1803.03664v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.03664</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network-based methods represent the state-of-the-art in question
generation from text. Existing work focuses on generating only questions from
text without concerning itself with answer generation. Moreover, our analysis
shows that handling rare words and generating the most appropriate question
given a candidate answer are still challenges facing existing approaches. We
present a novel two-stage process to generate question-answer pairs from the
text. For the first stage, we present alternatives for encoding the span of the
pivotal answer in the sentence using Pointer Networks. In our second stage, we
employ sequence to sequence models for question generation, enhanced with rich
linguistic features. Finally, global attention and answer encoding are used for
generating the question most relevant to the answer. We motivate and
linguistically analyze the role of each component in our framework and consider
compositions of these. This analysis is supported by extensive experimental
evaluations. Using standard evaluation metrics as well as human evaluations,
our experimental results validate the significant improvement in the quality of
questions generated by our framework over the state-of-the-art. The technique
presented here represents another step towards more automated reading
comprehension assessment. We also present a live system \footnote{Demo of the
system is available at
\url{https://www.cse.iitb.ac.in/~vishwajeet/autoqg.html}.} to demonstrate the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vishwajeet Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boorla_K/0/1/0/all/0/1&quot;&gt;Kireeti Boorla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meena_Y/0/1/0/all/0/1&quot;&gt;Yogesh Meena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1&quot;&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03827">
<title>Face2Text: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions. (arXiv:1803.03827v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.03827</link>
<description rdf:parseType="Literal">&lt;p&gt;The past few years have witnessed renewed interest in NLP tasks at the
interface between vision and language. One intensively-studied problem is that
of automatically generating text from images. In this paper, we extend this
problem to the more specific domain of face description. Unlike scene
descriptions, face descriptions are more fine-grained and rely on attributes
extracted from the image, rather than objects and relations. Given that no data
exists for this task, we present an ongoing crowdsourcing study to collect a
corpus of descriptions of face images taken `in the wild&apos;. To gain a better
understanding of the variation we find in face description and the possible
issues that this may raise, we also conducted an annotation study on a subset
of the corpus. Primarily, we found descriptions to refer to a mixture of
attributes, not only physical, but also emotional and inferential, which is
bound to create further challenges for current image-to-text methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1&quot;&gt;Albert Gatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1&quot;&gt;Marc Tanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muscat_A/0/1/0/all/0/1&quot;&gt;Adrian Muscat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paggio_P/0/1/0/all/0/1&quot;&gt;Patrizia Paggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrugia_R/0/1/0/all/0/1&quot;&gt;Reuben A. Farrugia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1&quot;&gt;Claudia Borg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilleri_K/0/1/0/all/0/1&quot;&gt;Kenneth P. Camilleri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosner_M/0/1/0/all/0/1&quot;&gt;Mike Rosner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plas_L/0/1/0/all/0/1&quot;&gt;Lonneke van der Plas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03834">
<title>Learning and analyzing vector encoding of symbolic representations. (arXiv:1803.03834v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.03834</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a formal language with expressions denoting general symbol
structures and queries which access information in those structures. A
sequence-to-sequence network processing this language learns to encode symbol
structures and query them. The learned representation (approximately) shares a
simple linearity property with theoretical techniques for performing this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1&quot;&gt;Roland Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1&quot;&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smolensky_P/0/1/0/all/0/1&quot;&gt;Paul Smolensky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04263">
<title>Intelligible Artificial Intelligence. (arXiv:1803.04263v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.04263</link>
<description rdf:parseType="Literal">&lt;p&gt;Since Artificial Intelligence (AI) software uses techniques like deep
lookahead search and stochastic optimization of huge neural networks to fit
mammoth datasets, it often results in complex behavior that is difficult for
people to understand. Yet organizations are deploying AI algorithms in many
mission-critical settings. In order to trust their behavior, we must make it
intelligible --- either by using inherently interpretable models or by
developing methods for explaining otherwise overwhelmingly complex decisions by
local approximation, vocabulary alignment, and interactive dialog.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1&quot;&gt;Daniel S. Weld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_G/0/1/0/all/0/1&quot;&gt;Gagan Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.04582">
<title>The Opacity of Backbones and Backdoors Under a Weak Assumption. (arXiv:1706.04582v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.04582</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoors and backbones of Boolean formulas are hidden structural properties.
A natural goal, already in part realized, is that solver algorithms seek to
obtain substantially better performance by exploiting these structures.
&lt;/p&gt;
&lt;p&gt;However, the present paper is not intended to improve the performance of SAT
solvers, but rather is a cautionary paper. In particular, the theme of this
paper is that there is a potential chasm between the existence of such
structures in the Boolean formula and being able to effectively exploit them.
This does not mean that these structures are not useful to solvers. It does
mean that one must be very careful not to assume that it is computationally
easy to go from the existence of a structure to being able to get one&apos;s hands
on it and/or being able to exploit the structure.
&lt;/p&gt;
&lt;p&gt;For example, in this paper we show that, under the assumption that P $\neq$
NP, there are easily recognizable sets of Boolean formulas for which it is hard
to determine whether they have a large backbone. We also show that, also under
the assumption P $\neq$ NP, there are easily recognizable families of Boolean
formulas with strong backdoors that are easy to find, yet for which it is hard
to determine whether they are satisfiable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemaspaandra_L/0/1/0/all/0/1&quot;&gt;Lane A. Hemaspaandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narvaez_D/0/1/0/all/0/1&quot;&gt;David E. Narv&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04676">
<title>KBLRN : End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features. (arXiv:1709.04676v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04676</link>
<description rdf:parseType="Literal">&lt;p&gt;We present KBLRN, a framework for end-to-end learning of knowledge base
representations from latent, relational, and numerical features. KBLRN
integrates feature types with a novel combination of neural representation
learning and probabilistic product of experts models. To the best of our
knowledge, KBLRN is the first approach that learns representations of knowledge
bases by integrating latent, relational, and numerical features. We show that
instances of KBLRN outperform existing methods on a range of knowledge base
completion tasks. We contribute a novel data sets enriching commonly used
knowledge base completion benchmarks with numerical features. We have made the
data sets available for further research. We also investigate the impact
numerical features have on the KB completion performance of KBLRN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garcia-Duran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03764">
<title>Variance Networks: When Expectation Does Not Meet Your Expectations. (arXiv:1803.03764v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03764</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose variance networks, a new model that stores the
learned information in the variances of the network weights. Surprisingly, no
information gets stored in the expectations of the weights, therefore if we
replace these weights with their expectations, we would obtain a random guess
quality prediction. We provide a numerical criterion that uses the loss
curvature to determine which random variables can be replaced with their
expected values, and find that only a small fraction of weights is needed for
ensembling. Variance networks represent a diverse ensemble that is more robust
to adversarial attacks than conventional low-variance ensembles. The success of
this model raises several counter-intuitive implications for the training and
application of Deep Learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neklyudov_K/0/1/0/all/0/1&quot;&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1&quot;&gt;Arsenii Ashukha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03880">
<title>Combating Adversarial Attacks Using Sparse Representations. (arXiv:1803.03880v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03880</link>
<description rdf:parseType="Literal">&lt;p&gt;It is by now well-known that small adversarial perturbations can induce
classification errors in deep neural networks (DNNs). In this paper, we make
the case that sparse representations of the input data are a crucial tool for
combating such attacks. For linear classifiers, we show that a sparsifying
front end is provably effective against $\ell_{\infty}$-bounded attacks,
reducing output distortion due to the attack by a factor of roughly $K / N$
where $N$ is the data dimension and $K$ is the sparsity level. We then extend
this concept to DNNs, showing that a &quot;locally linear&quot; model can be used to
develop a theoretical foundation for crafting attacks and defenses.
Experimental results for the MNIST dataset show the efficacy of the proposed
sparsifying front end.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gopalakrishnan_S/0/1/0/all/0/1&quot;&gt;Soorya Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marzi_Z/0/1/0/all/0/1&quot;&gt;Zhinus Marzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Madhow_U/0/1/0/all/0/1&quot;&gt;Upamanyu Madhow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1&quot;&gt;Ramtin Pedarsani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03965">
<title>BEBP: An Poisoning Method Against Machine Learning Based IDSs. (arXiv:1803.03965v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03965</link>
<description rdf:parseType="Literal">&lt;p&gt;In big data era, machine learning is one of fundamental techniques in
intrusion detection systems (IDSs). However, practical IDSs generally update
their decision module by feeding new data then retraining learning models in a
periodical way. Hence, some attacks that comprise the data for training or
testing classifiers significantly challenge the detecting capability of machine
learning-based IDSs. Poisoning attack, which is one of the most recognized
security threats towards machine learning-based IDSs, injects some adversarial
samples into the training phase, inducing data drifting of training data and a
significant performance decrease of target IDSs over testing data. In this
paper, we adopt the Edge Pattern Detection (EPD) algorithm to design a novel
poisoning method that attack against several machine learning algorithms used
in IDSs. Specifically, we propose a boundary pattern detection algorithm to
efficiently generate the points that are near to abnormal data but considered
to be normal ones by current classifiers. Then, we introduce a Batch-EPD
Boundary Pattern (BEBP) detection algorithm to overcome the limitation of the
number of edge pattern points generated by EPD and to obtain more useful
adversarial samples. Based on BEBP, we further present a moderate but effective
poisoning method called chronic poisoning attack. Extensive experiments on
synthetic and three real network data sets demonstrate the performance of the
proposed poisoning method against several well-known machine learning
algorithms and a practical intrusion detection method named FMIFS-LSSVM-IDS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wentao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dongxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04042">
<title>Interpreting Deep Classifier by Visual Distillation of Dark Knowledge. (arXiv:1803.04042v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04042</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting black box classifiers, such as deep networks, allows an analyst
to validate a classifier before it is deployed in a high-stakes setting. A
natural idea is to visualize the deep network&apos;s representations, so as to &quot;see
what the network sees&quot;. In this paper, we demonstrate that standard dimension
reduction methods in this setting can yield uninformative or even misleading
visualizations. Instead, we present DarkSight, which visually summarizes the
predictions of a classifier in a way inspired by notion of dark knowledge.
DarkSight embeds the data points into a low-dimensional space such that it is
easy to compress the deep classifier into a simpler one, essentially combining
model compression and dimension reduction. We compare DarkSight against t-SNE
both qualitatively and quantitatively, demonstrating that DarkSight
visualizations are more informative. Our method additionally yields a new
confidence measure based on dark knowledge by quantifying how unusual a given
vector of predictions is.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1&quot;&gt;Dae Hoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1&quot;&gt;Chang Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1&quot;&gt;Charles Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04062">
<title>Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing---and Back. (arXiv:1803.04062v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04062</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep multitask learning boosts performance by sharing learned structure
across related tasks. This paper adapts ideas from deep multitask learning to
the setting where only a single task is available. The method is formalized as
pseudo-task augmentation, in which models are trained with multiple decoders
for each task. Pseudo-tasks simulate the effect of training towards
closely-related tasks drawn from the same universe. In a suite of experiments,
pseudo-task augmentation is shown to improve performance on single-task
learning problems. When combined with multitask learning, further improvements
are achieved, including state-of-the-art performance on the CelebA dataset,
showing that pseudo-task augmentation and multitask learning have complementary
value. All in all, pseudo-task augmentation is a broadly applicable and
efficient way to boost performance in deep learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyerson_E/0/1/0/all/0/1&quot;&gt;Elliot Meyerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04186">
<title>R3Net: Random Weights, Rectifier Linear Units and Robustness for Artificial Neural Network. (arXiv:1803.04186v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04186</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a neural network architecture with randomized features, a
sign-splitter, followed by rectified linear units (ReLU). We prove that our
architecture exhibits robustness to the input perturbation: the output feature
of the neural network exhibits a Lipschitz continuity in terms of the input
perturbation. We further show that the network output exhibits a discrimination
ability that inputs that are not arbitrarily close generate output vectors
which maintain distance between each other obeying a certain lower bound. This
ensures that two different inputs remain discriminable while contracting the
distance in the output feature space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venkitaraman_A/0/1/0/all/0/1&quot;&gt;Arun Venkitaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Javid_A/0/1/0/all/0/1&quot;&gt;Alireza M. Javid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Saikat Chatterjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04193">
<title>Extreme Learning Machine for Graph Signal Processing. (arXiv:1803.04193v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04193</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we improve extreme learning machines for regression tasks
using a graph signal processing based regularization. We assume that the target
signal for prediction or regression is a graph signal. With this assumption, we
use the regularization to enforce that the output of an extreme learning
machine is smooth over a given graph. Simulation results with real data confirm
that such regularization helps significantly when the available training data
is limited in size and corrupted by noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venkitaraman_A/0/1/0/all/0/1&quot;&gt;Arun Venkitaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Saikat Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Handel_P/0/1/0/all/0/1&quot;&gt;Peter H&amp;#xe4;ndel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04223">
<title>Leveraging Crowdsourcing Data For Deep Active Learning - An Application: Learning Intents in Alexa. (arXiv:1803.04223v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04223</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a generic Bayesian framework that enables any deep
learning model to actively learn from targeted crowds. Our framework inherits
from recent advances in Bayesian deep learning, and extends existing work by
considering the targeted crowdsourcing approach, where multiple annotators with
unknown expertise contribute an uncontrolled amount (often limited) of
annotations. Our framework leverages the low-rank structure in annotations to
learn individual annotator expertise, which then helps to infer the true labels
from noisy and sparse annotations. It provides a unified Bayesian model to
simultaneously infer the true labels and train the deep learning model in order
to reach an optimal learning efficacy. Finally, our framework exploits the
uncertainty of the deep learning model during prediction as well as the
annotators&apos; estimated expertise to minimize the number of required annotations
and annotators for optimally training the deep learning model.
&lt;/p&gt;
&lt;p&gt;We evaluate the effectiveness of our framework for intent classification in
Alexa (Amazon&apos;s personal assistant), using both synthetic and real-world
datasets. Experiments show that our framework can accurately learn annotator
expertise, infer true labels, and effectively reduce the amount of annotations
in model training as compared to state-of-the-art approaches. We further
discuss the potential of our proposed framework in bridging machine learning
and crowdsourcing towards improved human-in-the-loop systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drake_T/0/1/0/all/0/1&quot;&gt;Thomas Drake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damianou_A/0/1/0/all/0/1&quot;&gt;Andreas Damianou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maarek_Y/0/1/0/all/0/1&quot;&gt;Yoelle Maarek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04300">
<title>Neural Conditional Gradients. (arXiv:1803.04300v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04300</link>
<description rdf:parseType="Literal">&lt;p&gt;The move from hand-designed to learned optimizers in machine learning has
been quite successful for gradient-based and -free optimizers. When facing a
constrained problem, however, maintaining feasibility typically requires a
projection step, which might be computationally expensive and not
differentiable. We show how the design of projection-free convex optimization
algorithms can be cast as a learning problem based on Frank-Wolfe Networks:
recurrent networks implementing the Frank-Wolfe algorithm aka. conditional
gradients. This allows them to learn to exploit structure when, e.g.,
optimizing over rank-1 matrices. Our LSTM-learned optimizers outperform
hand-designed as well learned but unconstrained ones. We demonstrate this for
training support vector machines and softmax classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1&quot;&gt;Christian Bauckhage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04347">
<title>Classifying Online Dating Profiles on Tinder using FaceNet Facial Embeddings. (arXiv:1803.04347v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.04347</link>
<description rdf:parseType="Literal">&lt;p&gt;A method to produce personalized classification models to automatically
review online dating profiles on Tinder is proposed, based on the user&apos;s
historical preference. The method takes advantage of a FaceNet facial
classification model to extract features which may be related to facial
attractiveness. The embeddings from a FaceNet model were used as the features
to describe an individual&apos;s face. A user reviewed 8,545 online dating profiles.
For each reviewed online dating profile, a feature set was constructed from the
profile images which contained just one face. Two approaches are presented to
go from the set of features for each face, to a set of profile features. A
simple logistic regression trained on the embeddings from just 20 profiles
could obtain a 65% validation accuracy. A point of diminishing marginal returns
was identified to occur around 80 profiles, at which the model accuracy of 73%
would only improve marginally after reviewing a significant number of
additional profiles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jekel_C/0/1/0/all/0/1&quot;&gt;Charles F Jekel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haftka_R/0/1/0/all/0/1&quot;&gt;Raphael T. Haftka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04364">
<title>Maturation Trajectories of Cortical Resting-State Networks Depend on the Mediating Frequency Band. (arXiv:1803.04364v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1803.04364</link>
<description rdf:parseType="Literal">&lt;p&gt;The functional significance of resting state networks and their abnormal
manifestations in psychiatric disorders are firmly established, as is the
importance of the cortical rhythms in mediating these networks. Resting state
networks are known to undergo substantial reorganization from childhood to
adulthood, but whether distinct cortical rhythms, which are generated by
separable neural mechanisms and are often manifested abnormally in psychiatric
conditions, mediate maturation differentially, remains unknown. Using
magnetoencephalography (MEG) to map frequency band specific maturation of
resting state networks from age 7 to 29 in 162 participants (31 independent),
we found significant changes with age in networks mediated by the beta
(13-30Hz) and gamma (31-80Hz) bands. More specifically, gamma band mediated
networks followed an expected asymptotic trajectory, but beta band mediated
networks followed a linear trajectory. Network integration increased with age
in gamma band mediated networks, while local segregation increased with age in
beta band mediated networks. Spatially, the hubs that changed in importance
with age in the beta band mediated networks had relatively little overlap with
those that showed the greatest changes in the gamma band mediated networks.
These findings are relevant for our understanding of the neural mechanisms of
cortical maturation, in both typical and atypical development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Sheraz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hashmi_J/0/1/0/all/0/1&quot;&gt;Javeria Hashmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mamashli_F/0/1/0/all/0/1&quot;&gt;Fahimeh Mamashli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Michmizos_K/0/1/0/all/0/1&quot;&gt;Konstantinos Michmizos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kitzbichler_M/0/1/0/all/0/1&quot;&gt;Manfred Kitzbichler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bharadwaj_H/0/1/0/all/0/1&quot;&gt;Hari Bharadwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bekhti_Y/0/1/0/all/0/1&quot;&gt;Yousra Bekhti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ganesan_S/0/1/0/all/0/1&quot;&gt;Santosh Ganesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Garel_K/0/1/0/all/0/1&quot;&gt;Keri A Garel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Whitfield_Gabrieli_S/0/1/0/all/0/1&quot;&gt;Susan Whitfield-Gabrieli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gollub_R/0/1/0/all/0/1&quot;&gt;Randy Gollub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kong_J/0/1/0/all/0/1&quot;&gt;Jian Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vaina_L/0/1/0/all/0/1&quot;&gt;Lucia M Vaina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rana_K/0/1/0/all/0/1&quot;&gt;Kunjan Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Stufflebeam_S/0/1/0/all/0/1&quot;&gt;Steven Stufflebeam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hamalainen_M/0/1/0/all/0/1&quot;&gt;Matti Hamalainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kenet_T/0/1/0/all/0/1&quot;&gt;Tal Kenet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04386">
<title>Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches. (arXiv:1803.04386v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04386</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic neural net weights are used in a variety of contexts, including
regularization, Bayesian neural nets, exploration in reinforcement learning,
and evolution strategies. Unfortunately, due to the large number of weights,
all the examples in a mini-batch typically share the same weight perturbation,
thereby limiting the variance reduction effect of large mini-batches. We
introduce flipout, an efficient method for decorrelating the gradients within a
mini-batch by implicitly sampling pseudo-independent weight perturbations for
each example. Empirically, flipout achieves the ideal linear variance reduction
for fully connected networks, convolutional networks, and RNNs. We find
significant speedups in training neural networks with multiplicative Gaussian
perturbations. We show that flipout is effective at regularizing LSTMs, and
outperforms previous methods. Flipout also enables us to vectorize evolution
strategies: in our experiments, a single GPU with flipout can handle the same
throughput as at least 40 CPU cores using existing methods, equivalent to a
factor-of-4 cost reduction on Amazon Web Services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yeming Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicol_P/0/1/0/all/0/1&quot;&gt;Paul Vicol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1&quot;&gt;Jimmy Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04025">
<title>Learning Large-Scale Bayesian Networks with the sparsebn Package. (arXiv:1703.04025v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04025</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning graphical models from data is an important problem with wide
applications, ranging from genomics to the social sciences. Nowadays datasets
often have upwards of thousands---sometimes tens or hundreds of thousands---of
variables and far fewer samples. To meet this challenge, we have developed a
new R package called sparsebn for learning the structure of large, sparse
graphical models with a focus on Bayesian networks. While there are many
existing software packages for this task, this package focuses on the unique
setting of learning large networks from high-dimensional data, possibly with
interventions. As such, the methods provided place a premium on scalability and
consistency in a high-dimensional setting. Furthermore, in the presence of
interventions, the methods implemented here achieve the goal of learning a
causal network from data. Additionally, the sparsebn package is fully
compatible with existing software packages for network analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aragam_B/0/1/0/all/0/1&quot;&gt;Bryon Aragam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaying Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qing Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04757">
<title>Separation of time scales and direct computation of weights in deep neural networks. (arXiv:1703.04757v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04757</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence is revolutionizing our lives at an ever increasing
pace. At the heart of this revolution is the recent advancements in deep neural
networks (DNN), learning to perform sophisticated, high-level tasks. However,
training DNNs requires massive amounts of data and is very computationally
intensive. Gaining analytical understanding of the solutions found by DNNs can
help us devise more efficient training algorithms, replacing the commonly used
mthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and
show that, indeed, direct computation of the solutions is possible in many
cases. We show that a high performing setup used in DNNs introduces a
separation of time-scales in the training dynamics, allowing SGD to train
layers from the lowest (closest to input) to the highest. We then show that for
each layer, the distribution of solutions found by SGD can be estimated using a
class-based principal component analysis (PCA) of the layer&apos;s input. This
finding allows us to forgo SGD entirely and directly derive the DNN parameters
using this class-based PCA, which can be well estimated using significantly
less data than SGD. We implement these results on image datasets MNIST, CIFAR10
and CIFAR100 and find that, in fact, layers derived using our class-based PCA
perform comparable or superior to neural networks of the same size and
architecture trained using SGD. We also confirm that the class-based PCA often
converges using a fraction of the data required for SGD. Thus, using our method
training time can be reduced both by requiring less training data than SGD, and
by eliminating layers in the costly backpropagation step of the training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehmamy_N/0/1/0/all/0/1&quot;&gt;Nima Dehmamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohani_N/0/1/0/all/0/1&quot;&gt;Neda Rohani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1&quot;&gt;Aggelos Katsaggelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11278">
<title>Approximating Continuous Functions by ReLU Nets of Minimal Width. (arXiv:1710.11278v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11278</link>
<description rdf:parseType="Literal">&lt;p&gt;This article concerns the expressive power of depth in deep feed-forward
neural nets with ReLU activations. Specifically, we answer the following
question: for a fixed $d_{in}\geq 1,$ what is the minimal width $w$ so that
neural nets with ReLU activations, input dimension $d_{in}$, hidden layer
widths at most $w,$ and arbitrary depth can approximate any continuous,
real-valued function of $d_{in}$ variables arbitrarily well? It turns out that
this minimal width is exactly equal to $d_{in}+1.$ That is, if all the hidden
layer widths are bounded by $d_{in}$, then even in the infinite depth limit,
ReLU nets can only express a very limited class of functions, and, on the other
hand, any continuous function on the $d_{in}$-dimensional unit cube can be
approximated to arbitrary precision by ReLU nets in which all hidden layers
have width exactly $d_{in}+1.$ Our construction in fact shows that any
continuous function $f:[0,1]^{d_{in}}\to\mathbb R^{d_{out}}$ can be
approximated by a net of width $d_{in}+d_{out}$. We obtain quantitative depth
estimates for such an approximation in terms of the modulus of continuity of
$f$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sellke_M/0/1/0/all/0/1&quot;&gt;Mark Sellke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01558">
<title>Wasserstein Auto-Encoders. (arXiv:1711.01558v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01558</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building
a generative model of the data distribution. WAE minimizes a penalized form of
the Wasserstein distance between the model distribution and the target
distribution, which leads to a different regularizer than the one used by the
Variational Auto-Encoder (VAE). This regularizer encourages the encoded
training distribution to match the prior. We compare our algorithm with several
other techniques and show that it is a generalization of adversarial
auto-encoders (AAE). Our experiments show that WAE shares many of the
properties of VAEs (stable training, encoder-decoder architecture, nice latent
manifold structure) while generating samples of better quality, as measured by
the FID score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tolstikhin_I/0/1/0/all/0/1&quot;&gt;Ilya Tolstikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bousquet_O/0/1/0/all/0/1&quot;&gt;Olivier Bousquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gelly_S/0/1/0/all/0/1&quot;&gt;Sylvain Gelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoelkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Schoelkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04877">
<title>Estimating prediction error for complex samples. (arXiv:1711.04877v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04877</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-uniform random samples are commonly generated in multiple scientific
fields ranging from economics to medicine. Complex sampling designs afford
research with increased precision for estimating parameters of interest in less
prevalent sub-populations. With a growing interest in using complex samples to
generate prediction models for numerous outcomes it is necessary to account for
the sampling design that gave rise to the data in order to assess the
generalized predictive utility of a proposed prediction rule. Specifically,
after learning a prediction rule based on a complex sample, it is of interest
to estimate the rule&apos;s error rate when applied to unobserved members of the
population. Efron proposed a general class of covariance-inflated prediction
error estimators that assumed the available training data is representative of
the target population for which the prediction rule is to be applied. We extend
Efron&apos;s estimator to the complex sample context by incorporating
Horvitz-Thompson sampling weights and show that it is consistent for the true
generalization error rate when applied to the underlying superpopulation giving
rise to the training sample. The resulting Horvitz-Thompson-Efron (HTE)
estimator is equivalent to dAIC---a recent extension of AIC to survey sampling
data---and is more widely applicable. The proposed methodology is assessed via
empirical simulations and is applied to data predicting renal function that was
obtained from the National Health and Nutrition Examination Survey (NHANES).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holbrook_A/0/1/0/all/0/1&quot;&gt;Andrew Holbrook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lumley_T/0/1/0/all/0/1&quot;&gt;Thomas Lumley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gillen_D/0/1/0/all/0/1&quot;&gt;Daniel Gillen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00828">
<title>Tensor Train Neighborhood Preserving Embedding. (arXiv:1712.00828v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00828</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a Tensor Train Neighborhood Preserving Embedding
(TTNPE) to embed multi-dimensional tensor data into low dimensional tensor
subspace. Novel approaches to solve the optimization problem in TTNPE are
proposed. For this embedding, we evaluate novel trade-off gain among
classification, computation, and dimensionality reduction (storage) for
supervised learning. It is shown that compared to the state-of-the-arts tensor
embedding methods, TTNPE achieves superior trade-off in classification,
computation, and dimensionality reduction in MNIST handwritten digits and
Weizmann face datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1&quot;&gt;Shuchin Aeron&lt;/a&gt;</dc:creator>
</item></rdf:RDF>