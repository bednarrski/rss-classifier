<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06964"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06943"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07152"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09681"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07091"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07169"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07237"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07265"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05565"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03958"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06518"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.06964">
<title>GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute Learning. (arXiv:1804.06964v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.06964</link>
<description rdf:parseType="Literal">&lt;p&gt;A key problem in deep multi-attribute learning is to effectively discover the
inter-attribute correlation structures. Typically, the conventional deep
multi-attribute learning approaches follow the pipeline of manually designing
the network architectures based on task-specific expertise prior knowledge and
careful network tunings, leading to the inflexibility for various complicated
scenarios in practice. Motivated by addressing this problem, we propose an
efficient greedy neural architecture search approach (GNAS) to automatically
discover the optimal tree-like deep architecture for multi-attribute learning.
In a greedy manner, GNAS divides the optimization of global architecture into
the optimizations of individual connections step by step. By iteratively
updating the local architectures, the global tree-like architecture gets
converged where the bottom layers are shared across relevant attributes and the
branches in top layers more encode attribute-specific features. Experiments on
three benchmark multi-attribute datasets show the effectiveness and compactness
of neural architectures derived by GNAS, and also demonstrate the efficiency of
GNAS in searching neural architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhongfei/0/1/0/all/0/1&quot;&gt;Zhongfei&lt;/a&gt; (Mark) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1&quot;&gt;Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1&quot;&gt;Alexander Hauptmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07127">
<title>Hierarchical Behavioral Repertoires with Unsupervised Descriptors. (arXiv:1804.07127v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.07127</link>
<description rdf:parseType="Literal">&lt;p&gt;Enabling artificial agents to automatically learn complex, versatile and
high-performing behaviors is a long-lasting challenge. This paper presents a
step in this direction with hierarchical behavioral repertoires that stack
several behavioral repertoires to generate sophisticated behaviors. Each
repertoire of this architecture uses the lower repertoires to create complex
behaviors as sequences of simpler ones, while only the lowest repertoire
directly controls the agent&apos;s movements. This paper also introduces a novel
approach to automatically define behavioral descriptors thanks to an
unsupervised neural network that organizes the produced high-level behaviors.
The experiments show that the proposed architecture enables a robot to learn
how to draw digits in an unsupervised manner after having learned to draw lines
and arcs. Compared to traditional behavioral repertoires, the proposed
architecture reduces the dimensionality of the optimization problems by orders
of magnitude and provides behaviors with a twice better fitness. More
importantly, it enables the transfer of knowledge between robots: a
hierarchical repertoire evolved for a robotic arm to draw digits can be
transferred to a humanoid robot by simply changing the lowest layer of the
hierarchy. This enables the humanoid to draw digits although it has never been
trained for this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demiris_Y/0/1/0/all/0/1&quot;&gt;Yiannis Demiris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07144">
<title>Human Activity Recognition using Recurrent Neural Networks. (arXiv:1804.07144v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.07144</link>
<description rdf:parseType="Literal">&lt;p&gt;Human activity recognition using smart home sensors is one of the bases of
ubiquitous computing in smart environments and a topic undergoing intense
research in the field of ambient assisted living. The increasingly large amount
of data sets calls for machine learning methods. In this paper, we introduce a
deep learning model that learns to classify human activities without using any
prior knowledge. For this purpose, a Long Short Term Memory (LSTM) Recurrent
Neural Network was applied to three real world smart home datasets. The results
of these experiments show that the proposed approach outperforms the existing
ones in terms of accuracy and performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1&quot;&gt;Deepika Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merdivan_E/0/1/0/all/0/1&quot;&gt;Erinc Merdivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psychoula_I/0/1/0/all/0/1&quot;&gt;Ismini Psychoula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kropf_J/0/1/0/all/0/1&quot;&gt;Johannes Kropf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanke_S/0/1/0/all/0/1&quot;&gt;Sten Hanke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holzinger_A/0/1/0/all/0/1&quot;&gt;Andreas Holzinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06876">
<title>Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods. (arXiv:1804.06876v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.06876</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new benchmark, WinoBias, for coreference resolution focused on
gender bias. Our corpus contains Winograd-schema style sentences with entities
corresponding to people referred by their occupation (e.g. the nurse, the
doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a
neural coreference system all link gendered pronouns to pro-stereotypical
entities with higher accuracy than anti-stereotypical entities, by an average
difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation
approach that, in combination with existing word-embedding debiasing
techniques, removes the bias demonstrated by these systems in WinoBias without
significantly affecting their performance on existing coreference benchmark
datasets. Our dataset and code are available at &lt;a href=&quot;http://winobias.org.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianlu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1&quot;&gt;Mark Yatskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1&quot;&gt;Vicente Ordonez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06896">
<title>A Multi-task Selected Learning Approach for Solving New Type 3D Bin Packing Problem. (arXiv:1804.06896v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06896</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a new type of 3D bin packing problem (BPP), in which a
number of cuboid-shaped items must be put into a bin one by one orthogonally.
The objective is to find a way to place these items that can minimize the
surface area of the bin. This problem is based on the fact that there is no
fixed-sized bin in many real business scenarios and the cost of a bin is
proportional to its surface area. Based on previous research on 3D BPP, the
surface area is determined by the sequence, spatial locations and orientations
of items. It is a new NP-hard combinatorial optimization problem on
unfixed-sized bin packing, for which we propose a multi-task framework based on
Selected Learning, generating the sequence and orientations of items packed
into the bin simultaneously. During training steps, Selected Learning chooses
one of loss functions derived from Deep Reinforcement Learning and Supervised
Learning corresponding to the training procedure. Numerical results show that
the method proposed significantly outperforms Lego baselines by a substantial
gain of 7.52%. Moreover, we produce large scale 3D Bin Packing order data set
for studying bin packing problems and will release it to the research
community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haoyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1&quot;&gt;Lu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaodong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinghui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jiangwen Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06898">
<title>Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input. (arXiv:1804.06898v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.06898</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate that current state-of-the-art approaches to Automated Essay
Scoring (AES) are not well-suited to capturing adversarially crafted input of
grammatical but incoherent sequences of sentences. We develop a neural model of
local coherence that can effectively learn connectedness features between
sentences, and propose a framework for integrating and jointly training the
local coherence model with a state-of-the-art AES model. We evaluate our
approach against a number of baselines and experimentally demonstrate its
effectiveness on both the AES task and the task of flagging adversarial input,
further contributing to the development of an approach that strengthens the
validity of neural essay scoring models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farag_Y/0/1/0/all/0/1&quot;&gt;Youmna Farag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1&quot;&gt;Helen Yannakoudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briscoe_T/0/1/0/all/0/1&quot;&gt;Ted Briscoe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06943">
<title>K-Nearest Oracles Borderline Dynamic Classifier Ensemble Selection. (arXiv:1804.06943v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06943</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic Ensemble Selection (DES) techniques aim to select locally competent
classifiers for the classification of each new test sample. Most DES techniques
estimate the competence of classifiers using a given criterion over the region
of competence of the test sample (its the nearest neighbors in the validation
set). The K-Nearest Oracles Eliminate (KNORA-E) DES selects all classifiers
that correctly classify all samples in the region of competence of the test
sample, if such classifier exists, otherwise, it removes from the region of
competence the sample that is furthest from the test sample, and the process
repeats. When the region of competence has samples of different classes,
KNORA-E can reduce the region of competence in such a way that only samples of
a single class remain in the region of competence, leading to the selection of
locally incompetent classifiers that classify all samples in the region of
competence as being from the same class. In this paper, we propose two DES
techniques: K-Nearest Oracles Borderline (KNORA-B) and K-Nearest Oracles
Borderline Imbalanced (KNORA-BI). KNORA-B is a DES technique based on KNORA-E
that reduces the region of competence but maintains at least one sample from
each class that is in the original region of competence. KNORA-BI is a
variation of KNORA-B for imbalance datasets that reduces the region of
competence but maintains at least one minority class sample if there is any in
the original region of competence. Experiments are conducted comparing the
proposed techniques with 19 DES techniques from the literature using 40
datasets. The results show that the proposed techniques achieved interesting
results, with KNORA-BI outperforming state-of-art techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1&quot;&gt;Dayvid V. R. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalcanti_G/0/1/0/all/0/1&quot;&gt;George D. C. Cavalcanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porpino_T/0/1/0/all/0/1&quot;&gt;Thyago N. Porpino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07013">
<title>An Integrated Development Environment for Planning Domain Modeling. (arXiv:1804.07013v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.07013</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to make the task, description of planning domains and problems, more
comprehensive for non-experts in planning, the visual representation has been
used in planning domain modeling in recent years. However, current knowledge
engineering tools with visual modeling, like itSIMPLE (Vaquero et al. 2012) and
VIZ (Vodr\&apos;a\v{z}ka and Chrpa 2010), are less efficient than the traditional
method of hand-coding by a PDDL expert using a text editor, and rarely involved
in finetuning planning domains depending on the plan validation. Aim at this,
we present an integrated development environment KAVI for planning domain
modeling inspired by itSIMPLE and VIZ. KAVI using an abstract domain knowledge
base to improve the efficiency of planning domain visual modeling. By
integrating planners and a plan validator, KAVI proposes a method to fine-tune
planning domains based on the plan validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuncong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1&quot;&gt;Hankz Hankui Zhuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07047">
<title>Cell Selection with Deep Reinforcement Learning in Sparse Mobile Crowdsensing. (arXiv:1804.07047v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.07047</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse Mobile CrowdSensing (MCS) is a novel MCS paradigm where data inference
is incorporated into the MCS process for reducing sensing costs while its
quality is guaranteed. Since the sensed data from different cells (sub-areas)
of the target sensing area will probably lead to diverse levels of inference
data quality, cell selection (i.e., choose which cells of the target area to
collect sensed data from participants) is a critical issue that will impact the
total amount of data that requires to be collected (i.e., data collection
costs) for ensuring a certain level of quality. To address this issue, this
paper proposes a Deep Reinforcement learning based Cell selection mechanism for
Sparse MCS, called DR-Cell. First, we properly model the key concepts in
reinforcement learning including state, action, and reward, and then propose to
use a deep recurrent Q-network for learning the Q-function that can help decide
which cell is a better choice under a certain state during cell selection.
Furthermore, we leverage the transfer learning techniques to reduce the amount
of data required for training the Q-function if there are multiple correlated
MCS tasks that need to be conducted in the same target area. Experiments on
various real-life sensing datasets verify the effectiveness of DR-Cell over the
state-of-the-art cell selection mechanisms in Sparse MCS by reducing up to 15%
of sensed cells with the same data inference quality guarantee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1&quot;&gt;En Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongjian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07090">
<title>Low Rank Structure of Learned Representations. (arXiv:1804.07090v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07090</link>
<description rdf:parseType="Literal">&lt;p&gt;A key feature of neural networks, particularly deep convolutional neural
networks, is their ability to &quot;learn&quot; useful representations from data. The
very last layer of a neural network is then simply a linear model trained on
these &quot;learned&quot; representations. Despite their numerous applications in other
tasks such as classification, retrieval, clustering etc., a.k.a. transfer
learning, not much work has been published that investigates the structure of
these representations or whether structure can be imposed on them during the
training process.
&lt;/p&gt;
&lt;p&gt;In this paper, we study the dimensionality of the learned representations by
models that have proved highly succesful for image classification. We focus on
ResNet-18, ResNet-50 and VGG-19 and observe that when trained on CIFAR10 or
CIFAR100 datasets, the learned representations exhibit a fairly low rank
structure. We propose a modification to the training procedure, which further
encourages low rank representations of activations at various stages in the
neural network. Empirically, we show that this has implications for compression
and robustness to adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1&quot;&gt;Amartya Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1&quot;&gt;Varun Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H. S. Torr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07152">
<title>Scalable attribute-aware network embedding with localily. (arXiv:1804.07152v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07152</link>
<description rdf:parseType="Literal">&lt;p&gt;Adding attributes for nodes to network embedding helps to improve the ability
of the learned joint representation to depict features from topology and
attributes simultaneously. Recent research on the joint embedding has exhibited
a promising performance on a variety of tasks by jointly embedding the two
spaces. However, due to the indispensable requirement of globality based
information, present approaches contain a flaw of in-scalability. Here we
propose \emph{SANE}, a scalable attribute-aware network embedding algorithm
with locality, to learn the joint representation from topology and attributes.
By enforcing the alignment of a local linear relationship between each node and
its K-nearest neighbors in topology and attribute space, the joint embedding
representations are more informative comparing with a single representation
from topology or attributes alone. And we argue that the locality in
\emph{SANE} is the key to learning the joint representation at scale. By using
several real-world networks from diverse domains, We demonstrate the efficacy
of \emph{SANE} in performance and scalability aspect. Overall, for performance
on label classification, SANE successfully reaches up to the highest F1-score
on most datasets, and even closer to the baseline method that needs label
information as extra inputs, compared with other state-of-the-art joint
representation algorithms. What&apos;s more, \emph{SANE} has an up to 71.4\%
performance gain compared with the single topology-based algorithm. For
scalability, we have demonstrated the linearly time complexity of \emph{SANE}.
In addition, we intuitively observe that when the network size scales to
100,000 nodes, the &quot;learning joint embedding&quot; step of \emph{SANE} only takes
$\approx10$ seconds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhining Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzumura_T/0/1/0/all/0/1&quot;&gt;Toyotaro Suzumura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1&quot;&gt;Guangmin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07193">
<title>Lipschitz Continuity in Model-based Reinforcement Learning. (arXiv:1804.07193v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07193</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based reinforcement-learning methods learn transition and reward models
and use them to guide behavior. We analyze the impact of learning models that
are Lipschitz continuous---the distance between function values for two inputs
is bounded by a linear function of the distance between the inputs. Our first
result shows a tight bound on model errors for multi-step predictions with
Lipschitz continuous models. We go on to prove an error bound for the
value-function estimate arising from such models and show that the estimated
value function is itself Lipschitz continuous. We conclude with empirical
results that demonstrate significant benefits to enforcing Lipschitz continuity
of neural net models during reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadi_K/0/1/0/all/0/1&quot;&gt;Kavosh Asadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1&quot;&gt;Dipendra Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Littman_M/0/1/0/all/0/1&quot;&gt;Michael L. Littman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07270">
<title>A Dynamic Boosted Ensemble Learning Based on Random Forest. (arXiv:1804.07270v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07270</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Dynamic Boosted Random Forest (DBRF), a novel ensemble algorithm
that incorporates the notion of hard example mining into Random Forest (RF) and
thus combines the high accuracy of Boosting algorithm with the strong
generalization of Bagging algorithm. Specifically, we propose to measure the
quality of each leaf node of every decision tree in the random forest to
determine hard examples. By iteratively training and then removing easy
examples and noise examples from training data, we evolve the random forest to
focus on hard examples dynamically so as to learn decision boundaries better.
Data can be cascaded through these random forests learned in each iteration in
sequence to generate predictions, thus making RF deep. We also propose to use
evolution mechanism, stacking mechanism and smart iteration mechanism to
improve the performance of the model. DBRF outperforms RF on three UCI datasets
and achieved state-of-the-art results compared to other deep models. Moreover,
we show that DBRF is also a new way of sampling and can be very useful when
learning from unbalanced data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xingzhang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1&quot;&gt;Chen Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leilei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Ye Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1&quot;&gt;Dongdong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jingxi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02732">
<title>The Higher-Order Prover Leo-III (Extended Version). (arXiv:1802.02732v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02732</link>
<description rdf:parseType="Literal">&lt;p&gt;The automated theorem prover Leo-III for classical higher-order logic with
Henkin semantics and choice is presented. Leo-III is based on extensional
higher-order paramodulation and accepts every common TPTP dialect (FOF, TFF,
THF), including their recent extensions to rank-1 polymorphism (TF1, TH1). In
addition, the prover natively supports almost every normal higher-order modal
logic. Leo-III cooperates with first-order reasoning tools using translations
to many-sorted first-order logic and produces verifiable proof certificates.
The prover is evaluated on heterogeneous benchmark sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steen_A/0/1/0/all/0/1&quot;&gt;Alexander Steen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1&quot;&gt;Christoph Benzm&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01271">
<title>An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. (arXiv:1803.01271v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01271</link>
<description rdf:parseType="Literal">&lt;p&gt;For most deep learning practitioners, sequence modeling is synonymous with
recurrent networks. Yet recent results indicate that convolutional
architectures can outperform recurrent networks on tasks such as audio
synthesis and machine translation. Given a new sequence modeling task or
dataset, which architecture should one use? We conduct a systematic evaluation
of generic convolutional and recurrent architectures for sequence modeling. The
models are evaluated across a broad range of standard tasks that are commonly
used to benchmark recurrent networks. Our results indicate that a simple
convolutional architecture outperforms canonical recurrent networks such as
LSTMs across a diverse range of tasks and datasets, while demonstrating longer
effective memory. We conclude that the common association between sequence
modeling and recurrent networks should be reconsidered, and convolutional
networks should be regarded as a natural starting point for sequence modeling
tasks. To assist related work, we have made code available at
&lt;a href=&quot;http://github.com/locuslab/TCN&quot;&gt;this http URL&lt;/a&gt; .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shaojie Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1&quot;&gt;Vladlen Koltun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09681">
<title>I/O Logic in HOL --- First Steps. (arXiv:1803.09681v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09681</link>
<description rdf:parseType="Literal">&lt;p&gt;A semantical embedding of input/output logic in classical higher-order logic
is presented. This embedding enables the mechanisation and automation of
reasoning tasks in input/output logic with off-the-shelf higher-order theorem
provers and proof assistants. The key idea for the solution presented here
results from the analysis of an inaccurate previous embedding attempt, which we
will discuss as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1&quot;&gt;Christoph Benzm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parent_X/0/1/0/all/0/1&quot;&gt;Xavier Parent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03126">
<title>Data2Vis: Automatic Generation of Data Visualizations Using Sequence to Sequence Recurrent Neural Networks. (arXiv:1804.03126v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03126</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapidly creating effective visualizations using expressive grammars is
challenging for users who have limited time and limited skills in statistics
and data visualization. Even high-level, dedicated visualization tools often
require users to manually select among data attributes, decide which
transformations to apply, and specify mappings between visual encoding
variables and raw or transformed attributes.
&lt;/p&gt;
&lt;p&gt;In this paper we introduce Data2Vis, a neural translation model for
automatically generating visualizations from given datasets. We formulate
visualization generation as a sequence to sequence translation problem where
data specifications are mapped to visualization specifications in a declarative
language (Vega-Lite). To this end, we train a multilayered attention-based
recurrent neural network (RNN) with long short-term memory (LSTM) units on a
corpus of visualization specifications.
&lt;/p&gt;
&lt;p&gt;Qualitative results show that our model learns the vocabulary and syntax for
a valid visualization specification, appropriate transformations (count, bins,
mean) and how to use common data selection patterns that occur within data
visualizations. Data2Vis generates visualizations that are comparable to
manually-created visualizations in a fraction of the time, with potential to
learn more complex visualization strategies at scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dibia_V/0/1/0/all/0/1&quot;&gt;Victor Dibia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1&quot;&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05044">
<title>Specifying, Monitoring, and Executing Workflows in Linked Data Environments. (arXiv:1804.05044v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05044</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an ontology for representing workflows over components with
Read-Write Linked Data interfaces and give an operational semantics to the
ontology via a rule language. Workflow languages have been successfully applied
for modelling behaviour in enterprise information systems, in which the data is
often managed in a relational database. Linked Data interfaces have been widely
deployed on the web to support data integration in very diverse domains,
increasingly also in scenarios involving the Internet of Things, in which
application behaviour is often specified using imperative programming
languages. With our work we aim to combine workflow languages, which allow for
the high-level specification of application behaviour by non-expert users, with
Linked Data, which allows for decentralised data publication and integrated
data access. We show that our ontology is expressive enough to cover the basic
workflow patterns and demonstrate the applicability of our approach with a
prototype system that observes pilots carrying out tasks in a mixed-reality
aircraft cockpit. On a synthetic benchmark from the building automation domain,
the runtime scales linearly with the size of the number of Internet of Things
devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafer_T/0/1/0/all/0/1&quot;&gt;Tobias K&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harth_A/0/1/0/all/0/1&quot;&gt;Andreas Harth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05997">
<title>A New Decidable Class of Tuple Generating Dependencies: The Triangularly-Guarded Class. (arXiv:1804.05997v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05997</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a new class of tuple-generating dependencies
(TGDs) called triangularly-guarded TGDs, which are TGDs with certain
restrictions on the atomic derivation track embedded in the underlying rule
set. We show that conjunctive query answering under this new class of TGDs is
decidable. We further show that this new class strictly contains some other
decidable classes such as weak-acyclic, guarded, sticky and shy, which, to the
best of our knowledge, provides a unified representation of all these
aforementioned classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asuncion_V/0/1/0/all/0/1&quot;&gt;Vernon Asuncion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06909">
<title>Modeling and Simultaneously Removing Bias via Adversarial Neural Networks. (arXiv:1804.06909v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06909</link>
<description rdf:parseType="Literal">&lt;p&gt;In real world systems, the predictions of deployed Machine Learned models
affect the training data available to build subsequent models. This introduces
a bias in the training data that needs to be addressed. Existing solutions to
this problem attempt to resolve the problem by either casting this in the
reinforcement learning framework or by quantifying the bias and re-weighting
the loss functions. In this work, we develop a novel Adversarial Neural Network
(ANN) model, an alternative approach which creates a representation of the data
that is invariant to the bias. We take the Paid Search auction as our working
example and ad display position features as the confounding features for this
setting. We show the success of this approach empirically on both synthetic
data as well as real world paid search auction data from a major search engine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;John Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1&quot;&gt;Joel Pfeiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kai Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1&quot;&gt;Rishabh Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_D/0/1/0/all/0/1&quot;&gt;Denis Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilad_Bachrach_R/0/1/0/all/0/1&quot;&gt;Ran Gilad-Bachrach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyles_L/0/1/0/all/0/1&quot;&gt;Levi Boyles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1&quot;&gt;Eren Manavoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07010">
<title>Forward-Backward Stochastic Neural Networks: Deep Learning of High-dimensional Partial Differential Equations. (arXiv:1804.07010v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.07010</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical numerical methods for solving partial differential equations suffer
from the curse dimensionality mainly due to their reliance on meticulously
generated spatio-temporal grids. Inspired by modern deep learning based
techniques for solving forward and inverse problems associated with partial
differential equations, we circumvent the tyranny of numerical discretization
by devising an algorithm that is scalable to high-dimensions. In particular, we
approximate the unknown solution by a deep neural network which essentially
enables us to benefit from the merits of automatic differentiation. To train
the aforementioned neural network we leverage the well-known connection between
high-dimensional partial differential equations and forward-backward stochastic
differential equations. In fact, independent realizations of a standard
Brownian motion will act as training data. We test the effectiveness of our
approach for a couple of benchmark problems spanning a number of scientific
domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman
equations, both in 100-dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raissi_M/0/1/0/all/0/1&quot;&gt;Maziar Raissi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07091">
<title>Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly Detection. (arXiv:1804.07091v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.07091</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic detection of anomalies in space- and time-varying measurements is
an important tool in several fields, e.g., fraud detection, climate analysis,
or healthcare monitoring. We present an algorithm for detecting anomalous
regions in multivariate spatio-temporal time-series, which allows for spotting
the interesting parts in large amounts of data, including video and text data.
In opposition to existing techniques for detecting isolated anomalous data
points, we propose the &quot;Maximally Divergent Intervals&quot; (MDI) framework for
unsupervised detection of coherent spatial regions and time intervals
characterized by a high Kullback-Leibler divergence compared with all other
data given. In this regard, we define an unbiased Kullback-Leibler divergence
that allows for ranking regions of different size and show how to enable the
algorithm to run on large-scale data sets in reasonable time using an interval
proposal technique. Experiments on both synthetic and real data from various
domains, such as climate analysis, video surveillance, and text forensics,
demonstrate that our method is widely applicable and a valuable tool for
finding interesting events in different types of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barz_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Barz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodner_E/0/1/0/all/0/1&quot;&gt;Erik Rodner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garcia_Y/0/1/0/all/0/1&quot;&gt;Yanira Guanche Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Denzler_J/0/1/0/all/0/1&quot;&gt;Joachim Denzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07169">
<title>Large-scale Nonlinear Variable Selection via Kernel Random Features. (arXiv:1804.07169v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07169</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method for input variable selection in nonlinear regression.
The method is embedded into a kernel regression machine that can model general
nonlinear functions, not being a priori limited to additive models. This is the
first kernel-based variable selection method applicable to large datasets. It
sidesteps the typical poor scaling properties of kernel methods by mapping the
inputs into a relatively low-dimensional space of random features. The
algorithm discovers the variables relevant for the regression task together
with learning the prediction model through learning the appropriate nonlinear
random feature maps. We demonstrate the outstanding performance of our method
on a set of large-scale synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregorova_M/0/1/0/all/0/1&quot;&gt;Magda Gregorov&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1&quot;&gt;Alexandros Kalousis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchand_Maillet_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Marchand-Maillet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07237">
<title>Multi-view Hybrid Embedding: A Divide-and-Conquer Approach. (arXiv:1804.07237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.07237</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel cross-view classification algorithm where the gallery and
probe data come from different views. A popular approach to tackle this problem
is the multi-view subspace learning (MvSL) that aims to learn a latent subspace
shared by multi-view data. Despite promising results obtained on some
applications, the performance of existing methods deteriorates dramatically
when the multi-view data is sampled from nonlinear manifolds or suffers from
heavy outliers. To circumvent this drawback, motivated by the
Divide-and-Conquer strategy, we propose Multi-view Hybrid Embedding (MvHE), a
unique method of dividing the problem of cross-view classification into three
subproblems and building one model for each subproblem. Specifically, the first
model is designed to remove view discrepancy, whereas the second and third
models attempt to discover the intrinsic nonlinear structure and to increase
discriminability in intra-view and inter-view samples respectively. The kernel
extension is conducted to further boost the representation power of MvHE.
Extensive experiments are conducted on four benchmark datasets. Our methods
demonstrate overwhelming advantages against the state-of-the-art MvSL based
cross-view classification approaches in terms of classification accuracy and
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiamiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1&quot;&gt;Xinge You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_M/0/1/0/all/0/1&quot;&gt;Mengjun Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1&quot;&gt;Xiao-Yuan Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C. L. Philip Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07265">
<title>Deep Transfer Network with Joint Distribution Adaptation: A New Intelligent Fault Diagnosis Framework for Industry Application. (arXiv:1804.07265v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07265</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, an increasing popularity of deep learning model for
intelligent condition monitoring and diagnosis as well as prognostics used for
mechanical systems and structures has been observed. In the previous studies,
however, a major assumption accepted by default, is that the training and
testing data are taking from same feature distribution. Unfortunately, this
assumption is mostly invalid in real application, resulting in a certain lack
of applicability for the traditional diagnosis approaches. Inspired by the idea
of transfer learning that leverages the knowledge learnt from rich labeled data
in source domain to facilitate diagnosing a new but similar target task, a new
intelligent fault diagnosis framework, i.e., deep transfer network (DTN), which
generalizes deep learning model to domain adaptation scenario, is proposed in
this paper. By extending the marginal distribution adaptation (MDA) to joint
distribution adaptation (JDA), the proposed framework can exploit the
discrimination structures associated with the labeled data in source domain to
adapt the conditional distribution of unlabeled target data, and thus guarantee
a more accurate distribution matching. Extensive empirical evaluations on three
fault datasets validate the applicability and practicability of DTN, while
achieving many state-of-the-art transfer results in terms of diverse operating
conditions, fault severities and fault types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Te Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenguang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongxiang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07275">
<title>Deep Triplet Ranking Networks for One-Shot Recognition. (arXiv:1804.07275v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07275</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the breakthroughs achieved by deep learning models in conventional
supervised learning scenarios, their dependence on sufficient labeled training
data in each class prevents effective applications of these deep models in
situations where labeled training instances for a subset of novel classes are
very sparse -- in the extreme case only one instance is available for each
class. To tackle this natural and important challenge, one-shot learning, which
aims to exploit a set of well labeled base classes to build classifiers for the
new target classes that have only one observed instance per class, has recently
received increasing attention from the research community. In this paper we
propose a novel end-to-end deep triplet ranking network to perform one-shot
learning. The proposed approach learns class universal image embeddings on the
well labeled base classes under a triplet ranking loss, such that the instances
from new classes can be categorized based on their similarity with the one-shot
instances in the learned embedding space. Moreover, our approach can naturally
incorporate the available one-shot instances from the new classes into the
embedding learning process to improve the triplet ranking model. We conduct
experiments on two popular datasets for one-shot learning. The results show the
proposed approach achieves better performance than the state-of-the- art
comparison methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Meng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuhong Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05565">
<title>Towards Neural Phrase-based Machine Translation. (arXiv:1706.05565v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05565</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our
method explicitly models the phrase structures in output sequences using
Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence
modeling method. To mitigate the monotonic alignment requirement of SWAN, we
introduce a new layer to perform (soft) local reordering of input sequences.
Different from existing neural machine translation (NMT) approaches, NPMT does
not use attention-based decoding mechanisms. Instead, it directly outputs
phrases in a sequential order and can decode in linear time. Our experiments
show that NPMT achieves superior performances on IWSLT 2014
German-English/English-German and IWSLT 2015 English-Vietnamese machine
translation tasks compared with strong NMT baselines. We also observe that our
method produces meaningful phrases in output languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Sen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sitao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dengyong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Li Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03958">
<title>Interdependent Gibbs Samplers. (arXiv:1804.03958v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03958</link>
<description rdf:parseType="Literal">&lt;p&gt;Gibbs sampling, as a model learning method, is known to produce the most
accurate results available in a variety of domains, and is a de facto standard
in these domains. Yet, it is also well known that Gibbs random walks usually
have bottlenecks, sometimes termed &quot;local maxima&quot;, and thus samplers often
return suboptimal solutions. In this paper we introduce a variation of the
Gibbs sampler which yields high likelihood solutions significantly more often
than the regular Gibbs sampler.
&lt;/p&gt;
&lt;p&gt;Specifically, we show that combining multiple samplers, with certain
dependence (coupling) between them, results in higher likelihood solutions.
This side-steps the well known issue of identifiability, which has been the
obstacle to combining samplers in previous work. We evaluate the approach on a
Latent Dirichlet Allocation model, and also on HMM&apos;s, where precise computation
of likelihoods and comparisons to the standard EM algorithm are possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kozdoba_M/0/1/0/all/0/1&quot;&gt;Mark Kozdoba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06216">
<title>Learning Sparse Latent Representations with the Deep Copula Information Bottleneck. (arXiv:1804.06216v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06216</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep latent variable models are powerful tools for representation learning.
In this paper, we adopt the deep information bottleneck model, identify its
shortcomings and propose a model that circumvents them. To this end, we apply a
copula transformation which, by restoring the invariance properties of the
information bottleneck method, leads to disentanglement of the features in the
latent space. Building on that, we show how this transformation translates to
sparsity of the latent space in the new model. We evaluate our method on
artificial and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wieczorek_A/0/1/0/all/0/1&quot;&gt;Aleksander Wieczorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wieser_M/0/1/0/all/0/1&quot;&gt;Mario Wieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murezzan_D/0/1/0/all/0/1&quot;&gt;Damian Murezzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roth_V/0/1/0/all/0/1&quot;&gt;Volker Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06234">
<title>Clustering Analysis on Locally Asymptotically Self-similar Processes. (arXiv:1804.06234v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06234</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we design algorithms for clustering locally asymptotically
self-similar stochastic processes. We show a sufficient condition on the
dissimilarity measure that leads to the consistency of the algorithms for
clustering offline and online data settings, respectively. As an example of
application, clustering synthetic data sampled from multifractional Brownian
motions is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peng_Q/0/1/0/all/0/1&quot;&gt;Qidi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_N/0/1/0/all/0/1&quot;&gt;Nan Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ran Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06518">
<title>Online Non-Additive Path Learning under Full and Partial Information. (arXiv:1804.06518v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06518</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the online path learning problem in a graph with non-additive
gains/losses. Various settings of full information, semi-bandit, and full
bandit are explored. We give an efficient implementation of EXP3 algorithm for
the full bandit setting with any (non-additive) gain. Then, focusing on the
large family of non-additive count-based gains, we construct an intermediate
graph which has equivalent gains that are additive. By operating on this
intermediate graph, we are able to use algorithms like Component Hedge and
ComBand for the first time for non-additive gains. Finally, we apply our
methods to the important application of ensemble structured prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cortes_C/0/1/0/all/0/1&quot;&gt;Corinna Cortes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_V/0/1/0/all/0/1&quot;&gt;Vitaly Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1&quot;&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmanian_H/0/1/0/all/0/1&quot;&gt;Holakou Rahmanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warmuth_M/0/1/0/all/0/1&quot;&gt;Manfred K. Warmuth&lt;/a&gt;</dc:creator>
</item></rdf:RDF>