<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11023"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08693"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02086"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02261"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11347"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.02257">
<title>Interoceptive robustness through environment-mediated morphological development. (arXiv:1804.02257v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02257</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically, AI researchers and roboticists try to realize intelligent behavior
in machines by tuning parameters of a predefined structure (body plan and/or
neural network architecture) using evolutionary or learning algorithms. Another
but not unrelated longstanding property of these systems is their brittleness
to slight aberrations, as highlighted by the growing deep learning literature
on adversarial examples. Here we show robustness can be achieved by evolving
the geometry of soft robots, their control systems, and how their material
properties develop in response to one particular interoceptive stimulus
(engineering stress) during their lifetimes. By doing so we realized robots
that were equally fit but more robust to extreme material defects (such as
might occur during fabrication or by damage thereafter) than robots that did
not develop during their lifetimes, or developed in response to a different
interoceptive stimulus (pressure). This suggests that the interplay between
changes in the containing systems of agents (body plan and/or neural
architecture) at different temporal scales (evolutionary and developmental)
along different modalities (geometry, material properties, synaptic weights)
and in response to different signals (interoceptive and external perception)
all dictate those agents&apos; abilities to evolve or learn capable and robust
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kriegman_S/0/1/0/all/0/1&quot;&gt;Sam Kriegman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corucci_F/0/1/0/all/0/1&quot;&gt;Francesco Corucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh C. Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02341">
<title>Compositional Obverter Communication Learning From Raw Visual Input. (arXiv:1804.02341v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02341</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the distinguishing aspects of human language is its compositionality,
which allows us to describe complex environments with limited vocabulary.
Previously, it has been shown that neural network agents can learn to
communicate in a highly structured, possibly compositional language based on
disentangled input (e.g. hand- engineered features). Humans, however, do not
learn to communicate based on well-summarized features. In this work, we train
neural agents to simultaneously develop visual perception from raw image
pixels, and learn to communicate with a sequence of discrete symbols. The
agents play an image description game where the image contains factors such as
colors and shapes. We train the agents using the obverter technique where an
agent introspects to generate messages that maximize its own understanding.
Through qualitative analysis, visualization and a zero-shot test, we show that
the agents can develop, out of raw image pixels, a language with compositional
properties, given a proper pressure from the environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1&quot;&gt;Angeliki Lazaridou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1&quot;&gt;Nando de Freitas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11023">
<title>A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management. (arXiv:1711.11023v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11023</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid
the significant effort needed to hand-craft the required dialogue flow, the
Dialogue Management (DM) module can be cast as a continuous Markov Decision
Process (MDP) and trained through Reinforcement Learning (RL). Several RL
models have been investigated over recent years. However, the lack of a common
benchmarking framework makes it difficult to perform a fair comparison between
different models and their capability to generalise to different environments.
Therefore, this paper proposes a set of challenging simulated environments for
dialogue model development and evaluation. To provide some baselines, we
investigate a number of representative parametric algorithms, namely deep
reinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and
compare them to a non-parametric model, GP-SARSA. Both the environments and
policy models are implemented using the publicly available PyDial toolkit and
released on-line, in order to establish a testbed framework for further
experiments and to facilitate experimental reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Casanueva_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;igo Casanueva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Budzianowski_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Budzianowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Pei-Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mrksic_N/0/1/0/all/0/1&quot;&gt;Nikola Mrk&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wen_T/0/1/0/all/0/1&quot;&gt;Tsung-Hsien Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ultes_S/0/1/0/all/0/1&quot;&gt;Stefan Ultes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rojas_Barahona_L/0/1/0/all/0/1&quot;&gt;Lina Rojas-Barahona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Young_S/0/1/0/all/0/1&quot;&gt;Steve Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gasic_M/0/1/0/all/0/1&quot;&gt;Milica Ga&amp;#x161;i&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02047">
<title>Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond. (arXiv:1804.02047v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.02047</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art pedestrian detection models have achieved great success in
many benchmarks. However, these models require lots of annotation information
and the labeling process usually takes much time and efforts. In this paper, we
propose a method to generate labeled pedestrian data and adapt them to support
the training of pedestrian detectors. The proposed framework is built on the
Generative Adversarial Network (GAN) with multiple discriminators, trying to
synthesize realistic pedestrians and learn the background context
simultaneously. To handle the pedestrians of different sizes, we adopt the
Spatial Pyramid Pooling (SPP) layer in the discriminator. We conduct
experiments on two benchmarks. The results show that our framework can smoothly
synthesize pedestrians on background images of variations and different levels
of details. To quantitatively evaluate our approach, we add the generated
samples into training data of the baseline pedestrian detectors and show the
synthetic images are able to improve the detectors&apos; performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1&quot;&gt;Xi Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08693">
<title>Fooling Vision and Language Models Despite Localization and Attention Mechanism. (arXiv:1709.08693v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08693</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks are known to succeed on classifiers, but it has been an
open question whether more complex vision systems are vulnerable. In this
paper, we study adversarial examples for vision and language models, which
incorporate natural language understanding and complex structures such as
attention, localization, and modular architectures. In particular, we
investigate attacks on a dense captioning model and on two visual question
answering (VQA) models. Our evaluation shows that we can generate adversarial
examples with a high success rate (i.e., &amp;gt; 90%) for these models. Our work
sheds new light on understanding adversarial attacks on vision systems which
have a language component and shows that attention, bounding box localization,
and compositional internal structures are vulnerable to adversarial attacks.
These observations will inform future work towards building effective defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaojun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1&quot;&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07280">
<title>Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. (arXiv:1711.07280v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07280</link>
<description rdf:parseType="Literal">&lt;p&gt;A robot that can carry out a natural-language instruction has been a dream
since before the Jetsons cartoon series imagined a life of leisure mediated by
a fleet of attentive robot helpers. It is a dream that remains stubbornly
distant. However, recent advances in vision and language methods have made
incredible progress in closely related areas. This is significant because a
robot interpreting a natural-language navigation instruction on the basis of
what it sees is carrying out a vision and language process that is similar to
Visual Question Answering. Both tasks can be interpreted as visually grounded
sequence-to-sequence translation problems, and many of the same methods are
applicable. To enable and encourage the application of vision and language
methods to the problem of interpreting visually-grounded navigation
instructions, we present the Matterport3D Simulator -- a large-scale
reinforcement learning environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision and language tasks, we
provide the first benchmark dataset for visually-grounded natural language
navigation in real buildings -- the Room-to-Room (R2R) dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1&quot;&gt;Peter Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1&quot;&gt;Damien Teney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruce_J/0/1/0/all/0/1&quot;&gt;Jake Bruce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Mark Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1&quot;&gt;Niko S&amp;#xfc;nderhauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1&quot;&gt;Stephen Gould&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1&quot;&gt;Anton van den Hengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02086">
<title>Hierarchical Disentangled Representations. (arXiv:1804.02086v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02086</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep latent-variable models learn representations of high-dimensional data in
an unsupervised manner. A number of recent efforts have focused on learning
representations that disentangle statistically independent axes of variation,
often by introducing suitable modifications of the objective function. We
synthesize this growing body of literature by formulating a generalization of
the evidence lower bound that explicitly represents the trade-offs between
sparsity of the latent code, bijectivity of representations, and coverage of
the support of the empirical data distribution. Our objective is also suitable
to learning hierarchical representations that disentangle blocks of variables
whilst allowing for some degree of correlations within blocks. Experiments on a
range of datasets demonstrate that learned representations contain
interpretable features, are able to learn discrete attributes, and generalize
to unseen combinations of factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Esmaeili_B/0/1/0/all/0/1&quot;&gt;Babak Esmaeili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sarthak Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narayanaswamy_S/0/1/0/all/0/1&quot;&gt;Siddharth Narayanaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paige_B/0/1/0/all/0/1&quot;&gt;Brooks Paige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meent_J/0/1/0/all/0/1&quot;&gt;Jan-Willem van de Meent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02261">
<title>Chatter Classification in Turning Using Machine Learning and Topological Data Analysis. (arXiv:1804.02261v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02261</link>
<description rdf:parseType="Literal">&lt;p&gt;Chatter identification and detection in machining processes has been an
active area of research in the past two decades. Part of the challenge in
studying chatter is that machining equations that describe its occurrence are
often nonlinear delay differential equations. The majority of the available
tools for chatter identification rely on defining a metric that captures the
characteristics of chatter, and a threshold that signals its occurrence. The
difficulty in choosing these parameters can be somewhat alleviated by utilizing
machine learning techniques. However, even with a successful classification
algorithm, the transferability of typical machine learning methods from one
data set to another remains very limited. In this paper we combine supervised
machine learning with Topological Data Analysis (TDA) to obtain a descriptor of
the process which can detect chatter. The features we use are derived from the
persistence diagram of an attractor reconstructed from the time series via
Takens embedding. We test the approach using deterministic and stochastic
turning models, where the stochasticity is introduced via the cutting
coefficient term. Our results show a 97% successful classification rate on the
deterministic model labeled by the stability diagram obtained using the
spectral element method. The features gleaned from the deterministic model are
then utilized for characterization of chatter in a stochastic turning model
where there are very limited analysis methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khasawneh_F/0/1/0/all/0/1&quot;&gt;Firas A. Khasawneh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Munch_E/0/1/0/all/0/1&quot;&gt;Elizabeth Munch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perea_J/0/1/0/all/0/1&quot;&gt;Jose A. Perea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02370">
<title>Minimal Support Vector Machine. (arXiv:1804.02370v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.02370</link>
<description rdf:parseType="Literal">&lt;p&gt;Support Vector Machine (SVM) is an efficient classification approach, which
finds a hyperplane to separate data from different classes. This hyperplane is
determined by support vectors. In existing SVM formulations, the objective
function uses L2 norm or L1 norm on slack variables. The number of support
vectors is a measure of generalization errors. In this work, we propose a
Minimal SVM, which uses L0.5 norm on slack variables. The result model further
reduces the number of support vectors and increases the classification
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Chris Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07581">
<title>Riemann-Theta Boltzmann Machine. (arXiv:1712.07581v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07581</link>
<description rdf:parseType="Literal">&lt;p&gt;A general Boltzmann machine with continuous visible and discrete integer
valued hidden states is introduced. Under mild assumptions about the connection
matrices, the probability density function of the visible units can be solved
for analytically, yielding a novel parametric density function involving a
ratio of Riemann-Theta functions. The conditional expectation of a hidden state
for given visible states can also be calculated analytically, yielding a
derivative of the logarithmic Riemann-Theta function. The conditional
expectation can be used as activation function in a feedforward neural network,
thereby increasing the modelling capacity of the network. Both the Boltzmann
machine and the derived feedforward neural network can be successfully trained
via standard gradient- and non-gradient-based optimization techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krefl_D/0/1/0/all/0/1&quot;&gt;Daniel Krefl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carrazza_S/0/1/0/all/0/1&quot;&gt;Stefano Carrazza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haghighat_B/0/1/0/all/0/1&quot;&gt;Babak Haghighat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kahlen_J/0/1/0/all/0/1&quot;&gt;Jens Kahlen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11347">
<title>Learning to Adapt: Meta-Learning for Model-Based Control. (arXiv:1803.11347v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.11347</link>
<description rdf:parseType="Literal">&lt;p&gt;Although reinforcement learning methods can achieve impressive results in
simulation, the real world presents two major challenges: generating samples is
exceedingly expensive, and unexpected perturbations can cause proficient but
narrowly-learned policies to fail at test time. In this work, we propose to
learn how to quickly and effectively adapt online to new situations as well as
to perturbations. To enable sample-efficient meta-learning, we consider
learning online adaptation in the context of model-based reinforcement
learning. Our approach trains a global model such that, when combined with
recent data, the model can be be rapidly adapted to the local context. Our
experiments demonstrate that our approach can enable simulated agents to adapt
their behavior online to novel terrains, to a crippled leg, and in
highly-dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clavera_I/0/1/0/all/0/1&quot;&gt;Ignasi Clavera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1&quot;&gt;Anusha Nagabandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fearing_R/0/1/0/all/0/1&quot;&gt;Ronald S. Fearing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item></rdf:RDF>