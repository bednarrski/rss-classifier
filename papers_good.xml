<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07040"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04054"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06935"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07004"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.07826"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05488"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02629"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.07040">
<title>The NarrativeQA Reading Comprehension Challenge. (arXiv:1712.07040v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.07040</link>
<description rdf:parseType="Literal">&lt;p&gt;Reading comprehension (RC)---in contrast to information retrieval---requires
integrating information and reasoning about events, entities, and their
relations across a full document. Question answering is conventionally used to
assess RC ability, in both artificial agents and children learning to read.
However, existing RC datasets and tasks are dominated by questions that can be
solved by selecting answers using superficial information (e.g., local context
similarity or global term frequency); they thus fail to test for the essential
integrative aspect of RC. To encourage progress on deeper comprehension of
language, we present a new dataset and set of tasks in which the reader must
answer questions about stories by reading entire books or movie scripts. These
tasks are designed so that successfully answering their questions requires
understanding the underlying narrative rather than relying on shallow pattern
matching or salience. We show that although humans solve the tasks easily,
standard RC models struggle on the tasks presented here. We provide an analysis
of the dataset and the challenges it presents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocisky_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Ko&amp;#x10d;isk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_J/0/1/0/all/0/1&quot;&gt;Jonathan Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1&quot;&gt;Phil Blunsom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1&quot;&gt;Chris Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermann_K/0/1/0/all/0/1&quot;&gt;Karl Moritz Hermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melis_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe1;bor Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1&quot;&gt;Edward Grefenstette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04054">
<title>Shifting Mean Activation Towards Zero with Bipolar Activation Functions. (arXiv:1709.04054v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04054</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple extension to the ReLU-family of activation functions that
allows them to shift the mean activation across a layer towards zero. Combined
with proper weight initialization, this alleviates the need for normalization
layers. We explore the training of deep vanilla recurrent neural networks
(RNNs) with up to 144 layers, and show that bipolar activation functions help
learning in this setting. On the Penn Treebank and Text8 language modeling
tasks we obtain competitive results, improving on the best reported results for
non-gated networks. In experiments with convolutional neural networks without
batch normalization, we find that bipolar activations produce a faster drop in
training error, and results in a lower test error on the CIFAR-10
classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eidnes_L/0/1/0/all/0/1&quot;&gt;Lars Eidnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nokland_A/0/1/0/all/0/1&quot;&gt;Arild N&amp;#xf8;kland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06935">
<title>Mining Smart Card Data for Travelers&apos; Mini Activities. (arXiv:1712.06935v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.06935</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of public transport modeling and simulation, we address the
problem of mismatch between simulated transit trips and observed ones. We point
to the weakness of the current travel demand modeling process; the trips it
generates are over-optimistic and do not reflect the real passenger choices. We
introduce the notion of mini activities the travelers do during the trips; they
can explain the deviation of simulated trips from the observed trips. We
propose to mine the smart card data to extract the mini activities. We develop
a technique to integrate them in the generated trips and learn such an
integration from two available sources, the trip history and trip planner
recommendations. For an input travel demand, we build a Markov chain over the
trip collection and apply the Monte Carlo Markov Chain algorithm to integrate
mini activities in such a way that the selected characteristics converge to the
desired distributions. We test our method in different settings on the
passenger trip collection of Nancy, France. We report experimental results
demonstrating a very important mismatch reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1&quot;&gt;Boris Chidlovskii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07004">
<title>Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case Study. (arXiv:1712.07004v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.07004</link>
<description rdf:parseType="Literal">&lt;p&gt;Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram
features when learning from textual data. They are also compatible with the use
of word embeddings so that word similarities can be accounted for. While the
original any-gram kernels are implemented on top of tree kernels, we propose a
new approach which is independent of tree kernels and is more efficient. We
also propose a more effective way to make use of word embeddings than the
original any-gram formulation. When applied to the task of sentiment
classification, our new formulation achieves significantly better performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaljahi_R/0/1/0/all/0/1&quot;&gt;Rasoul Kaljahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1&quot;&gt;Jennifer Foster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.07826">
<title>Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations. (arXiv:1702.07826v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1702.07826</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce AI rationalization, an approach for generating explanations of
autonomous system behavior as if a human had performed the behavior. We
describe a rationalization technique that uses neural machine translation to
translate internal state-action representations of an autonomous agent into
natural language. We evaluate our technique in the Frogger game environment,
training an autonomous game playing agent to rationalize its action choices
using natural language. A natural language training corpus is collected from
human players thinking out loud as they play the game. We motivate the use of
rationalization as an approach to explanation generation and show the results
of two experiments evaluating the effectiveness of rationalization. Results of
these evaluations show that neural machine translation is able to accurately
generate rationalizations that describe agent behavior, and that
rationalizations are more satisfying to humans than other alternative methods
of explanation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsan_U/0/1/0/all/0/1&quot;&gt;Upol Ehsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_B/0/1/0/all/0/1&quot;&gt;Brent Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1&quot;&gt;Larry Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1&quot;&gt;Mark O. Riedl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07008">
<title>Privacy-Preserving Adversarial Networks. (arXiv:1712.07008v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1712.07008</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a data-driven framework for optimizing privacy-preserving data
release mechanisms toward the information-theoretically optimal tradeoff
between minimizing distortion of useful data and concealing sensitive
information. Our approach employs adversarially-trained neural networks to
implement randomized mechanisms and to perform a variational approximation of
mutual information privacy. We empirically validate our Privacy-Preserving
Adversarial Networks (PPAN) framework with experiments conducted on discrete
and continuous synthetic data, as well as the MNIST handwritten digits dataset.
With the synthetic data, we find that our model-agnostic PPAN approach achieves
tradeoff points very close to the optimal tradeoffs that are
analytically-derived from model knowledge. In experiments with the MNIST data,
we visually demonstrate a learned tradeoff between minimizing the pixel-level
distortion versus concealing the written digit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripathy_A/0/1/0/all/0/1&quot;&gt;Ardhendu Tripathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishwar_P/0/1/0/all/0/1&quot;&gt;Prakash Ishwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07107">
<title>Adversarial Examples: Attacks and Defenses for Deep Learning. (arXiv:1712.07107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.07107</link>
<description rdf:parseType="Literal">&lt;p&gt;With rapid progress and great successes in a wide spectrum of applications,
deep learning is being applied in many safety-critical environments. However,
deep neural networks have been recently found vulnerable to well-designed input
samples, called \textit{adversarial examples}. Adversarial examples are
imperceptible to human but can easily fool deep neural networks in the
testing/deploying stage. The vulnerability to adversarial examples becomes one
of the major risks for applying deep neural networks in safety-critical
scenarios. Therefore, the attacks and defenses on adversarial examples draw
great attention.
&lt;/p&gt;
&lt;p&gt;In this paper, we review recent findings on adversarial examples against deep
neural networks, summarize the methods for generating adversarial examples, and
propose a taxonomy of these methods. Under the taxonomy, applications and
countermeasures for adversarial examples are investigated. We further elaborate
on adversarial examples and explore the challenges and the potential solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qile Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1&quot;&gt;Rajendra Rana Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07113">
<title>Query-Efficient Black-box Adversarial Examples. (arXiv:1712.07113v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.07113</link>
<description rdf:parseType="Literal">&lt;p&gt;Current neural network-based image classifiers are susceptible to adversarial
examples, even in the black-box setting, where the attacker is limited to query
access without access to gradients. Previous methods --- substitute networks
and coordinate-based finite-difference methods --- are either unreliable or
query-inefficient, making these methods impractical for certain problems.
&lt;/p&gt;
&lt;p&gt;We introduce a new method for reliably generating adversarial examples under
more restricted, practical black-box threat models. First, we apply natural
evolution strategies to perform black-box attacks using two to three orders of
magnitude fewer queries than previous methods. Second, we introduce a new
algorithm to perform targeted adversarial attacks in the partial-information
setting, where the attacker only has access to a limited number of target
classes. Using these techniques, we successfully perform the first targeted
adversarial attack against a commercially deployed machine learning system, the
Google Cloud Vision API, in the partial information setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilyas_A/0/1/0/all/0/1&quot;&gt;Andrew Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engstrom_L/0/1/0/all/0/1&quot;&gt;Logan Engstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_A/0/1/0/all/0/1&quot;&gt;Anish Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jessy Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05488">
<title>A Geometric View of Optimal Transportation and Generative Model. (arXiv:1710.05488v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05488</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we show the intrinsic relations between optimal transportation
and convex geometry, especially the variational approach to solve Alexandrov
problem: constructing a convex polytope with prescribed face normals and
volumes. This leads to a geometric interpretation to generative models, and
leads to a novel framework for generative models. By using the optimal
transportation view of GAN model, we show that the discriminator computes the
Kantorovich potential, the generator calculates the transportation map. For a
large class of transportation costs, the Kantorovich potential can give the
optimal transportation map by a close-form formula. Therefore, it is sufficient
to solely optimize the discriminator. This shows the adversarial competition
can be avoided, and the computational architecture can be simplified.
Preliminary experimental results show the geometric method outperforms WGAN for
approximating probability measures with multiple clusters in low dimensional
space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1&quot;&gt;Na Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kehua Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Li Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yau_S/0/1/0/all/0/1&quot;&gt;Shing-Tung Yau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_D/0/1/0/all/0/1&quot;&gt;David Xianfeng Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01558">
<title>Wasserstein Auto-Encoders. (arXiv:1711.01558v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01558</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building
a generative model of the data distribution. WAE minimizes a penalized form of
the Wasserstein distance between the model distribution and the target
distribution, which leads to a different regularizer than the one used by the
Variational Auto-Encoder (VAE). This regularizer encourages the encoded
training distribution to match the prior. We compare our algorithm with several
other techniques and show that it is a generalization of adversarial
auto-encoders (AAE). Our experiments show that WAE shares many of the
properties of VAEs (stable training, encoder-decoder architecture, nice latent
manifold structure) while generating samples of better quality, as measured by
the FID score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tolstikhin_I/0/1/0/all/0/1&quot;&gt;Ilya Tolstikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bousquet_O/0/1/0/all/0/1&quot;&gt;Olivier Bousquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gelly_S/0/1/0/all/0/1&quot;&gt;Sylvain Gelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoelkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Schoelkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02629">
<title>Differentially Private Variational Dropout. (arXiv:1712.02629v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02629</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks with their large number of parameters are highly
flexible learning systems. The high flexibility in such networks brings with
some serious problems such as overfitting, and regularization is used to
address this problem. A currently popular and effective regularization
technique for controlling the overfitting is dropout. Often, large data
collections required for neural networks contain sensitive information such as
the medical histories of patients, and the privacy of the training data should
be protected. In this paper, we modify the recently proposed variational
dropout technique which provided an elegant Bayesian interpretation to dropout,
and show that the intrinsic noise in the variational dropout can be exploited
to obtain a degree of differential privacy. The iterative nature of training
neural networks presents a challenge for privacy-preserving estimation since
multiple iterations increase the amount of noise added. We overcome this by
using a relaxed notion of differential privacy, called concentrated
differential privacy, which provides tighter estimates on the overall privacy
loss. We demonstrate the accuracy of our privacy-preserving variational dropout
algorithm on benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermis_B/0/1/0/all/0/1&quot;&gt;Beyza Ermis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cemgil_A/0/1/0/all/0/1&quot;&gt;Ali Taylan Cemgil&lt;/a&gt;</dc:creator>
</item></rdf:RDF>