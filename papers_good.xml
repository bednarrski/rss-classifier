<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10009"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10120"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10134"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10393"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10259"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10260"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05163"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09830"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09902"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.09996">
<title>Learning End-to-End Goal-Oriented Dialog with Multiple Answers. (arXiv:1808.09996v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09996</link>
<description rdf:parseType="Literal">&lt;p&gt;In a dialog, there can be multiple valid next utterances at any point. The
present end-to-end neural methods for dialog do not take this into account.
They learn with the assumption that at any time there is only one correct next
utterance. In this work, we focus on this problem in the goal-oriented dialog
setting where there are different paths to reach a goal. We propose a new
method, that uses a combination of supervised learning and reinforcement
learning approaches to address this issue. We also propose a new and more
effective testbed, permuted-bAbI dialog tasks, by introducing multiple valid
next utterances to the original-bAbI dialog tasks, which allows evaluation of
goal-oriented dialog systems in a more realistic setting. We show that there is
a significant drop in performance of existing end-to-end neural methods from
81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on
permuted-bAbI dialog tasks. We also show that our proposed method improves the
performance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1&quot;&gt;Janarthanan Rajendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganhotra_J/0/1/0/all/0/1&quot;&gt;Jatin Ganhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satinder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polymenakos_L/0/1/0/all/0/1&quot;&gt;Lazaros Polymenakos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10009">
<title>Learning a Policy for Opportunistic Active Learning. (arXiv:1808.10009v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.10009</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning identifies data points to label that are expected to be the
most useful in improving a supervised model. Opportunistic active learning
incorporates active learning into interactive tasks that constrain possible
queries during interactions. Prior work has shown that opportunistic active
learning can be used to improve grounding of natural language descriptions in
an interactive object retrieval task. In this work, we use reinforcement
learning for such an object retrieval task, to learn a policy that effectively
trades off task completion with model improvement that would benefit future
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1&quot;&gt;Aishwarya Padmakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1&quot;&gt;Raymond J. Mooney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10120">
<title>ExpIt-OOS: Towards Learning from Planning in Imperfect Information Games. (arXiv:1808.10120v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10120</link>
<description rdf:parseType="Literal">&lt;p&gt;The current state of the art in playing many important perfect information
games, including Chess and Go, combines planning and deep reinforcement
learning with self-play. We extend this approach to imperfect information games
and present ExIt-OOS, a novel approach to playing imperfect information games
within the Expert Iteration framework and inspired by AlphaZero. We use Online
Outcome Sampling, an online search algorithm for imperfect information games in
place of MCTS. While training online, our neural strategy is used to improve
the accuracy of playouts in OOS, allowing a learning and planning feedback loop
for imperfect information games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitchen_A/0/1/0/all/0/1&quot;&gt;Andy Kitchen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benedetti_M/0/1/0/all/0/1&quot;&gt;Michela Benedetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10134">
<title>Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and Learning based Vehicle Longitude Dynamic Calibrating Algorithm. (arXiv:1808.10134v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.10134</link>
<description rdf:parseType="Literal">&lt;p&gt;For any autonomous driving vehicle, control module determines its road
performance and safety, i.e. its precision and stability should stay within a
carefully-designed range. Nonetheless, control algorithms require vehicle
dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are
obscure to calibrate in real time. As a result, to achieve reasonable
performance, most, if not all, research-oriented autonomous vehicles do manual
calibrations in a one-by-one fashion. Since manual calibration is not
sustainable once entering into mass production stage for industrial purposes,
we here introduce a machine-learning based auto-calibration system for
autonomous driving vehicles. In this paper, we will show how we build a
data-driven longitudinal calibration procedure using machine learning
techniques. We first generated offline calibration tables from human driving
data. The offline table serves as an initial guess for later uses and it only
needs twenty-minutes data collection and process. We then used an
online-learning algorithm to appropriately update the initial table (the
offline table) based on real-time performance analysis. This longitudinal
auto-calibration system has been deployed to more than one hundred Baidu Apollo
self-driving vehicles (including hybrid family vehicles and electronic
delivery-only vehicles) since April 2018. By August 27, 2018, it had been
tested for more than two thousands hours, ten thousands kilometers (6,213
miles) and yet proven to be effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dingfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Qi Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10393">
<title>Learning End-to-end Autonomous Driving using Guided Auxiliary Supervision. (arXiv:1808.10393v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10393</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to drive faithfully in highly stochastic urban settings remains an
open problem. To that end, we propose a Multi-task Learning from Demonstration
(MT-LfD) framework which uses supervised auxiliary task prediction to guide the
main task of predicting the driving commands. Our framework involves an
end-to-end trainable network for imitating the expert demonstrator&apos;s driving
commands. The network intermediately predicts visual affordances and action
primitives through direct supervision which provide the aforementioned
auxiliary supervised guidance. We demonstrate that such joint learning and
supervised guidance facilitates hierarchical task decomposition, assisting the
agent to learn faster, achieve better driving performance and increases
transparency of the otherwise black-box end-to-end network. We run our
experiments to validate the MT-LfD framework in CARLA, an open-source urban
driving simulator. We introduce multiple non-player agents in CARLA and induce
temporal noise in them for realistic stochasticity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Ashish Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1&quot;&gt;Adithya Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1&quot;&gt;Anbumani Subramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09888">
<title>KDSL: a Knowledge-Driven Supervised Learning Framework for Word Sense Disambiguation. (arXiv:1808.09888v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09888</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose KDSL, a new word sense disambiguation (WSD) framework that
utilizes knowledge to automatically generate sense-labeled data for supervised
learning. First, from WordNet, we automatically construct a semantic knowledge
base called DisDict, which provides refined feature words that highlight the
differences among word senses, i.e., synsets. Second, we automatically generate
new sense-labeled data by DisDict from unlabeled corpora. Third, these
generated data, together with manually labeled data and unlabeled data, are fed
to a neural framework conducting supervised and unsupervised learning jointly
to model the semantic relations among synsets, feature words and their
contexts. The experimental results show that KDSL outperforms several
representative state-of-the-art methods on various major benchmarks.
Interestingly, it performs relatively well even when manually labeled data is
unavailable, thus provides a new promising backoff strategy for WSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1&quot;&gt;Shi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianmin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruili Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10259">
<title>Analyze Unstructured Data Patterns for Conceptual Representation. (arXiv:1808.10259v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.10259</link>
<description rdf:parseType="Literal">&lt;p&gt;Online news media provides aggregated news and stories from different sources
all over the world and up-to-date news coverage. The main goal of this study is
to have a solution that considered as a homogeneous source for the news and to
represent the news in a new conceptual framework. Furthermore, the user can
easily find different updated news in a fast way through the designed
interface. The Mobile App implementation is based on modeling the multi-level
conceptual analysis discipline. Discovering main concepts of any domain is
captured from the hidden unstructured data that are analyzed by the proposed
solution. Concepts are discovered through analyzing data patterns to be
structured into a tree-based interface for easy navigation for the end user,
through the discovered news concepts. Our final experiment results showing that
analyzing the news before displaying to the end-user and restructuring the
final output in a conceptual multilevel structure, that producing new display
frame for the end user to find the related information to his interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aqle_A/0/1/0/all/0/1&quot;&gt;Aboubakr Aqle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Thani_D/0/1/0/all/0/1&quot;&gt;Dena Al-Thani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaoua_A/0/1/0/all/0/1&quot;&gt;Ali Jaoua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10260">
<title>Understanding Latent Factors Using a GWAP. (arXiv:1808.10260v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.10260</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems relying on latent factor models often appear as black
boxes to their users. Semantic descriptions for the factors might help to
mitigate this problem. Achieving this automatically is, however, a
non-straightforward task due to the models&apos; statistical nature. We present an
output-agreement game that represents factors by means of sample items and
motivates players to create such descriptions. A user study shows that the
collected output actually reflects real-world characteristics of the factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunkel_J/0/1/0/all/0/1&quot;&gt;Johannes Kunkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loepp_B/0/1/0/all/0/1&quot;&gt;Benedikt Loepp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziegler_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Ziegler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10307">
<title>Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation. (arXiv:1808.10307v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1808.10307</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have consistently outperformed traditional machine
learning models in various classification tasks, including image
classification. As such, they have become increasingly prevalent in many real
world applications including those where security is of great concern. Such
popularity, however, may attract attackers to exploit the vulnerabilities of
the deployed deep learning models and launch attacks against security-sensitive
applications. In this paper, we focus on a specific type of data poisoning
attack, which we refer to as a {\em backdoor injection attack}. The main goal
of the adversary performing such attack is to generate and inject a backdoor
into a deep learning model that can be triggered to recognize certain embedded
patterns with a target label of the attacker&apos;s choice. Additionally, a backdoor
injection attack should occur in a stealthy manner, without undermining the
efficacy of the victim model. Specifically, we propose two approaches for
generating a backdoor that is hardly perceptible yet effective in poisoning the
model. We consider two attack settings, with backdoor injection carried out
either before model training or during model updating. We carry out extensive
experimental evaluations under various assumptions on the adversary model, and
demonstrate that such attacks can be effective and achieve a high attack
success rate (above $90\%$) at a small cost of model accuracy loss (below
$1\%$) with a small injection rate (around $1\%$), even under the weakest
assumption wherein the adversary has no knowledge either of the original
training data or the classifier model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Cong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Haoti Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Squicciarini_A/0/1/0/all/0/1&quot;&gt;Anna Squicciarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Sencun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1&quot;&gt;David Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10350">
<title>IEA: Inner Ensemble Average within a convolutional neural network. (arXiv:1808.10350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10350</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensemble learning is a method of combining multiple trained models to improve
the model accuracy. We introduce the usage of such methods, specifically
ensemble average inside Convolutional Neural Networks (CNNs) architectures. By
Inner Average Ensemble (IEA) of multiple convolutional neural layers (CNLs)
replacing the single CNLs inside the CNN architecture, the accuracy of the CNN
increased. A visual and a similarity score analysis of the features generated
from IEA explains why it boosts the model performance. Empirical results using
different benchmarking datasets and well-known deep model architectures shows
that IEA outperforms the ordinary CNL used in CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abduallah A. Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claudel_C/0/1/0/all/0/1&quot;&gt;Christian Claudel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08294">
<title>Learning Context-Sensitive Convolutional Filters for Text Processing. (arXiv:1709.08294v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08294</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinghan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1&quot;&gt;Martin Renqiang Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yitong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00047">
<title>Matrix completion with deterministic pattern - a geometric perspective. (arXiv:1802.00047v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00047</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the matrix completion problem with a deterministic pattern of
observed entries. In this setting, we aim to answer the question: under what
condition there will be (at least locally) unique solution to the matrix
completion problem, i.e., the underlying true matrix is identifiable. We answer
the question from a certain point of view and outline a geometric perspective.
We give an algebraically verifiable sufficient condition, which we call the
well-posedness condition, for the local uniqueness of MRMC solutions. We argue
that this condition is necessary for local stability of MRMC solutions, and we
show that the condition is generic using the characteristic rank. We also argue
that the low-rank approximation approaches are more stable than MRMC and
further propose a sequential statistical testing procedure to determine the
&quot;true&quot; rank from observed entries. Finally, we provide numerical examples aimed
at verifying validity of the presented theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_A/0/1/0/all/0/1&quot;&gt;Alexander Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05163">
<title>A Simple but Hard-to-Beat Baseline for Session-based Recommendations. (arXiv:1808.05163v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05163</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) models have been recently introduced in
the domain of top-$N$ session-based recommendations. An ordered collection of
past items the user has interacted with in a session (or sequence) are embedded
into a 2-dimensional latent matrix, and treated as an image. The convolution
and pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we propose
a simple, but very effective generative model that is capable of learning
high-level representation from both short- and long-range dependencies. The
network architecture of the proposed model is formed of a stack of holed
convolutional layers, which can efficiently increase the receptive fields
without relying on the pooling operation. Another contribution is the effective
use of residual block structure in recommender systems, which can ease the
optimization for much deeper networks. The proposed generative model attains
state-of-the-art accuracy with less training time in the session-based
recommendation task. It accordingly can be used as a powerful session-based
recommendation baseline to beat in future, especially when there are long
sequences of user feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1&quot;&gt;Fajie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzoglou_A/0/1/0/all/0/1&quot;&gt;Alexandros Karatzoglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arapakis_I/0/1/0/all/0/1&quot;&gt;Ioannis Arapakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1&quot;&gt;Joemon M Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06791">
<title>LRMM: Learning to Recommend with Missing Modalities. (arXiv:1808.06791v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06791</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning has shown promising performance in content-based
recommendation due to the auxiliary user and item information of multiple
modalities such as text and images. However, the problem of incomplete and
missing modality is rarely explored and most existing methods fail in learning
a recommendation model with missing or corrupted modalities. In this paper, we
propose LRMM, a novel framework that mitigates not only the problem of missing
modalities but also more generally the cold-start problem of recommender
systems. We propose modality dropout (m-drop) and a multimodal sequential
autoencoder (m-auto) to learn multimodal representations for complementing and
imputing missing modalities. Extensive experiments on real-world Amazon data
show that LRMM achieves state-of-the-art performance on rating prediction
tasks. More importantly, LRMM is more robust to previous methods in alleviating
data-sparsity and the cold-start problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08316">
<title>A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08316</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring entity relatedness is a fundamental task for many natural language
processing and information retrieval applications. Prior work often studies
entity relatedness in static settings and an unsupervised manner. However,
entities in real-world are often involved in many different relationships,
consequently entity-relations are very dynamic over time. In this work, we
propose a neural networkbased approach for dynamic entity relatedness,
leveraging the collective attention as supervision. Our model is capable of
learning rich and different entity representations in a joint framework.
Through extensive experiments on large-scale datasets, we demonstrate that our
method achieves better results than competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tuan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1&quot;&gt;Wolfgang Nejdl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09830">
<title>Searching Toward Pareto-Optimal Device-Aware Neural Architectures. (arXiv:1808.09830v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09830</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in Neural Architectural Search (NAS) have achieved
state-of-the-art performance in many tasks such as image classification and
language understanding. However, most existing works only optimize for model
accuracy and largely ignore other important factors imposed by the underlying
hardware and devices, such as latency and energy, when making inference. In
this paper, we first introduce the problem of NAS and provide a survey on
recent works. Then we deep dive into two recent advancements on extending NAS
into multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net
are capable of optimizing accuracy and other objectives imposed by devices,
searching for neural architectures that can be best deployed on a wide spectrum
of devices: from embedded systems and mobile devices to workstations.
Experimental results are poised to show that architectures found by MONAS and
DPP-Net achieves Pareto optimality w.r.t the given objectives for various
devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1&quot;&gt;An-Chieh Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jin-Dong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chi-Hung Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shu-Huan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Min Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Chieh Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jia-Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1&quot;&gt;Da-Cheng Juan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09902">
<title>Extreme Value Theory for Open Set Classification - GPD and GEV Classifiers. (arXiv:1808.09902v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09902</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification tasks usually assume that all possible classes are present
during the training phase. This is restrictive if the algorithm is used over a
long time and possibly encounters samples from unknown classes. The recently
introduced extreme value machine, a classifier motivated by extreme value
theory, addresses this problem and achieves competitive performance in specific
cases. We show that this algorithm can fail when the geometries of known and
unknown classes differ. To overcome this problem, we propose two new algorithms
relying on approximations from extreme value theory. We show the effectiveness
of our classifiers in simulations and on the LETTER and MNIST data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vignotto_E/0/1/0/all/0/1&quot;&gt;Edoardo Vignotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Engelke_S/0/1/0/all/0/1&quot;&gt;Sebastian Engelke&lt;/a&gt;</dc:creator>
</item></rdf:RDF>