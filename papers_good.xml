<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05512"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05695"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.07981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05438"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05556"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05630"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05654"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05790"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.04114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09511"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.05512">
<title>Data Clustering using a Hybrid of Fuzzy C-Means and Quantum-behaved Particle Swarm Optimization. (arXiv:1712.05512v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.05512</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzy clustering has become a widely used data mining technique and plays an
important role in grouping, traversing and selectively using data for user
specified applications. The deterministic Fuzzy C-Means (FCM) algorithm may
result in suboptimal solutions when applied to multidimensional data in
real-world, time-constrained problems. In this paper the Quantum-behaved
Particle Swarm Optimization (QPSO) with a fully connected topology is coupled
with the Fuzzy C-Means Clustering algorithm and is tested on a suite of
datasets from the UCI Machine Learning Repository. The global search ability of
the QPSO algorithm helps in avoiding stagnation in local optima while the soft
clustering approach of FCM helps to partition data based on membership
probabilities. Clustering performance indices such as F-Measure, Accuracy,
Quantization Error, Intercluster and Intracluster distances are reported for
competitive techniques such as PSO K-Means, QPSO K-Means and QPSO FCM over all
datasets considered. Experimental results indicate that QPSO FCM provides
comparable and in most cases superior results when compared to the others.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Saptarshi Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basak_S/0/1/0/all/0/1&quot;&gt;Sanchita Basak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_R/0/1/0/all/0/1&quot;&gt;Richard Alan Peters II&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05695">
<title>Lightweight Neural Networks. (arXiv:1712.05695v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the weights in a Lightweight Neural Network have a value of zero,
while the remaining ones are either +1 or -1. These universal approximators
require approximately 1.1 bits/weight of storage, posses a quick forward pass
and achieve classification accuracies similar to conventional continuous-weight
networks. Their training regimen focuses on error reduction initially, but
later emphasizes discretization of weights. They ignore insignificant inputs,
remove unnecessary weights, and drop unneeded hidden neurons. We have
successfully tested them on the MNIST, credit card fraud, and credit card
defaults data sets using networks having 2 to 16 hidden layers and up to 4.4
million weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Altaf H. Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06673">
<title>Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational Compositional Operators for Analogy Detection. (arXiv:1709.06673v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06673</link>
<description rdf:parseType="Literal">&lt;p&gt;Representing the semantic relations that exist between two given words (or
entities) is an important first step in a wide-range of NLP applications such
as analogical reasoning, knowledge base completion and relational information
retrieval. A simple, yet surprisingly accurate method for representing a
relation between two words is to compute the vector offset (\PairDiff) between
their corresponding word embeddings. Despite the empirical success, it remains
unclear as to whether \PairDiff is the best operator for obtaining a relational
representation from word embeddings. We conduct a theoretical analysis of
generalised bilinear operators that can be used to measure the $\ell_{2}$
relational distance between two word-pairs. We show that, if the word
embeddings are standardised and uncorrelated, such an operator will be
independent of bilinear terms, and can be simplified to a linear form, where
\PairDiff is a special case. For numerous word embedding types, we empirically
verify the uncorrelation assumption, demonstrating the general applicability of
our theoretical result. Moreover, we experimentally discover \PairDiff from the
bilinear relation composition operator on several benchmark analogy datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakami_H/0/1/0/all/0/1&quot;&gt;Huda Hakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1&quot;&gt;Danushka Bollegala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohei_H/0/1/0/all/0/1&quot;&gt;Hayashi Kohei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05136">
<title>Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05136</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently on sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1&quot;&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kappel_D/0/1/0/all/0/1&quot;&gt;David Kappel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1&quot;&gt;Wolfgang Maass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legenstein_R/0/1/0/all/0/1&quot;&gt;Robert Legenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05403">
<title>Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis. (arXiv:1712.05403v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.05403</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a
given document with respect to a given aspect entity. While neural network
architectures have been successful in predicting the overall polarity of
sentences, aspect-specific sentiment analysis still remains as an open problem.
In this paper, we propose a novel method for integrating aspect information
into the neural model. More specifically, we incorporate aspect information
into the neural model by modeling word-aspect relationships. Our novel model,
\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative
relationships between sentence words and aspect which allows our model to
adaptively focus on the correct words given an aspect term. This ameliorates
the flaws of other state-of-the-art models that utilize naive concatenations to
model word-aspect similarity. Instead, our model adopts circular convolution
and circular correlation to model the similarity between aspect and words and
elegantly incorporates this within a differentiable neural attention framework.
Finally, our model is end-to-end differentiable and highly related to
convolution-correlation (holographic like) memories. Our proposed neural model
achieves state-of-the-art performance on benchmark datasets, outperforming
ATAE-LSTM by $4\%-5\%$ on average across multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siu Cheung Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05474">
<title>AI2-THOR: An Interactive 3D Environment for Visual AI. (arXiv:1712.05474v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.05474</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce The House Of inteRactions (THOR), a framework for visual AI
research, available at &lt;a href=&quot;http://ai2thor.allenai.org.&quot;&gt;this http URL&lt;/a&gt; AI2-THOR consists of near
photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes
and interact with objects to perform tasks. AI2-THOR enables research in many
different domains including but not limited to deep reinforcement learning,
imitation learning, learning by interaction, planning, visual question
answering, unsupervised representation learning, object detection and
segmentation, and learning models of cognition. The goal of AI2-THOR is to
facilitate building visually intelligent models and push the research forward
in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolve_E/0/1/0/all/0/1&quot;&gt;Eric Kolve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1&quot;&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_D/0/1/0/all/0/1&quot;&gt;Daniel Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05497">
<title>What Can This Robot Do? Learning from Appearance and Experiments. (arXiv:1712.05497v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.05497</link>
<description rdf:parseType="Literal">&lt;p&gt;When presented with an unknown robot (subject) how can an autonomous agent
(learner) figure out what this new robot can do? The subject&apos;s appearance can
provide cues to its physical as well as cognitive capabilities. Seeing a
humanoid can make one wonder if it can kick balls, climb stairs or recognize
faces. What if the learner can request the subject to perform these tasks? We
present an approach to make the learner build a model of the subject at a task
based on the latter&apos;s appearance and refine it by experimentation. Apart from
the subject&apos;s inherent capabilities, certain extrinsic factors may affect its
performance at a task. Based on the subject&apos;s appearance and prior knowledge
about the task a learner can identify a set of potential factors, a subset of
which we assume are controllable. Our approach picks values of controllable
factors to generate the most informative experiments to test the subject at.
Additionally, we present a metric to determine if a factor should be
incorporated in the model. We present results of our approach on modeling a
humanoid robot at the task of kicking a ball. Firstly, we show that actively
picking values for controllable factors, even in noisy experiments, leads to
faster learning of the subject&apos;s model for the task. Secondly, starting from a
minimal set of factors our metric identifies the set of relevant factors to
incorporate in the model. Lastly, we show that the refined model better
represents the subject&apos;s performance at the task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadke_A/0/1/0/all/0/1&quot;&gt;Ashwin Khadke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1&quot;&gt;Manuela Veloso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05514">
<title>Inverse Reinforce Learning with Nonparametric Behavior Clustering. (arXiv:1712.05514v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.05514</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse Reinforcement Learning (IRL) is the task of learning a single reward
function given a Markov Decision Process (MDP) without defining the reward
function, and a set of demonstrations generated by humans/experts. However, in
practice, it may be unreasonable to assume that human behaviors can be
explained by one reward function since they may be inherently inconsistent.
Also, demonstrations may be collected from various users and aggregated to
infer and predict user&apos;s behaviors. In this paper, we introduce the
Non-parametric Behavior Clustering IRL algorithm to simultaneously cluster
demonstrations and learn multiple reward functions from demonstrations that may
be generated from more than one behaviors. Our method is iterative: It
alternates between clustering demonstrations into different behavior clusters
and inverse learning the reward functions until convergence. It is built upon
the Expectation-Maximization formulation and non-parametric clustering in the
IRL setting. Further, to improve the computation efficiency, we remove the need
of completely solving multiple IRL problems for multiple clusters during the
iteration steps and introduce a resampling technique to avoid generating too
many unlikely clusters. We demonstrate the convergence and efficiency of the
proposed method through learning multiple driver behaviors from demonstrations
generated from a grid-world environment and continuous trajectories collected
from autonomous robot cars using the Gazebo robot simulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1&quot;&gt;Siddharthan Rajasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05558">
<title>CoDraw: Visual Dialog for Collaborative Drawing. (arXiv:1712.05558v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.05558</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a goal-driven collaborative task that contains
vision, language, and action in a virtual environment as its core components.
Specifically, we develop a collaborative `Image Drawing&apos; game between two
agents, called CoDraw. Our game is grounded in a virtual world that contains
movable clip art objects. Two players, Teller and Drawer, are involved. The
Teller sees an abstract scene containing multiple clip arts in a semantically
meaningful configuration, while the Drawer tries to reconstruct the scene on an
empty canvas using available clip arts. The two players communicate via two-way
communication using natural language. We collect the CoDraw dataset of ~10K
dialogs consisting of 138K messages exchanged between a Teller and a Drawer
from Amazon Mechanical Turk (AMT). We analyze our dataset and present three
models to model the players&apos; behaviors, including an attention model to
describe and draw multiple clip arts at each round. The attention models are
quantitatively compared to the other models to show how the conventional
approaches work for this new task. We also present qualitative visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin-Hwa Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1&quot;&gt;Devi Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05734">
<title>Information Processing by Networks of Quantum Decision Makers. (arXiv:1712.05734v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/1712.05734</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest a model of a multi-agent society of decision makers taking
decisions being based on two criteria, one is the utility of the prospects and
the other is the attractiveness of the considered prospects. The model is the
generalization of quantum decision theory, developed earlier for single
decision makers realizing one-step decisions, in two principal aspects. First,
several decision makers are considered simultaneously, who interact with each
other through information exchange. Second, a multistep procedure is treated,
when the agents exchange information many times. Several decision makers
exchanging information and forming their judgement, using quantum rules, form a
kind of a quantum information network, where collective decisions develop in
time as a result of information exchange. In addition to characterizing
collective decisions that arise in human societies, such networks can describe
dynamical processes occurring in artificial quantum intelligence composed of
several parts or in a cluster of quantum computers. The practical usage of the
theory is illustrated on the dynamic disjunction effect for which three
quantitative predictions are made: (i) the probabilistic behavior of decision
makers at the initial stage of the process is described; (ii) the decrease of
the difference between the initial prospect probabilities and the related
utility factors is proved; (iii) the existence of a common consensus after
multiple exchange of information is predicted. The predicted numerical values
are in very good agreement with empirical data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yukalov_V/0/1/0/all/0/1&quot;&gt;V.I. Yukalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yukalova_E/0/1/0/all/0/1&quot;&gt;E.P. Yukalova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sornette_D/0/1/0/all/0/1&quot;&gt;D. Sornette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05785">
<title>Sentiment Predictability for Stocks. (arXiv:1712.05785v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.05785</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present our findings and experiments for stock-market
prediction using various textual sentiment analysis tools, such as mood
analysis and event extraction, as well as prediction models, such as LSTMs and
specific convolutional architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prosky_J/0/1/0/all/0/1&quot;&gt;Jordan Prosky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xingyou Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1&quot;&gt;Andrew Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Michael Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.07981">
<title>The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns. (arXiv:1604.07981v3 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/1604.07981</link>
<description rdf:parseType="Literal">&lt;p&gt;Characterising tractable fragments of the constraint satisfaction problem
(CSP) is an important challenge in theoretical computer science and artificial
intelligence. Forbidding patterns (generic sub-instances) provides a means of
defining CSP fragments which are neither exclusively language-based nor
exclusively structure-based. It is known that the class of binary CSP instances
in which the broken-triangle pattern (BTP) does not occur, a class which
includes all tree-structured instances, are decided by arc consistency (AC), a
ubiquitous reduction operation in constraint solvers. We provide a
characterisation of simple partially-ordered forbidden patterns which have this
AC-solvability property. It turns out that BTP is just one of five such
AC-solvable patterns. The four other patterns allow us to exhibit new tractable
classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1&quot;&gt;Martin C. Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zivny_S/0/1/0/all/0/1&quot;&gt;Stanislav &amp;#x17d;ivn&amp;#xfd;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07387">
<title>How morphological development can guide evolution. (arXiv:1711.07387v2 [q-bio.PE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07387</link>
<description rdf:parseType="Literal">&lt;p&gt;Organisms result from multiple adaptive processes occurring and interacting
at different time scales. One such interaction is that between development and
evolution. In modeling studies, it has been shown that development sweeps over
a series of traits in a single agent, and sometimes exposes promising static
traits. Subsequent evolution can then canalize these rare traits. Thus,
development can, under the right conditions, increase evolvability. Here, we
report on a previously unknown phenomenon when embodied agents are allowed to
develop and evolve: Evolution discovers body plans which are robust to control
changes, these body plans become genetically assimilated, yet controllers for
these agents are not assimilated. This allows evolution to continue climbing
fitness gradients by tinkering with the developmental programs for controllers
within these permissive body plans. This exposes a previously unknown detail
about the Baldwin effect: instead of all useful traits becoming genetically
assimilated, only phenotypic traits that render the agent robust to changes in
other traits become assimilated. We refer to this phenomenon as differential
canalization. This finding also has important implications for the evolutionary
design of artificial and embodied agents such as robots: robots that are robust
to internal changes in their controllers may also be robust to external changes
in their environment, such as transferal from simulation to reality, or
deployment in novel environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kriegman_S/0/1/0/all/0/1&quot;&gt;Sam Kriegman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05181">
<title>Rasa: Open Source Language Understanding and Dialogue Management. (arXiv:1712.05181v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1712.05181</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source
python libraries for building conversational software. Their purpose is to make
machine-learning based dialogue management and language understanding
accessible to non-specialist software developers. In terms of design
philosophy, we aim for ease of use, and bootstrapping from minimal (or no)
initial training data. Both packages are extensively documented and ship with a
comprehensive suite of tests. The code is available at
https://github.com/RasaHQ/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bocklisch_T/0/1/0/all/0/1&quot;&gt;Tom Bocklisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faulkner_J/0/1/0/all/0/1&quot;&gt;Joey Faulkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1&quot;&gt;Nick Pawlowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1&quot;&gt;Alan Nichol&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05438">
<title>Stochastic Particle Gradient Descent for Infinite Ensembles. (arXiv:1712.05438v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.05438</link>
<description rdf:parseType="Literal">&lt;p&gt;The superior performance of ensemble methods with infinite models are well
known. Most of these methods are based on optimization problems in
infinite-dimensional spaces with some regularization, for instance, boosting
methods and convex neural networks use $L^1$-regularization with the
non-negative constraint. However, due to the difficulty of handling
$L^1$-regularization, these problems require early stopping or a rough
approximation to solve it inexactly. In this paper, we propose a new ensemble
learning method that performs in a space of probability measures, that is, our
method can handle the $L^1$-constraint and the non-negative constraint in a
rigorous way. Such an optimization is realized by proposing a general purpose
stochastic optimization method for learning probability measures via
parameterization using transport maps on base models. As a result of running
the method, a transport map to output an infinite ensemble is obtained, which
forms a residual-type network. From the perspective of functional gradient
methods, we give a convergence rate as fast as that of a stochastic
optimization method for finite dimensional nonconvex problems. Moreover, we
show an interior optimality property of a local optimality condition used in
our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nitanda_A/0/1/0/all/0/1&quot;&gt;Atsushi Nitanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1&quot;&gt;Taiji Suzuki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05510">
<title>Graph-Sparse Logistic Regression. (arXiv:1712.05510v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.05510</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Graph-Sparse Logistic Regression, a new algorithm for
classification for the case in which the support should be sparse but connected
on a graph. We val- idate this algorithm against synthetic data and benchmark
it against L1-regularized Logistic Regression. We then explore our technique in
the bioinformatics context of proteomics data on the interactome graph. We make
all our experimental code public and provide GSLR as an open source package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeNail_A/0/1/0/all/0/1&quot;&gt;Alexander LeNail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Johnathan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehrenberger_T/0/1/0/all/0/1&quot;&gt;Tobias Ehrenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachs_K/0/1/0/all/0/1&quot;&gt;Karen Sachs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fraenkel_E/0/1/0/all/0/1&quot;&gt;Ernest Fraenkel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05556">
<title>Safe Policy Search with Gaussian Process Models. (arXiv:1712.05556v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.05556</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to optimise the parameters of a policy which will be used
to safely perform a given task in a data-efficient manner. We train a Gaussian
process model to capture the system dynamics, based on the PILCO framework. Our
model has useful analytic properties, which allow closed form computation of
error gradients and estimating the probability of violating given state space
constraints. During training, as well as operation, only policies that are
deemed safe are implemented on the real system, minimising the risk of failure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polymenakos_K/0/1/0/all/0/1&quot;&gt;Kyriakos Polymenakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05630">
<title>Sparse principal component analysis via random projections. (arXiv:1712.05630v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.05630</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new method for sparse principal component analysis, based on
the aggregation of eigenvector information from carefully-selected random
projections of the sample covariance matrix. Unlike most alternative
approaches, our algorithm is non-iterative, so is not vulnerable to a bad
choice of initialisation. Our theory provides great detail on the statistical
and computational trade-off in our procedure, revealing a subtle interplay
between the effective sample size and the number of random projections that are
required to achieve the minimax optimal rate. Numerical studies provide further
insight into the procedure and confirm its highly competitive finite-sample
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gataric_M/0/1/0/all/0/1&quot;&gt;Milana Gataric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tengyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Samworth_R/0/1/0/all/0/1&quot;&gt;Richard J. Samworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05654">
<title>Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice. (arXiv:1712.05654v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.05654</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a generic scheme for accelerating gradient-based optimization
methods in the sense of Nesterov. The approach, called Catalyst, builds upon
the inexact acceler- ated proximal point algorithm for minimizing a convex
objective function, and consists of approximately solving a sequence of
well-chosen auxiliary problems, leading to faster convergence. One of the key
to achieve acceleration in theory and in practice is to solve these
sub-problems with appropriate accuracy by using the right stopping criterion
and the right warm-start strategy. In this paper, we give practical guidelines
to use Catalyst and present a comprehensive theoretical analysis of its global
complexity. We show that Catalyst applies to a large class of algorithms,
including gradient descent, block coordinate descent, incremental algorithms
such as SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For
all of these methods, we provide acceleration and explicit sup- port for
non-strongly convex objectives. We conclude with extensive experiments showing
that acceleration is useful in practice, especially for ill-conditioned
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongzhou Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mairal_J/0/1/0/all/0/1&quot;&gt;Julien Mairal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1&quot;&gt;Zaid Harchaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05689">
<title>BT-Nets: Simplifying Deep Neural Networks via Block Term Decomposition. (arXiv:1712.05689v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.05689</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep neural networks (DNNs) have been regarded as the
state-of-the-art classification methods in a wide range of applications,
especially in image classification. Despite the success, the huge number of
parameters blocks its deployment to situations with light computing resources.
Researchers resort to the redundancy in the weights of DNNs and attempt to find
how fewer parameters can be chosen while preserving the accuracy at the same
time. Although several promising results have been shown along this research
line, most existing methods either fail to significantly compress a
well-trained deep network or require a heavy fine-tuning process for the
compressed network to regain the original performance. In this paper, we
propose the \textit{Block Term} networks (BT-nets) in which the commonly used
fully-connected layers (FC-layers) are replaced with block term layers
(BT-layers). In BT-layers, the inputs and the outputs are reshaped into two
low-dimensional high-order tensors, then block-term decomposition is applied as
tensor operators to connect them. We conduct extensive experiments on benchmark
datasets to demonstrate that BT-layers can achieve a very large compression
ratio on the number of parameters while preserving the representation power of
the original FC-layers as much as possible. Specifically, we can get a higher
performance while requiring fewer parameters compared with the tensor train
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guangxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jinmian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haiqin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Di Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05690">
<title>Sockeye: A Toolkit for Neural Machine Translation. (arXiv:1712.05690v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.05690</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe Sockeye (version 1.12), an open-source sequence-to-sequence
toolkit for Neural Machine Translation (NMT). Sockeye is a production-ready
framework for training and applying models as well as an experimental platform
for researchers. Written in Python and built on MXNet, the toolkit offers
scalable training and inference for the three most prominent encoder-decoder
architectures: attentional recurrent neural networks, self-attentional
transformers, and fully convolutional networks. Sockeye also supports a wide
range of optimizers, normalization and regularization techniques, and inference
improvements from current NMT literature. Users can easily run standard
training recipes, explore different model settings, and incorporate new ideas.
In this paper, we highlight Sockeye&apos;s features and benchmark it against other
NMT toolkits on two language arcs from the 2017 Conference on Machine
Translation (WMT): English-German and Latvian-English. We report competitive
BLEU scores across all three architectures, including an overall best score for
Sockeye&apos;s transformer implementation. To facilitate further comparison, we
release all system outputs and training scripts used in our experiments. The
Sockeye toolkit is free software released under the Apache 2.0 license.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hieber_F/0/1/0/all/0/1&quot;&gt;Felix Hieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domhan_T/0/1/0/all/0/1&quot;&gt;Tobias Domhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denkowski_M/0/1/0/all/0/1&quot;&gt;Michael Denkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1&quot;&gt;David Vilar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1&quot;&gt;Artem Sokolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifton_A/0/1/0/all/0/1&quot;&gt;Ann Clifton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1&quot;&gt;Matt Post&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05754">
<title>Understanding Career Progression in Baseball Through Machine Learning. (arXiv:1712.05754v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.05754</link>
<description rdf:parseType="Literal">&lt;p&gt;Professional baseball players are increasingly guaranteed expensive long-term
contracts, with over 70 deals signed in excess of \$90 million, mostly in the
last decade. These are substantial sums compared to a typical franchise
valuation of \$1-2 billion. Hence, the players to whom a team chooses to give
such a contract can have an enormous impact on both competitiveness and profit.
Despite this, most published approaches examining career progression in
baseball are fairly simplistic. We applied four machine learning algorithms to
the problem and soundly improved upon existing approaches, particularly for
batting data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bierig_B/0/1/0/all/0/1&quot;&gt;Brian Bierig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hollenbeck_J/0/1/0/all/0/1&quot;&gt;Jonathan Hollenbeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stroud_A/0/1/0/all/0/1&quot;&gt;Alexander Stroud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05790">
<title>Deep Burst Denoising. (arXiv:1712.05790v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.05790</link>
<description rdf:parseType="Literal">&lt;p&gt;Noise is an inherent issue of low-light image capture, one which is
exacerbated on mobile devices due to their narrow apertures and small sensors.
One strategy for mitigating noise in a low-light situation is to increase the
shutter time of the camera, thus allowing each photosite to integrate more
light and decrease noise variance. However, there are two downsides of long
exposures: (a) bright regions can exceed the sensor range, and (b) camera and
scene motion will result in blurred images. Another way of gathering more light
is to capture multiple short (thus noisy) frames in a &quot;burst&quot; and intelligently
integrate the content, thus avoiding the above downsides. In this paper, we use
the burst-capture strategy and implement the intelligent integration via a
recurrent fully convolutional deep neural net (CNN). We build our novel,
multiframe architecture to be a simple addition to any single frame denoising
model, and design to handle an arbitrary number of noisy input frames. We show
that it achieves state of the art denoising results on our burst dataset,
improving on the best published multi-frame techniques, such as VBM4D and
FlexISP. Finally, we explore other applications of image enhancement by
integrating content from multiple frames and demonstrate that our DNN
architecture generalizes well to image super-resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godard_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Godard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matzen_K/0/1/0/all/0/1&quot;&gt;Kevin Matzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uyttendaele_M/0/1/0/all/0/1&quot;&gt;Matt Uyttendaele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.04114">
<title>Inferring the parameters of a Markov process from snapshots of the steady state. (arXiv:1707.04114v3 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/1707.04114</link>
<description rdf:parseType="Literal">&lt;p&gt;We seek to infer the parameters of an ergodic Markov process from samples
taken independently from the steady state. Our focus is on non-equilibrium
processes, where the steady state is not described by the Boltzmann measure,
but is generally unknown and hard to compute, which prevents the application of
established equilibrium inference methods. We propose a quantity we call
propagator likelihood, which takes on the role of the likelihood in equilibrium
processes. This propagator likelihood is based on fictitious transitions
between those configurations of the system which occur in the samples. The
propagator likelihood can be derived by minimising the relative entropy between
the empirical distribution and a distribution generated by propagating the
empirical distribution forward in time. Maximising the propagator likelihood
leads to an efficient reconstruction of the parameters of the underlying model
in different systems, both with discrete configurations and with continuous
configurations. We apply the method to non-equilibrium models from statistical
physics and theoretical biology, including the asymmetric simple exclusion
process (ASEP), the kinetic Ising model, and replicator dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Dettmer_S/0/1/0/all/0/1&quot;&gt;Simon Lee Dettmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Berg_J/0/1/0/all/0/1&quot;&gt;Johannes Berg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05673">
<title>Semi-supervised learning. (arXiv:1709.05673v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05673</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning deals with the problem of how, if possible, to take
advantage of a huge amount of not classified data, to perform classification,
in situations when, typically, the labelled data are few. Even though this is
not always possible (it depends on how useful is to know the distribution of
the unlabelled data in the inference of the labels), several algorithm have
been proposed recently. A new algorithm is proposed, that under almost
neccesary conditions, attains asymptotically the performance of the best
theoretical rule, when the size of unlabeled data tends to infinity. The set of
necessary assumptions, although reasonables, show that semi-parametric
classification only works for very well conditioned problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cholaquidis_A/0/1/0/all/0/1&quot;&gt;Alejandro Cholaquidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fraiman_R/0/1/0/all/0/1&quot;&gt;Ricardo Fraiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sued_M/0/1/0/all/0/1&quot;&gt;Mariela Sued&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07954">
<title>A Novel Bayesian Cluster Enumeration Criterion for Unsupervised Learning. (arXiv:1710.07954v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07954</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive a new Bayesian Information Criterion (BIC) from first principles by
formulating the problem of estimating the number of clusters in an observed
data set as maximization of the posterior probability of the candidate models.
Given that some mild assumptions are satisfied, we provide a general BIC
expression for a broad class of data distributions. This serves as an important
milestone when deriving the BIC for specific data distributions. Along this
line, we provide a closed-form BIC expression for multivariate Gaussian
distributed observations. We show that incorporating data structure of the
clustering problem into the derivation of the BIC results in an expression
whose penalty term is different from that of the original BIC. We propose a
two-step cluster enumeration algorithm. First, a model-based unsupervised
learning algorithm partitions the data according to a given set of candidate
models. Subsequently, the optimal cluster number is determined as the one
associated to the model for which the proposed BIC is maximal. The performance
of the proposed criterion is tested using synthetic and real data sets. Despite
the fact that the original BIC is a generic criterion which does not include
information about the specific model selection problem at hand, it has been
widely used in the literature to estimate the number of clusters in an observed
data set. We, therefore, consider it as a benchmark comparison. Simulation
results show that our proposed criterion outperforms the existing cluster
enumeration methods that are based on the original BIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Teklehaymanot_F/0/1/0/all/0/1&quot;&gt;Freweyni K. Teklehaymanot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Muma_M/0/1/0/all/0/1&quot;&gt;Michael Muma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zoubir_A/0/1/0/all/0/1&quot;&gt;Abdelhak M. Zoubir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07792">
<title>Hierarchical internal representation of spectral features in deep convolutional networks trained for EEG decoding. (arXiv:1711.07792v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07792</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there is increasing interest and research on the interpretability
of machine learning models, for example how they transform and internally
represent EEG signals in Brain-Computer Interface (BCI) applications. This can
help to understand the limits of the model and how it may be improved, in
addition to possibly provide insight about the data itself. Schirrmeister et
al. (2017) have recently reported promising results for EEG decoding with deep
convolutional neural networks (ConvNets) trained in an end-to-end manner and,
with a causal visualization approach, showed that they learn to use spectral
amplitude changes in the input. In this study, we investigate how ConvNets
represent spectral features through the sequence of intermediate stages of the
network. We show higher sensitivity to EEG phase features at earlier stages and
higher sensitivity to EEG amplitude features at later stages. Intriguingly, we
observed a specialization of individual stages of the network to the classical
EEG frequency bands alpha, beta, and high gamma. Furthermore, we find first
evidence that particularly in the last convolutional layer, the network learns
to detect more complex oscillatory patterns beyond spectral phase and
amplitude, reminiscent of the representation of complex visual features in
later layers of ConvNets in computer vision tasks. Our findings thus provide
insights into how ConvNets hierarchically represent spectral EEG features in
their intermediate layers and suggest that ConvNets can exploit and might help
to better understand the compositional structure of EEG time series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartmann_K/0/1/0/all/0/1&quot;&gt;Kay Gregor Hartmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09511">
<title>Highly Efficient Human Action Recognition with Quantum Genetic Algorithm Optimized Support Vector Machine. (arXiv:1711.09511v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09511</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose the use of quantum genetic algorithm to optimize the
support vector machine (SVM) for human action recognition. The Microsoft Kinect
sensor can be used for skeleton tracking, which provides the joints&apos; position
data. However, how to extract the motion features for representing the dynamics
of a human skeleton is still a challenge due to the complexity of human motion.
We present a highly efficient features extraction method for action
classification, that is, using the joint angles to represent a human skeleton
and calculating the variance of each angle during an action time window. Using
the proposed representation, we compared the human action classification
accuracy of two approaches, including the optimized SVM based on quantum
genetic algorithm and the conventional SVM with grid search. Experimental
results on the MSR-12 dataset show that the conventional SVM achieved an
accuracy of $ 93.85\% $. The proposed approach outperforms the conventional
method with an accuracy of $ 96.15\% $.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yafeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shimin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhikai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_E/0/1/0/all/0/1&quot;&gt;Enjie Ding&lt;/a&gt;</dc:creator>
</item></rdf:RDF>