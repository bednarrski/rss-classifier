<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09633"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09648"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09890"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04343"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09501"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09540"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09830"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09902"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09935"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09940"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09484"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05240"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.09551">
<title>Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?. (arXiv:1808.09551v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09551</link>
<description rdf:parseType="Literal">&lt;p&gt;Character-level features are currently used in different neural network-based
natural language processing algorithms. However, little is known about the
character-level patterns those models learn. Moreover, models are often
compared only quantitatively while a qualitative analysis is missing. In this
paper, we investigate which character-level patterns neural networks learn and
if those patterns coincide with manually-defined word segmentations and
annotations. To that end, we extend the contextual decomposition technique
(Murdoch et al. 2018) to convolutional neural networks which allows us to
compare convolutional neural networks and bidirectional long short-term memory
networks. We evaluate and compare these models for the task of morphological
tagging on three morphologically different languages and show that these models
implicitly discover understandable linguistic rules. Our implementation can be
found at https://github.com/FredericGodin/ContextualDecomposition-NLP .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;deric Godin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demuynck_K/0/1/0/all/0/1&quot;&gt;Kris Demuynck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dambre_J/0/1/0/all/0/1&quot;&gt;Joni Dambre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neve_W/0/1/0/all/0/1&quot;&gt;Wesley De Neve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1&quot;&gt;Thomas Demeester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09633">
<title>Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment. (arXiv:1808.09633v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09633</link>
<description rdf:parseType="Literal">&lt;p&gt;Network embeddings, which learn low-dimensional representations for each
vertex in a large-scale network, have received considerable attention in recent
years. For a wide range of applications, vertices in a network are typically
accompanied by rich textual information such as user profiles, paper abstracts,
etc. We propose to incorporate semantic features into network embeddings by
matching important words between text sequences for all pairs of vertices. We
introduce a word-by-word alignment framework that measures the compatibility of
embeddings between word pairs, and then adaptively accumulates these alignment
features with a simple yet effective aggregation function. In experiments, we
evaluate the proposed framework on three real-world benchmarks for downstream
tasks, including link prediction and multi-label vertex classification. Results
demonstrate that our model outperforms state-of-the-art network embedding
methods by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinghan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1&quot;&gt;Ricardo Henao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09648">
<title>From VQA to Multimodal CQA: Adapting Visual QA Models for Community QA Tasks. (arXiv:1808.09648v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09648</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present novel methods to adapt visual QA models for
community QA tasks of practical significance - automated question category
classification and finding experts for question answering - on questions
containing both text and image. To the best of our knowledge, this is the first
work to tackle the multimodality challenge in CQA, and is an enabling step
towards basic question-answering on image-based CQA. First, we analyze the
differences between visual QA and community QA datasets, discussing the
limitations of applying VQA models directly to CQA tasks, and then we propose
novel augmentations to VQA-based models to best address those limitations. Our
model, with the augmentations of an image-text combination method tailored for
CQA and use of auxiliary tasks for learning better grounding features,
significantly outperforms the text-only and VQA model baselines for both tasks
on real-world CQA data from Yahoo! Chiebukuro, a Japanese counterpart of Yahoo!
Answers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Avikalp Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hsin Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujita_S/0/1/0/all/0/1&quot;&gt;Sumio Fujita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09888">
<title>KDSL: a Knowledge-Driven Supervised Learning Framework for Word Sense Disambiguation. (arXiv:1808.09888v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09888</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose KDSL, a new word sense disambiguation (WSD) framework that
utilizes knowledge to automatically generate sense-labeled data for supervised
learning. First, from WordNet, we au- tomatically construct a semantic
knowledge base called DisDict, which provides refined feature words that
highlight the differences among word senses, i.e., synsets. Second, we
automatically generate new sense-labeled data by DisDict from unlabeled
corpora. Third, these generated data, together with manually labeled data, are
fed to a supervised learning neural network to model the semantic relations
among synsets, feature words and their contexts. Jointly with the supervised
learning process, we also implement unsupervised learning on unlabeled data as
an auxiliary task. The experimental results show that KDSL outperforms several
representative state-of-the-art methods on various major benchmarks.
Interestingly, it performs relatively well even when manually labeled data is
unavailable, thus provides a new promising backoff strategy for WSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1&quot;&gt;Shi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianmin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruili Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09890">
<title>An Adaptive Conversational Bot Framework. (arXiv:1808.09890v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09890</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we enable users to heavily specify criteria for database queries in a
user-friendly way? This paper describes a general framework of a conversational
bot that extracts meaningful information from user&apos;s sentences, that asks
subsequent questions to complete missing information, and that adjusts its
questions and information-extraction parameters for later conversations
depending on users&apos; behavior. Additionally, we provide a comparison of existing
tools and give novel techniques to implement such framework. Finally, we
exemplify the framework with a bot to query movies in a database, whose code is
available for Microsoft employees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etinger_I/0/1/0/all/0/1&quot;&gt;Isak Czeresnia Etinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09942">
<title>Neural Compositional Denotational Semantics for Question Answering. (arXiv:1808.09942v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09942</link>
<description rdf:parseType="Literal">&lt;p&gt;Answering compositional questions requiring multi-step reasoning is
challenging. We introduce an end-to-end differentiable model for interpreting
questions about a knowledge graph (KG), which is inspired by formal approaches
to semantics. Each span of text is represented by a denotation in a KG and a
vector that captures ungrounded aspects of meaning. Learned composition modules
recursively combine constituent spans, culminating in a grounding for the
complete sentence which answers the question. For example, to interpret &quot;not
green&quot;, the model represents &quot;green&quot; as a set of KG entities and &quot;not&quot; as a
trainable ungrounded vector---and then uses this vector to parameterize a
composition function that performs a complement operation. For each sentence,
we build a parse chart subsuming all possible parses, allowing the model to
jointly learn both the composition operators and output structure by gradient
descent from end-task supervision. The model learns a variety of challenging
semantic operators, such as quantifiers, disjunctions and composed relations,
and infers latent syntactic structure. It also generalizes well to longer
questions than seen in its training data, in contrast to RNN, its tree-based
variants, and semantic parsing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Nitish Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08156">
<title>Identifiability of Gaussian Structural Equation Models with Dependent Errors Having Equal Variances. (arXiv:1806.08156v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.08156</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we prove that some Gaussian structural equation models with
dependent errors having equal variances are identifiable from their
corresponding Gaussian distributions. Specifically, we prove identifiability
for the Gaussian structural equation models that can be represented as
Andersson-Madigan-Perlman chain graphs (Andersson et al., 2001). These chain
graphs were originally developed to represent independence models. However,
they are also suitable for representing causal models with additive noise
(Pe\~na, 2016. Our result implies then that these causal models can be
identified from observational data alone. Our result generalizes the result by
Peters and B\&quot;uhlmann (2014), who considered independent errors having equal
variances. The suitability of the equal error variances assumption should be
assessed on a per domain basis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1&quot;&gt;Jose M. Pe&amp;#xf1;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04343">
<title>REGMAPR - Text Matching Made Easy. (arXiv:1808.04343v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04343</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple model for textual matching problems. Starting from a
Siamese architecture, we augment word embeddings with two features based on
exact and paraphrase match between words in the two sentences being considered.
We train the model using four types of regularization on datasets for textual
entailment, paraphrase detection and semantic relatedness. Our model performs
comparably or better than more complex architectures; achieving
state-of-the-art results for paraphrase detection on the SICK dataset and for
textual entailment on the SNLI dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1&quot;&gt;Siddhartha Brahma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09293">
<title>A Summary Description of the A2RD Project. (arXiv:1808.09293v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09293</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper briefly describes the Autonomous Architecture Over Restricted
Domains project (A2RD). In It begins with the description of the context upon
which the project is focused, and in the sequence describes the A2RD abstract
and implementation models. Finish by presenting the environment conceptual
model named Structure for Knowledge Acquisition, Use and Collaboration Inter
A2RD Agents (SKAU), showing where stand the components, inputs and facilities
required to interact among the intelligent agents of the various
implementations in their respective and restricted routing domains (Autonomous
Systems) that build the Internet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09489">
<title>Convergence of Krasulina Scheme. (arXiv:1808.09489v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09489</link>
<description rdf:parseType="Literal">&lt;p&gt;Principal component analysis (PCA) is one of the most commonly used
statistical procedures with a wide range of applications. Consider the points
$X_1, X_2,..., X_n$ are vectors drawn i.i.d. from a distribution with mean zero
and covariance $\Sigma$, where $\Sigma$ is unknown. Let $A_n = X_nX_n^T$, then
$E[A_n] = \Sigma$. This paper consider the problem of finding the least
eigenvalue and eigenvector of matrix $\Sigma$. A classical such estimator are
due to Krasulina\cite{krasulina_method_1969}. We are going to state the
convergence proof of Krasulina for the least eigenvalue and corresponding
eigenvector, and then find their convergence rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiangning Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09501">
<title>Concentrated Differentially Private Gradient Descent with Adaptive per-Iteration Privacy Budget. (arXiv:1808.09501v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09501</link>
<description rdf:parseType="Literal">&lt;p&gt;Iterative algorithms, like gradient descent, are common tools for solving a
variety of problems, such as model fitting. For this reason, there is interest
in creating differentially private versions of them. However, their conversion
to differentially private algorithms is often naive. For instance, a fixed
number of iterations are chosen, the privacy budget is split evenly among them,
and at each iteration, parameters are updated with a noisy gradient. In this
paper, we show that gradient-based algorithms can be improved by a more careful
allocation of privacy budget per iteration. Intuitively, at the beginning of
the optimization, gradients are expected to be large, so that they do not need
to be measured as accurately. However, as the parameters approach their optimal
values, the gradients decrease and hence need to be measured more accurately.
We add a basic line-search capability that helps the algorithm decide when more
accurate gradient measurements are necessary. Our gradient descent algorithm
works with the recently introduced zCDP version of differential privacy. It
outperforms prior algorithms for model fitting and is competitive with the
state-of-the-art for $(\epsilon,\delta)$-differential privacy, a strictly
weaker definition than zCDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1&quot;&gt;Daniel Kifer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09540">
<title>Lipschitz regularized Deep Neural Networks converge and generalize. (arXiv:1808.09540v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09540</link>
<description rdf:parseType="Literal">&lt;p&gt;Lipschitz regularized neural networks augment the usual fidelity term used in
training with a regularization term corresponding the excess Lipschitz constant
of the network compared to the Lipschitz constant of the data. We prove that
Lipschitz regularized neural networks converge, and provide a rate, in the
limit as the number of data points $n\to\infty$. We consider the regime where
perfect fitting of data is possible, which means the size of the network grows
with $n$. There are two regimes: in the case of perfect labels, we prove
convergence to the label function which corresponds to zero loss. In the case
of corrupted labels which occurs when the Lipschitz constant of the data blows
up, we prove convergence to a regularized label function which is the solution
of a limiting variational problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oberman_A/0/1/0/all/0/1&quot;&gt;Adam M Oberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1&quot;&gt;Jeff Calder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09830">
<title>Searching Toward Pareto-Optimal Device-Aware Neural Architectures. (arXiv:1808.09830v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09830</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in Neural Architectural Search (NAS) have achieved
state-of-the-art performance in many tasks such as image classification and
language understanding. However, most existing works only optimize for model
accuracy and largely ignore other important factors imposed by the underlying
hardware and devices, such as latency and energy, when making inference. In
this paper, we first introduce the problem of NAS and provide a survey on
recent works. Then we deep dive into two recent advancements on extending NAS
into multiple-objective frameworks: MONAS and DPP-Net. Both MONAS and DPP-Net
are capable of optimizing accuracy and other objectives imposed by devices,
searching for neural architectures that can be best deployed on a wide spectrum
of devices: from embedded systems and mobile devices to workstations.
Experimental results are poised to show that architectures found by MONAS and
DPP-Net achieves Pareto optimality w.r.t the given objectives for various
devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1&quot;&gt;An-Chieh Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jin-Dong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chi-Hung Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shu-Huan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Min Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Chieh Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jia-Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1&quot;&gt;Da-Cheng Juan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09889">
<title>Zero-shot Transfer Learning for Semantic Parsing. (arXiv:1808.09889v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09889</link>
<description rdf:parseType="Literal">&lt;p&gt;While neural networks have shown impressive performance on large datasets,
applying these models to tasks where little data is available remains a
challenging problem.
&lt;/p&gt;
&lt;p&gt;In this paper we propose to use feature transfer in a zero-shot experimental
setting on the task of semantic parsing.
&lt;/p&gt;
&lt;p&gt;We first introduce a new method for learning the shared space between
multiple domains based on the prediction of the domain label for each example.
&lt;/p&gt;
&lt;p&gt;Our experiments support the superiority of this method in a zero-shot
experimental setting in terms of accuracy metrics compared to state-of-the-art
techniques.
&lt;/p&gt;
&lt;p&gt;In the second part of this paper we study the impact of individual domains
and examples on semantic parsing performance.
&lt;/p&gt;
&lt;p&gt;We use influence functions to this aim and investigate the sensitivity of
domain-label classification loss on each example.
&lt;/p&gt;
&lt;p&gt;Our findings reveal that cross-domain adversarial attacks identify useful
examples for training even from the domains the least similar to the target
domain. Augmenting our training data with these influential examples further
boosts our accuracy at both the token and the sequence level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadashkarimi_J/0/1/0/all/0/1&quot;&gt;Javid Dadashkarimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1&quot;&gt;Alexander Fabbri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatikonda_S/0/1/0/all/0/1&quot;&gt;Sekhar Tatikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1&quot;&gt;Dragomir R. Radev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09902">
<title>Extreme Value Theory for Open Set Classification - GPD and GEV Classifiers. (arXiv:1808.09902v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09902</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification tasks usually assume that all possible classes are present
during the training phase. This is restrictive if the algorithm is used over a
long time and possibly encounters samples from unknown classes. The recently
introduced extreme value machine, a classifier motivated by extreme value
theory, addresses this problem and achieves competitive performance in specific
cases. We show that this algorithm can fail when the geometries of known and
unknown classes differ. To overcome this problem, we propose two new algorithms
relying on approximations from extreme value theory. We show the effectiveness
of our classifiers in simulations and on the LETTER and MNIST data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vignotto_E/0/1/0/all/0/1&quot;&gt;Edoardo Vignotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Engelke_S/0/1/0/all/0/1&quot;&gt;Sebastian Engelke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09907">
<title>Dropout with Tabu Strategy for Regularizing Deep Neural Networks. (arXiv:1808.09907v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09907</link>
<description rdf:parseType="Literal">&lt;p&gt;Dropout has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons in deep neural networks (DNN). It
randomly drops units with a probability $p$ during the training stage of DNN.
Dropout also provides a way of approximately combining exponentially many
different neural network architectures efficiently. In this work, we add a
diversification strategy into dropout, which aims at generating more different
neural network architectures in a proper times of iterations. The dropped units
in last forward propagation will be marked. Then the selected units for
dropping in the current FP will be kept if they have been marked in the last
forward propagation. We only mark the units from the last forward propagation.
We call this new technique Tabu Dropout. Tabu Dropout has no extra parameters
compared with the standard Dropout and also it is computationally cheap. The
experiments conducted on MNIST, Fashion-MNIST datasets show that Tabu Dropout
improves the performance of the standard dropout.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zongjie Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattar_A/0/1/0/all/0/1&quot;&gt;Abdul Sattar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kaile Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09935">
<title>Attention-based Neural Text Segmentation. (arXiv:1808.09935v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09935</link>
<description rdf:parseType="Literal">&lt;p&gt;Text segmentation plays an important role in various Natural Language
Processing (NLP) tasks like summarization, context understanding, document
indexing and document noise removal. Previous methods for this task require
manual feature engineering, huge memory requirements and large execution times.
To the best of our knowledge, this paper is the first one to present a novel
supervised neural approach for text segmentation. Specifically, we propose an
attention-based bidirectional LSTM model where sentence embeddings are learned
using CNNs and the segments are predicted based on contextual information. This
model can automatically handle variable sized context information. Compared to
the existing competitive baselines, the proposed model shows a performance
improvement of ~7% in WinDiff score on three benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badjatiya_P/0/1/0/all/0/1&quot;&gt;Pinkesh Badjatiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurisinkel_L/0/1/0/all/0/1&quot;&gt;Litton J Kurisinkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Manish Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1&quot;&gt;Vasudeva Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09940">
<title>Deep Reinforcement Learning in Portfolio Management. (arXiv:1808.09940v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/1808.09940</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we implement two state-of-art continuous reinforcement
learning algorithms, Deep Deterministic Policy Gradient (DDPG) and Proximal
Policy Optimization (PPO) in portfolio management. Both of them are widely-used
in game playing and robot control. What&apos;s more, PPO has appealing theoretical
propeties which is hopefully potential in portfolio management. We present the
performances of them under different settings, including different learning
rate, objective function, markets, feature combinations, in order to provide
insights for parameter tuning, features selection and data preparation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kangkang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Junhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanran Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08294">
<title>Learning Context-Aware Convolutional Filters for Text Processing. (arXiv:1709.08294v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08294</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-aware convolutional filters for
text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-aware filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-aware
filters, we further validate and rationalize the effectiveness of proposed
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinghan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_M/0/1/0/all/0/1&quot;&gt;Martin Renqiang Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yitong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04212">
<title>Word2Vec applied to Recommendation: Hyperparameters Matter. (arXiv:1804.04212v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;Skip-gram with negative sampling, a popular variant of Word2vec originally
designed and tuned to create word embeddings for Natural Language Processing,
has been used to create item embeddings with successful applications in
recommendation. While these fields do not share the same type of data, neither
evaluate on the same tasks, recommendation applications tend to use the same
already tuned hyperparameters values, even if optimal hyperparameters values
are often known to be data and task dependent. We thus investigate the marginal
importance of each hyperparameter in a recommendation setting through large
hyperparameter grid searches on various datasets. Results reveal that
optimizing neglected hyperparameters, namely negative sampling distribution,
number of epochs, subsampling parameter and window-size, significantly improves
performance on a recommendation task, and can increase it by an order of
magnitude. Importantly, we find that optimal hyperparameters configurations for
Natural Language Processing tasks and Recommendation tasks are noticeably
different.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1&quot;&gt;Hugo Caselles-Dupr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lesaint_F/0/1/0/all/0/1&quot;&gt;Florian Lesaint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Royo_Letelier_J/0/1/0/all/0/1&quot;&gt;Jimena Royo-Letelier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09484">
<title>Multi-Level Deep Cascade Trees for Conversion Rate Prediction. (arXiv:1805.09484v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09484</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing effective and efficient recommendation methods is very challenging
for modern e-commerce platforms (e.g., Taobao). In this paper, we tackle this
problem by proposing multi-Level Deep Cascade Trees (ldcTree), which is a novel
decision tree ensemble approach. It leverages deep cascade structures by
stacking Gradient Boosting Decision Trees (GBDT) to effectively learn feature
representation. In addition, we propose to utilize the cross-entropy in each
tree of the preceding GBDT as the input feature representation for next level
GBDT, which has a clear explanation, i.e., a traversal from root to leaf nodes
in the next level GBDT corresponds to the combination of certain traversals in
the preceding GBDT. The deep cascade structure and the combination rule enable
the proposed ldcTree to have a stronger distributed feature representation
ability. Moreover, we propose an ensemble ldcTree to take full use of weak and
strong correlation features. Experimental results on off-line dataset and
online deployment demonstrate the effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Quan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Keping Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1&quot;&gt;Taiwei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1&quot;&gt;Fuyu Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Pipei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05240">
<title>Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks. (arXiv:1808.05240v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05240</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantized deep neural networks (QDNNs) are attractive due to their much lower
memory storage and faster inference speed than their regular full precision
counterparts. To maintain the same performance level especially at low
bit-widths, QDNNs must be retrained. Their training involves piecewise constant
activation functions and discrete weights, hence mathematical challenges arise.
We introduce the notion of coarse derivative and propose the blended coarse
gradient descent (BCGD) algorithm, for training fully quantized neural
networks. Coarse gradient is generally not a gradient of any function but an
artificial ascent direction. The weight update of BCGD goes by coarse gradient
correction of a weighted average of the full precision weights and their
quantization (the so-called blending), which yields sufficient descent in the
objective value and thus accelerates the training. Our experiments demonstrate
that this simple blending technique is very effective for quantization at
extremely low bit-width such as binarization. In full quantization of ResNet-18
for ImageNet classification task, BCGD gives 64.36% top-1 accuracy with binary
weights across all layers and 4-bit adaptive activation. If the weights in the
first and last layers are kept in full precision, this number increases to
65.46%. As theoretical justification, we provide the convergence analysis of
coarse gradient descent for a two-layer neural network model with Gaussian
input data, and prove that the expected coarse gradient correlates positively
with the underlying true gradient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Penghang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1&quot;&gt;Stanley Osher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yingyong Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Jack Xin&lt;/a&gt;</dc:creator>
</item></rdf:RDF>