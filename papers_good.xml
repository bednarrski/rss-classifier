<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06511"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06660"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06739"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.06824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06424"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06763"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08111"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04327"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06518"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00598"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06901"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.06511">
<title>Fast Weight Long Short-Term Memory. (arXiv:1804.06511v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.06511</link>
<description rdf:parseType="Literal">&lt;p&gt;Associative memory using fast weights is a short-term memory mechanism that
substantially improves the memory capacity and time scale of recurrent neural
networks (RNNs). As recent studies introduced fast weights only to regular
RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs. In
this work, we report a significant synergy between long short-term memory
(LSTM) networks and fast weight associative memories. We show that this
combination, in learning associative retrieval tasks, results in much faster
training and lower test error, a performance boost most prominent at high
memory task difficulties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1&quot;&gt;T. Anderson Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1&quot;&gt;Sharath Nittur Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06660">
<title>Short Term Electric Load Forecast with Artificial Neural Networks. (arXiv:1804.06660v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.06660</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents issues regarding short term electric load forecasting
using feedforward and Elman recurrent neural networks. The study cases were
developed using measured data representing electrical energy consume from Banat
area. There were considered 35 different types of structure for both
feedforward and recurrent network cases. For each type of neural network
structure were performed many trainings and best solution was selected. The
issue of forecasting the load on short term is essential in the effective
energetic consume management in an open market environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasar_C/0/1/0/all/0/1&quot;&gt;Cristian Vasar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szeidert_I/0/1/0/all/0/1&quot;&gt;Iosif Szeidert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filip_I/0/1/0/all/0/1&quot;&gt;Ioan Filip&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prostean_G/0/1/0/all/0/1&quot;&gt;Gabriela Prostean&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06739">
<title>Are ResNets Provably Better than Linear Predictors?. (arXiv:1804.06739v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06739</link>
<description rdf:parseType="Literal">&lt;p&gt;A residual network (or ResNet) is a standard deep neural net architecture,
with state-of-the-art performance across numerous applications. The main
premise of ResNets is that they allow the training of each layer to focus on
fitting just the residual of the previous layer&apos;s output and the target output.
Thus, we should expect that the trained network is no worse than what we can
obtain if we remove the residual layers and train a shallower network instead.
However, due to the non-convexity of the optimization problem, it is not at all
clear that ResNets indeed achieve this behavior, rather than getting stuck at
some arbitrarily poor local minimum. In this paper, we rigorously prove that
arbitrarily deep, nonlinear ResNets indeed exhibit this behavior, in the sense
that the optimization landscape contains no local minima with value above what
can be obtained with a linear predictor (namely a 1-layer network). Notably, we
show this under minimal or no assumptions on the precise network architecture,
data distribution, or loss function used. We also provide a quantitative
analysis of second-order stationary points for this problem, and show that with
a certain tweak to the architecture, training the network with standard
stochastic gradient descent achieves an objective value no worse than any fixed
linear predictor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1&quot;&gt;Ohad Shamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.06824">
<title>Learning Convolutional Text Representations for Visual Question Answering. (arXiv:1705.06824v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.06824</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
&quot;CNN Inception + Gate&quot; model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shuiwang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03745">
<title>Evolutionary Architecture Search For Deep Multitask Networks. (arXiv:1803.03745v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03745</link>
<description rdf:parseType="Literal">&lt;p&gt;Multitask learning, i.e. learning several tasks at once with the same neural
network, can improve performance in each of the tasks. Designing deep neural
network architectures for multitask learning is a challenge: There are many
ways to tie the tasks together, and the design choices matter. The size and
complexity of this problem exceeds human design ability, making it a compelling
domain for evolutionary optimization. Using the existing state of the art soft
ordering architecture as the starting point, methods for evolving the modules
of this architecture and for evolving the overall topology or routing between
modules are evaluated in this paper. A synergetic approach of evolving custom
routings with evolved, shared modules for each task is found to be very
powerful, significantly improving the state of the art in the Omniglot
multitask, multialphabet character recognition domain. This result demonstrates
how evolution can be instrumental in advancing deep neural network and complex
system design in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jason Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyerson_E/0/1/0/all/0/1&quot;&gt;Elliot Meyerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06424">
<title>Terrain RL Simulator. (arXiv:1804.06424v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06424</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide $89$ challenging simulation environments that range in difficulty.
The difficulty of solving a task is linked not only to the number of dimensions
in the action space but also to the size and shape of the distribution of
configurations the agent experiences. Therefore, we are releasing a number of
simulation environments that include randomly generated terrain. The library
also provides simple mechanisms to create new environments with different agent
morphologies and the option to modify the distribution of generated terrain. We
believe using these and other more complex simulations will help push the field
closer to creating human-level intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1&quot;&gt;Glen Berseth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xue Bin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1&quot;&gt;Michiel van de Panne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06458">
<title>Deep Probabilistic Programming Languages: A Qualitative Study. (arXiv:1804.06458v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06458</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep probabilistic programming languages try to combine the advantages of
deep learning with those of probabilistic programming languages. If successful,
this would be a big step forward in machine learning and programming languages.
Unfortunately, as of now, this new crop of languages is hard to use and
understand. This paper addresses this problem directly by explaining deep
probabilistic programming languages and indirectly by characterizing their
current strengths and weaknesses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baudart_G/0/1/0/all/0/1&quot;&gt;Guillaume Baudart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirzel_M/0/1/0/all/0/1&quot;&gt;Martin Hirzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandel_L/0/1/0/all/0/1&quot;&gt;Louis Mandel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06461">
<title>An Adaptive Clipping Approach for Proximal Policy Optimization. (arXiv:1804.06461v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06461</link>
<description rdf:parseType="Literal">&lt;p&gt;Very recently proximal policy optimization (PPO) algorithms have been
proposed as first-order optimization methods for effective reinforcement
learning. While PPO is inspired by the same learning theory that justifies
trust region policy optimization (TRPO), PPO substantially simplifies algorithm
design and improves data efficiency by performing multiple epochs of
\emph{clipped policy optimization} from sampled data. Although clipping in PPO
stands for an important new mechanism for efficient and reliable policy update,
it may fail to adaptively improve learning performance in accordance with the
importance of each sampled state. To address this issue, a new surrogate
learning objective featuring an adaptive clipping mechanism is proposed in this
paper, enabling us to develop a new algorithm, known as PPO-$\lambda$.
PPO-$\lambda$ optimizes policies repeatedly based on a theoretical target for
adaptive policy improvement. Meanwhile, destructively large policy update can
be effectively prevented through both clipping and adaptive control of a
hyperparameter $\lambda$ in PPO-$\lambda$, ensuring high learning reliability.
PPO-$\lambda$ enjoys the same simple and efficient design as PPO. Empirically
on several Atari game playing tasks and benchmark control tasks, PPO-$\lambda$
also achieved clearly better performance than PPO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yiming Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06763">
<title>A General Account of Argumentation with Preferences. (arXiv:1804.06763v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06763</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper builds on the recent ASPIC+ formalism, to develop a general
framework for argumentation with preferences. We motivate a revised definition
of conflict free sets of arguments, adapt ASPIC+ to accommodate a broader range
of instantiating logics, and show that under some assumptions, the resulting
framework satisfies key properties and rationality postulates. We then show
that the generalised framework accommodates Tarskian logic instantiations
extended with preferences, and then study instantiations of the framework by
classical logic approaches to argumentation. We conclude by arguing that
ASPIC+&apos;s modelling of defeasible inference rules further testifies to the
generality of the framework, and then examine and counter recent critiques of
Dung&apos;s framework and its extensions to accommodate preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modgil_S/0/1/0/all/0/1&quot;&gt;Sanjay Modgil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakken_H/0/1/0/all/0/1&quot;&gt;Henry Prakken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06769">
<title>CoNet: Collaborative Cross Networks for Cross-Domain Recommendation. (arXiv:1804.06769v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1804.06769</link>
<description rdf:parseType="Literal">&lt;p&gt;The cross-domain recommendation technique is an effective way of alleviating
the data sparsity in recommender systems by leveraging the knowledge from
relevant domains. Transfer learning is a class of algorithms underlying these
techniques. In this paper, we propose a novel transfer learning approach for
cross-domain recommendation by using neural networks as the base model. We
assume that hidden layers in two base networks are connected by cross mappings,
leading to the collaborative cross networks (CoNet). CoNet enables dual
knowledge transfer across domains by introducing cross connections from one
base network to another and vice versa. CoNet is achieved in multi-layer
feedforward networks by adding dual connections and joint loss functions, which
can be trained efficiently by back-propagation. The proposed model is evaluated
on two real-world datasets and it outperforms baseline models by relative
improvements of 3.56\% in MRR and 8.94\% in NDCG, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Herbert Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06819">
<title>Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. (arXiv:1804.06819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06819</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an active learning architecture that allows a robot to actively
learn which data collection strategy is most efficient for acquiring motor
skills to achieve multiple outcomes, and generalise over its experience to
achieve new outcomes. The robot explores its environment both via interactive
learning and goal-babbling. It learns at the same time when, who and what to
actively imitate from several available teachers, and learns when not to use
social guidance but use active goal-oriented self-exploration. This is
formalised in the framework of life-long strategic learning. The proposed
architecture, called Socially Guided Intrinsic Motivation with Active Choice of
Teacher and Strategy (SGIM-ACTS), relies on hierarchical active decisions of
what and how to learn driven by empirical evaluation of learning progress for
each learning strategy. We illustrate with an experiment where a simulated
robot learns to control its arm for realising two kinds of different outcomes.
It has to choose actively and hierarchically at each learning episode: 1) what
to learn: which outcome is most interesting to select as a goal to focus on for
goal-directed exploration; 2) how to learn: which data collection strategy to
use among self-exploration, mimicry and emulation; 3) once he has decided when
and what to imitate by choosing mimicry or emulation, then he has to choose who
to imitate, from a set of different teachers. We show that SGIM-ACTS learns
significantly more efficiently than using single learning strategies, and
coherently selects the best strategy with respect to the chosen outcome, taking
advantage of the available teachers (with different levels of skills).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1&quot;&gt;Sao Mai Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08111">
<title>A Popperian Falsification of Artificial Intelligence - Lighthill Defended. (arXiv:1704.08111v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1704.08111</link>
<description rdf:parseType="Literal">&lt;p&gt;The area of computation called artificial intelligence (AI) is falsified by
describing a previous 1972 falsification of AI by British applied mathematician
James Lighthill. It is explained how Lighthill&apos;s arguments continue to apply to
current AI. It is argued that AI should use the Popperian scientific method in
which it is the duty of every scientist to attempt to falsify theories and if
theories are falsified to replace or modify them. The paper describes the
Popperian method in detail and discusses Paul Nurse&apos;s application of the method
to cell biology that also involves questions of mechanism and behavior.
Arguments used by Lighthill in his original 1972 report that falsified AI are
discussed. The Lighthill arguments are then shown to apply to current AI. The
argument uses recent scholarship to explain Lighthill&apos;s assumptions and to show
how the arguments based on those assumptions continue to falsify modern AI. An
important focus of the argument involves Hilbert&apos;s philosophical programme that
defined knowledge and truth as provable formal sentences. Current AI takes the
Hilbert programme as dogma beyond criticism while Lighthill as a mid 20th
century applied mathematician had abandoned it. The paper uses recent
scholarship to explain John von Neumann&apos;s criticism of AI that I claim was
assumed by Lighthill. The paper discusses computer chess programs to show
Lighthill&apos;s combinatorial explosion still applies to AI but not humans. An
argument showing that Turing Machines (TM) are not the correct description of
computation is given. The paper concludes by advocating studying computation as
Peter Naur&apos;s Dataology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_S/0/1/0/all/0/1&quot;&gt;Steven Meyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00987">
<title>A Language for Function Signature Representations. (arXiv:1804.00987v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00987</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work by (Richardson and Kuhn, 2017a,b; Richardson et al., 2018) looks
at semantic parser induction and question answering in the domain of source
code libraries and APIs. In this brief note, we formalize the representations
being learned in these studies and introduce a simple domain specific language
and a systematic translation from this language to first-order logic. By
recasting the target representations in terms of classical logic, we aim to
broaden the applicability of existing code datasets for investigating more
complex natural language understanding and reasoning problems in the software
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1&quot;&gt;Kyle Richardson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04327">
<title>Attention-based Group Recommendation. (arXiv:1804.04327v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04327</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems are widely used in big information-based companies such
as Google, Twitter, LinkedIn, and Netflix. A recommender system deals with the
problem of information overload by filtering important information fragments
according to users&apos; preferences. In light of the increasing success of deep
learning, recent studies have proved the benefits of using deep learning in
various recommendation tasks. However, most proposed techniques only aim to
target individuals, which cannot be efficiently applied in group
recommendation.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a deep learning architecture to solve the group
recommendation problem. On the one hand, as different individual preferences in
a group necessitate preference trade-offs in making group recommendations, it
is essential that the recommendation model can discover substitutes among user
behaviors. On the other hand, it has been observed that a user as an individual
and as a group member behaves differently. To tackle such problems, we propose
using an attention mechanism to capture the impact of each user in a group.
Specifically, our model automatically learns the influence weight of each user
in a group and recommends items to the group based on its members&apos; weighted
preferences. We conduct extensive experiments on four datasets. Our model
significantly outperforms baseline methods and shows promising results in
applying deep learning to the group recommendation problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinh_T/0/1/0/all/0/1&quot;&gt;Tran Dang Quang Vinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Nguyen Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1&quot;&gt;Gao Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiao-Li Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06518">
<title>Online Non-Additive Path Learning under Full and Partial Information. (arXiv:1804.06518v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06518</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the online path learning problem in a graph with non-additive
gains/losses. Various settings of full information, semi-bandit, and full
bandit are explored. We give an efficient implementation of EXP3 algorithm for
the full bandit setting with any (non-additive) gain. Then, focusing on the
large family of non-additive count-based gains, we construct an intermediate
graph which has equivalent gains that are additive. By operating on this
intermediate graph, we are able to use algorithms like Component Hedge and
ComBand for the first time for non-additive gains. Finally, we apply our
methods to the important application of ensemble structured prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cortes_C/0/1/0/all/0/1&quot;&gt;Corinna Cortes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_V/0/1/0/all/0/1&quot;&gt;Vitaly Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1&quot;&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmanian_H/0/1/0/all/0/1&quot;&gt;Holakou Rahmanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warmuth_M/0/1/0/all/0/1&quot;&gt;Manfred K. Warmuth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06537">
<title>Understanding Convolutional Neural Network Training with Information Theory. (arXiv:1804.06537v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06537</link>
<description rdf:parseType="Literal">&lt;p&gt;Using information theoretic concepts to understand and explore the inner
organization of deep neural networks (DNNs) remains a big challenge. Recently,
the concept of an information plane began to shed light on the analysis of
multilayer perceptrons (MLPs). We provided an in-depth insight into stacked
autoencoders (SAEs) using a novel matrix-based Renyi&apos;s {\alpha}-entropy
functional, enabling for the first time the analysis of the dynamics of
learning using information flow in real-world scenario involving complex
network architecture and large data. Despite the great potential of these past
works, there are several open questions when it comes to applying information
theoretic concepts to understand convolutional neural networks (CNNs). These
include for instance the accurate estimation of information quantities among
multiple variables, and the many different training methodologies. By extending
the novel matrix-based Renyi&apos;s {\alpha}-entropy functional to a multivariate
scenario, this paper presents a systematic method to analyze CNNs training
using information theory. Our results validate two fundamental data processing
inequalities in CNNs, and also have direct impacts on previous work concerning
the training and design of CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06546">
<title>Deep Generative Networks For Sequence Prediction. (arXiv:1804.06546v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06546</link>
<description rdf:parseType="Literal">&lt;p&gt;This thesis investigates unsupervised time series representation learning for
sequence prediction problems, i.e. generating nice-looking input samples given
a previous history, for high dimensional input sequences by decoupling the
static input representation from the recurrent sequence representation. We
introduce three models based on Generative Stochastic Networks (GSN) for
unsupervised sequence learning and prediction. Experimental results for these
three models are presented on pixels of sequential handwritten digit (MNIST)
data, videos of low-resolution bouncing balls, and motion capture data. The
main contribution of this thesis is to provide evidence that GSNs are a viable
framework to learn useful representations of complex sequential input data, and
to suggest a new framework for deep generative models to learn complex
sequences by decoupling static input representations from dynamic time
dependency representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beissinger_M/0/1/0/all/0/1&quot;&gt;Markus Beissinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06561">
<title>A Mean Field View of the Landscape of Two-Layers Neural Networks. (arXiv:1804.06561v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.06561</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-layer neural networks are among the most powerful models in machine
learning, yet the fundamental reasons for this success defy mathematical
understanding. Learning a neural network requires to optimize a non-convex
high-dimensional objective (risk function), a problem which is usually attacked
using stochastic gradient descent (SGD). Does SGD converge to a global optimum
of the risk or only to a local optimum? In the first case, does this happen
because local minima are absent, or because SGD somehow avoids them? In the
second, why do local minima reached by SGD have good generalization properties?
&lt;/p&gt;
&lt;p&gt;In this paper we consider a simple case, namely two-layers neural networks,
and prove that -in a suitable scaling limit- SGD dynamics is captured by a
certain non-linear partial differential equation (PDE) that we call
distributional dynamics (DD). We then consider several specific examples, and
show how DD can be used to prove convergence of SGD to networks with
nearlyideal generalization error. This description allows to &apos;average-out&apos; some
of the complexities of the landscape of neural networks, and can be used to
prove a general convergence result for noisy SGD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Song Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Andrea Montanari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phan-Minh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00598">
<title>Controllable Generative Adversarial Network. (arXiv:1708.00598v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00598</link>
<description rdf:parseType="Literal">&lt;p&gt;Although it is recently introduced, in last few years, generative adversarial
network (GAN) has been shown many promising results to generate realistic
samples. However, it is hardly able to control generated samples since input
variables for a generator are from a random distribution. Some attempts have
been made to control generated samples from a generator, but they have not
shown good performances with difficult problems. Conditional GAN is one of the
most popular GAN structure to control generated samples; however, conditional
GAN frequently fails to generate samples with minor labels which are hardly
distinguishable in a training set. Here, we propose controllable GAN
(ControlGAN) in this paper. ControlGAN shows powerful performance to control
generated samples by making the generator concentrate more on input labels. In
this paper, ControlGAN is evaluated with a face image dataset and a room image
dataset. Furthermore, we demonstrate that the training of ControlGAN is an
actual learning, by feeding a label that had never been trained during the
training process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seok_J/0/1/0/all/0/1&quot;&gt;Junhee Seok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08868">
<title>Learning Generative ConvNets via Multi-grid Modeling and Sampling. (arXiv:1709.08868v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08868</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a multi-grid method for learning energy-based generative
ConvNet models of images. For each grid, we learn an energy-based probabilistic
model where the energy function is defined by a bottom-up convolutional neural
network (ConvNet or CNN). Learning such a model requires generating synthesized
examples from the model. Within each iteration of our learning algorithm, for
each observed training image, we generate synthesized images at multiple grids
by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of
the training image. The synthesized image at each subsequent grid is obtained
by a finite-step MCMC initialized from the synthesized image generated at the
previous coarser grid. After obtaining the synthesized examples, the parameters
of the models at multiple grids are updated separately and simultaneously based
on the differences between synthesized and observed examples. We show that this
multi-grid method can learn realistic energy-based generative ConvNet models,
and it outperforms the original contrastive divergence (CD) and persistent CD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruiqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junpei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06901">
<title>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement. (arXiv:1802.06901v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06901</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a conditional non-autoregressive neural sequence model based on
iterative refinement. The proposed model is designed based on the principles of
latent variable models and denoising autoencoders, and is generally applicable
to any sequence generation task. We extensively evaluate the proposed model on
machine translation (En-De and En-Ro) and image caption generation, and observe
that it significantly speeds up decoding while maintaining the generation
quality comparable to the autoregressive counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1&quot;&gt;Elman Mansimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;</dc:creator>
</item></rdf:RDF>