<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02501"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.04401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.00369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.10371"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07061"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.08012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00313"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02292"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01930"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02390"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02441"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02488"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02512"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02609"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02629"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02743"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1501.05108"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.00252"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.10073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.01158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.07875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02828"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.07180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08042"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02162"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00725"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.02501">
<title>CNNs are Globally Optimal Given Multi-Layer Support. (arXiv:1712.02501v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.02501</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gradient Descent (SGD) is the central workhorse for training
modern CNNs. Although giving impressive empirical performance it can be slow to
converge. In this paper we explore a novel strategy for training a CNN using an
alternation strategy that offers substantial speedups during training. We make
the following contributions: (i) replace the ReLU non-linearity within a CNN
with positive hard-thresholding, (ii) reinterpret this non-linearity as a
binary state vector making the entire CNN linear if the multi-layer support is
known, and (iii) demonstrate that under certain conditions a global optima to
the CNN can be found through local descent. We then employ a novel alternation
strategy (between weights and support) for CNN training that leads to
substantially faster convergence rates, nice theoretical properties, and
achieving state of the art results across large scale datasets (e.g. ImageNet)
as well as other standard benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1&quot;&gt;Chen Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1&quot;&gt;Simon Lucey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1511.04401">
<title>Symbol Grounding Association in Multimodal Sequences with Missing Elements. (arXiv:1511.04401v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1511.04401</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we extend a symbolic association framework for being able to
handle missing elements in multimodal sequences. The general scope of the work
is the symbolic associations of object-word mappings as it happens in language
development in infants. In other words, two different representations of the
same abstract concepts can associate in both directions. This scenario has been
long interested in Artificial Intelligence, Psychology, and Neuroscience. In
this work, we extend a recent approach for multimodal sequences (visual and
audio) to also cope with missing elements in one or both modalities. Our method
uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based
on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We
propose to include an extra step for the combination with the max operation for
exploiting the common elements between both sequences. The motivation behind is
that the combination acts as a condition selector for choosing the best
representation from both LSTMs. We evaluated the proposed extension in the
following scenarios: missing elements in one modality (visual or audio) and
missing elements in both modalities (visual and sound). The performance of our
extension reaches better results than the original model and similar results to
individual LSTM trained in each modality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1&quot;&gt;Thomas M. Breuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1&quot;&gt;Marcus Liwicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.00369">
<title>New Ideas for Brain Modelling 3. (arXiv:1612.00369v6 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1612.00369</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers a process for the creation and subsequent firing of
sequences of neuronal patterns, as might be found in the human brain. The scale
is one of larger patterns emerging from an ensemble mass, possibly through some
type of energy equation and a reduction procedure. The links between the
patterns can be formed naturally, as a residual effect of the pattern creation
itself. This paper follows-on closely from the earlier research, including two
earlier papers in the series and uses the ideas of entropy and cohesion. With a
small addition, it is possible to show how the inter-pattern links can be
determined. A new compact Grid form of an earlier Counting Mechanism is also
demonstrated. It is possible to explain how a very basic repeating structure
can form the arbitrary patterns and activation sequences between them, and a
key question of how nodes synchronise may even be answerable. The paper
finishes with an implementation architecture, for the realisation and storage
of knowledge and memory, as part of a general design, based on distributed
neural components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.10371">
<title>Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks. (arXiv:1703.10371v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1703.10371</link>
<description rdf:parseType="Literal">&lt;p&gt;Biological plastic neural networks are systems of extraordinary computational
capabilities shaped by evolution, development, and lifetime learning. The
interplay of these elements leads to the emergence of adaptive behavior and
intelligence. Inspired by such intricate natural phenomena, Evolved Plastic
Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed
plastic neural networks with a large variety of dynamics, architectures, and
plasticity rules: these artificial systems are composed of inputs, outputs, and
plastic components that change in response to experiences in an environment.
These systems may autonomously discover novel adaptive algorithms, and lead to
hypotheses on the emergence of biological adaptation. EPANNs have seen
considerable progress over the last two decades. Current scientific and
technological advances in artificial neural networks are now setting the
conditions for radically new approaches and results. In particular, the
limitations of hand-designed networks could be overcome by more flexible and
innovative solutions. This paper brings together a variety of inspiring ideas
that define the field of EPANNs. The main methods and results are reviewed.
Finally, new opportunities and developments are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltoggio_A/0/1/0/all/0/1&quot;&gt;Andrea Soltoggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1&quot;&gt;Sebastian Risi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07061">
<title>Forecasting day-ahead electricity prices in Europe: the importance of considering market integration. (arXiv:1708.07061v3 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07061</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the increasing integration among electricity markets, in this
paper we propose two different methods to incorporate market integration in
electricity price forecasting and to improve the predictive performance. First,
we propose a deep neural network that considers features from connected markets
to improve the predictive accuracy in a local market. To measure the importance
of these features, we propose a novel feature selection algorithm that, by
using Bayesian optimization and functional analysis of variance, evaluates the
effect of the features on the algorithm performance. In addition, using market
integration, we propose a second model that, by simultaneously predicting
prices from two markets, improves the forecasting accuracy even further. As a
case study, we consider the electricity market in Belgium and the improvements
in forecasting accuracy when using various French electricity features. We show
that the two proposed models lead to improvements that are statistically
significant. Particularly, due to market integration, the predictive accuracy
is improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage
error). In addition, we show that the proposed feature selection algorithm is
able to perform a correct assessment, i.e. to discard the irrelevant features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Lago_J/0/1/0/all/0/1&quot;&gt;Jesus Lago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Ridder_F/0/1/0/all/0/1&quot;&gt;Fjo De Ridder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Vrancx_P/0/1/0/all/0/1&quot;&gt;Peter Vrancx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Schutter_B/0/1/0/all/0/1&quot;&gt;Bart De Schutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.08012">
<title>Deep learning with convolutional neural networks for decoding and visualization of EEG pathology. (arXiv:1708.08012v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.08012</link>
<description rdf:parseType="Literal">&lt;p&gt;We apply convolutional neural networks (ConvNets) to the task of
distinguishing pathological from normal EEG recordings in the Temple University
Hospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet
architectures recently shown to decode task-related information from EEG at
least as well as established algorithms designed for this purpose. In decoding
EEG pathology, both ConvNets reached substantially better accuracies (about 6%
better, ~85% vs. ~79%) than the only published result for this dataset, and
were still better when using only 1 minute of each recording for training and
only six seconds of each recording for testing. We used automated methods to
optimize architectural hyperparameters and found intriguingly different ConvNet
architectures, e.g., with max pooling as the only nonlinearity. Visualizations
of the ConvNet decoding behavior showed that they used spectral power changes
in the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside
other features, consistent with expectations derived from spectral analysis of
the EEG data and from the textual medical reports. Analysis of the textual
medical reports also highlighted the potential for accuracy increases by
integrating contextual information, such as the age of subjects. In summary,
the ConvNets and visualization techniques used in this study constitute a next
step towards clinically useful automated EEG diagnosis and establish a new
baseline for future work on this topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gemein_L/0/1/0/all/0/1&quot;&gt;Lukas Gemein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eggensperger_K/0/1/0/all/0/1&quot;&gt;Katharina Eggensperger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06114">
<title>Geometric Semantic Genetic Programming Algorithm and Slump Prediction. (arXiv:1709.06114v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06114</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on the performance of recycled concrete as building material in the
current world is an important subject. Given the complex composition of
recycled concrete, conventional methods for forecasting slump scarcely obtain
satisfactory results. Based on theory of nonlinear prediction method, we
propose a recycled concrete slump prediction model based on geometric semantic
genetic programming (GSGP) and combined it with recycled concrete features.
Tests show that the model can accurately predict the recycled concrete slump by
using the established prediction model to calculate the recycled concrete slump
with different mixing ratios in practical projects and by comparing the
predicted values with the experimental values. By comparing the model with
several other nonlinear prediction models, we can conclude that GSGP has higher
accuracy and reliability than conventional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Juncai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qingwen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00313">
<title>Avoiding Your Teacher&apos;s Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00313</link>
<description rdf:parseType="Literal">&lt;p&gt;Training deep neural networks requires massive amounts of training data, but
for many tasks only limited labeled data is available. This makes weak
supervision attractive, using weak or noisy signals like the output of
heuristic methods or user click-through data for training. In a semi-supervised
setting, we can use a large set of data with weak labels to pretrain a neural
network and then fine-tune the parameters with a small amount of data with true
labels. This feels intuitively sub-optimal as these two independent stages
leave the model unaware about the varying label quality. What if we could
somehow inform the model about the label quality? In this paper, we propose a
semi-supervised learning method where we train two neural networks in a
multi-task fashion: a &quot;target network&quot; and a &quot;confidence network&quot;. The target
network is optimized to perform a given task and is trained using a large set
of unlabeled data that are weakly annotated. We propose to weight the gradient
updates to the target network using the scores provided by the second
confidence network, which is trained on a small amount of supervised data. Thus
we avoid that the weight updates computed from noisy labels harm the quality of
the target network model. We evaluate our learning strategy on two different
tasks: document ranking and sentiment classification. The results demonstrate
that our approach not only enhances the performance compared to the baselines
but also speeds up the learning process from weak labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1&quot;&gt;Aliaksei Severyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1&quot;&gt;Sascha Rothe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamps_J/0/1/0/all/0/1&quot;&gt;Jaap Kamps&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00519">
<title>An Elementary Analysis of the Probability That a Binomial Random Variable Exceeds Its Expectation. (arXiv:1712.00519v2 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00519</link>
<description rdf:parseType="Literal">&lt;p&gt;We give an elementary proof of the fact that a binomial random variable $X$
with parameters $n$ and $0.29/n \le p &amp;lt; 1$ with probability at least $1/4$
strictly exceeds its expectation. We also show that for $1/n \le p &amp;lt; 1 - 1/n$,
$X$ exceeds its expectation by more than one with probability at least
$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to
infinity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02494">
<title>Adversarial Examples that Fool Detectors. (arXiv:1712.02494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.02494</link>
<description rdf:parseType="Literal">&lt;p&gt;An adversarial example is an example that has been adjusted to produce a
wrong label when presented to a system at test time. To date, adversarial
example constructions have been demonstrated for classifiers, but not for
detectors. If adversarial examples that could fool a detector exist, they could
be used to (for example) maliciously create security hazards on roads populated
with smart vehicles. In this paper, we demonstrate a construction that
successfully fools two standard detectors, Faster RCNN and YOLO. The existence
of such examples is surprising, as attacking a classifier is very different
from attacking a detector, and that the structure of detectors - which must
search for their own bounding box, and which cannot estimate that box very
accurately - makes it quite likely that adversarial patterns are strongly
disrupted. We show that our construction produces adversarial examples that
generalize well across sequences digitally, even though large perturbations are
needed. We also show that our construction yields physical objects that are
adversarial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiajun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sibai_H/0/1/0/all/0/1&quot;&gt;Hussein Sibai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabry_E/0/1/0/all/0/1&quot;&gt;Evan Fabry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02734">
<title>ChemNet: A Transferable and Generalizable Deep Neural Network for Small-Molecule Property Prediction. (arXiv:1712.02734v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02734</link>
<description rdf:parseType="Literal">&lt;p&gt;With access to large datasets, deep neural networks (DNN) have achieved
human-level accuracy in image and speech recognition tasks. However, in
chemistry, availability of large standardized and labelled datasets is scarce,
and many chemical properties of research interest, chemical data is inherently
small and fragmented. In this work, we explore transfer learning techniques in
conjunction with the existing Chemception CNN model, to create a transferable
and generalizable deep neural network for small-molecule property prediction.
Our latest model, ChemNet learns in a semi-supervised manner from inexpensive
labels computed from the ChEMBL database. When fine-tuned to the Tox21, HIV and
FreeSolv dataset, which are 3 separate chemical properties that ChemNet was not
originally trained on, we demonstrate that ChemNet exceeds the performance of
existing Chemception models and other contemporary DNN models. Furthermore, as
ChemNet has been pre-trained on a large diverse chemical database, it can be
used as a general-purpose plug-and-play deep neural network for the prediction
of novel small-molecule chemical properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02292">
<title>Measuring Relations Between Concepts In Conceptual Spaces. (arXiv:1707.02292v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02292</link>
<description rdf:parseType="Literal">&lt;p&gt;The highly influential framework of conceptual spaces provides a geometric
way of representing knowledge. Instances are represented by points in a
high-dimensional space and concepts are represented by regions in this space.
Our recent mathematical formalization of this framework is capable of
representing correlations between different domains in a geometric way. In this
paper, we extend our formalization by providing quantitative mathematical
definitions for the notions of concept size, subsethood, implication,
similarity, and betweenness. This considerably increases the representational
power of our formalization by introducing measurable ways of describing
relations between concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechberger_L/0/1/0/all/0/1&quot;&gt;Lucas Bechberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhnberger_K/0/1/0/all/0/1&quot;&gt;Kai-Uwe K&amp;#xfc;hnberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05412">
<title>IKBT: solving closed-form Inverse Kinematics with Behavior Tree. (arXiv:1711.05412v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05412</link>
<description rdf:parseType="Literal">&lt;p&gt;Serial robot arms have complicated kinematic equations which must be solved
to write effective arm planning and control software (the Inverse Kinematics
Problem). Existing software packages for inverse kinematics often rely on
numerical methods which have significant shortcomings. Here we report a new
symbolic inverse kinematics solver which overcomes the limitations of numerical
methods, and the shortcomings of previous symbolic software packages. We
integrate Behavior Trees, an execution planning framework previously used for
controlling intelligent robot behavior, to organize the equation solving
process, and a modular architecture for each solution technique. The system
successfully solved, generated a LaTex report, and generated a Python code
template for 18 out of 19 example robots of 4-6 DOF. The system is readily
extensible, maintainable, and multi-platform with few dependencies. The
complete package is available with a Modified BSD license on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dianmu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1&quot;&gt;Blake Hannaford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07979">
<title>Posterior Sampling for Large Scale Reinforcement Learning. (arXiv:1711.07979v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07979</link>
<description rdf:parseType="Literal">&lt;p&gt;Posterior sampling for reinforcement learning (PSRL) is a popular algorithm
for learning to control an unknown Markov decision process (MDP). PSRL
maintains a distribution over MDP parameters and in an episodic fashion samples
MDP parameters, computes the optimal policy for them and executes it. A special
case of PSRL is where at the end of each episode the MDP resets to the initial
state distribution. Extensions of this idea to general MDPs without state
resetting has so far produced non-practical algorithms and in some cases buggy
theoretical analysis. This is due to the difficulty of analyzing regret under
episode switching schedules that depend on random variables of the true
underlying model. We propose a solution to this problem that involves using a
deterministic, model-independent episode switching schedule, and establish a
Bayes regret bound under mild assumptions. Our algorithm termed deterministic
schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space
complexity. Our result is more generally applicable to continuous state action
problems. We demonstrate how this algorithm is well suited for sequential
recommendation problems such as points of interest (POI). We derive a general
procedure for parameterizing the underlying MDPs, to create action condition
dynamics from passive data, that do not contain actions. We prove that such
parameterization satisfies the assumptions of our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theocharous_G/0/1/0/all/0/1&quot;&gt;Georgios Theocharous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Zheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1&quot;&gt;Yasin Abbasi-Yadkori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1&quot;&gt;Nikos Vlassis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01930">
<title>Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors. (arXiv:1712.01930v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01930</link>
<description rdf:parseType="Literal">&lt;p&gt;Personal electronic devices such as smartphones give access to a broad range
of behavioral signals that can be used to learn about the characteristics and
preferences of individuals. In this study we explore the connection between
demographic and psychological attributes and digital records for a cohort of
7,633 people, closely representative of the US population with respect to
gender, age, geographical distribution, education, and income. We collected
self-reported assessments on validated psychometric questionnaires based on
both the Moral Foundations and Basic Human Values theories, and combined this
information with passively-collected multi-modal digital data from web browsing
behavior, smartphone usage and demographic data. Then, we designed a machine
learning framework to infer both the demographic and psychological attributes
from the behavioral data. In a cross-validated setting, our model is found to
predict demographic attributes with good accuracy (weighted AUC scores of 0.90
for gender, 0.71 for age, 0.74 for ethnicity). Our weighted AUC scores for
Moral Foundation attributes (0.66) and Human Values attributes (0.60) suggest
that accurate prediction of complex psychometric attributes is more challenging
but feasible. This connection might prove useful for designing personalized
services, communication strategies, and interventions, and can be used to
sketch a portrait of people with similar worldviews.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalimeri_K/0/1/0/all/0/1&quot;&gt;Kyriaki Kalimeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beiro_M/0/1/0/all/0/1&quot;&gt;Mariano G. Beiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delfino_M/0/1/0/all/0/1&quot;&gt;Matteo Delfino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raleigh_R/0/1/0/all/0/1&quot;&gt;Robert Raleigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cattuto_C/0/1/0/all/0/1&quot;&gt;Ciro Cattuto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02369">
<title>Achieving the time of $1$-NN, but the accuracy of $k$-NN. (arXiv:1712.02369v1 [math.ST])</title>
<link>http://arxiv.org/abs/1712.02369</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple approach which, given distributed computing resources,
can nearly achieve the accuracy of $k$-NN prediction, while matching (or
improving) the faster prediction time of $1$-NN. The approach consists of
aggregating denoised $1$-NN predictors over a small number of distributed
subsamples. We show, both theoretically and experimentally, that small
subsample sizes suffice to attain similar performance as $k$-NN, without
sacrificing the computational efficiency of $1$-NN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Lirong Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kpotufe_S/0/1/0/all/0/1&quot;&gt;Samory Kpotufe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02390">
<title>Noisy Natural Gradient as Variational Inference. (arXiv:1712.02390v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.02390</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining the flexibility of deep learning with Bayesian uncertainty
estimation has long been a goal in our field, and many modern approaches are
based on variational Bayes. Unfortunately, one is forced to choose between
overly simplistic variational families (e.g. fully factorized) or expensive and
complicated inference procedures. We show that natural gradient ascent with
adaptive weight noise can be interpreted as fitting a variational posterior to
maximize the evidence lower bound (ELBO). This insight allows us to train full
covariance, fully factorized, and matrix variate Gaussian variational
posteriors using noisy versions of natural gradient, Adam, and K-FAC,
respectively. On standard regression benchmarks, our noisy K-FAC algorithm
makes better predictions and matches HMC&apos;s predictive variances better than
existing methods. Its improved uncertainty estimates lead to more efficient
exploration in the settings of active learning and intrinsic motivation for
reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guodong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shengyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02412">
<title>Estimating the error variance in a high-dimensional linear model. (arXiv:1712.02412v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.02412</link>
<description rdf:parseType="Literal">&lt;p&gt;The lasso has been studied extensively as a tool for estimating the
coefficient vector in the high-dimensional linear model; however, considerably
less is known about estimating the error variance. Indeed, most well-known
theoretical properties of the lasso, including recent advances in selective
inference with the lasso, are established under the assumption that the
underlying error variance is known. Yet the error variance in practice is, of
course, unknown. In this paper, we propose the natural lasso estimator for the
error variance, which maximizes a penalized likelihood objective. A key aspect
of the natural lasso is that the likelihood is expressed in terms of the
natural parameterization of the multiparameter exponential family of a Gaussian
with unknown mean and variance. The result is a remarkably simple estimator
with provably good performance in terms of mean squared error. These
theoretical results do not require placing any assumptions on the design matrix
or the true regression coefficients. We also propose a companion estimator,
called the organic lasso, which theoretically does not require tuning of the
regularization parameter. Both estimators do well compared to preexisting
methods, especially in settings where successful recovery of the true support
of the coefficient vector is hard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Guo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1&quot;&gt;Jacob Bien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02432">
<title>Sparse learning of stochastic dynamic equations. (arXiv:1712.02432v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02432</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid increase of available data for complex systems, there is great
interest in the extraction of physically relevant information from massive
datasets. Recently, a framework called Sparse Identification of Nonlinear
Dynamics (SINDy) has been introduced to identify the governing equations of
dynamical systems from simulation data. In this study, we extend SINDy to
stochastic dynamical systems, which are frequently used to model biophysical
processes. We prove the asymptotic correctness of stochastics SINDy in the
infinite data limit, both in the original and projected variables. We discuss
algorithms to solve the sparse regression problem arising from the practical
implementation of SINDy, and show that cross validation is an essential tool to
determine the right level of sparsity. We demonstrate the proposed methodology
on two test systems, namely, the diffusion in a one-dimensional potential, and
the projected dynamics of a two-dimensional diffusion process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boninsegna_L/0/1/0/all/0/1&quot;&gt;Lorenzo Boninsegna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nuske_F/0/1/0/all/0/1&quot;&gt;Feliks N&amp;#xfc;ske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Clementi_C/0/1/0/all/0/1&quot;&gt;Cecilia Clementi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02441">
<title>A Novel Model for Arbitration between Planning and Habitual Control Systems. (arXiv:1712.02441v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1712.02441</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well established that humans decision making and instrumental control
uses multiple systems, some which use habitual action selection and some which
require deliberate planning. Deliberate planning systems use predictions of
action-outcomes using an internal model of the agent&apos;s environment, while
habitual action selection systems learn to automate by repeating previously
rewarded actions. Habitual control is computationally efficient but may be
inflexible in changing environments. Conversely, deliberate planning may be
computationally expensive, but flexible in dynamic environments. This paper
proposes a general architecture comprising both control paradigms by
introducing an arbitrator that controls which subsystem is used at any time.
This system is implemented for a target-reaching task with a simulated
two-joint robotic arm that comprises a supervised internal model and deep
reinforcement learning. Through permutation of target-reaching conditions, we
demonstrate that the proposed is capable of rapidly learning kinematics of the
system without a priori knowledge, and is robust to (A) changing environmental
reward and kinematics, and (B) occluded vision. The arbitrator model is
compared to exclusive deliberate planning with the internal model and exclusive
habitual control instances of the model. The results show how such a model can
harness the benefits of both systems, using fast decisions in reliable
circumstances while optimizing performance in changing environments. In
addition, the proposed model learns very fast. Finally, the system which
includes internal models is able to reach the target under the visual
occlusion, while the pure habitual system is unable to operate sufficiently
under such conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1&quot;&gt;Farzaneh S. Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trappenberg_T/0/1/0/all/0/1&quot;&gt;Thomas P. Trappenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02488">
<title>Cost-sensitive detection with variational autoencoders for environmental acoustic sensing. (arXiv:1712.02488v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02488</link>
<description rdf:parseType="Literal">&lt;p&gt;Environmental acoustic sensing involves the retrieval and processing of audio
signals to better understand our surroundings. While large-scale acoustic data
make manual analysis infeasible, they provide a suitable playground for machine
learning approaches. Most existing machine learning techniques developed for
environmental acoustic sensing do not provide flexible control of the trade-off
between the false positive rate and the false negative rate. This paper
presents a cost-sensitive classification paradigm, in which the
hyper-parameters of classifiers and the structure of variational autoencoders
are selected in a principled Neyman-Pearson framework. We examine the
performance of the proposed approach using a dataset from the HumBug project
which aims to detect the presence of mosquitoes using sound collected by simple
embedded devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kiskin_I/0/1/0/all/0/1&quot;&gt;Ivan Kiskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zilli_D/0/1/0/all/0/1&quot;&gt;Davide Zilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sinka_M/0/1/0/all/0/1&quot;&gt;Marianne Sinka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Henry Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Kathy Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02512">
<title>Gini-regularized Optimal Transport with an Application to Spatio-Temporal Forecasting. (arXiv:1712.02512v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02512</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapidly growing product lines and services require a finer-granularity
forecast that considers geographic locales. However the open question remains,
how to assess the quality of a spatio-temporal forecast? In this manuscript we
introduce a metric to evaluate spatio-temporal forecasts. This metric is based
on an Opti- mal Transport (OT) problem. The metric we propose is a constrained
OT objec- tive function using the Gini impurity function as a regularizer. We
demonstrate through computer experiments both the qualitative and the
quantitative charac- teristics of the Gini regularized OT problem. Moreover, we
show that the Gini regularized OT problem converges to the classical OT
problem, when the Gini regularized problem is considered as a function of
{\lambda}, the regularization parame-ter. The convergence to the classical OT
solution is faster than the state-of-the-art Entropic-regularized OT[Cuturi,
2013] and results in a numerically more stable algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_L/0/1/0/all/0/1&quot;&gt;Lucas Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Razoumov_L/0/1/0/all/0/1&quot;&gt;Leo Razoumov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Lin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02519">
<title>Convergence Rates of Variational Posterior Distributions. (arXiv:1712.02519v1 [math.ST])</title>
<link>http://arxiv.org/abs/1712.02519</link>
<description rdf:parseType="Literal">&lt;p&gt;We study convergence rates of variational posterior distributions for
nonparametric and high-dimensional inference. We formulate general conditions
on prior, likelihood, and variational class that characterize the convergence
rates. Under similar &quot;prior mass and testing&quot; conditions considered in the
literature, the rate is found to be the sum of two terms. The first term stands
for the convergence rate of the true posterior distribution, and the second
term is contributed by the variational approximation error. For a class of
priors that admit the structure of a mixture of product measures, we propose a
novel prior mass condition, under which the variational approximation error of
the generalized mean-field class is dominated by convergence rate of the true
posterior. We demonstrate the applicability of our general results for various
models, prior distributions and variational classes by deriving convergence
rates of the corresponding variational posteriors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fengshuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chao Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02527">
<title>Learning Random Fourier Features by Hybrid Constrained Optimization. (arXiv:1712.02527v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02527</link>
<description rdf:parseType="Literal">&lt;p&gt;The kernel embedding algorithm is an important component for adapting kernel
methods to large datasets. Since the algorithm consumes a major computation
cost in the testing phase, we propose a novel teacher-learner framework of
learning computation-efficient kernel embeddings from specific data. In the
framework, the high-precision embeddings (teacher) transfer the data
information to the computation-efficient kernel embeddings (learner). We
jointly select informative embedding functions and pursue an orthogonal
transformation between two embeddings. We propose a novel approach of
constrained variational expectation maximization (CVEM), where the alternate
direction method of multiplier (ADMM) is applied over a nonconvex domain in the
maximization step. We also propose two specific formulations based on the
prevalent Random Fourier Feature (RFF), the masked and blocked version of
Computation-Efficient RFF (CERF), by imposing a random binary mask or a block
structure on the transformation matrix. By empirical studies of several
applications on different real-world datasets, we demonstrate that the CERF
significantly improves the performance of kernel methods upon the RFF, under
certain arithmetic operation requirements, and suitable for structured matrix
multiplication in Fastfood type algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wangni_J/0/1/0/all/0/1&quot;&gt;Jianqiao Wangni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhuo_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02609">
<title>Solving internal covariate shift in deep learning with linked neurons. (arXiv:1712.02609v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02609</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a novel solution to the problem of internal covariate
shift and dying neurons using the concept of linked neurons. We define the
neuron linkage in terms of two constraints: first, all neuron activations in
the linkage must have the same operating point. That is to say, all of them
share input weights. Secondly, a set of neurons is linked if and only if there
is at least one member of the linkage that has a non-zero gradient in regard to
the input of the activation function. This means that for any input in the
activation function, there is at least one member of the linkage that operates
in a non-flat and non-zero area. This simple change has profound implications
in the network learning dynamics. In this article we explore the consequences
of this proposal and show that by using this kind of units, internal covariate
shift is implicitly solved. As a result of this, the use of linked neurons
allows to train arbitrarily large networks without any architectural or
algorithmic trick, effectively removing the need of using re-normalization
schemes such as Batch Normalization, which leads to halving the required
training time. It also solves the problem of the need for standarized input
data. Results show that the units using the linkage not only do effectively
solve the aforementioned problems, but are also a competitive alternative with
respect to state-of-the-art with very promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molina_C/0/1/0/all/0/1&quot;&gt;Carles Roger Riera Molina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vila_O/0/1/0/all/0/1&quot;&gt;Oriol Pujol Vila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02629">
<title>Differentially Private Variational Dropout. (arXiv:1712.02629v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02629</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks with their large number of parameters are highly
flexible learning systems. The high flexibility in such networks brings with
some serious problems such as overfitting, and regularization is used to
address this problem. A currently popular and effective regularization
technique for controlling the overfitting is dropout. Often, large data
collections required for neural networks contain sensitive information such as
the medical histories of patients, and the privacy of the training data should
be protected. In this paper, we modify the recently proposed variational
dropout technique which provided an elegant Bayesian interpretation to dropout,
and show that the intrinsic noise in the variational dropout can be exploited
to obtain a degree of differential privacy. The iterative nature of training
neural networks presents a challenge for privacy-preserving estimation since
multiple iterations increase the amount of noise added. We overcome this by
using a relaxed notion of differential privacy, called concentrated
differential privacy, which provides tighter estimates on the overall privacy
loss. We demonstrate the accuracy of our privacy-preserving variational dropout
algorithm on benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermis_B/0/1/0/all/0/1&quot;&gt;Beyza Ermis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cemgil_A/0/1/0/all/0/1&quot;&gt;Ali Taylan Cemgil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02640">
<title>High-dimensional robust regression and outliers detection with SLOPE. (arXiv:1712.02640v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02640</link>
<description rdf:parseType="Literal">&lt;p&gt;The problems of outliers detection and robust regression in a
high-dimensional setting are fundamental in statistics, and have numerous
applications. Following a recent set of works providing methods for
simultaneous robust regression and outliers detection, we consider in this
paper a model of linear regression with individual intercepts, in a
high-dimensional setting. We introduce a new procedure for simultaneous
estimation of the linear regression coefficients and intercepts, using two
dedicated sorted-$\ell_1$ penalizations, also called SLOPE. We develop a
complete theory for this problem: first, we provide sharp upper bounds on the
statistical estimation error of both the vector of individual intercepts and
regression coefficients. Second, we give an asymptotic control on the False
Discovery Rate (FDR) and statistical power for support selection of the
individual intercepts. As a consequence, this paper is the first to introduce a
procedure with guaranteed FDR and statistical power control for outliers
detection under the mean-shift model. Numerical illustrations, with a
comparison to recent alternative approaches, are provided on both simulated and
several real-world datasets. Experiments are conducted using an open-source
software written in Python and C++.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Virouleau_A/0/1/0/all/0/1&quot;&gt;Alain Virouleau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guilloux_A/0/1/0/all/0/1&quot;&gt;Agathe Guilloux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bogdan_M/0/1/0/all/0/1&quot;&gt;Malgorzata Bogdan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02658">
<title>Using SVDD in SimpleMKL for 3D-Shapes Filtering. (arXiv:1712.02658v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02658</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes the adaptation of Support Vector Data Description (SVDD)
to the multiple kernel case (MK-SVDD), based on SimpleMKL. It also introduces a
variant called Slim-MK-SVDD that is able to produce a tighter frontier around
the data. For the sake of comparison, the equivalent methods are also developed
for One-Class SVM, known to be very similar to SVDD for certain shapes of
kernels.
&lt;/p&gt;
&lt;p&gt;Those algorithms are illustrated in the context of 3D-shapes filtering and
outliers detection. For the 3D-shapes problem, the objective is to be able to
select a sub-category of 3D-shapes, each sub-category being learned with our
algorithm in order to create a filter. For outliers detection, we apply the
proposed algorithms for unsupervised outliers detection as well as for the
supervised case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loosli_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;lle Loosli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aboubacar_H/0/1/0/all/0/1&quot;&gt;Hattoibe Aboubacar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02675">
<title>Is My Model Flexible Enough? Information-Theoretic Model Check. (arXiv:1712.02675v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02675</link>
<description rdf:parseType="Literal">&lt;p&gt;The choice of model class is fundamental in statistical learning and system
identification, no matter whether the class is derived from physical principles
or is a generic black-box. We develop a method to evaluate the specified model
class by assessing its capability of reproducing data that is similar to the
observed data record. This model check is based on the information-theoretic
properties of models viewed as data generators and is applicable to e.g.
sequential data and nonlinear dynamical models. The method can be understood as
a specific two-sided posterior predictive test. We apply the
information-theoretic model check to both synthetic and real data and compare
it with a classical whiteness test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Svensson_A/0/1/0/all/0/1&quot;&gt;Andreas Svensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zachariah_D/0/1/0/all/0/1&quot;&gt;Dave Zachariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02679">
<title>AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training. (arXiv:1712.02679v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.02679</link>
<description rdf:parseType="Literal">&lt;p&gt;Highly distributed training of Deep Neural Networks (DNNs) on future compute
platforms (offering 100 of TeraOps/s of computational capacity) is expected to
be severely communication constrained. To overcome this limitation, new
gradient compression techniques are needed that are computationally friendly,
applicable to a wide variety of layers seen in Deep Neural Networks and
adaptable to variations in network architectures as well as their
hyper-parameters. In this paper we introduce a novel technique - the Adaptive
Residual Gradient Compression (AdaComp) scheme. AdaComp is based on localized
selection of gradient residues and automatically tunes the compression rate
depending on local activity. We show excellent results on a wide spectrum of
state of the art Deep Learning models in multiple domains (vision, speech,
language), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers
(SGD with momentum, Adam) and network parameters (number of learners,
minibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate
end-to-end compression rates of ~200X for fully-connected and recurrent layers,
and ~40X for convolutional layers, without any noticeable degradation in model
accuracies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chia-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jungwook Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brand_D/0/1/0/all/0/1&quot;&gt;Daniel Brand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Ankur Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Kailash Gopalakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02743">
<title>End-to-end Learning of Deterministic Decision Trees. (arXiv:1712.02743v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.02743</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional decision trees have a number of favorable properties, including
interpretability, a small computational footprint and the ability to learn from
little training data. However, they lack a key quality that has helped fuel the
deep learning revolution: that of being end-to-end trainable, and to learn from
scratch those features that best allow to solve a given supervised learning
problem. Recent work (Kontschieder 2015) has addressed this deficit, but at the
cost of losing a main attractive trait of decision trees: the fact that each
sample is routed along a small subset of tree nodes only. We here propose a
model and Expectation-Maximization training scheme for decision trees that are
fully probabilistic at train time, but after a deterministic annealing process
become deterministic at test time. We also analyze the learned oblique split
parameters on image datasets and show that Neural Networks can be trained at
each split node. In summary, we present the first end-to-end learning scheme
for deterministic decision trees and present results on par with or superior to
published standard oblique decision tree algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hehn_T/0/1/0/all/0/1&quot;&gt;Thomas Hehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hamprecht_F/0/1/0/all/0/1&quot;&gt;Fred A. Hamprecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1501.05108">
<title>BDgraph: An R Package for Bayesian Structure Learning in Graphical Models. (arXiv:1501.05108v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1501.05108</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphical models provide powerful tools to uncover complicated patterns in
multivariate data and are commonly used in Bayesian statistics and machine
learning. In this paper, we introduce an R package BDgraph which performs
Bayesian structure learning for general undirected graphical models with either
continuous or discrete variables. The package efficiently implements recent
improvements in the Bayesian literature. To speed up computations, the
computationally intensive tasks have been implemented in C++ and interfaced
with R. In addition, the package contains several functions for simulation and
visualization, as well as two multivariate datasets taken from the literature
and are used to describe the package capabilities. The paper includes a brief
overview of the statistical methods which have been implemented in the package.
The main body of the paper explains how to use the package. Furthermore, we
illustrate the package&apos;s functionality in both real and artificial examples, as
well as in an extensive simulation study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohammadi_A/0/1/0/all/0/1&quot;&gt;Abdolreza Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wit_E/0/1/0/all/0/1&quot;&gt;Ernst C. Wit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.00252">
<title>Fast Rates for General Unbounded Loss Functions: from ERM to Generalized Bayes. (arXiv:1605.00252v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1605.00252</link>
<description rdf:parseType="Literal">&lt;p&gt;We present new excess risk bounds for general unbounded loss functions
including log loss and squared loss, where the distribution of the losses may
be heavy-tailed. The bounds hold for general estimators, but they are optimized
when applied to $\eta$-generalized Bayesian, MDL, and ERM estimators. When
applied with log loss, the bounds imply convergence rates for generalized
Bayesian inference under misspecification in terms of a generalization of the
Hellinger metric as long as the learning rate $\eta$ is set correctly. For
general loss functions, our bounds rely on two separate conditions: the
$v$-GRIP (generalized reversed information projection) conditions, which
control the lower tail of the excess loss; and the newly introduced witness
condition, which controls the upper tail. The parameter $v$ in the $v$-GRIP
conditions determines the achievable rate and is akin to the exponent in the
well-known Tsybakov margin condition and the Bernstein condition for bounded
losses, which the $v$-GRIP conditions generalize; favorable $v$ in combination
with small model complexity leads to $\tilde{O}(1/n)$ rates. The witness
condition allows us to connect the excess risk to an &apos;annealed&apos; version
thereof, by which we generalize several previous results connecting Hellinger
and R\&apos;enyi divergence to KL divergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grunwald_P/0/1/0/all/0/1&quot;&gt;Peter D. Gr&amp;#xfc;nwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1&quot;&gt;Nishant A. Mehta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.10073">
<title>Complex-valued Gaussian Process Regression for Time Series Analysis. (arXiv:1611.10073v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.10073</link>
<description rdf:parseType="Literal">&lt;p&gt;The construction of synthetic complex-valued signals from real-valued
observations is an important step in many time series analysis techniques. The
most widely used approach is based on the Hilbert transform, which maps the
real-valued signal into its quadrature component. In this paper, we define a
probabilistic generalization of this approach. We model the observable
real-valued signal as the real part of a latent complex-valued Gaussian
process. In order to obtain the appropriate statistical relationship between
its real and imaginary parts, we define two new classes of complex-valued
covariance functions. Through an analysis of simulated chirplets and stochastic
oscillations, we show that the resulting Gaussian process complex-valued signal
provides a better estimate of the instantaneous amplitude and frequency than
the established approaches. Furthermore, the complex-valued Gaussian process
regression allows to incorporate prior information about the structure in
signal and noise and thereby to tailor the analysis to the features of the
signal. As a example, we analyze the non-stationary dynamics of brain
oscillations in the alpha band, as measured using magneto-encephalography.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ambrogioni_L/0/1/0/all/0/1&quot;&gt;Luca Ambrogioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maris_E/0/1/0/all/0/1&quot;&gt;Eric Maris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.01158">
<title>Properties and Bayesian fitting of restricted Boltzmann machines. (arXiv:1612.01158v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1612.01158</link>
<description rdf:parseType="Literal">&lt;p&gt;A restricted Boltzmann machine (RBM) is an undirected graphical model
constructed for discrete or continuous random variables, with two layers, one
hidden and one visible, and no conditional dependency within a layer. In recent
years, RBMs have risen to prominence due to their connection to deep learning.
By treating a hidden layer of one RBM as the visible layer in a second RBM, a
deep architecture can be created. RBMs are thought to thereby have the ability
to encode very complex and rich structures in data, making them attractive for
supervised learning. However, the generative behavior of RBMs is largely
unexplored. In this paper, we discuss the relationship between RBM parameter
specification in the binary case and model properties such as degeneracy,
instability and uninterpretability. We also describe the difficulties that
arise in likelihood-based and Bayes fitting of such (highly flexible) models,
especially as Gibbs sampling (quasi-Bayes) methods are often advocated for the
RBM model structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaplan_A/0/1/0/all/0/1&quot;&gt;Andee Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nordman_D/0/1/0/all/0/1&quot;&gt;Daniel Nordman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vardeman_S/0/1/0/all/0/1&quot;&gt;Stephen Vardeman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.07875">
<title>Wasserstein GAN. (arXiv:1701.07875v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1701.07875</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new algorithm named WGAN, an alternative to traditional GAN
training. In this new model, we show that we can improve the stability of
learning, get rid of problems like mode collapse, and provide meaningful
learning curves useful for debugging and hyperparameter searches. Furthermore,
we show that the corresponding optimization problem is sound, and provide
extensive theoretical work highlighting the deep connections to other distances
between distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arjovsky_M/0/1/0/all/0/1&quot;&gt;Martin Arjovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chintala_S/0/1/0/all/0/1&quot;&gt;Soumith Chintala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;on Bottou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02828">
<title>Integral Transforms from Finite Data: An Application of Gaussian Process Regression to Fourier Analysis. (arXiv:1704.02828v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02828</link>
<description rdf:parseType="Literal">&lt;p&gt;Computing accurate estimates of the Fourier transform of analog signals from
discrete data points is important in many fields of science and engineering.
The conventional approach of performing the discrete Fourier transform of the
data implicitly assumes periodicity and bandlimitedness of the signal. In this
paper, we use Gaussian process regression to estimate the Fourier transform (or
any other integral transform) without making these assumptions. This is
possible because the posterior expectation of Gaussian process regression maps
a finite set of samples to a function defined on the whole real line, expressed
as a linear combination of covariance functions. We estimate the covariance
function from the data using an appropriately designed gradient ascent method
that constrains the solution to a linear combination of tractable kernel
functions. This procedure results in a posterior expectation of the analog
signal whose Fourier transform can be obtained analytically by exploiting
linearity. Our simulations show that the new method leads to sharper and more
precise estimation of the spectral density both in noise-free and
noise-corrupted signals. We further validate the method in two real-world
applications: the analysis of the yearly fluctuation in atmospheric CO2 level
and the analysis of the spectral content of brain signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ambrogioni_L/0/1/0/all/0/1&quot;&gt;Luca Ambrogioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maris_E/0/1/0/all/0/1&quot;&gt;Eric Maris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01686">
<title>Limitations on Variance-Reduction and Acceleration Schemes for Finite Sum Optimization. (arXiv:1706.01686v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01686</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the conditions under which one is able to efficiently apply
variance-reduction and acceleration schemes on finite sum optimization
problems. First, we show that, perhaps surprisingly, the finite sum structure
by itself, is not sufficient for obtaining a complexity bound of
$\tilde{\cO}((n+L/\mu)\ln(1/\epsilon))$ for $L$-smooth and $\mu$-strongly
convex individual functions - one must also know which individual function is
being referred to by the oracle at each iteration. Next, we show that for a
broad class of first-order and coordinate-descent finite sum algorithms
(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated&apos;
complexity bound of $\tilde{\cO}((n+\sqrt{n L/\mu})\ln(1/\epsilon))$, unless
the strong convexity parameter is given explicitly. Lastly, we show that when
this class of algorithms is used for minimizing $L$-smooth and convex finite
sums, the optimal complexity bound is $\tilde{\cO}(n+L/\epsilon)$, assuming
that (on average) the same update rule is used in every iteration, and
$\tilde{\cO}(n+\sqrt{nL/\epsilon})$, otherwise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Arjevani_Y/0/1/0/all/0/1&quot;&gt;Yossi Arjevani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.07180">
<title>Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.07180</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a general framework --compressive statistical learning-- for
resource-efficient large-scale learning: the training collection is compressed
in one pass into a low-dimensional sketch (a vector of random empirical
generalized moments) that captures the information relevant to the considered
learning task. A near-minimizer of the risk is computed from the sketch through
the solution of a nonlinear least squares problem. We investigate sufficient
sketch sizes to control the generalization error of this procedure. The
framework is illustrated on compressive clustering, compressive Gaussian
mixture Modeling with fixed known variance, and compressive PCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt; (PANAMA), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blanchard_G/0/1/0/all/0/1&quot;&gt;Gilles Blanchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1&quot;&gt;Nicolas Keriven&lt;/a&gt; (PANAMA, UR1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Traonmilin_Y/0/1/0/all/0/1&quot;&gt;Yann Traonmilin&lt;/a&gt; (PANAMA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11303">
<title>Algorithmic learning of probability distributions from random data in the limit. (arXiv:1710.11303v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11303</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of identifying a probability distribution for some given
randomly sampled data in the limit, in the context of algorithmic learning
theory as proposed recently by Vinanyi and Chater. We show that there exists a
computable partial learner for the computable probability measures, while by
Bienvenu, Monin and Shen it is known that there is no computable learner for
the computable probability measures. Our main result is the characterization of
the oracles that compute explanatory learners for the computable (continuous)
probability measures as the high oracles. This provides an analogue of a
well-known result of Adleman and Blum in the context of learning computable
probability distributions. We also discuss related learning notions such as
behaviorally correct learning and orther variations of explanatory learning, in
the context of learning probability distributions from data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barmpalias_G/0/1/0/all/0/1&quot;&gt;George Barmpalias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephan_F/0/1/0/all/0/1&quot;&gt;Frank Stephan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06494">
<title>Improved Bayesian Compression. (arXiv:1711.06494v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06494</link>
<description rdf:parseType="Literal">&lt;p&gt;Compression of Neural Networks (NN) has become a highly studied topic in
recent years. The main reason for this is the demand for industrial scale usage
of NNs such as deploying them on mobile devices, storing them efficiently,
transmitting them via band-limited channels and most importantly doing
inference at scale. In this work, we propose to join the Soft-Weight Sharing
and Variational Dropout approaches that show strong results to define a new
state-of-the-art in terms of model compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Federici_M/0/1/0/all/0/1&quot;&gt;Marco Federici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ullrich_K/0/1/0/all/0/1&quot;&gt;Karen Ullrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08042">
<title>&quot;I know it when I see it&quot;. Visualization and Intuitive Interpretability. (arXiv:1711.08042v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08042</link>
<description rdf:parseType="Literal">&lt;p&gt;Most research on the interpretability of machine learning systems focuses on
the development of a more rigorous notion of interpretability. I suggest that a
better understanding of the deficiencies of the intuitive notion of
interpretability is needed as well. I show that visualization enables but also
impedes intuitive interpretability, as it presupposes two levels of technical
pre-interpretation: dimensionality reduction and regularization. Furthermore, I
argue that the use of positive concepts to emulate the distributed semantic
structure of machine learning models introduces a significant human bias into
the model. As a consequence, I suggest that, if intuitive interpretability is
needed, singular representations of internal model states should be avoided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Offert_F/0/1/0/all/0/1&quot;&gt;Fabian Offert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00535">
<title>Survival-Supervised Topic Modeling with Anchor Words: Characterizing Pancreatitis Outcomes. (arXiv:1712.00535v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00535</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new approach for topic modeling that is supervised by survival
analysis. Specifically, we build on recent work on unsupervised topic modeling
with so-called anchor words by providing supervision through an elastic-net
regularized Cox proportional hazards model. In short, an anchor word being
present in a document provides strong indication that the document is partially
about a specific topic. For example, by seeing &quot;gallstones&quot; in a document, we
are fairly certain that the document is partially about medicine. Our proposed
method alternates between learning a topic model and learning a survival model
to find a local minimum of a block convex optimization problem. We apply our
proposed approach to predicting how long patients with pancreatitis admitted to
an intensive care unit (ICU) will stay in the ICU. Our approach is as accurate
as the best of a variety of baselines while being more interpretable than any
of the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;George H. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weiss_J/0/1/0/all/0/1&quot;&gt;Jeremy C. Weiss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02162">
<title>A trans-disciplinary review of deep learning research for water resources scientists. (arXiv:1712.02162v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02162</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL), a new-generation artificial neural network research, has
made profound strides in recent years. This review paper is intended to provide
water resources scientists with a simple technical overview, trans-disciplinary
progress update, and potentially inspirations about DL. Effective
architectures, more accessible data, advances in regularization, and new
computing power enabled the success of DL. A trans-disciplinary review reveals
that DL is rapidly transforming myriad scientific disciplines including
high-energy physics, astronomy, chemistry, genomics and remote sensing, where
systematic DL toolkits, innovative customizations, and sub-disciplines have
emerged. However, with a few exceptions, its adoption in hydrology has so far
been gradual. The literature suggests that novel regularization techniques can
effectively prevent high-capacity deep networks from overfitting. As a result,
in most scientific disciplines, DL models demonstrated superior predictive and
generalization performance to conventional methods. Meanwhile, less noticed is
that DL may also serve as a scientific exploratory tool. A new area termed &quot;AI
neuroscience&quot;, has been born. This budding sub-discipline is accumulating a
significant body of work, e.g., distilling knowledge obtained in DL networks to
interpretable models, attributing decisions to inputs via back-propagation of
relevance, or visualization of activations. These methods are designed to
interpret the decision process of deep networks and derive insights. While
scientists so far have mostly been using customized, ad-hoc methods for
interpretation, vast opportunities await for DL to propel advancement in water
science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chaopeng Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00725">
<title>Sentiment Classification using Images and Label Embeddings. (arXiv:1712.00725v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.00725</link>
<description rdf:parseType="Literal">&lt;p&gt;In this project we analysed how much semantic information images carry, and
how much value image data can add to sentiment analysis of the text associated
with the images. To better understand the contribution from images, we compared
models which only made use of image data, models which only made use of text
data, and models which combined both data types. We also analysed if this
approach could help sentiment classifiers generalize to unknown sentiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graesser_L/0/1/0/all/0/1&quot;&gt;Laura Graesser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_L/0/1/0/all/0/1&quot;&gt;Lakshay Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1&quot;&gt;Evelina Bakhturina&lt;/a&gt;</dc:creator>
</item></rdf:RDF>