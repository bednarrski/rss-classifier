<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-22T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08194"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06661"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07187"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07228"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07036"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07292"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05247"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06148"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01251"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.03001">
<title>Learning Functions in Large Networks requires Modularity and produces Multi-Agent Dynamics. (arXiv:1807.03001v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03001</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks are abundant in biological systems. Small sized over-represented
network motifs have been discovered, and it has been suggested that these
constitute functional building blocks. We ask whether larger dynamical network
motifs exist in biological networks, thus contributing to the higher-order
organization of a network. To end this, we introduce a gradient descent machine
learning (ML) approach and genetic algorithms to learn larger functional motifs
in contrast to an (unfeasible) exhaustive search. We use the French Flag (FF)
and Switch functional motif as case studies motivated from biology. While our
algorithm successfully learns large functional motifs, we identify a threshold
size of approximately 20 nodes beyond which learning breaks down. Therefore we
investigate the stability of the motifs. We find that the size of the real
negative eigenvalues of the Jacobian decreases with increasing system size,
thus conferring instability. Finally, without imposing learning an input-output
for all the components of the network, we observe that unconstrained middle
components of the network still learn the desired function, a form of
homogeneous team learning. We conclude that the size limitation of
learnability, most likely due to stability constraints, impose a definite
requirement for modularity in networked systems while enabling team learning
within unconstrained parts of the module. Thus, the observation that community
structures and modularity are abundant in biological networks could be
accounted for by a computational compositional network structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;C. H. Huck Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ooi_R/0/1/0/all/0/1&quot;&gt;Rise Ooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hiscock_T/0/1/0/all/0/1&quot;&gt;Tom Hiscock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eguiluz_V/0/1/0/all/0/1&quot;&gt;Victor Eguiluz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08194">
<title>Towards Distributed Coevolutionary GANs. (arXiv:1807.08194v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08194</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have become one of the dominant
methods for deep generative modeling. Despite their demonstrated success on
multiple vision tasks, GANs are difficult to train and much research has been
dedicated towards understanding and improving their gradient-based learning
dynamics. Here, we investigate the use of coevolution, a class of black-box
(gradient-free) co-optimization techniques and a powerful tool in evolutionary
computing, as a supplement to gradient-based GAN training techniques.
Experiments on a simple model that exhibits several of the GAN gradient-based
dynamics (e.g., mode collapse, oscillatory behavior, and vanishing gradients)
show that coevolution is a promising framework for escaping degenerate GAN
training behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Dujaili_A/0/1/0/all/0/1&quot;&gt;Abdullah Al-Dujaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmiedlechner_T/0/1/0/all/0/1&quot;&gt;Tom Schmiedlechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemberg_a/0/1/0/all/0/1&quot;&gt;and Erik Hemberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1&quot;&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06661">
<title>A Hybrid Differential Evolution Approach to Designing Deep Convolutional Neural Networks for Image Classification. (arXiv:1808.06661v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06661</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have demonstrated their superiority in
image classification, and evolutionary computation (EC) methods have recently
been surging to automatically design the architectures of CNNs to save the
tedious work of manually designing CNNs. In this paper, a new hybrid
differential evolution (DE) algorithm with a newly added crossover operator is
proposed to evolve the architectures of CNNs of any lengths, which is named
DECNN. There are three new ideas in the proposed DECNN method. Firstly, an
existing effective encoding scheme is refined to cater for variable-length CNN
architectures; Secondly, the new mutation and crossover operators are developed
for variable-length DE to optimise the hyperparameters of CNNs; Finally, the
new second crossover is introduced to evolve the depth of the CNN
architectures. The proposed algorithm is tested on six widely-used benchmark
datasets and the results are compared to 12 state-of-the-art methods, which
shows the proposed method is vigorously competitive to the state-of-the-art
algorithms. Furthermore, the proposed method is also compared with a method
using particle swarm optimisation with a similar encoding strategy named IPPSO,
and the proposed DECNN outperforms IPPSO in terms of the accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yanan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07049">
<title>Catastrophic Importance of Catastrophic Forgetting. (arXiv:1808.07049v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07049</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes some of the possibilities of artificial neural networks
that open up after solving the problem of catastrophic forgetting. A simple
model and reinforcement learning applications of existing methods are also
proposed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ierusalem_A/0/1/0/all/0/1&quot;&gt;Albert Ierusalem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07050">
<title>Vicious Circle Principle and Logic Programs with Aggregates. (arXiv:1808.07050v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07050</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents a knowledge representation language $\mathcal{A}log$ which
extends ASP with aggregates. The goal is to have a language based on simple
syntax and clear intuitive and mathematical semantics. We give some properties
of $\mathcal{A}log$, an algorithm for computing its answer sets, and comparison
with other approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelfond_M/0/1/0/all/0/1&quot;&gt;Michael Gelfond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanlin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07104">
<title>Aiming to Know You Better Perhaps Makes Me a More Engaging Dialogue Partner. (arXiv:1808.07104v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07104</link>
<description rdf:parseType="Literal">&lt;p&gt;There have been several attempts to define a plausible motivation for a
chit-chat dialogue agent that can lead to engaging conversations. In this work,
we explore a new direction where the agent specifically focuses on discovering
information about its interlocutor. We formalize this approach by defining a
quantitative metric. We propose an algorithm for the agent to maximize it. We
validate the idea with human evaluation where our system outperforms various
baselines. We demonstrate that the metric indeed correlates with the human
judgments of engagingness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1&quot;&gt;Yury Zemlyanskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1&quot;&gt;Fei Sha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07187">
<title>Neural Latent Extractive Document Summarization. (arXiv:1808.07187v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07187</link>
<description rdf:parseType="Literal">&lt;p&gt;Extractive summarization models need sentence level labels, which are usually
created with rule-based methods since most summarization datasets only have
document summary pairs. These labels might be suboptimal. We propose a latent
variable extractive model, where sentences are viewed as latent variables and
sentences with activated variables are used to infer gold summaries. During
training, the loss can come directly from gold summaries. Experiments on
CNN/Dailymail dataset show our latent extractive model outperforms a strong
extractive baseline trained on rule-based labels and also performs
competitively with several recent models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1&quot;&gt;Mirella Lapata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07228">
<title>Hierarchical Neural Network for Extracting Knowledgeable Snippets and Documents. (arXiv:1808.07228v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07228</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we focus on extracting knowledgeable snippets and annotating
knowledgeable documents from Web corpus, consisting of the documents from
social media and We-media. Informally, knowledgeable snippets refer to the text
describing concepts, properties of entities, or relations among entities, while
knowledgeable documents are the ones with enough knowledgeable snippets. These
knowledgeable snippets and documents could be helpful in multiple applications,
such as knowledge base construction and knowledge-oriented service. Previous
studies extracted the knowledgeable snippets using the pattern-based method.
Here, we propose the semantic-based method for this task. Specifically, a CNN
based model is developed to extract knowledgeable snippets and annotate
knowledgeable documents simultaneously. Additionally, a &quot;low-level sharing,
high-level splitting&quot; structure of CNN is designed to handle the documents from
different content domains. Compared with building multiple domain-specific
CNNs, this joint model not only critically saves the training time, but also
improves the prediction accuracy visibly. The superiority of the proposed
method is demonstrated in a real dataset from Wechat public platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Ganbin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1&quot;&gt;Rongyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1&quot;&gt;Xiang Ao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Leyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qing He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07270">
<title>Learning to Support: Exploiting Structure Information in Support Sets for One-Shot Learning. (arXiv:1808.07270v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07270</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning shows very good performance when trained on large labeled data
sets. The problem of training a deep net on a few or one sample per class
requires a different learning approach which can generalize to unseen classes
using only a few representatives of these classes. This problem has previously
been approached by meta-learning. Here we propose a novel meta-learner which
shows state-of-the-art performance on common benchmarks for one/few shot
classification. Our model features three novel components: First is a
feed-forward embedding that takes random class support samples (after a
customary CNN embedding) and transfers them to a better class representation in
terms of a classification problem. Second is a novel attention mechanism,
inspired by competitive learning, which causes class representatives to compete
with each other to become a temporary class prototype with respect to the query
point. This mechanism allows switching between representatives depending on the
position of the query point. Once a prototype is chosen for each class, the
predicated label is computed using a simple attention mechanism over prototypes
of all considered classes. The third feature is the ability of our meta-learner
to incorporate deeper CNN embedding, enabling larger capacity. Finally, to ease
the training procedure and reduce overfitting, we averages the top $t$ models
(evaluated on the validation) over the optimization trajectory. We show that
this approach can be viewed as an approximation to an ensemble, which saves the
factor of $t$ in training and test times and the factor of of $t$ in the
storage of the final model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinchao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_S/0/1/0/all/0/1&quot;&gt;Stuart J. Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osadchy_M/0/1/0/all/0/1&quot;&gt;Margarita Osadchy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07272">
<title>Deep Adaptive Temporal Pooling for Activity Recognition. (arXiv:1808.07272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.07272</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have recently achieved competitive accuracy for human
activity recognition. However, there is room for improvement, especially in
modeling long-term temporal importance and determining the activity relevance
of different temporal segments in a video. To address this problem, we propose
a learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP).
DATP applies a self-attention mechanism to adaptively pool the classification
scores of different video segments. Specifically, using frame-level features,
DATP regresses importance of different temporal segments and generates weights
for them. Remarkably, DATP is trained using only the video-level label. There
is no need of additional supervision except video-level activity class label.
We conduct extensive experiments to investigate various input features and
different weight models. Experimental results show that DATP can learn to
assign large weights to key video segments. More importantly, DATP can improve
training of frame-level feature extractor. This is because relevant temporal
segments are assigned large weights during back-propagation. Overall, we
achieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sibo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekhar_V/0/1/0/all/0/1&quot;&gt;Vijay Chandrasekhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_B/0/1/0/all/0/1&quot;&gt;Bappaditya Mandal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07374">
<title>Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation. (arXiv:1808.07374v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07374</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the Neural Machine Translation (NMT) models are based on the
sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped
with the attention mechanism. However, the conventional attention mechanism
treats the decoding at each time step equally with the same matrix, which is
problematic since the softness of the attention for different types of words
(e.g. content words and function words) should differ. Therefore, we propose a
new model with a mechanism called Self-Adaptive Control of Temperature (SACT)
to control the softness of attention by means of an attention temperature.
Experimental results on the Chinese-English translation and English-Vietnamese
translation demonstrate that our model outperforms the baseline models, and the
analysis and the case study show that our model can attend to the most relevant
elements in the source-side contexts and generate the translation of high
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Muyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qi Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07036">
<title>QuAC : Question Answering in Context. (arXiv:1808.07036v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07036</link>
<description rdf:parseType="Literal">&lt;p&gt;We present QuAC, a dataset for Question Answering in Context that contains
14K information-seeking QA dialogs (100K questions in total). The interactions
involve two crowd workers: (1) a student who poses a sequence of freeform
questions to learn as much as possible about a hidden Wikipedia text, and (2) a
teacher who answers the questions by providing short excerpts from the text.
QuAC introduces challenges not found in existing machine comprehension
datasets: its questions are often more open-ended, unanswerable, or only
meaningful within the dialog context, as we show in a detailed qualitative
evaluation. We also report results for a number of reference models, including
a recently state-of-the-art reading comprehension architecture extended to
model dialog context. Our best model underperforms humans by 20 F1, suggesting
that there is significant room for future work on this data. Dataset, baseline,
and leaderboard are available at &lt;a href=&quot;http://quac.ai.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Eunsol Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1&quot;&gt;Mohit Iyyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1&quot;&gt;Mark Yatskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1&quot;&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07181">
<title>Efficient sparse Hessian based algorithms for the clustered lasso problem. (arXiv:1808.07181v1 [math.OC])</title>
<link>http://arxiv.org/abs/1808.07181</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on solving the clustered lasso problem, which is a least squares
problem with the $\ell_1$-type penalties imposed on both the coefficients and
their pairwise differences to learn the group structure of the regression
parameters. Here we first reformulate the clustered lasso regularizer as a
weighted ordered-lasso regularizer, which is essential in reducing the
computational cost from $O(n^2)$ to $O(n\log (n))$. We then propose an inexact
semismooth Newton augmented Lagrangian (SSNAL) algorithm to solve the clustered
lasso problem or its dual via this equivalent formulation, depending on whether
the sample size is larger than the dimension of the features. An essential
component of the SSNAL algorithm is the computation of the generalized Jacobian
of the proximal mapping of the clustered lasso regularizer. Based on the new
formulation, we derive an efficient procedure for its computation.
Comprehensive results on the global convergence and local linear convergence of
the SSNAL algorithm are established. For the purpose of exposition and
comparison, we also summarize/design several first-order methods that can be
used to solve the problem under consideration, but with the key improvement
from the new formulation of the clustered lasso regularizer. As a demonstration
of the applicability of our algorithms, numerical experiments on the clustered
lasso problem are performed. The experiments show that the SSNAL algorithm
substantially outperforms the best alternative algorithm for the clustered
lasso problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Meixia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Defeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Toh_K/0/1/0/all/0/1&quot;&gt;Kim-Chuan Toh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07233">
<title>Neural Architecture Optimization. (arXiv:1808.07233v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07233</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic neural architecture design has shown its potential in discovering
powerful neural network architectures. Existing methods, no matter based on
reinforcement learning or evolutionary algorithms (EA), conduct architecture
search in a discrete space, which is highly inefficient. In this paper, we
propose a simple and efficient method to automatic neural architecture design
based on continuous optimization. We call this new approach neural architecture
optimization (NAO). There are three key components in our proposed approach:
(1) An encoder embeds/maps neural network architectures into a continuous
space. (2) A predictor takes the continuous representation of a network as
input and predicts its accuracy. (3) A decoder maps a continuous representation
of a network back to its architecture. The performance predictor and the
encoder enable us to perform gradient based optimization in the continuous
space to find the embedding of a new architecture with potentially better
accuracy. Such a better embedding is then decoded to a network by the decoder.
Experiments show that the architecture discovered by our method is very
competitive for image classification task on CIFAR-10 and language modeling
task on PTB, outperforming or on par with the best results of previous
architecture search methods with a significantly reduction of computational
resources. Specifically we obtain $2.07\%$ test set error rate for CIFAR-10
image classification task and $55.9$ test set perplexity of PTB language
modeling task. The best discovered architectures on both tasks are successfully
transferred to other tasks such as CIFAR-100 and WikiText-2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Renqian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1&quot;&gt;Fei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07243">
<title>Controversy Rules - Discovering Regions Where Classifiers (Dis-)Agree Exceptionally. (arXiv:1808.07243v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07243</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding regions for which there is higher controversy among different
classifiers is insightful with regards to the domain and our models. Such
evaluation can falsify assumptions, assert some, or also, bring to the
attention unknown phenomena. The present work describes an algorithm, which is
based on the Exceptional Model Mining framework, and enables that kind of
investigations. We explore several public datasets and show the usefulness of
this approach in classification tasks. We show in this paper a few interesting
observations about those well explored datasets, some of which are general
knowledge, and other that as far as we know, were not reported before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeev_Ben_Mordehai_O/0/1/0/all/0/1&quot;&gt;Oren Zeev-Ben-Mordehai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duivesteijn_W/0/1/0/all/0/1&quot;&gt;Wouter Duivesteijn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07288">
<title>Clustering and Labelling Auction Fraud Data. (arXiv:1808.07288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07288</link>
<description rdf:parseType="Literal">&lt;p&gt;Although shill bidding is a common auction fraud, it is however very tough to
detect. Due to the unavailability and lack of training data, in this study, we
build a high-quality labeled shill bidding dataset based on recently collected
auctions from eBay. Labeling shill biding instances with multidimensional
features is a critical phase for the fraud classification task. For this
purpose, we introduce a new approach to systematically label the fraud data
with the help of the hierarchical clustering CURE that returns remarkable
results as illustrated in the experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alzahrani_A/0/1/0/all/0/1&quot;&gt;Ahmad Alzahrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1&quot;&gt;Samira Sadaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07292">
<title>k-meansNet: When k-means Meets Differentiable Programming. (arXiv:1808.07292v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07292</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study how to make clustering benefiting from differentiable
programming whose basic idea is treating the neural network as a language
instead of a machine learning method. To this end, we recast the vanilla
$k$-means as a novel feedforward neural network in an elegant way. Our
contribution is two-fold. On the one hand, the proposed \textit{k}-meansNet is
a neural network implementation of the vanilla \textit{k}-means, which enjoys
four advantages highly desired, i.e., robustness to initialization, fast
inference speed, the capability of handling new coming data, and provable
convergence. On the other hand, this work may provide novel insights into
differentiable programming. More specifically, most existing differentiable
programming works unroll an \textbf{optimizer} as a \textbf{recurrent neural
network}, namely, the neural network is employed to solve an existing
optimization problem. In contrast, we reformulate the \textbf{objective
function} of \textit{k}-means as a \textbf{feedforward neural network}, namely,
we employ the neural network to describe a problem. In such a way, we advance
the boundary of differentiable programming by treating the neural network as
from an alternative optimization approach to the problem formulation. Extensive
experimental studies show that our method achieves promising performance
comparing with 12 clustering methods on some challenging datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07383">
<title>Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding. (arXiv:1808.07383v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07383</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention
mechanism for sentence embedding. We design DSA by modifying dynamic routing in
capsule network (Sabouretal.,2017) for natural language processing. DSA attends
to informative words with a dynamic weight vector. We achieve new
state-of-the-art results among sentence encoding methods in Stanford Natural
Language Inference (SNLI) dataset with the least number of parameters, while
showing comparative results in Stanford Sentiment Treebank (SST) dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1&quot;&gt;Deunsol Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongbok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;SangKeun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06464">
<title>A unified deep artificial neural network approach to partial differential equations in complex geometries. (arXiv:1711.06464v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06464</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we use deep feedforward artificial neural networks to
approximate solutions to partial differential equations in complex geometries.
We show how to modify the backpropagation algorithm to compute the partial
derivatives of the network output with respect to the space variables which is
needed to approximate the differential operator. The method is based on an
ansatz for the solution which requires nothing but feedforward neural networks
and an unconstrained gradient based optimization method such as gradient
descent or a quasi-Newton method.
&lt;/p&gt;
&lt;p&gt;We show an example where classical mesh based methods cannot be used and
neural networks can be seen as an attractive alternative. Finally, we highlight
the benefits of deep compared to shallow neural networks and device some other
convergence enhancing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berg_J/0/1/0/all/0/1&quot;&gt;Jens Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nystrom_K/0/1/0/all/0/1&quot;&gt;Kaj Nystr&amp;#xf6;m&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05247">
<title>Channel Charting: Locating Users within the Radio Environment using Channel State Information. (arXiv:1807.05247v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05247</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose channel charting (CC), a novel framework in which a multi-antenna
network element learns a chart of the radio geometry in its surrounding area.
The channel chart captures the local spatial geometry of the area so that
points that are close in space will also be close in the channel chart and vice
versa. CC works in a fully unsupervised manner, i.e., learning is only based on
channel state information (CSI) that is passively collected at a single point
in space, but from multiple transmit locations in the area over time. The
method then extracts channel features that characterize large-scale fading
properties of the wireless channel. Finally, the channel charts are generated
with tools from dimensionality reduction, manifold learning, and deep neural
networks. The network element performing CC may be, for example, a
multi-antenna base-station in a cellular system and the charted area in the
served cell. Logical relationships related to the position and movement of a
transmitter, e.g., a user equipment (UE), in the cell can then be directly
deduced from comparing measured radio channel characteristics to the channel
chart. The unsupervised nature of CC enables a range of new applications in UE
localization, network planning, user scheduling, multipoint connectivity,
hand-over, cell search, user grouping, and other cognitive tasks that rely on
CSI and UE movement relative to the base-station, without the need of
information from global navigation satellite systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Studer_C/0/1/0/all/0/1&quot;&gt;Christoph Studer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medjkouh_S/0/1/0/all/0/1&quot;&gt;Sa&amp;#xef;d Medjkouh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonultas_E/0/1/0/all/0/1&quot;&gt;Emre G&amp;#xf6;n&amp;#xfc;lta&amp;#x15f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirkkonen_O/0/1/0/all/0/1&quot;&gt;Olav Tirkkonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11374">
<title>Weakly-Supervised Deep Learning of Heat Transport via Physics Informed Loss. (arXiv:1807.11374v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11374</link>
<description rdf:parseType="Literal">&lt;p&gt;In typical machine learning tasks and applications, it is necessary to obtain
or create large labeled datasets in order to to achieve high performance.
Unfortunately, large labeled datasets are not always available and can be
expensive to source, creating a bottleneck towards more widely applicable
machine learning. The paradigm of weak supervision offers an alternative that
allows for integration of domain-specific knowledge by enforcing constraints
that a correct solution to the learning problem will obey over the output
space. In this work, we explore the application of this paradigm to 2-D
physical systems governed by non-linear differential equations. We demonstrate
that knowledge of the partial differential equations governing a system can be
encoded into the loss function of a neural network via an appropriately chosen
convolutional kernel. We demonstrate this by showing that the steady-state
solution to the 2-D heat equation can be learned directly from initial
conditions by a convolutional neural network, in the absence of labeled
training data. We also extend recent work in the progressive growing of fully
convolutional networks to achieve high accuracy (&amp;lt; 1.5% error) at multiple
scales of the heat-flow problem, including at the very large scale (1024x1024).
Finally, we demonstrate that this method can be used to speed up exact
calculation of the solution to the differential equations via finite
difference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Rishi Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Farimani_A/0/1/0/all/0/1&quot;&gt;Amir Barati Farimani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gomes_J/0/1/0/all/0/1&quot;&gt;Joe Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eastman_P/0/1/0/all/0/1&quot;&gt;Peter Eastman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04295">
<title>Understanding training and generalization in deep learning by Fourier analysis. (arXiv:1808.04295v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04295</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: It is still an open research area to theoretically understand why
Deep Neural Networks (DNNs)---equipped with many more parameters than training
data and trained by (stochastic) gradient-based methods---often achieve
remarkably low generalization error. Contribution: We study DNN training by
Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic)
gradient-based methods endows low-frequency components of the target function
with a higher priority during the training; ii) Small initialization leads to
good generalization ability of DNN while preserving the DNN&apos;s ability of
fitting any function. These results are further confirmed by experiments of
DNNs fitting the following datasets, i.e., natural images, one-dimensional
functions and MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiqin John Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04572">
<title>Small Sample Learning in Big Data Era. (arXiv:1808.04572v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04572</link>
<description rdf:parseType="Literal">&lt;p&gt;As a promising area in artificial intelligence, a new learning paradigm,
called Small Sample Learning (SSL), has been attracting prominent research
attention in the recent years. In this paper, we aim to present a survey to
comprehensively introduce the current techniques proposed on this topic.
Specifically, current SSL techniques can be mainly divided into two categories.
The first category of SSL approaches can be called &quot;concept learning&quot;, which
emphasizes learning new concepts from only few related observations. The
purpose is mainly to simulate human learning behaviors like recognition,
generation, imagination, synthesis and analysis. The second category is called
&quot;experience learning&quot;, which usually co-exists with the large sample learning
manner of conventional machine learning. This category mainly focuses on
learning with insufficient samples, and can also be called small data learning
in some literatures. More extensive surveys on both categories of SSL
techniques are introduced and some neuroscience evidences are provided to
clarify the rationality of the entire SSL regime, and the relationship with
human learning process. Some discussions on the main challenges and possible
future research directions along this line are also presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1&quot;&gt;Jun Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zongben Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06148">
<title>Generalized Bregman and Jensen divergences which include some f-divergences. (arXiv:1808.06148v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06148</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce new classes of divergences by extending the
definitions of the Bregman divergence and the skew Jensen divergence. These new
divergence classes (g-Bregman divergence and skew g-Jensen divergence) satisfy
some properties similar to the Bregman or skew Jensen divergence. We show these
g-divergences include divergences which belong to a class of f-divergence (the
Hellinger distance, the chi-square divergence and the alpha-divergence in
addition to the Kullback-Leibler divergence). Moreover, we derive an inequality
between the skew g-Jensen divergence and the g-Bregman divergence and show this
inequality is a generalization of Lin&apos;s inequality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nishiyama_T/0/1/0/all/0/1&quot;&gt;Tomohiro Nishiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01251">
<title>Training behavior of deep neural network in frequency domain. (arXiv:1807.01251v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.01251</link>
<description rdf:parseType="Literal">&lt;p&gt;Why deep neural networks (DNNs) capable of overfitting often generalize well
in practice is a mystery in deep learning. Existing works indicate that this
observation holds for both complicated real datasets and simple datasets of
one-dimensional (1-d) functions. In this work, for natural images and
low-frequency dominant 1-d functions, we empirically found that a DNN with
common settings first quickly captures the dominant low-frequency components,
and then relatively slowly captures high-frequency ones. We call this
phenomenon Frequency Principle (F-Principle). F-Principle can be observed over
various DNN setups of different activation functions, layer structures and
training algorithms in our experiments. F-Principle can be used to understand
(i) the behavior of DNN training in the information plane and (ii) why DNNs
often generalize well albeit its ability of overfitting. This F-Principle
potentially can provide insights into understanding the general principle
underlying DNN optimization and generalization for real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qin J. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yanyang Xiao&lt;/a&gt;</dc:creator>
</item></rdf:RDF>