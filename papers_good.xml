<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08660"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08712"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.08573">
<title>Etymo: A New Discovery Engine for AI Research. (arXiv:1801.08573v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1801.08573</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Etymo (https://etymo.io), a discovery engine to facilitate
artificial intelligence (AI) research and development. It aims to help readers
navigate a large number of AI-related papers published every week by using a
novel form of search that finds relevant papers and displays related papers in
a graphical interface. Etymo constructs and maintains an adaptive
similarity-based network of research papers as an all-purpose knowledge graph
for ranking, recommendation, and visualisation. The network is constantly
evolving and can learn from user feedback to adjust itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weijian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deakin_J/0/1/0/all/0/1&quot;&gt;Jonathan Deakin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Higham_N/0/1/0/all/0/1&quot;&gt;Nicholas J. Higham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07791">
<title>PointCNN. (arXiv:1801.07791v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07791</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple and general framework for feature learning from point
cloud. The key to the success of CNNs is the convolution operator that is
capable of leveraging spatially-local correlation in data represented densely
in grids (e.g. images). However, point cloud are irregular and unordered, thus
a direct convolving of kernels against the features associated with the points
will result in deserting the shape information while being variant to the
orders. To address these problems, we propose to learn a X-transformation from
the input points, and then use it to simultaneously weight the input features
associated with the points and permute them into latent potentially canonical
order, before the element-wise product and sum operations are applied. The
proposed method is a generalization of typical CNNs into learning features from
point cloud, thus we call it PointCNN. Experiments show that PointCNN achieves
on par or better performance than state-of-the-art methods on multiple
challenging benchmark datasets and tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_R/0/1/0/all/0/1&quot;&gt;Rui Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mingchao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoquan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08459">
<title>Finding ReMO (Related Memory Object): A Simple Neural Architecture for Text based Reasoning. (arXiv:1801.08459v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08459</link>
<description rdf:parseType="Literal">&lt;p&gt;To solve the text-based question and answering task that requires relational
reasoning, it is necessary to memorize a large amount of information and find
out the question relevant information from the memory. Most approaches were
based on external memory and four components proposed by Memory Network. The
distinctive component among them was the way of finding the necessary
information and it contributes to the performance. Recently, a simple but
powerful neural network module for reasoning called Relation Network (RN) has
been introduced. We analyzed RN from the view of Memory Network, and realized
that its MLP component is able to reveal the complicate relation between
question and object pair. Motivated from it, we introduce which uses MLP to
find out relevant information on Memory Network architecture. It shows new
state-of-the-art results in jointly trained bAbI-10k story-based question
answering tasks and bAbI dialog-based question answering tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jihyung Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hyochang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sungzoon Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08640">
<title>Transparent Model Distillation. (arXiv:1801.08640v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08640</link>
<description rdf:parseType="Literal">&lt;p&gt;Model distillation was originally designed to distill knowledge from a large,
complex teacher model to a faster, simpler student model without significant
loss in prediction accuracy. We investigate model distillation for another goal
-- transparency -- investigating if fully-connected neural networks can be
distilled into models that are transparent or interpretable in some sense. Our
teacher models are multilayer perceptrons, and we try two types of student
models: (1) tree-based generalized additive models (GA2Ms), a type of boosted,
short tree (2) gradient boosted trees (GBTs). More transparent student models
are forthcoming. Our results are not yet conclusive. GA2Ms show some promise
for distilling binary classification teachers, but not yet regression. GBTs are
not &quot;directly&quot; interpretable but may be promising for regression teachers. GA2M
models may provide a computationally viable alternative to additive
decomposition methods for global function approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Sarah Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Caruana_R/0/1/0/all/0/1&quot;&gt;Rich Caruana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1&quot;&gt;Giles Hooker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gordo_A/0/1/0/all/0/1&quot;&gt;Albert Gordo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08660">
<title>Context Models for OOV Word Translation in Low-Resource Languages. (arXiv:1801.08660v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.08660</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-vocabulary word translation is a major problem for the translation of
low-resource languages that suffer from a lack of parallel training data. This
paper evaluates the contributions of target-language context models towards the
translation of OOV words, specifically in those cases where OOV translations
are derived from external knowledge sources, such as dictionaries. We develop
both neural and non-neural context models and evaluate them within both
phrase-based and self-attention based neural machine translation systems. Our
results show that neural language models that integrate additional context
beyond the current sentence are the most effective in disambiguating possible
OOV word translations. We present an efficient second-pass lattice-rescoring
method for wide-context neural language models and demonstrate performance
improvements over state-of-the-art self-attention based neural MT systems in
five out of six low-resource language pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Angli Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1&quot;&gt;Katrin Kirchhoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08702">
<title>Improving Bi-directional Generation between Different Modalities with Variational Autoencoders. (arXiv:1801.08702v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08702</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate deep generative models that can exchange multiple modalities
bi-directionally, e.g., generating images from corresponding texts and vice
versa. A major approach to achieve this objective is to train a model that
integrates all the information of different modalities into a joint
representation and then to generate one modality from the corresponding other
modality via this joint representation. We simply applied this approach to
variational autoencoders (VAEs), which we call a joint multimodal variational
autoencoder (JMVAE). However, we found that when this model attempts to
generate a large dimensional modality missing at the input, the joint
representation collapses and this modality cannot be generated successfully.
Furthermore, we confirmed that this difficulty cannot be resolved even using a
known solution. Therefore, in this study, we propose two models to prevent this
difficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that
these methods can prevent the difficulty above and that they generate
modalities bi-directionally with equal or higher likelihood than conventional
VAE methods, which generate in only one direction. Moreover, we confirm that
these methods can obtain the joint representation appropriately, so that they
can generate various variations of modality by moving over the joint
representation or changing the value of another modality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suzuki_M/0/1/0/all/0/1&quot;&gt;Masahiro Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nakayama_K/0/1/0/all/0/1&quot;&gt;Kotaro Nakayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matsuo_Y/0/1/0/all/0/1&quot;&gt;Yutaka Matsuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08712">
<title>Classification of sparsely labeled spatio-temporal data through semi-supervised adversarial learning. (arXiv:1801.08712v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08712</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Generative Adversarial Networks (GAN) have emerged as a
powerful method for learning the mapping from noisy latent spaces to realistic
data samples in high-dimensional space. So far, the development and application
of GANs have been predominantly focused on spatial data such as images. In this
project, we aim at modeling of spatio-temporal sensor data instead, i.e.
dynamic data over time. The main goal is to encode temporal data into a global
and low-dimensional latent vector that captures the dynamics of the
spatio-temporal signal. To this end, we incorporate auto-regressive RNNs,
Wasserstein GAN loss, spectral norm weight constraints and a semi-supervised
learning scheme into InfoGAN, a method for retrieval of meaningful latents in
adversarial learning. To demonstrate the modeling capability of our method, we
encode full-body skeletal human motion from a large dataset representing 60
classes of daily activities, recorded in a multi-Kinect setup. Initial results
indicate competitive classification performance of the learned latent
representations, compared to direct CNN/RNN inference. In future work, we plan
to apply this method on a related problem in the medical domain, i.e. on
recovery of meaningful latents in gait analysis of patients with vertigo and
balance disorders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mirchev_A/0/1/0/all/0/1&quot;&gt;Atanas Mirchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahmadi_S/0/1/0/all/0/1&quot;&gt;Seyed-Ahmad Ahmadi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>