<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03453"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04359"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04545"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04600"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04614"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.04503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04362"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04411"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04685"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04730"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09859"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00931"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01630"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.03453">
<title>The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities. (arXiv:1803.03453v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03453</link>
<description rdf:parseType="Literal">&lt;p&gt;Biological evolution provides a creative fount of complex and subtle
adaptations, often surprising the scientists who discover them. However,
because evolution is an algorithmic process that transcends the substrate in
which it occurs, evolution&apos;s creativity is not limited to nature. Indeed, many
researchers in the field of digital evolution have observed their evolving
algorithms and organisms subverting their intentions, exposing unrecognized
bugs in their code, producing unexpected adaptations, or exhibiting outcomes
uncannily convergent with ones in nature. Such stories routinely reveal
creativity by evolution in these digital worlds, but they rarely fit into the
standard scientific narrative. Instead they are often treated as mere obstacles
to be overcome, rather than results that warrant study in their own right. The
stories themselves are traded among researchers through oral tradition, but
that mode of information transmission is inefficient and prone to error and
outright loss. Moreover, the fact that these stories tend to be shared only
among practitioners means that many natural scientists do not realize how
interesting and lifelike digital organisms are and how natural their evolution
can be. To our knowledge, no collection of such anecdotes has been published
before. This paper is the crowd-sourced product of researchers in the fields of
artificial life and evolutionary computation who have provided first-hand
accounts of such cases. It thus serves as a written, fact-checked collection of
scientifically important and even entertaining stories. In doing so we also
present here substantial evidence that the existence and importance of
evolutionary surprises extends beyond the natural world, and may indeed be a
universal property of all complex evolving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misevic_D/0/1/0/all/0/1&quot;&gt;Dusan Misevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adami_C/0/1/0/all/0/1&quot;&gt;Christoph Adami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altenberg_L/0/1/0/all/0/1&quot;&gt;Lee Altenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaulieu_J/0/1/0/all/0/1&quot;&gt;Julie Beaulieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bentley_P/0/1/0/all/0/1&quot;&gt;Peter J. Bentley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_S/0/1/0/all/0/1&quot;&gt;Samuel Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beslon_G/0/1/0/all/0/1&quot;&gt;Guillaume Beslon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bryson_D/0/1/0/all/0/1&quot;&gt;David M. Bryson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrabaszcz_P/0/1/0/all/0/1&quot;&gt;Patryk Chrabaszcz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doncieux_S/0/1/0/all/0/1&quot;&gt;Stephane Doncieux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_F/0/1/0/all/0/1&quot;&gt;Fred C. Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellefsen_K/0/1/0/all/0/1&quot;&gt;Kai Olav Ellefsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldt_R/0/1/0/all/0/1&quot;&gt;Robert Feldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_S/0/1/0/all/0/1&quot;&gt;Stephan Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forrest_S/0/1/0/all/0/1&quot;&gt;Stephanie Forrest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frenoy_A/0/1/0/all/0/1&quot;&gt;Antoine Fr&amp;#xe9;noy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goff_L/0/1/0/all/0/1&quot;&gt;Leni Le Goff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabowski_L/0/1/0/all/0/1&quot;&gt;Laura M. Grabowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodjat_B/0/1/0/all/0/1&quot;&gt;Babak Hodjat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_L/0/1/0/all/0/1&quot;&gt;Laurent Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knibbe_C/0/1/0/all/0/1&quot;&gt;Carole Knibbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krcah_P/0/1/0/all/0/1&quot;&gt;Peter Krcah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenski_R/0/1/0/all/0/1&quot;&gt;Richard E. Lenski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacCurdy_R/0/1/0/all/0/1&quot;&gt;Robert MacCurdy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maestre_C/0/1/0/all/0/1&quot;&gt;Carlos Maestre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitri_S/0/1/0/all/0/1&quot;&gt;Sara Mitri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moriarty_D/0/1/0/all/0/1&quot;&gt;David E. Moriarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ofria_C/0/1/0/all/0/1&quot;&gt;Charles Ofria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parizeau_M/0/1/0/all/0/1&quot;&gt;Marc Parizeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsons_D/0/1/0/all/0/1&quot;&gt;David Parsons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennock_R/0/1/0/all/0/1&quot;&gt;Robert T. Pennock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Punch_W/0/1/0/all/0/1&quot;&gt;William F. Punch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_T/0/1/0/all/0/1&quot;&gt;Thomas S. Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenauer_M/0/1/0/all/0/1&quot;&gt;Marc Schoenauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shulte_E/0/1/0/all/0/1&quot;&gt;Eric Shulte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sims_K/0/1/0/all/0/1&quot;&gt;Karl Sims&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taddei_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Taddei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarapore_D/0/1/0/all/0/1&quot;&gt;Danesh Tarapore&lt;/a&gt;, et al. (4 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04359">
<title>Mind Your Language: Learning Visually Grounded Dialog in a Multi-Agent Setting. (arXiv:1808.04359v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04359</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of visually grounded dialog involves learning goal-oriented
cooperative dialog between autonomous agents who exchange information about a
scene through several rounds of questions and answers. We posit that requiring
agents to adhere to rules of human language while also maximizing information
exchange is an ill-posed problem, and observe that humans do not stray from a
common language, because they are social creatures and have to communicate with
many people everyday, and it is far easier to stick to a common language even
at the cost of some efficiency loss. Using this as inspiration, we propose and
evaluate a multi-agent dialog framework where each agent interacts with, and
learns from, multiple agents, and show that this results in more relevant and
coherent dialog (as judged by human evaluators) without sacrificing task
performance (as judged by quantitative metrics).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Akshat Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurumurthy_S/0/1/0/all/0/1&quot;&gt;Swaminathan Gurumurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vasu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04364">
<title>D-PAGE: Diverse Paraphrase Generation. (arXiv:1808.04364v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.04364</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the diversity aspect of paraphrase generation.
Prior deep learning models employ either decoding methods or add random input
noise for varying outputs. We propose a simple method Diverse Paraphrase
Generation (D-PAGE), which extends neural machine translation (NMT) models to
support the generation of diverse paraphrases with implicit rewriting patterns.
Our experimental results on two real-world benchmark datasets demonstrate that
our model generates at least one order of magnitude more diverse outputs than
the baselines in terms of a new evaluation metric Jeffrey&apos;s Divergence. We have
also conducted extensive experiments to understand various properties of our
model with a focus on diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiongkai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1&quot;&gt;Lizhen Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lexing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04365">
<title>What is wrong with style transfer for texts?. (arXiv:1808.04365v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.04365</link>
<description rdf:parseType="Literal">&lt;p&gt;A number of recent machine learning papers work with an automated style
transfer for texts and, counter to intuition, demonstrate that there is no
consensus formulation of this NLP task. Different researchers propose different
algorithms, datasets and target metrics to address it. This short opinion paper
aims to discuss possible formalization of this NLP task in anticipation of a
further growing interest to it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1&quot;&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1&quot;&gt;Ivan P. Yamshchikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04444">
<title>Character-Level Language Modeling with Deeper Self-Attention. (arXiv:1808.04444v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.04444</link>
<description rdf:parseType="Literal">&lt;p&gt;LSTMs and other RNN variants have shown strong performance on character-level
language modeling. These models are typically trained using truncated
backpropagation through time, and it is common to assume that their success
stems from their ability to remember long-term contexts. In this paper, we show
that a deep (64-layer) transformer model with fixed context outperforms RNN
variants by a large margin, achieving state of the art on two popular
benchmarks- 1.13 bits per character on text8 and 1.06 on enwik8. To get good
results at this depth, we show that it is important to add auxiliary losses,
both at intermediate network layers and intermediate sequence positions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1&quot;&gt;Rami Al-Rfou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_D/0/1/0/all/0/1&quot;&gt;Dokook Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1&quot;&gt;Noah Constant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Mandy Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1&quot;&gt;Llion Jones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04456">
<title>Multimodal Deep Neural Networks using Both Engineered and Learned Representations for Biodegradability Prediction. (arXiv:1808.04456v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04456</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms excel at extracting patterns from raw data. Through
representation learning and automated feature engineering on large datasets,
such models have been highly successful in computer vision and natural language
applications. However, in many other technical domains, large datasets on which
to learn representations from may not be feasible. In this work, we develop a
novel multimodal CNN-MLP neural network architecture that utilizes both
domain-specific feature engineering as well as learned representations from raw
data. We illustrate the effectiveness of such an approach in the chemical
sciences, for predicting chemical properties, where labeled data is scarce
owing to the high costs associated with acquiring labels through experimental
measurements. By training on both raw chemical data and using engineered
chemical features, while leveraging weak supervised learning and transfer
learning methods, we show that the multimodal CNN-MLP network is more accurate
than either a standalone CNN or MLP network that uses only raw data or
engineered features respectively. Using this multimodal network, we then
develop the DeepBioD model for predicting chemical biodegradability, which
achieves an error classification rate of 0.125 that is 27% lower than the
current state-of-the-art. Thus, our work indicates that combining traditional
feature engineering with representation learning on raw data can be an
effective approach, particularly in situations where labeled training data is
limited. Such a framework can also be potentially applied to other technical
fields, where substantial research efforts into feature engineering has been
established.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakloth_K/0/1/0/all/0/1&quot;&gt;Khusheemn Sakloth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfaendtner_J/0/1/0/all/0/1&quot;&gt;Jim Pfaendtner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04468">
<title>Risk-Sensitive Generative Adversarial Imitation Learning. (arXiv:1808.04468v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04468</link>
<description rdf:parseType="Literal">&lt;p&gt;We study risk-sensitive imitation learning where the agent&apos;s goal is to
perform at least as well as the expert in terms of a risk profile. We first
formulate our risk-sensitive imitation learning setting. We consider the
generative adversarial approach to imitation learning (GAIL) and derive an
optimization problem for our formulation, which we call risk-sensitive GAIL
(RS-GAIL). We then derive two different versions of our RS-GAIL optimization
problem that aim at matching the risk profiles of the agent and the expert
w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop
risk-sensitive generative adversarial imitation learning algorithms based on
these optimization problems. We evaluate the performance of our JS-based
algorithm and compare it with GAIL and the risk-averse imitation learning
(RAIL) algorithm in two MuJoCo tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacotte_J/0/1/0/all/0/1&quot;&gt;Jonathan Lacotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1&quot;&gt;Yinlam Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04503">
<title>Shared Multi-Task Imitation Learning for Indoor Self-Navigation. (arXiv:1808.04503v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04503</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep imitation learning enables robots to learn from expert demonstrations to
perform tasks such as lane following or obstacle avoidance. However, in the
traditional imitation learning framework, one model only learns one task, and
thus it lacks of the capability to support a robot to perform various different
navigation tasks with one model in indoor environments. This paper proposes a
new framework, Shared Multi-headed Imitation Learning(SMIL), that allows a
robot to perform multiple tasks with one model without switching among
different models. We model each task as a sub-policy and design a multi-headed
policy to learn the shared information among related tasks by summing up
activations from all sub-policies. Compared to single or non-shared
multi-headed policies, this framework is able to leverage correlated
information among tasks to increase performance.We have implemented this
framework using a robot based on NVIDIA TX2 and performed extensive experiments
in indoor environments with different baseline solutions. The results
demonstrate that SMIL has doubled the performance over nonshared multi-headed
policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Junhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hanqing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kageza_A/0/1/0/all/0/1&quot;&gt;Aaron Kageza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlQarni_S/0/1/0/all/0/1&quot;&gt;Saeed AlQarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shaoen Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04545">
<title>MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics. (arXiv:1808.04545v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04545</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-term human motion can be represented as a series of motion
modes---motion sequences that capture short-term temporal dynamics---with
transitions between them. We leverage this structure and present a novel Motion
Transformation Variational Auto-Encoders (MT-VAE) for learning motion sequence
generation. Our model jointly learns a feature embedding for motion modes (that
the motion sequence can be reconstructed from) and a feature transformation
that represents the transition of one motion mode to the next motion mode. Our
model is able to generate multiple diverse and plausible motion sequences in
the future from the same input. We apply our approach to both facial and full
body motion, and demonstrate applications like analogy-based motion transfer
and video synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xinchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1&quot;&gt;Akash Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1&quot;&gt;Ruben Villegas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Sunkavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadap_S/0/1/0/all/0/1&quot;&gt;Sunil Hadap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yumer_E/0/1/0/all/0/1&quot;&gt;Ersin Yumer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04600">
<title>Reconciling Irrational Human Behavior with AI based Decision Making: A Quantum Probabilistic Approach. (arXiv:1808.04600v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04600</link>
<description rdf:parseType="Literal">&lt;p&gt;There are many examples of human decision making which cannot be modeled by
classical probabilistic and logic models, on which the current AI systems are
based. Hence the need for a modeling framework which can enable intelligent
systems to detect and predict cognitive biases in human decisions to facilitate
better human-agent interaction. We give a few examples of irrational behavior
and use a generalized probabilistic model inspired by the mathematical
framework of Quantum Theory to model and explain such behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uprety_S/0/1/0/all/0/1&quot;&gt;Sagar Uprety&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawei Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04614">
<title>Explaining Queries over Web Tables to Non-Experts. (arXiv:1808.04614v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.04614</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing a reliable natural language (NL) interface for querying tables has
been a longtime goal of researchers in both the data management and natural
language processing (NLP) communities. Such an interface receives as input an
NL question, translates it into a formal query, executes the query and returns
the results. Errors in the translation process are not uncommon, and users
typically struggle to understand whether their query has been mapped correctly.
We address this problem by explaining the obtained formal queries to non-expert
users. Two methods for query explanations are presented: the first translates
queries into NL, while the second method provides a graphic representation of
the query cell-based provenance (in its execution on a given table). Our
solution augments a state-of-the-art NL interface over web tables, enhancing it
in both its training and deployment phase. Experiments, including a user study
conducted on Amazon Mechanical Turk, show our solution to improve both the
correctness and reliability of an NL interface.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deutch_D/0/1/0/all/0/1&quot;&gt;Daniel Deutch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1&quot;&gt;Amir Globerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milo_T/0/1/0/all/0/1&quot;&gt;Tova Milo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1&quot;&gt;Tomer Wolfson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04673">
<title>Mining Threat Intelligence about Open-Source Projects and Libraries from Code Repository Issues and Bug Reports. (arXiv:1808.04673v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1808.04673</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-Source Projects and Libraries are being used in software development
while also bearing multiple security vulnerabilities. This use of third party
ecosystem creates a new kind of attack surface for a product in development. An
intelligent attacker can attack a product by exploiting one of the
vulnerabilities present in linked projects and libraries.
&lt;/p&gt;
&lt;p&gt;In this paper, we mine threat intelligence about open source projects and
libraries from bugs and issues reported on public code repositories. We also
track library and project dependencies for installed software on a client
machine. We represent and store this threat intelligence, along with the
software dependencies in a security knowledge graph. Security analysts and
developers can then query and receive alerts from the knowledge graph if any
threat intelligence is found about linked libraries and projects, utilized in
their products.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neil_L/0/1/0/all/0/1&quot;&gt;Lorenzo Neil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sudip Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Anupam Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01433">
<title>Interactive Grounded Language Acquisition and Generalization in a 2D World. (arXiv:1802.01433v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01433</link>
<description rdf:parseType="Literal">&lt;p&gt;We build a virtual agent for learning language in a 2D maze-like world. The
agent sees images of the surrounding environment, listens to a virtual teacher,
and takes actions to receive rewards. It interactively learns the teacher&apos;s
language from scratch based on two language use cases: sentence-directed
navigation and question answering. It learns simultaneously the visual
representations of the world, the language, and the action control. By
disentangling language grounding from other computational routines and sharing
a concept detection function between language grounding and prediction, the
agent reliably interpolates and extrapolates to interpret sentences that
contain new word combinations or new words missing from training sentences. The
new words are transferred from the answers of language prediction. Such a
language ability is trained and evaluated on a population of over 1.6 million
distinct sentences consisting of 119 object words, 8 color words, 9
spatial-relation words, and 50 grammatical words. The proposed model
significantly outperforms five comparison methods for interpreting zero-shot
sentences. In addition, we demonstrate human-interpretable intermediate outputs
of the model in the appendix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haonan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haichao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09133">
<title>Improving Native Ads CTR Prediction by Large Scale Event Embedding and Recurrent Networks. (arXiv:1804.09133v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09133</link>
<description rdf:parseType="Literal">&lt;p&gt;Click through rate (CTR) prediction is very important for Native
advertisement but also hard as there is no direct query intent. In this paper
we propose a large-scale event embedding scheme to encode the each user
browsing event by training a Siamese network with weak supervision on the
users&apos; consecutive events. The CTR prediction problem is modeled as a
supervised recurrent neural network, which naturally model the user history as
a sequence of events. Our proposed recurrent models utilizing pretrained event
embedding vectors and an attention layer to model the user history. Our
experiments demonstrate that our model significantly outperforms the baseline
and some variants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsana_M/0/1/0/all/0/1&quot;&gt;Mehul Parsana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poola_K/0/1/0/all/0/1&quot;&gt;Krishna Poola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yajun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiguang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.04503">
<title>Deep Learning for Computational Chemistry. (arXiv:1701.04503v1 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1701.04503</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise and fall of artificial neural networks is well documented in the
scientific literature of both computer science and computational chemistry. Yet
almost two decades later, we are now seeing a resurgence of interest in deep
learning, a machine learning algorithm based on multilayer neural networks.
Within the last few years, we have seen the transformative impact of deep
learning in many domains, particularly in speech recognition and computer
vision, to the extent that the majority of expert practitioners in those field
are now regularly eschewing prior established models in favor of deep learning
models. In this review, we provide an introductory overview into the theory of
deep neural networks and their unique properties that distinguish them from
traditional machine learning algorithms used in cheminformatics. By providing
an overview of the variety of emerging applications of deep neural networks, we
highlight its ubiquity and broad applicability to a wide range of challenges in
the field, including QSAR, virtual screening, protein structure prediction,
quantum chemistry, materials design and property prediction. In reviewing the
performance of deep neural networks, we observed a consistent outperformance
against non-neural networks state-of-the-art models across disparate research
topics, and deep neural network based models often exceeded the &quot;glass ceiling&quot;
expectations of their respective tasks. Coupled with the maturity of
GPU-accelerated computing for training deep neural networks and the exponential
growth of chemical data on which to train these networks on, we anticipate that
deep learning algorithms will be a valuable tool for computational chemistry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02734">
<title>Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for Transferable Chemical Property Prediction. (arXiv:1712.02734v2 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.02734</link>
<description rdf:parseType="Literal">&lt;p&gt;With access to large datasets, deep neural networks (DNN) have achieved
human-level accuracy in image and speech recognition tasks. However, in
chemistry, data is inherently small and fragmented. In this work, we develop an
approach of using rule-based knowledge for training ChemNet, a transferable and
generalizable deep neural network for chemical property prediction that learns
in a weak-supervised manner from large unlabeled chemical databases. When
coupled with transfer learning approaches to predict other smaller datasets for
chemical properties that it was not originally trained on, we show that
ChemNet&apos;s accuracy outperforms contemporary DNN models that were trained using
conventional supervised learning. Furthermore, we demonstrate that the ChemNet
pre-training approach is equally effective on both CNN (Chemception) and RNN
(SMILES2vec) models, indicating that this approach is network architecture
agnostic and is effective across multiple data modalities. Our results indicate
a pre-trained ChemNet that incorporates chemistry domain knowledge, enables the
development of generalizable neural networks for more accurate prediction of
novel chemical properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04362">
<title>A Domain Guided CNN Architecture for Predicting Age from Structural Brain Images. (arXiv:1808.04362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04362</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the wide success of convolutional neural networks (CNNs) applied to
natural images, researchers have begun to apply them to neuroimaging data. To
date, however, exploration of novel CNN architectures tailored to neuroimaging
data has been limited. Several recent works fail to leverage the 3D structure
of the brain, instead treating the brain as a set of independent 2D slices.
Approaches that do utilize 3D convolutions rely on architectures developed for
object recognition tasks in natural 2D images. Such architectures make
assumptions about the input that may not hold for neuroimaging. For example,
existing architectures assume that patterns in the brain exhibit translation
invariance. However, a pattern in the brain may have different meaning
depending on where in the brain it is located. There is a need to explore novel
architectures that are tailored to brain images. We present two simple
modifications to existing CNN architectures based on brain image structure.
Applied to the task of brain age prediction, our network achieves a mean
absolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline
that achieves a MAE of 1.6 years. Our results suggest that lessons learned from
developing models on natural images may not directly transfer to neuroimaging
tasks. Instead, there remains a large space of unexplored questions regarding
model development in this area, whose answers may differ from conventional
wisdom.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sturmfels_P/0/1/0/all/0/1&quot;&gt;Pascal Sturmfels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutherford_S/0/1/0/all/0/1&quot;&gt;Saige Rutherford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angstadt_M/0/1/0/all/0/1&quot;&gt;Mike Angstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peterson_M/0/1/0/all/0/1&quot;&gt;Mark Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sripada_C/0/1/0/all/0/1&quot;&gt;Chandra Sripada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1&quot;&gt;Jenna Wiens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04411">
<title>Murmur Detection Using Parallel Recurrent &amp; Convolutional Neural Networks. (arXiv:1808.04411v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1808.04411</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose a novel technique for classification of the
Murmurs in heart sound. We introduce a novel deep neural network architecture
using parallel combination of the Recurrent Neural Network (RNN) based
Bidirectional Long Short-Term Memory (BiLSTM) &amp;amp; Convolutional Neural Network
(CNN) to learn visual and time-dependent characteristics of Murmur in PCG
waveform. Set of acoustic features are presented to our proposed deep neural
network to discriminate between Normal and Murmur class. The proposed method
was evaluated on a large dataset using 5-fold cross-validation, resulting in a
sensitivity and specificity of 96 +- 0.6 % , 100 +- 0 % respectively and F1
Score of 98 +- 0.3 %.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1&quot;&gt;Shahnawaz Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_R/0/1/0/all/0/1&quot;&gt;Rohan Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Soma Bandyopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04446">
<title>Visual Reasoning with Multi-hop Feature Modulation. (arXiv:1808.04446v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04446</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in computer vision and natural language processing have
spurred interest in challenging multi-modal tasks such as visual
question-answering and visual dialogue. For such tasks, one successful approach
is to condition image-based convolutional network computation on language via
Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and
shifting. We propose to generate the parameters of FiLM layers going up the
hierarchy of a convolutional network in a multi-hop fashion rather than all at
once, as in prior work. By alternating between attending to the language input
and generating FiLM layer parameters, this approach is better able to scale to
settings with longer input sequences such as dialogue. We demonstrate that
multi-hop FiLM generation achieves state-of-the-art for the short input
sequence task ReferIt --- on-par with single-hop FiLM generation --- while also
significantly outperforming prior state-of-the-art and single-hop FiLM
generation on the GuessWhat?! visual dialogue task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1&quot;&gt;Florian Strub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seurin_M/0/1/0/all/0/1&quot;&gt;Mathieu Seurin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1&quot;&gt;Harm de Vries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mary_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;mie Mary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1&quot;&gt;Philippe Preux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1&quot;&gt;Olivier Pietquin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04475">
<title>Kernel Flows: from learning kernels from data into the abyss. (arXiv:1808.04475v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.04475</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning can be seen as approximating an unknown function by interpolating
the training data. Kriging offers a solution to this problem based on the prior
specification of a kernel. We explore a numerical approximation approach to
kernel selection/construction based on the simple premise that a kernel must be
good if the number of interpolation points can be halved without significant
loss in accuracy (measured using the intrinsic RKHS norm $\|\cdot\|$ associated
with the kernel). We first test and motivate this idea on a simple problem of
recovering the Green&apos;s function of an elliptic PDE (with inhomogeneous
coefficients) from the sparse observation of one of its solutions. Next we
consider the problem of learning non-parametric families of deep kernels of the
form $K_1(F_n(x),F_n(x&apos;))$ with $F_{n+1}=(I_d+\epsilon G_{n+1})\circ F_n$ and
$G_{n+1} \in \operatorname{Span}\{K_1(F_n(x_i),\cdot)\}$. With the proposed
approach constructing the kernel becomes equivalent to integrating a stochastic
data driven dynamical system, which allows for the training of very deep
(bottomless) networks and the exploration of their properties. These networks
learn by constructing flow maps in the kernel and input spaces via incremental
data-dependent deformations/perturbations (appearing as the cooperative
counterpart of adversarial examples) and, at profound depths, they (1) can
achieve accurate classification from only one data point per class (2) appear
to learn archetypes of each class (3) expand distances between points that are
in different classes and contract distances between points in the same class.
For kernels parameterized by the weights of Convolutional Neural Network,
minimizing approximation errors incurred by halving random subsets of
interpolation points, appears to outperform training (the same CNN
architecture) with relative entropy and dropout.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Owhadi_H/0/1/0/all/0/1&quot;&gt;Houman Owhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yoo_G/0/1/0/all/0/1&quot;&gt;Gene Ryan Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04538">
<title>Text-to-Image-to-Text Translation using Cycle Consistent Adversarial Networks. (arXiv:1808.04538v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04538</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-Image translation has been an active area of research in the recent
past. The ability for a network to learn the meaning of a sentence and generate
an accurate image that depicts the sentence shows ability of the model to think
more like humans. Popular methods on text to image translation make use of
Generative Adversarial Networks (GANs) to generate high quality images based on
text input, but the generated images don&apos;t always reflect the meaning of the
sentence given to the model as input. We address this issue by using a
captioning network to caption on generated images and exploit the distance
between ground truth captions and generated captions to improve the network
further. We show extensive comparisons between our method and existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorti_S/0/1/0/all/0/1&quot;&gt;Satya Krishna Gorti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jeremy Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04572">
<title>Small Sample Learning in Big Data Era. (arXiv:1808.04572v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04572</link>
<description rdf:parseType="Literal">&lt;p&gt;As a promising area in artificial intelligence, a new learning paradigm,
called Small Sample Learning (SSL), has been attracting prominent research
attention in the recent years. In this paper, we aim to present a survey to
comprehensively introduce the current techniques proposed on this topic.
Specifically, current SSL techniques can be mainly divided into two categories.
The first category of SSL approaches can be called &quot;concept learning&quot;, which
emphasizes learning new concepts from only few related observations. The
purpose is mainly to simulate human learning behaviors like recognition,
generation, imagination, synthesis and analysis. The second category is called
&quot;experience learning&quot;, which usually co-exists with the large sample learning
manner of conventional machine learning. This category mainly focuses on
learning with insufficient samples, and can also be called small data learning
in some literatures. More extensive surveys on both categories of SSL
techniques are introduced and some neuroscience evidences are provided to
clarify the rationality of the entire SSL regime, and the relationship with
human learning process. Some discussions on the main challenges and possible
future research directions along this line are also presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1&quot;&gt;Jun Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zongben Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04685">
<title>Learning ReLU Networks on Linearly Separable Data: Algorithm, Optimality, and Generalization. (arXiv:1808.04685v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.04685</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks with ReLU activations have achieved great empirical success
in various domains. However, existing results for learning ReLU networks either
pose assumptions on the underlying data distribution being e.g. Gaussian, or
require the network size and/or training size to be sufficiently large. In this
context, the problem of learning a two-layer ReLU network is approached in a
binary classification setting, where the data are linearly separable and a
hinge loss criterion is adopted. Leveraging the power of random noise, this
contribution presents a novel stochastic gradient descent (SGD) algorithm,
which can provably train any single-hidden-layer ReLU network to attain global
optimality, despite the presence of infinitely many bad local minima and saddle
points in general. This result is the first of its kind, requiring no
assumptions on the data distribution, training/network size, or initialization.
Convergence of the resultant iterative algorithm to a global minimum is
analyzed by establishing both an upper bound and a lower bound on the number of
effective (non-zero) updates to be performed. Furthermore, generalization
guarantees are developed for ReLU networks trained with the novel SGD. These
guarantees highlight a fundamental difference (at least in the worst case)
between learning a ReLU network as well as a leaky ReLU network in terms of
sample complexity. Numerical tests using synthetic data and real images
validate the effectiveness of the algorithm and the practical merits of the
theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04730">
<title>Analyzing Inverse Problems with Invertible Neural Networks. (arXiv:1808.04730v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04730</link>
<description rdf:parseType="Literal">&lt;p&gt;In many tasks, in particular in natural science, the goal is to determine
hidden system parameters from a set of measurements. Often, the forward process
from parameter- to measurement-space is a well-defined function, whereas the
inverse problem is ambiguous: one measurement may map to multiple different
sets of parameters. In this setting, the posterior parameter distribution,
conditioned on an input measurement, has to be determined. We argue that a
particular class of neural networks is well suited for this task -- so-called
Invertible Neural Networks (INNs). Although INNs are not new, they have, so
far, received little attention in literature. While classical neural networks
attempt to solve the ambiguous inverse problem directly, INNs are able to learn
it jointly with the well-defined forward process, using additional latent
output variables to capture the information otherwise lost. Given a specific
measurement and sampled latent variables, the inverse pass of the INN provides
a full distribution over parameter space. We verify experimentally, on
artificial data and real-world problems from astrophysics and medicine, that
INNs are a powerful analysis tool to find multi-modalities in parameter space,
to uncover parameter correlations, and to identify unrecoverable parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardizzone_L/0/1/0/all/0/1&quot;&gt;Lynton Ardizzone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruse_J/0/1/0/all/0/1&quot;&gt;Jakob Kruse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirkert_S/0/1/0/all/0/1&quot;&gt;Sebastian Wirkert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahner_D/0/1/0/all/0/1&quot;&gt;Daniel Rahner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_E/0/1/0/all/0/1&quot;&gt;Eric W. Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klessen_R/0/1/0/all/0/1&quot;&gt;Ralf S. Klessen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1&quot;&gt;Lena Maier-Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1&quot;&gt;Carsten Rother&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1&quot;&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09859">
<title>Kernel k-Groups via Hartigan&apos;s Method. (arXiv:1710.09859v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09859</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy statistics was proposed by Sz\&apos;{e}kely in the 80&apos;s inspired by
Newton&apos;s gravitational potential in classical mechanics, and it provides a
model-free hypothesis test for equality of distributions. In its original form,
energy statistics was formulated in Euclidean spaces. More recently, it was
generalized to metric spaces of negative type. In this paper, we consider a
formulation for the clustering problem using a weighted version of energy
statistics in spaces of negative type. We show that this approach leads to a
quadratically constrained quadratic program in the associated kernel space,
establishing connections with graph partitioning problems and kernel methods in
unsupervised machine learning. To find local solutions of such an optimization
problem, we propose an extension of Hartigan&apos;s method to kernel spaces. Our
method has the same computational cost as kernel k-means algorithm, which is
based on Lloyd&apos;s heuristic, but our numerical results show an improved
performance, especially in high dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franca_G/0/1/0/all/0/1&quot;&gt;Guilherme Fran&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rizzo_M/0/1/0/all/0/1&quot;&gt;Maria L. Rizzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03133">
<title>Outfit Generation and Style Extraction via Bidirectional LSTM and Autoencoder. (arXiv:1807.03133v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03133</link>
<description rdf:parseType="Literal">&lt;p&gt;When creating an outfit, style is a criterion in selecting each fashion item.
This means that style can be regarded as a feature of the overall outfit.
However, in various previous studies on outfit generation, there have been few
methods focusing on global information obtained from an outfit. To address this
deficiency, we have incorporated an unsupervised style extraction module into a
model to learn outfits. Using the style information of an outfit as a whole,
the proposed model succeeded in generating outfits more flexibly without
requiring additional information. Moreover, the style information extracted by
the proposed model is easy to interpret. The proposed model was evaluated on
two human-generated outfit datasets. In a fashion item prediction task (missing
prediction task), the proposed model outperformed a baseline method. In a style
extraction task, the proposed model extracted some easily distinguishable
styles. In an outfit generation task, the proposed model generated an outfit
while controlling its styles. This capability allows us to generate fashionable
outfits according to various preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamura_T/0/1/0/all/0/1&quot;&gt;Takuma Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goto_R/0/1/0/all/0/1&quot;&gt;Ryosuke Goto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00931">
<title>Machine Learning of Space-Fractional Differential Equations. (arXiv:1808.00931v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00931</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven discovery of &quot;hidden physics&quot; -- i.e., machine learning of
differential equation models underlying observed data -- has recently been
approached by embedding the discovery problem into a Gaussian Process
regression of spatial data, treating and discovering unknown equation
parameters as hyperparameters of a modified &quot;physics informed&quot; Gaussian Process
kernel. This kernel includes the parametrized differential operators applied to
a prior covariance kernel. We extend this framework to linear space-fractional
differential equations. The methodology is compatible with a wide variety of
fractional operators in $\mathbb{R}^d$ and stationary covariance kernels,
including the Matern class, and can optimize the Matern parameter during
training. We provide a user-friendly and feasible way to perform fractional
derivatives of kernels, via a unified set of d-dimensional Fourier integral
formulas amenable to generalized Gauss-Laguerre quadrature.
&lt;/p&gt;
&lt;p&gt;The implementation of fractional derivatives has several benefits. First, it
allows for discovering fractional-order PDEs for systems characterized by heavy
tails or anomalous diffusion, bypassing the analytical difficulty of fractional
calculus. Data sets exhibiting such features are of increasing prevalence in
physical and financial domains. Second, a single fractional-order archetype
allows for a derivative of arbitrary order to be learned, with the order itself
being a parameter in the regression. This is advantageous even when used for
discovering integer-order equations; the user is not required to assume a
&quot;dictionary&quot; of derivatives of various orders, and directly controls the
parsimony of the models being discovered. We illustrate on several examples,
including fractional-order interpolation of advection-diffusion and modeling
relative stock performance in the S&amp;amp;P 500 with alpha-stable motion via a
fractional diffusion equation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulian_M/0/1/0/all/0/1&quot;&gt;Mamikon Gulian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raissi_M/0/1/0/all/0/1&quot;&gt;Maziar Raissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1&quot;&gt;Paris Perdikaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Karniadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01630">
<title>A Review of Learning with Deep Generative Models from perspective of graphical modeling. (arXiv:1808.01630v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01630</link>
<description rdf:parseType="Literal">&lt;p&gt;This document aims to provide a review on learning with deep generative
models (DGMs), which is an highly-active area in machine learning and more
generally, artificial intelligence. This review is not meant to be a tutorial,
but when necessary, we provide self-contained derivations for completeness.
This review has two features. First, though there are different perspectives to
classify DGMs, we choose to organize this review from the perspective of
graphical modeling, because the learning methods for directed DGMs and
undirected DGMs are fundamentally different. Second, we differentiate model
definitions from model learning algorithms, since different learning algorithms
can be applied to solve the learning problem on the same model, and an
algorithm can be applied to learn different models. We thus separate model
definition and model learning, with more emphasis on reviewing, differentiating
and connecting different learning algorithms. We also discuss promising future
research directions. This review is by no means comprehensive as the field is
evolving rapidly. The authors apologize in advance for any missed papers and
inaccuracies in descriptions. Corrections and comments are highly welcome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1&quot;&gt;Zhijian Ou&lt;/a&gt;</dc:creator>
</item></rdf:RDF>