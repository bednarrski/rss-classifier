<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04486"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04871"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.08602"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00443"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.04524"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.04156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04693"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04695"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05630"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.04326">
<title>Not All Ops Are Created Equal!. (arXiv:1801.04326v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04326</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient and compact neural network models are essential for enabling the
deployment on mobile and embedded devices. In this work, we point out that
typical design metrics for gauging the efficiency of neural network
architectures -- total number of operations and parameters -- are not
sufficient. These metrics may not accurately correlate with the actual
deployment metrics such as energy and memory footprint. We show that throughput
and energy varies by up to 5X across different neural network operation types
on an off-the-shelf Arm Cortex-M7 microcontroller. Furthermore, we show that
the memory required for activation data also need to be considered, apart from
the model parameters, for network architecture exploration studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Liangzhen Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suda_N/0/1/0/all/0/1&quot;&gt;Naveen Suda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04530">
<title>A Bio-inspired Collision Detecotr for Small Quadcopter. (arXiv:1801.04530v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.04530</link>
<description rdf:parseType="Literal">&lt;p&gt;Sense and avoid capability enables insects to fly versatilely and robustly in
dynamic complex environment. Their biological principles are so practical and
efficient that inspired we human imitating them in our flying machines. In this
paper, we studied a novel bio-inspired collision detector and its application
on a quadcopter. The detector is inspired from LGMD neurons in the locusts, and
modeled into an STM32F407 MCU. Compared to other collision detecting methods
applied on quadcopters, we focused on enhancing the collision selectivity in a
bio-inspired way that can considerably increase the computing efficiency during
an obstacle detecting task even in complex dynamic environment. We designed the
quadcopter&apos;s responding operation imminent collisions and tested this
bio-inspired system in an indoor arena. The observed results from the
experiments demonstrated that the LGMD collision detector is feasible to work
as a vision module for the quadcopter&apos;s collision avoidance task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiannan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Cheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhihua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1&quot;&gt;Shigang Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04406">
<title>On the convergence properties of GAN training. (arXiv:1801.04406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04406</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has shown local convergence of GAN training for absolutely
continuous data and generator distributions. In this note we show that the
requirement of absolute continuity is necessary: we describe a simple yet
prototypical counterexample showing that in the more realistic case of
distributions that are not absolutely continuous, unregularized GAN training is
generally not convergent. Furthermore, we discuss recent regularization
strategies that were proposed to stabilize GAN training. Our analysis shows
that while GAN training with instance noise or gradient penalties converges,
Wasserstein-GANs and Wasserstein-GANs-GP with a finite number of discriminator
updates per generator update do in general not converge to the equilibrium
point. We explain these results and show that both instance noise and gradient
penalties constitute solutions to the problem of purely imaginary eigenvalues
of the Jacobian of the gradient vector field. Based on our analysis, we also
propose a simplified gradient penalty with the same effects on local
convergence as more complicated penalties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mescheder_L/0/1/0/all/0/1&quot;&gt;Lars Mescheder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04486">
<title>Can Computers Create Art?. (arXiv:1801.04486v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04486</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses whether computers, using Artifical Intelligence (AI),
could create art. The first part concerns AI-based tools for assisting with art
making. The history of technologies that automated aspects of art is covered,
including photography and animation. In each case, we see initial fears and
denial of the technology, followed by acceptance, and a blossoming of new
creative and professional opportunities for artists. The hype and reality of
Artificial Intelligence (AI) tools for art making is discussed, together with
predictions about how AI tools will be used. The second part concerns AI
systems that could conceive of artwork, and be credited with authorship of an
artwork.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hertzmann_A/0/1/0/all/0/1&quot;&gt;Aaron Hertzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04819">
<title>Robots as Powerful Allies for the Study of Embodied Cognition from the Bottom Up. (arXiv:1801.04819v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04819</link>
<description rdf:parseType="Literal">&lt;p&gt;A large body of compelling evidence has been accumulated demonstrating that
embodiment - the agent&apos;s physical setup, including its shape, materials,
sensors and actuators - is constitutive for any form of cognition and as a
consequence, models of cognition need to be embodied. In contrast to methods
from empirical sciences to study cognition, robots can be freely manipulated
and virtually all key variables of their embodiment and control programs can be
systematically varied. As such, they provide an extremely powerful tool of
investigation. We present a robotic bottom-up or developmental approach,
focusing on three stages: (a) low-level behaviors like walking and reflexes,
(b) learning regularities in sensorimotor spaces, and (c) human-like cognition.
We also show that robotic based research is not only a productive path to
deepening our understanding of cognition, but that robots can strongly benefit
from human-like cognition in order to become more autonomous, robust,
resilient, and safe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1&quot;&gt;Matej Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeifer_R/0/1/0/all/0/1&quot;&gt;Rolf Pfeifer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04871">
<title>Building a Conversational Agent Overnight with Dialogue Self-Play. (arXiv:1801.04871v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04871</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Machines Talking To Machines (M2M), a framework combining
automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents
for goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with
just a task schema and an API client from the dialogue system developer, but it
is also customizable to cater to task-specific interactions. Compared to the
Wizard-of-Oz approach for data collection, M2M achieves greater diversity and
coverage of salient dialogue flows while maintaining the naturalness of
individual utterances. In the first phase, a simulated user bot and a
domain-agnostic system bot converse to exhaustively generate dialogue
&quot;outlines&quot;, i.e. sequences of template utterances and their semantic parses. In
the second phase, crowd workers provide contextual rewrites of the dialogues to
make the utterances more natural while preserving their meaning. The entire
process can finish within a few hours. We propose a new corpus of 3,000
dialogues spanning 2 domains collected with M2M, and present comparisons with
popular dialogue datasets on the quality and diversity of the surface forms and
dialogue flows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1&quot;&gt;Pararth Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1&quot;&gt;Dilek Hakkani-T&amp;#xfc;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1&quot;&gt;Gokhan T&amp;#xfc;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1&quot;&gt;Abhinav Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1&quot;&gt;Ankur Bapna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1&quot;&gt;Neha Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heck_L/0/1/0/all/0/1&quot;&gt;Larry Heck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.08602">
<title>A Review of 40 Years of Cognitive Architecture Research: Core Cognitive Abilities and Practical Applications. (arXiv:1610.08602v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1610.08602</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a broad overview of the last 40 years of research on
cognitive architectures. Although the number of existing architectures is
nearing several hundred, most of the existing surveys do not reflect this
growth and focus on a handful of well-established architectures. Thus, in this
survey we wanted to shift the focus towards a more inclusive and high-level
overview of the research on cognitive architectures. Our final set of 84
architectures includes 49 that are still actively developed, and borrow from a
diverse set of disciplines, spanning areas from psychoanalysis to neuroscience.
To keep the length of this paper within reasonable limits we discuss only the
core cognitive abilities, such as perception, attention mechanisms, action
selection, memory, learning and reasoning. In order to assess the breadth of
practical applications of cognitive architectures we gathered information on
over 900 practical projects implemented using the cognitive architectures in
our list. We use various visualization techniques to highlight overall trends
in the development of the field. In addition to summarizing the current
state-of-the-art in the cognitive architecture research, this survey describes
a variety of methods and ideas that have been tried and their relative success
in modeling human cognitive abilities, as well as which aspects of cognitive
behavior need more research with respect to their mechanistic counterparts and
thus can further inform how cognitive science might progress.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1&quot;&gt;Iuliia Kotseruba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1&quot;&gt;John K. Tsotsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00443">
<title>OptNet: Differentiable Optimization as a Layer in Neural Networks. (arXiv:1703.00443v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00443</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents OptNet, a network architecture that integrates
optimization problems (here, specifically in the form of quadratic programs) as
individual layers in larger end-to-end trainable deep networks. These layers
encode constraints and complex dependencies between the hidden states that
traditional convolutional and fully-connected layers often cannot capture. In
this paper, we explore the foundations for such an architecture: we show how
techniques from sensitivity analysis, bilevel optimization, and implicit
differentiation can be used to exactly differentiate through these layers and
with respect to layer parameters; we develop a highly efficient solver for
these layers that exploits fast GPU-based batch solves within a primal-dual
interior point method, and which provides backpropagation gradients with
virtually no additional cost on top of the solve; and we highlight the
application of these approaches in several problems. In one notable example, we
show that the method is capable of learning to play mini-Sudoku (4x4) given
just input and output games, with no a priori information about the rules of
the game; this highlights the ability of our architecture to learn hard
constraints better than other neural architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1&quot;&gt;Brandon Amos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.04524">
<title>Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks. (arXiv:1705.04524v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.04524</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods for arterial blood pressure (BP) estimation directly map the
input physiological signals to output BP values without explicitly modeling the
underlying temporal dependencies in BP dynamics. As a result, these models
suffer from accuracy decay over a long time and thus require frequent
calibration. In this work, we address this issue by formulating BP estimation
as a sequence prediction problem in which both the input and target are
temporal sequences. We propose a novel deep recurrent neural network (RNN)
consisting of multilayered Long Short-Term Memory (LSTM) networks, which are
incorporated with (1) a bidirectional structure to access larger-scale context
information of input sequence, and (2) residual connections to allow gradients
in deep RNN to propagate more effectively. The proposed deep RNN model was
tested on a static BP dataset, and it achieved root mean square error (RMSE) of
3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction
respectively, surpassing the accuracy of traditional BP prediction models. On a
multi-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81
mmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP
prediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,
respectively, which outperforms all previous models with notable improvement.
The experimental results suggest that modeling the temporal dependencies in BP
dynamics significantly improves the long-term BP prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Peng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiao-Rong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Ting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_F/0/1/0/all/0/1&quot;&gt;Fen Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1&quot;&gt;Ni Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.04156">
<title>Gradient descent GAN optimization is locally stable. (arXiv:1706.04156v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.04156</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the growing prominence of generative adversarial networks (GANs),
optimization in GANs is still a poorly understood topic. In this paper, we
analyze the &quot;gradient descent&quot; form of GAN optimization i.e., the natural
setting where we simultaneously take small gradient steps in both generator and
discriminator parameters. We show that even though GAN optimization does not
correspond to a convex-concave game (even for simple parameterizations), under
proper conditions, equilibrium points of this optimization procedure are still
\emph{locally asymptotically stable} for the traditional GAN formulation. On
the other hand, we show that the recently proposed Wasserstein GAN can have
non-convergent limit cycles near equilibrium. Motivated by this stability
analysis, we propose an additional regularization term for gradient descent GAN
updates, which \emph{is} able to guarantee local stability for both the WGAN
and the traditional GAN, and also shows practical promise in speeding up
convergence and addressing mode collapse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagarajan_V/0/1/0/all/0/1&quot;&gt;Vaishnavh Nagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01968">
<title>Faster Deep Q-learning using Neural Episodic Control. (arXiv:1801.01968v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01968</link>
<description rdf:parseType="Literal">&lt;p&gt;The Research on deep reinforcement learning to estimate Q-value by deep
learning has been active in recent years. In deep reinforcement learning, it is
important to efficiently learn the experiences that a agent has collected by
exploring the environment. In this research, we propose NEC2DQN that improves
learning speed of a algorithm with poor sample efficiency by using a algorithm
with good one at the beginning of learning, and we demonstrate it in
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishio_D/0/1/0/all/0/1&quot;&gt;Daichi Nishio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamane_S/0/1/0/all/0/1&quot;&gt;Satoshi Yamane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03737">
<title>Counterfactual equivalence for POMDPs, and underlying deterministic environments. (arXiv:1801.03737v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03737</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially Observable Markov Decision Processes (POMDPs) are rich environments
often used in machine learning. But the issue of information and causal
structures in POMDPs has been relatively little studied. This paper presents
the concepts of equivalent and counterfactually equivalent POMDPs, where agents
cannot distinguish which environment they are in though any observations and
actions. It shows that any POMDP is counterfactually equivalent, for any finite
number of turns, to a deterministic POMDP with all uncertainty concentrated
into the initial state. This allows a better understanding of POMDP
uncertainty, information, and learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Stuart Armstrong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02203">
<title>Indian Regional Movie Dataset for Recommender Systems. (arXiv:1801.02203v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1801.02203</link>
<description rdf:parseType="Literal">&lt;p&gt;Indian regional movie dataset is the first database of regional Indian
movies, users and their ratings. It consists of movies belonging to 18
different Indian regional languages and metadata of users with varying
demographics. Through this dataset, the diversity of Indian regional cinema and
its huge viewership is captured. We analyze the dataset that contains roughly
10K ratings of 919 users and 2,851 movies using some supervised and
unsupervised collaborative filtering techniques like Probabilistic Matrix
Factorization, Matrix Completion, Blind Compressed Sensing etc. The dataset
consists of metadata information of users like age, occupation, home state and
known languages. It also consists of metadata of movies like genre, language,
release year and cast. India has a wide base of viewers which is evident by the
large number of movies released every year and the huge box-office revenue.
This dataset can be used for designing recommendation systems for Indian users
and regional movies, which do not, yet, exist. The dataset can be downloaded
from \href{https://goo.gl/EmTPv6}{https://goo.gl/EmTPv6}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1&quot;&gt;Prerna Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_R/0/1/0/all/0/1&quot;&gt;Richa Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Angshul Majumdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04693">
<title>Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks. (arXiv:1801.04693v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04693</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning systems based on deep neural networks, being able to produce
state-of-the-art results on various perception tasks, have gained mainstream
adoption in many applications. However, they are shown to be vulnerable to
adversarial example attack, which generates malicious output by adding slight
perturbations to the input. Previous adversarial example crafting methods,
however, use simple metrics to evaluate the distances between the original
examples and the adversarial ones, which could be easily detected by human
eyes. In addition, these attacks are often not robust due to the inevitable
noises and deviation in the physical world. In this work, we present a new
adversarial example attack crafting method, which takes the human perceptual
system into consideration and maximizes the noise tolerance of the crafted
adversarial example. Experimental results demonstrate the efficacy of the
proposed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yannan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lingxiao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04695">
<title>Sparsity-based Defense against Adversarial Attacks on Linear Classifiers. (arXiv:1801.04695v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.04695</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks represent the state of the art in machine learning in a
growing number of fields, including vision, speech and natural language
processing. However, recent work raises important questions about the
robustness of such architectures, by showing that it is possible to induce
classification errors through tiny, almost imperceptible, perturbations.
Vulnerability to such &quot;adversarial attacks&quot;, or &quot;adversarial examples&quot;, has
been conjectured to be due to the excessive linearity of deep networks. In this
paper, we study this phenomenon in the setting of a linear classifier, and show
that it is possible to exploit sparsity in natural data to combat
$\ell_{\infty}$-bounded adversarial perturbations. Specifically, we demonstrate
the efficacy of a sparsifying front end via an ensemble averaged analysis, and
experimental results for the MNIST handwritten digit database. To the best of
our knowledge, this is the first work to provide a theoretically rigorous
framework for defense against adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marzi_Z/0/1/0/all/0/1&quot;&gt;Zhinus Marzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gopalakrishnan_S/0/1/0/all/0/1&quot;&gt;Soorya Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Madhow_U/0/1/0/all/0/1&quot;&gt;Upamanyu Madhow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1&quot;&gt;Ramtin Pedarsani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05630">
<title>Sparse principal component analysis via random projections. (arXiv:1712.05630v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1712.05630</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new method for sparse principal component analysis, based on
the aggregation of eigenvector information from carefully-selected random
projections of the sample covariance matrix. Unlike most alternative
approaches, our algorithm is non-iterative, so is not vulnerable to a bad
choice of initialisation. Our theory provides great detail on the statistical
and computational trade-off in our procedure, revealing a subtle interplay
between the effective sample size and the number of random projections that are
required to achieve the minimax optimal rate. Numerical studies provide further
insight into the procedure and confirm its highly competitive finite-sample
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gataric_M/0/1/0/all/0/1&quot;&gt;Milana Gataric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tengyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Samworth_R/0/1/0/all/0/1&quot;&gt;Richard J. Samworth&lt;/a&gt;</dc:creator>
</item></rdf:RDF>