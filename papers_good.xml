<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-08T19:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11417"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02906"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07267"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01284"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02765"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03148"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.00044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02641"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01682"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.03232">
<title>Feudal Reinforcement Learning for Dialogue Management in Large Domains. (arXiv:1803.03232v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.03232</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) is a promising approach to solve dialogue policy
optimisation. Traditional RL algorithms, however, fail to scale to large
domains due to the curse of dimensionality. We propose a novel Dialogue
Management architecture, based on Feudal RL, which decomposes the decision into
two steps; a first step where a master policy selects a subset of primitive
actions, and a second step where a primitive action is chosen from the selected
subset. The structural information included in the domain ontology is used to
abstract the dialogue state space, taking the decisions at each step using
different parts of the abstracted state. This, combined with an information
sharing mechanism between slots, increases the scalability to large domains. We
show that an implementation of this approach, based on Deep-Q Networks,
significantly outperforms previous state of the art in several dialogue domains
and environments, without the need of any additional reward signal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;igo Casanueva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Budzianowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Pei-Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1&quot;&gt;Stefan Ultes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1&quot;&gt;Lina Rojas-Barahona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1&quot;&gt;Bo-Hsiang Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1&quot;&gt;Milica Ga&amp;#x161;i&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11417">
<title>TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning. (arXiv:1710.11417v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11417</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining deep model-free reinforcement learning with on-line planning is a
promising approach to building on the successes of deep RL. On-line planning
with look-ahead trees has proven successful in environments where transition
models are known a priori. However, in complex environments where transition
models need to be learned from data, the deficiencies of learned models have
limited their utility for planning. To address these challenges, we propose
TreeQN, a differentiable, recursive, tree-structured model that serves as a
drop-in replacement for any value function network in deep RL with discrete
actions. TreeQN dynamically constructs a tree by recursively applying a
transition model in a learned abstract state space and then aggregating
predicted rewards and state-values using a tree backup to estimate Q-values. We
also propose ATreeC, an actor-critic variant that augments TreeQN with a
softmax layer to form a stochastic policy network. Both approaches are trained
end-to-end, such that the learned model is optimised for its actual use in the
tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a
box-pushing task, as well as n-step DQN and value prediction networks (Oh et
al. 2017) on multiple Atari games. Furthermore, we present ablation studies
that demonstrate the effect of different auxiliary losses on learning
transition models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farquhar_G/0/1/0/all/0/1&quot;&gt;Gregory Farquhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igl_M/0/1/0/all/0/1&quot;&gt;Maximilian Igl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04340">
<title>Data Augmentation Generative Adversarial Networks. (arXiv:1711.04340v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04340</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective training of neural networks requires much data. In the low-data
regime, parameters are underdetermined, and learnt networks generalise poorly.
Data Augmentation alleviates this by using existing data more effectively.
However standard data augmentation produces only limited plausible alternative
data. Given there is potential to generate a much broader set of augmentations,
we design and train a generative model to do data augmentation. The model,
based on image conditional Generative Adversarial Networks, takes data from a
source domain and learns to take any data item and generalise it to generate
other within-class data items. As this generative process does not depend on
the classes themselves, it can be applied to novel unseen classes of data. We
show that a Data Augmentation Generative Adversarial Network (DAGAN) augments
standard vanilla classifiers well. We also show a DAGAN can enhance few-shot
learning systems such as Matching Networks. We demonstrate these approaches on
Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In
our experiments we can see over 13% increase in accuracy in the low-data regime
experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face
(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%
(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Antoniou_A/0/1/0/all/0/1&quot;&gt;Antreas Antoniou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Storkey_A/0/1/0/all/0/1&quot;&gt;Amos Storkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Edwards_H/0/1/0/all/0/1&quot;&gt;Harrison Edwards&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02839">
<title>The emergent algebraic structure of RNNs and embeddings in NLP. (arXiv:1803.02839v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.02839</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the algebraic and geometric properties of a uni-directional GRU
and word embeddings trained end-to-end on a text classification task. A
hyperparameter search over word embedding dimension, GRU hidden dimension, and
a linear combination of the GRU outputs is performed. We conclude that words
naturally embed themselves in a Lie group and that RNNs form a nonlinear
representation of the group. Appealing to these results, we propose a novel
class of recurrent-like neural networks and a word embedding scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cantrell_S/0/1/0/all/0/1&quot;&gt;Sean A. Cantrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02852">
<title>Value Alignment, Fair Play, and the Rights of Service Robots. (arXiv:1803.02852v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1803.02852</link>
<description rdf:parseType="Literal">&lt;p&gt;Ethics and safety research in artificial intelligence is increasingly framed
in terms of &quot;alignment&quot; with human values and interests. I argue that Turing&apos;s
call for &quot;fair play for machines&quot; is an early and often overlooked contribution
to the alignment literature. Turing&apos;s appeal to fair play suggests a need to
correct human behavior to accommodate our machines, a surprising inversion of
how value alignment is treated today. Reflections on &quot;fair play&quot; motivate a
novel interpretation of Turing&apos;s notorious &quot;imitation game&quot; as a condition not
of intelligence but instead of value alignment: a machine demonstrates a
minimal degree of alignment (with the norms of conversation, for instance) when
it can go undetected when interrogated by a human. I carefully distinguish this
interpretation from the Moral Turing Test, which is not motivated by a
principle of fair play, but instead depends on imitation of human moral
behavior. Finally, I consider how the framework of fair play can be used to
situate the debate over robot rights within the alignment literature. I argue
that extending rights to service robots operating in public spaces is &quot;fair&quot; in
precisely the sense that it encourages an alignment of interests between humans
and machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estrada_D/0/1/0/all/0/1&quot;&gt;Daniel Estrada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02893">
<title>An efficient framework for learning sentence representations. (arXiv:1803.02893v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.02893</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose a simple and efficient framework for learning
sentence representations from unlabelled data. Drawing inspiration from the
distributional hypothesis and recent work on learning sentence representations,
we reformulate the problem of predicting the context in which a sentence
appears as a classification problem. Given a sentence and its context, a
classifier distinguishes context sentences from other contrastive sentences
based on their vector representations. This allows us to efficiently learn
different types of encoding functions, and we show that the model learns
high-quality sentence representations. We demonstrate that our sentence
representations outperform state-of-the-art unsupervised and supervised
representation learning methods on several downstream NLP tasks that involve
understanding sentence semantics while achieving an order of magnitude speedup
in training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1&quot;&gt;Lajanugen Logeswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02906">
<title>Simultaneous Task Allocation and Planning Under Uncertainty. (arXiv:1803.02906v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02906</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose novel techniques for task allocation and planning in multi-robot
systems operating in uncertain environments. Task allocation is performed
simultaneously with planning, which provides more detailed information about
individual robot behaviour, but also exploits the independence between tasks to
do so efficiently. We use Markov decision processes to model robot behaviour
and linear temporal logic to specify tasks and safety constraints. Building
upon techniques and tools from formal verification, we show how to generate a
sequence of multi-robot policies, iteratively refining them to reallocate tasks
if individual robots fail, and providing probabilistic guarantees on the
performance (and safe operation) of the team of robots under the resulting
policy. We implement our approach and evaluate it on a benchmark multi-robot
example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faruq_F/0/1/0/all/0/1&quot;&gt;Fatma Faruq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacerda_B/0/1/0/all/0/1&quot;&gt;Bruno Lacerda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawes_N/0/1/0/all/0/1&quot;&gt;Nick Hawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_D/0/1/0/all/0/1&quot;&gt;David Parker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02912">
<title>A Brandom-ian view of Reinforcement Learning towards strong-AI. (arXiv:1803.02912v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02912</link>
<description rdf:parseType="Literal">&lt;p&gt;The analytic philosophy of Robert Brandom, based on the ideas of pragmatism,
paints a picture of sapience, through inferentialism. In this paper, we present
a theory, that utilizes essential elements of Brandom&apos;s philosophy, towards the
objective of achieving strong-AI. We do this by connecting the constitutive
elements of reinforcement learning and the Game Of Giving and Asking For
Reasons. Further, following Brandom&apos;s prescriptive thoughts, we restructure the
popular reinforcement learning algorithm A3C, and show that RL algorithms can
be tuned towards the objective of strong-AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Atrisha Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02965">
<title>A Multi-Objective Deep Reinforcement Learning Framework. (arXiv:1803.02965v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.02965</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new multi-objective deep reinforcement learning (MODRL)
framework based on deep Q-networks. We propose linear and non-linear methods to
develop the MODRL framework that includes both single-policy and multi-policy
strategies. The experimental results on a deep sea treasure environment
indicate that the proposed approach is able to converge to the optimal Pareto
solutions. The proposed framework is generic, which allows implementation of
different deep reinforcement learning algorithms in various complex
environments. Details of the framework implementation can be referred to
&lt;a href=&quot;http://www.deakin.edu.au/~thanhthi/drl.htm.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh Thi Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07267">
<title>Search Engine Guided Non-Parametric Neural Machine Translation. (arXiv:1705.07267v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07267</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we extend an attention-based neural machine translation (NMT)
model by allowing it to access an entire training set of parallel sentence
pairs even after training. The proposed approach consists of two stages. In the
first stage--retrieval stage--, an off-the-shelf, black-box search engine is
used to retrieve a small subset of sentence pairs from a training set given a
source sentence. These pairs are further filtered based on a fuzzy matching
score based on edit distance. In the second stage--translation stage--, a novel
translation model, called translation memory enhanced NMT (TM-NMT), seamlessly
uses both the source sentence and a set of retrieved sentence pairs to perform
the translation. Empirical evaluation on three language pairs (En-Fr, En-De,
and En-Es) shows that the proposed approach significantly outperforms the
baseline approach and the improvement is more significant when more relevant
sentence pairs were retrieved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_V/0/1/0/all/0/1&quot;&gt;Victor O.K. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01284">
<title>Towards Synthesizing Complex Programs from Input-Output Examples. (arXiv:1706.01284v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01284</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep learning techniques have been developed to improve the
performance of program synthesis from input-output examples. Albeit its
significant progress, the programs that can be synthesized by state-of-the-art
approaches are still simple in terms of their complexity. In this work, we move
a significant step forward along this direction by proposing a new class of
challenging tasks in the domain of program synthesis from input-output
examples: learning a context-free parser from pairs of input programs and their
parse trees. We show that this class of tasks are much more challenging than
previously studied tasks, and the test accuracy of existing approaches is
almost 0%.
&lt;/p&gt;
&lt;p&gt;We tackle the challenges by developing three novel techniques inspired by
three novel observations, which reveal the key ingredients of using deep
learning to synthesize a complex program. First, the use of a
non-differentiable machine is the key to effectively restrict the search space.
Thus our proposed approach learns a neural program operating a domain-specific
non-differentiable machine. Second, recursion is the key to achieve
generalizability. Thus, we bake-in the notion of recursion in the design of our
non-differentiable machine. Third, reinforcement learning is the key to learn
how to operate the non-differentiable machine, but it is also hard to train the
model effectively with existing reinforcement learning algorithms from a cold
boot. We develop a novel two-phase reinforcement learning-based search
algorithm to overcome this issue. In our evaluation, we show that using our
novel approach, neural parsing programs can be learned to achieve 100% test
accuracy on test inputs that are 500x longer than the training samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02765">
<title>Realizing Intelligence. (arXiv:1803.02765v1 [q-bio.NC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.02765</link>
<description rdf:parseType="Literal">&lt;p&gt;Order exists in the world. The intelligence process enables us to realize
that order, to some extent. We provide a high level description of intelligence
using simple definitions, basic building blocks, a conceptual framework and
general hierarchy. This perspective includes multiple levels of abstraction
occurring in space and in time. The resulting model offers simple, useful ways
to help realize the essence of intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yaworsky_P/0/1/0/all/0/1&quot;&gt;Paul Yaworsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03148">
<title>Generating Differentially Private Datasets Using GANs. (arXiv:1803.03148v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03148</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a technique for generating artificial datasets that
retain statistical properties of the real data while providing differential
privacy guarantees with respect to this data. We include a Gaussian noise layer
in the discriminator of a generative adversarial network to make the output and
the gradients differentially private with respect to the training data, and
then use the generator component to synthesise privacy-preserving artificial
dataset. Our experiments show that under a reasonably small privacy budget we
are able to generate data of high quality and successfully train machine
learning models on this artificial data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triastcyn_A/0/1/0/all/0/1&quot;&gt;Aleksei Triastcyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1&quot;&gt;Boi Faltings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.00044">
<title>Penalizing Unfairness in Binary Classification. (arXiv:1707.00044v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.00044</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new approach for mitigating unfairness in learned classifiers.
In particular, we focus on binary classification tasks over individuals from
two populations, where, as our criterion for fairness, we wish to achieve
similar false positive rates in both populations, and similar false negative
rates in both populations. As a proof of concept, we implement our approach and
empirically evaluate its ability to achieve both fairness and accuracy, using
datasets from the fields of criminal risk assessment, credit, lending, and
college admissions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechavod_Y/0/1/0/all/0/1&quot;&gt;Yahav Bechavod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ligett_K/0/1/0/all/0/1&quot;&gt;Katrina Ligett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02641">
<title>Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition. (arXiv:1707.02641v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02641</link>
<description rdf:parseType="Literal">&lt;p&gt;Statisticians have made great progress in creating methods that reduce our
reliance on parametric assumptions. However this explosion in research has
resulted in a breadth of inferential strategies that both create opportunities
for more reliable inference as well as complicate the choices that an applied
researcher has to make and defend. Relatedly, researchers advocating for new
methods typically compare their method to at best 2 or 3 other causal inference
strategies and test using simulations that may or may not be designed to
equally tease out flaws in all the competing methods. The causal inference data
analysis challenge, &quot;Is Your SATT Where It&apos;s At?&quot;, launched as part of the 2016
Atlantic Causal Inference Conference, sought to make progress with respect to
both of these issues. The researchers creating the data testing grounds were
distinct from the researchers submitting methods whose efficacy would be
evaluated. Results from 30 competitors across the two versions of the
competition (black box algorithms and do-it-yourself analyses) are presented
along with post-hoc analyses that reveal information about the characteristics
of causal inference strategies and settings that affect performance. The most
consistent conclusion was that methods that flexibly model the response surface
perform better overall than methods that fail to do so. Finally new methods are
proposed that combine features of several of the top-performing submitted
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorie_V/0/1/0/all/0/1&quot;&gt;Vincent Dorie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hill_J/0/1/0/all/0/1&quot;&gt;Jennifer Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shalit_U/0/1/0/all/0/1&quot;&gt;Uri Shalit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scott_M/0/1/0/all/0/1&quot;&gt;Marc Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cervone_D/0/1/0/all/0/1&quot;&gt;Dan Cervone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08363">
<title>On Using Backpropagation for Speech Texture Generation and Voice Conversion. (arXiv:1712.08363v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08363</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by recent work on neural network image generation which rely on
backpropagation towards the network inputs, we present a proof-of-concept
system for speech texture synthesis and voice conversion based on two
mechanisms: approximate inversion of the representation learned by a speech
recognition neural network, and on matching statistics of neuron activations
between different source and target utterances. Similar to image texture
synthesis and neural style transfer, the system works by optimizing a cost
function with respect to the input waveform samples. To this end we use a
differentiable mel-filterbank feature extraction pipeline and train a
convolutional CTC speech recognition network. Our system is able to extract
speaker characteristics from very limited amounts of target speaker data, as
little as a few seconds, and can be used to generate realistic speech babble or
reconstruct an utterance in a different voice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1&quot;&gt;Jan Chorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1&quot;&gt;Ron J. Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1&quot;&gt;Rif A. Saurous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1&quot;&gt;Samy Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01682">
<title>Optimizing Slate Recommendations via Slate-CVAE. (arXiv:1803.01682v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01682</link>
<description rdf:parseType="Literal">&lt;p&gt;The slate recommendation problem aims to find the &quot;optimal&quot; ordering of a
subset of documents to be presented on a surface that we call &quot;slate&quot;. The
definition of &quot;optimal&quot; changes depending on the underlying applications but a
typical goal is to maximize user engagement with the slate. Solving this
problem at scale is hard due to the combinatorial explosion of documents to
show and their display positions on the slate. In this paper, we introduce
Slate Conditional Variational Auto-Encoders (Slate-CVAE) to generate optimal
slates. To the best of our knowledge, this is the first conditional generative
model that provides a unified framework for slate recommendation by direct
generation. Slate-CVAE automatically takes into account the format of the slate
and any biases that the representation causes, thus truly proposing the optimal
slate. Additionally, to deal with large corpora of documents, we present a
novel approach that uses pretrained document embeddings combined with a
soft-nearest-neighbors layer within our CVAE model. Experiments show that on
the simulated and real-world datasets, Slate-CVAE outperforms recommender
systems that consists of greedily ranking documents by a significant margin
while remaining scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ray Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gowal_S/0/1/0/all/0/1&quot;&gt;Sven Gowal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mann_T/0/1/0/all/0/1&quot;&gt;Timothy A. Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rezende_D/0/1/0/all/0/1&quot;&gt;Danilo J. Rezende&lt;/a&gt;</dc:creator>
</item></rdf:RDF>