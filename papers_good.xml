<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00728"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03308"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00643"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05263"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00559"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00598"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09069"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.00728">
<title>Evolving Mario Levels in the Latent Space of a Deep Convolutional Generative Adversarial Network. (arXiv:1805.00728v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00728</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) are a machine learning approach
capable of generating novel example outputs across a space of provided training
examples. Procedural Content Generation (PCG) of levels for video games could
benefit from such models, especially for games where there is a pre-existing
corpus of levels to emulate. This paper trains a GAN to generate levels for
Super Mario Bros using a level from the Video Game Level Corpus. The approach
successfully generates a variety of levels similar to one in the original
corpus, but is further improved by application of the Covariance Matrix
Adaptation Evolution Strategy (CMA-ES). Specifically, various fitness functions
are used to discover levels within the latent space of the GAN that maximize
desired properties. Simple static properties are optimized, such as a given
distribution of tile types. Additionally, the champion A* agent from the 2009
Mario AI competition is used to assess whether a level is playable, and how
many jumping actions are required to beat it. These fitness functions allow for
the discovery of levels that exist within the space of examples designed by
experts, and also guide the search towards levels that fulfill one or more
specified objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volz_V/0/1/0/all/0/1&quot;&gt;Vanessa Volz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrum_J/0/1/0/all/0/1&quot;&gt;Jacob Schrum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jialin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_S/0/1/0/all/0/1&quot;&gt;Simon M. Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1&quot;&gt;Adam Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1&quot;&gt;Sebastian Risi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06563">
<title>Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients. (arXiv:1712.06563v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06563</link>
<description rdf:parseType="Literal">&lt;p&gt;While neuroevolution (evolving neural networks) has a successful track record
across a variety of domains from reinforcement learning to artificial life, it
is rarely applied to large, deep neural networks. A central reason is that
while random mutation generally works in low dimensions, a random perturbation
of thousands or millions of weights is likely to break existing functionality,
providing no learning signal even if some individual weight changes were
beneficial. This paper proposes a solution by introducing a family of safe
mutation (SM) operators that aim within the mutation operator itself to find a
degree of change that does not alter network behavior too much, but still
facilitates exploration. Importantly, these SM operators do not require any
additional interactions with the environment. The most effective SM variant
capitalizes on the intriguing opportunity to scale the degree of mutation of
each individual weight according to the sensitivity of the network&apos;s outputs to
that weight, which requires computing the gradient of outputs with respect to
the weights (instead of the gradient of error, as in conventional deep
learning). This safe mutation through gradients (SM-G) operator dramatically
increases the ability of a simple genetic algorithm-based neuroevolution method
to find solutions in high-dimensional domains that require deep and/or
recurrent neural networks (which tend to be particularly brittle to mutation),
including domains that require processing raw pixels. By improving our ability
to evolve deep neural networks, this new safer approach to mutation expands the
scope of domains amenable to neuroevolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jay Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03308">
<title>Predictive Neural Networks. (arXiv:1802.03308v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03308</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks are a powerful means to cope with time series. We
show that already linearly activated recurrent neural networks can approximate
any time-dependent function f(t) given by a number of function values. The
approximation can effectively be learned by simply solving a linear equation
system; no backpropagation or similar methods are needed. Furthermore, the
network size can be reduced by taking only the most relevant components of the
network. Thus, in contrast to others, our approach not only learns network
weights but also the network architecture. The networks have interesting
properties: In the stationary case they end up in ellipse trajectories in the
long run, and they allow the prediction of further values and compact
representations of functions. We demonstrate this by several experiments, among
them multiple superimposed oscillators (MSO) and robotic soccer. Predictive
neural networks outperform the previous state-of-the-art for the MSO task with
a minimal number of units.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1&quot;&gt;Frieder Stolzenburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michael_O/0/1/0/all/0/1&quot;&gt;Olivia Michael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obst_O/0/1/0/all/0/1&quot;&gt;Oliver Obst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00643">
<title>Translating LPOD and CR-Prolog2 into Standard Answer Set Programs. (arXiv:1805.00643v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00643</link>
<description rdf:parseType="Literal">&lt;p&gt;Logic Programs with Ordered Disjunction (LPOD) is an extension of standard
answer set programs to handle preference using the construct of ordered
disjunction, and CR-Prolog2 is an extension of standard answer set programs
with consistency restoring rules and LPOD-like ordered disjunction. We present
reductions of each of these languages into the standard ASP language, which
gives us an alternative way to understand the extensions in terms of the
standard ASP language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhun Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00851">
<title>How does the AI understand what&apos;s going on. (arXiv:1805.00851v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00851</link>
<description rdf:parseType="Literal">&lt;p&gt;Most researchers regard AI as a static function without memory. This is one
of the few articles where AI is seen as a device with memory. When we have
memory, we can ask ourselves: &quot;Where am I?&quot;, and &quot;What is going on?&quot; When we
have no memory, we have to assume that we are always in the same place and that
the world is always in the same state.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobrev_D/0/1/0/all/0/1&quot;&gt;Dimiter Dobrev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04326">
<title>Learning with Opponent-Learning Awareness. (arXiv:1709.04326v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04326</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent settings are quickly gathering importance in machine learning.
This includes a plethora of recent work on deep multi-agent reinforcement
learning, but also can be extended to hierarchical RL, generative adversarial
networks and decentralised optimisation. In all these settings the presence of
multiple learning agents renders the training problem non-stationary and often
leads to unstable training or undesired final results. We present Learning with
Opponent-Learning Awareness (LOLA), a method in which each agent shapes the
anticipated learning of the other agents in the environment. The LOLA learning
rule includes an additional term that accounts for the impact of one agent&apos;s
policy on the anticipated parameter update of the other agents. Preliminary
results show that the encounter of two LOLA agents leads to the emergence of
tit-for-tat and therefore cooperation in the iterated prisoners&apos; dilemma, while
independent learning does not. In this domain, LOLA also receives higher
payouts compared to a naive learner, and is robust against exploitation by
higher order gradient-based methods. Applied to repeated matching pennies, LOLA
agents converge to the Nash equilibrium. In a round robin tournament we show
that LOLA agents can successfully shape the learning of a range of multi-agent
learning algorithms from literature, resulting in the highest average returns
on the IPD. We also show that the LOLA update rule can be efficiently
calculated using an extension of the policy gradient estimator, making the
method suitable for model-free RL. This method thus scales to large parameter
and input spaces and nonlinear function approximators. We also apply LOLA to a
grid world task with an embedded social dilemma using deep recurrent policies
and opponent modelling. Again, by explicitly considering the learning of the
other agent, LOLA agents learn to cooperate out of self-interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob N. Foerster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Richard Y. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1&quot;&gt;Maruan Al-Shedivat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05263">
<title>Knowledge-based Recurrent Attentive Neural Network for Small Object Detection. (arXiv:1803.05263v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05263</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate Traffic Sign Detection (TSD) can help intelligent systems make
better decisions according to the traffic regulations. TSD, regarded as a
typical small object detection problem in some way, is fundamental in Advanced
Driver Assistance Systems (ADAS) and self-driving. However, although deep
neural networks have achieved human even superhuman performance on several
tasks, due to their own limitations, small object detection is still an open
question. In this paper, we proposed a brain-inspired network, named as
KB-RANN, to handle this problem. Attention mechanism is an essential function
of our brain, we used a novel recurrent attentive neural network to improve the
detection accuracy in a fine-grained manner. Further, we combined domain
specific knowledge and intuitive knowledge to improve the efficiency.
Experimental result shows that our methods achieved better performance than
several popular methods widely used in object detection. More significantly, we
transplanted our method on our designed embedded system and deployed on our
self-driving car successfully.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kai Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuedong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00559">
<title>Decision Tree Design for Classification in Crowdsourcing Systems. (arXiv:1805.00559v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00559</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel sequential paradigm for classification in
crowdsourcing systems. Considering that workers are unreliable and they perform
the tests with errors, we study the construction of decision trees so as to
minimize the probability of mis-classification. By exploiting the connection
between probability of mis-classification and entropy at each level of the
decision tree, we propose two algorithms for decision tree design. Furthermore,
the worker assignment problem is studied when workers can be assigned to
different tests of the decision tree to provide a trade-off between
classification cost and resulting error performance. Numerical results are
presented for illustration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1&quot;&gt;Baocheng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qunwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_P/0/1/0/all/0/1&quot;&gt;Pramod K. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00784">
<title>Markov Chain Neural Networks. (arXiv:1805.00784v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00784</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we present a modified neural network model which is capable to
simulate Markov Chains. We show how to express and train such a network, how to
ensure given statistical properties reflected in the training data and we
demonstrate several applications where the network produces non-deterministic
outcomes. One example is a random walker model, e.g. useful for simulation of
Brownian motions or a natural Tic-Tac-Toe network which ensures
non-deterministic game behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1&quot;&gt;Maren Awiszus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1&quot;&gt;Bodo Rosenhahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00899">
<title>AI safety via debate. (arXiv:1805.00899v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00899</link>
<description rdf:parseType="Literal">&lt;p&gt;To make AI systems broadly useful for challenging real-world tasks, we need
them to learn complex human goals and preferences. One approach to specifying
complex goals asks humans to judge during training which agent behaviors are
safe and useful, but this approach can fail if the task is too complicated for
a human to directly judge. To help address this concern, we propose training
agents via self play on a zero sum debate game. Given a question or proposed
action, two agents take turns making short statements up to a limit, then a
human judges which of the agents gave the most true, useful information. In an
analogy to complexity theory, debate with optimal play can answer any question
in PSPACE given polynomial time judges (direct judging answers only NP
questions). In practice, whether debate works involves empirical questions
about humans and the tasks we want AIs to perform, plus theoretical questions
about the meaning of AI alignment. We report results on an initial MNIST
experiment where agents compete to convince a sparse classifier, boosting the
classifier&apos;s accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to
85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of
the debate model, focusing on potential weaknesses as the model scales up, and
we propose future human and computer experiments to test these properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Irving_G/0/1/0/all/0/1&quot;&gt;Geoffrey Irving&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Christiano_P/0/1/0/all/0/1&quot;&gt;Paul Christiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amodei_D/0/1/0/all/0/1&quot;&gt;Dario Amodei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00598">
<title>Controllable Generative Adversarial Network. (arXiv:1708.00598v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00598</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently introduced generative adversarial network (GAN) has been shown
numerous promising results to generate realistic samples. The essential task of
GAN is to control the features of samples generated from a random distribution.
While the current GAN structures, such as conditional GAN, successfully
generate samples with desired major features, they often fail to produce
detailed features that bring specific differences among samples. To overcome
this limitation, here we propose a controllable GAN (ControlGAN) structure. By
separating a feature classifier from a discriminator, the generator of
ControlGAN is designed to learn generating synthetic samples with the specific
detailed features. Evaluated with multiple image datasets, ControlGAN shows a
power to generate improved samples with well-controlled features. Furthermore,
we demonstrate that ControlGAN can generate intermediate features and opposite
features for interpolated and extrapolated input labels that are not used in
the training process. It implies that ControlGAN can significantly contribute
to the variety of generated samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seok_J/0/1/0/all/0/1&quot;&gt;Junhee Seok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09069">
<title>Active Learning with Logged Data. (arXiv:1802.09069v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09069</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider active learning with logged data, where labeled examples are
drawn conditioned on a predetermined logging policy, and the goal is to learn a
classifier on the entire population, not just conditioned on the logging
policy.
&lt;/p&gt;
&lt;p&gt;Prior work addresses this problem either when only logged data is available,
or purely in a controlled random experimentation setting where the logged data
is ignored. In this work, we combine both approaches to provide an algorithm
that uses logged data to bootstrap and inform experimentation, thus achieving
the best of both worlds. Our work is inspired by a connection between
controlled random experimentation and active learning, and modifies existing
disagreement-based active learning algorithms to exploit logged data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Songbai Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1&quot;&gt;Kamalika Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1&quot;&gt;Tara Javidi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>