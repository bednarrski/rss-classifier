<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03143"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.09332"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.07943"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00410"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03175"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02610"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.03143">
<title>Comparing heterogeneous entities using artificial neural networks of trainable weighted structural components and machine-learned activation functions. (arXiv:1801.03143v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.03143</link>
<description rdf:parseType="Literal">&lt;p&gt;To compare entities of differing types and structural components, the
artificial neural network paradigm was used to cross-compare structural
components between heterogeneous documents. Trainable weighted structural
components were input into machine-learned activation functions of the neurons.
The model was used for matching news articles and videos, where the inputs and
activation functions respectively consisted of term vectors and cosine
similarity measures between the weighted structural components. The model was
tested with different weights, achieving as high as 59.2% accuracy for matching
videos to news articles. A mobile application user interface for recommending
related videos for news articles was developed to demonstrate consumer value,
including its potential usefulness for cross-selling products from unrelated
categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wangperawong_A/0/1/0/all/0/1&quot;&gt;Artit Wangperawong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kriangchaivech_K/0/1/0/all/0/1&quot;&gt;Kettip Kriangchaivech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lanari_A/0/1/0/all/0/1&quot;&gt;Austin Lanari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lam_S/0/1/0/all/0/1&quot;&gt;Supui Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wangperawong_P/0/1/0/all/0/1&quot;&gt;Panthong Wangperawong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.09332">
<title>Parametric Exponential Linear Unit for Deep Convolutional Neural Networks. (arXiv:1605.09332v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1605.09332</link>
<description rdf:parseType="Literal">&lt;p&gt;Object recognition is an important task for improving the ability of visual
systems to perform complex scene understanding. Recently, the Exponential
Linear Unit (ELU) has been proposed as a key component for managing bias shift
in Convolutional Neural Networks (CNNs), but defines a parameter that must be
set by hand. In this paper, we propose learning a parameterization of ELU in
order to learn the proper activation shape at each layer in the CNNs. Our
results on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN,
Overfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU
(PELU) has better performances than the non-parametric ELU. We have observed as
much as a 7.28% relative error improvement on ImageNet with the NiN network,
with only 0.0003% parameter increase. Our visual examination of the non-linear
behaviors adopted by Vgg using PELU shows that the network took advantage of
the added flexibility by learning different activations at different layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trottier_L/0/1/0/all/0/1&quot;&gt;Ludovic Trottier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giguere_P/0/1/0/all/0/1&quot;&gt;Philippe Gigu&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaib_draa_B/0/1/0/all/0/1&quot;&gt;Brahim Chaib-draa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.07943">
<title>Role of zero synapses in unsupervised feature learning. (arXiv:1703.07943v4 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1703.07943</link>
<description rdf:parseType="Literal">&lt;p&gt;Synapses in real neural circuits can take discrete values, including zero
(silent or potential) synapses. The computational role of zero synapses in
unsupervised feature learning of unlabeled noisy data is still unclear, thus it
is important to understand how the sparseness of synaptic activity is shaped
during learning and its relationship with receptive field formation. Here, we
formulate this kind of sparse feature learning by a statistical mechanics
approach. We find that learning decreases the fraction of zero synapses, and
when the fraction decreases rapidly around a critical data size, an
intrinsically structured receptive field starts to develop. Further increasing
the data size refines the receptive field, while a very small fraction of zero
synapses remain to act as contour detectors. This phenomenon is discovered not
only in learning a handwritten digits dataset, but also in learning retinal
neural activity measured in a natural-movie-stimuli experiment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haiping Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00410">
<title>Visual art inspired by the collective feeding behavior of sand-bubbler crabs. (arXiv:1709.00410v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00410</link>
<description rdf:parseType="Literal">&lt;p&gt;Sand--bubblers are crabs of the genera Dotilla and Scopimera which are known
to produce remarkable patterns and structures at tropical beaches. From these
pattern-making abilities, we may draw inspiration for digital visual art. A
simple mathematical model is proposed and an algorithm is designed that may
create such sand-bubbler patterns artificially. In addition, design parameters
to modify the patterns are identified and analyzed by computational aesthetic
measures. Finally, an extension of the algorithm is discussed that may enable
controlling and guiding generative evolution of the art-making process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_H/0/1/0/all/0/1&quot;&gt;Hendrik Richter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03175">
<title>Precision and Recall for Range-Based Anomaly Detection. (arXiv:1801.03175v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03175</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical anomaly detection is principally concerned with point-based
anomalies, anomalies that occur at a single data point. In this paper, we
present a new mathematical model to express range-based anomalies, anomalies
that occur over a range (or period) of time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tae Jun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1&quot;&gt;Justin Gottschlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1&quot;&gt;Nesime Tatbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metcalf_E/0/1/0/all/0/1&quot;&gt;Eric Metcalf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zdonik_S/0/1/0/all/0/1&quot;&gt;Stan Zdonik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02668">
<title>Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time. (arXiv:1801.02668v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02668</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowd-powered conversational assistants have been shown to be more robust
than automated systems, but do so at the cost of higher response latency and
monetary costs. A promising direction is to combine the two approaches for high
quality, low latency, and low cost solutions. In this paper, we introduce
Evorus, a crowd-powered conversational assistant built to automate itself over
time by (i) allowing new chatbots to be easily integrated to automate more
scenarios, (ii) reusing prior crowd answers, and (iii) learning to
automatically approve response candidates. Our 5-month-long deployment with 80
participants and 281 conversations shows that Evorus can automate itself
without compromising conversation quality. Crowd-AI architectures have long
been proposed as a way to reduce cost and latency for crowd-powered systems;
Evorus demonstrates how automation can be introduced successfully in a deployed
system. Its architecture allows future researchers to make further innovation
on the underlying automated components in the context of a deployed open domain
dialog system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Ting-Hao &amp;#x27;Kenneth&amp;#x27; Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Joseph Chee Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1&quot;&gt;Jeffrey P. Bigham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03329">
<title>Weakly Supervised One-Shot Detection with Attention Siamese Networks. (arXiv:1801.03329v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of weakly supervised one-shot detection. In this task,
we attempt to perform a detection task over a set of unseen classes, when
training only using weak binary labels that indicate the existence of a class
instance in a given example. The model is conditioned on a single exemplar of
an unseen class and a target example that may or may not contain an instance of
the same class as the exemplar. A similarity map is computed by using a Siamese
neural network to map the exemplar and regions of the target example to a
latent representation space and then computing cosine similarity scores between
representations. An attention mechanism weights different regions in the target
example, and enables learning of the one-shot detection task using the weaker
labels alone. The model can be applied to detection tasks from different
domains, including computer vision object detection. We evaluate our attention
Siamese networks on a one-shot detection task from the audio domain, where it
detects audio keywords in spoken utterances. Our model considerably outperforms
a baseline approach and yields a 42.6% average precision for detection across
10 unseen classes. Moreover, architectural developments from computer vision
object detection models such as a region proposal network can be incorporated
into the model architecture, and results show that performance is expected to
improve by doing so.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keren_G/0/1/0/all/0/1&quot;&gt;Gil Keren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmitt_M/0/1/0/all/0/1&quot;&gt;Maximilian Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kehrenberg_T/0/1/0/all/0/1&quot;&gt;Thomas Kehrenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04757">
<title>Deriving optimal weights in deep neural networks. (arXiv:1703.04757v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04757</link>
<description rdf:parseType="Literal">&lt;p&gt;Training deep neural networks generally requires massive amounts of data and
is very computation intensive. We show here that it may be possible to
circumvent the expensive gradient descent procedure and derive the parameters
of a neural network directly from properties of the training data. We show
that, near convergence, the gradient descent equations for layers close to the
input can be linearized and become stochastic equations with noise related to
the covariance of data for each class. We derive the distribution of solutions
to these equations and discover that it is related to a &quot;supervised principal
component analysis.&quot; We implement these results on image datasets MNIST,
CIFAR10 and CIFAR100 and find that, indeed, pretrained layers using our
findings performs comparable or superior to neural networks of the same size
and architecture trained with gradient descent. Moreover, our pretrained layers
can often be calculated using a fraction of the training data, owing to the
quick convergence of the covariance matrix. Thus, our findings indicate that we
can cut the training time both by requiring only a fraction of the data used
for gradient descent, and by eliminating layers in the costly backpropagation
step of the training. Additionally, these findings partially elucidate the
inner workings of deep neural networks and allow us to mathematically calculate
optimal solutions for some stages of classification problems, thus
significantly boosting our ability to solve such problems efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehmamy_N/0/1/0/all/0/1&quot;&gt;Nima Dehmamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohani_N/0/1/0/all/0/1&quot;&gt;Neda Rohani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1&quot;&gt;Aggelos Katsaggelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01587">
<title>SpectralNet: Spectral Clustering using Deep Neural Networks. (arXiv:1801.01587v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is a leading and popular technique in unsupervised data
analysis. Two of its major limitations are scalability and generalization of
the spectral embedding (i.e., out-of-sample-extension). In this paper we
introduce a deep learning approach to spectral clustering that overcomes the
above shortcomings. Our network, which we call SpectralNet, learns a map that
embeds input data points into the eigenspace of their associated graph
Laplacian matrix and subsequently clusters them. We train SpectralNet using a
procedure that involves constrained stochastic optimization. Stochastic
optimization allows it to scale to large datasets, while the constraints, which
are implemented using a special-purpose output layer, allow us to keep the
network output orthogonal. Moreover, the map learned by SpectralNet naturally
generalizes the spectral embedding to unseen data points. To further improve
the quality of the clustering, we replace the standard pairwise Gaussian
affinities with affinities leaned from unlabeled data using a Siamese network.
Additional improvement can be achieved by applying the network to code
representations produced, e.g., by standard autoencoders. Our end-to-end
learning procedure is fully unsupervised. In addition, we apply VC dimension
theory to derive a lower bound on the size of SpectralNet. State-of-the-art
clustering results are reported on the Reuters dataset. Our implementation is
publicly available at https://github.com/kstant0725/SpectralNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Henry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nadler_B/0/1/0/all/0/1&quot;&gt;Boaz Nadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basri_R/0/1/0/all/0/1&quot;&gt;Ronen Basri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02610">
<title>Generating Adversarial Examples with Adversarial Networks. (arXiv:1801.02610v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02610</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been found to be vulnerable to adversarial
examples resulting from adding small-magnitude perturbations to inputs. Such
adversarial examples can mislead DNNs to produce adversary-selected results.
Different attack strategies have been proposed to generate adversarial
examples, but how to produce them with high perceptual quality and more
efficiently requires more research efforts. In this paper, we propose AdvGAN to
generate adversarial examples with generative adversarial networks (GANs),
which can learn and approximate the distribution of original instances. For
AdvGAN, once the generator is trained, it can generate adversarial
perturbations efficiently for any instance, so as to potentially accelerate
adversarial training as defenses. We apply AdvGAN in both semi-whitebox and
black-box attack settings. In semi-whitebox attacks, there is no need to access
the original target model after the generator is trained, in contrast to
traditional white-box attacks. In black-box attacks, we dynamically train a
distilled model for the black-box model and optimize the generator accordingly.
Adversarial examples generated by AdvGAN on different target models have high
attack success rate under state-of-the-art defenses compared to other attacks.
Our attack has placed the first with 92.76% accuracy on a public MNIST
black-box attack challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Warren He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item></rdf:RDF>