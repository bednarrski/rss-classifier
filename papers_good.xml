<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07770"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07729"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07828"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07980"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07661"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02798"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03360"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.07770">
<title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. (arXiv:1803.07770v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1803.07770</link>
<description rdf:parseType="Literal">&lt;p&gt;Decades of research on the neural code underlying spatial navigation have
revealed a diverse set of neural response properties. The Entorhinal Cortex
(EC) of the mammalian brain contains a rich set of spatial correlates,
including grid cells which encode space using tessellating patterns. However,
the mechanisms and functional significance of these spatial representations
remain largely mysterious. As a new way to understand these neural
representations, we trained recurrent neural networks (RNNs) to perform
navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find
that grid-like spatial response patterns emerge in trained networks, along with
units that exhibit other spatial correlates, including border cells and
band-like cells. All these different functional types of neurons have been
observed experimentally. The order of the emergence of grid-like and border
cells is also consistent with observations from developmental studies.
Together, our results suggest that grid cells, border cells and others as
observed in EC may be a natural solution for representing space efficiently
given the predominant recurrent connections in the neural circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cueva_C/0/1/0/all/0/1&quot;&gt;Christopher J. Cueva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xue-Xin Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07724">
<title>Attention on Attention: Architectures for Visual Question Answering (VQA). (arXiv:1803.07724v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.07724</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Question Answering (VQA) is an increasingly popular topic in deep
learning research, requiring coordination of natural language processing and
computer vision modules into a single architecture. We build upon the model
which placed first in the VQA Challenge by developing thirteen new attention
mechanisms and introducing a simplified classifier. We performed 300 GPU hours
of extensive hyperparameter and architecture searches and were able to achieve
an evaluation score of 64.78%, outperforming the existing state-of-the-art
single model&apos;s validation score of 63.15%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jasdeep Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_V/0/1/0/all/0/1&quot;&gt;Vincent Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nutkiewicz_A/0/1/0/all/0/1&quot;&gt;Alex Nutkiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07729">
<title>Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation. (arXiv:1803.07729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.07729</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing research studies on vision and language grounding for robot
navigation focus on improving model-free deep reinforcement learning (DRL)
models in synthetic environments. However, model-free DRL models do not
consider the dynamics in the real-world environments, and they often fail to
generalize to new scenes. In this paper, we take a radical approach to bridge
the gap between synthetic studies and real-world practices---We propose a
novel, planned-ahead hybrid reinforcement learning model that combines
model-free and model-based reinforcement learning to solve a real-world
vision-language navigation task. Our look-ahead module tightly integrates a
look-ahead policy model with an environment model that predicts the next state
and the reward. Experimental results suggest that our proposed method
significantly outperforms the baselines and achieves the best on the real-world
Room-to-Room dataset. Moreover, our scalable method is more generalizable when
transferring to unseen environments, and the relative success rate is increased
by 15.5% on the unseen test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongmin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07828">
<title>Expeditious Generation of Knowledge Graph Embeddings. (arXiv:1803.07828v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.07828</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Graph Embedding methods aim at representing entities and relations
in a knowledge base as points or vectors in a continuous vector space. Several
approaches using embeddings have shown promising results on tasks such as link
prediction, entity recommendation, question answering, and triplet
classification. However, only a few methods can compute low-dimensional
embeddings of very large knowledge bases. In this paper, we propose KG2Vec, a
novel approach to Knowledge Graph Embedding based on the skip-gram model.
Instead of using a predefined scoring function, we learn it relying on Long
Short-Term Memories. We evaluated the goodness of our embeddings on knowledge
graph completion and show that KG2Vec is comparable to the quality of the
scalable state-of-the-art approaches and can process large graphs by parsing
more than a hundred million triples in less than 6 hours on common hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soru_T/0/1/0/all/0/1&quot;&gt;Tommaso Soru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruberto_S/0/1/0/all/0/1&quot;&gt;Stefano Ruberto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1&quot;&gt;Diego Moussallem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marx_E/0/1/0/all/0/1&quot;&gt;Edgard Marx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteves_D/0/1/0/all/0/1&quot;&gt;Diego Esteves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1&quot;&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07980">
<title>Information Theoretic Interpretation of Deep learning. (arXiv:1803.07980v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07980</link>
<description rdf:parseType="Literal">&lt;p&gt;We interpret part of the experimental results of Shwartz-Ziv and Tishby
[2017]. Inspired by these results, we established a conjecture of the dynamics
of the machinary of deep neural network. This conjecture can be used to explain
the counterpart result by Saxe et al. [2018].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuekai Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08037">
<title>Similar Elements and Metric Labeling on Complete Graphs. (arXiv:1803.08037v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1803.08037</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a problem that involves finding similar elements in a collection
of sets. The problem is motivated by applications in machine learning and
pattern recognition. We formulate the similar elements problem as an
optimization and give an efficient approximation algorithm that finds a
solution within a factor of 2 of the optimal. The similar elements problem is a
special case of the metric labeling problem and we also give an efficient
2-approximation algorithm for the metric labeling problem on complete graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felzenszwalb_P/0/1/0/all/0/1&quot;&gt;Pedro F. Felzenszwalb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07170">
<title>Blaming humans in autonomous vehicle accidents: Shared responsibility across levels of automation. (arXiv:1803.07170v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07170</link>
<description rdf:parseType="Literal">&lt;p&gt;When a semi-autonomous car crashes and harms someone, how are blame and
causal responsibility distributed across the human and machine drivers? In this
article, we consider cases in which a pedestrian was hit and killed by a car
being operated under shared control of a primary and a secondary driver. We
find that when only one driver makes an error, that driver receives the blame
and is considered causally responsible for the harm, regardless of whether that
driver is a machine or a human. However, when both drivers make errors in cases
of shared control between a human and a machine, the blame and responsibility
attributed to the machine is reduced. This finding portends a public
under-reaction to the malfunctioning AI components of semi-autonomous cars and
therefore has a direct policy implication: a bottom-up regulatory scheme (which
operates through tort law that is adjudicated through the jury system) could
fail to properly regulate the safety of shared-control vehicles; instead, a
top-down scheme (enacted through federal laws) may be called for.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_E/0/1/0/all/0/1&quot;&gt;Edmond Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sydney Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1&quot;&gt;Max Kleiman-Weiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dsouza_S/0/1/0/all/0/1&quot;&gt;Sohan Dsouza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariff_A/0/1/0/all/0/1&quot;&gt;Azim Shariff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnefon_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Bonnefon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahwan_I/0/1/0/all/0/1&quot;&gt;Iyad Rahwan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07617">
<title>Online Learning: Sufficient Statistics and the Burkholder Method. (arXiv:1803.07617v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07617</link>
<description rdf:parseType="Literal">&lt;p&gt;We uncover a fairly general principle in online learning: If regret can be
(approximately) expressed as a function of certain &quot;sufficient statistics&quot; for
the data sequence, then there exists a special Burkholder function that 1) can
be used algorithmically to achieve the regret bound and 2) only depends on
these sufficient statistics, not the entire data sequence, so that the online
strategy is only required to keep the sufficient statistics in memory. This
characterization is achieved by bringing the full power of the Burkholder
Method --- originally developed for certifying probabilistic martingale
inequalities --- to bear on the online learning setting.
&lt;/p&gt;
&lt;p&gt;To demonstrate the scope and effectiveness of the Burkholder method, we
develop a novel online strategy for matrix prediction that attains a regret
bound corresponding to the variance term in matrix concentration inequalities.
We also present a linear-time/space prediction strategy for parameter free
supervised learning with linear classes and general smooth norms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dylan J. Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhlin_A/0/1/0/all/0/1&quot;&gt;Alexander Rakhlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1&quot;&gt;Karthik Sridharan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07634">
<title>Domain Adaptation with Randomized Expectation Maximization. (arXiv:1803.07634v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.07634</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation (DA) is the task of classifying an unlabeled dataset
(target) using a labeled dataset (source) from a related domain. The majority
of successful DA methods try to directly match the distributions of the source
and target data by transforming the feature space. Despite their success, state
of the art methods based on this approach are either involved or unable to
directly scale to data with many features. This article shows that domain
adaptation can be successfully performed by using a very simple randomized
expectation maximization (EM) method. We consider two instances of the method,
which involve logistic regression and support vector machine, respectively. The
underlying assumption of the proposed method is the existence of a good single
linear classifier for both source and target domain. The potential limitations
of this assumption are alleviated by the flexibility of the method, which can
directly incorporate deep features extracted from a pre-trained deep neural
network. The resulting algorithm is strikingly easy to implement and apply. We
test its performance on 36 real-life adaptation tasks over text and image data
with diverse characteristics. The method achieves state-of-the-art results,
competitive with those of involved end-to-end deep transfer-learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laarhoven_T/0/1/0/all/0/1&quot;&gt;Twan van Laarhoven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07661">
<title>Efficient Recurrent Neural Networks using Structured Matrices in FPGAs. (arXiv:1803.07661v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07661</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent Neural Networks (RNNs) are becoming increasingly important for time
series-related applications which require efficient and real-time
implementations. The recent pruning based work ESE suffers from degradation of
performance/energy efficiency due to the irregular network structure after
pruning. We propose block-circulant matrices for weight matrix representation
in RNNs, thereby achieving simultaneous model compression and acceleration. We
aim to implement RNNs in FPGA with highest performance and energy efficiency,
with certain accuracy requirement (negligible accuracy degradation).
Experimental results on actual FPGA deployments shows that the proposed
framework achieves a maximum energy efficiency improvement of 35.7$\times$
compared with ESE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qinru Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yun Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08010">
<title>Social Media Would Not Lie: Prediction of the 2016 Taiwan Election via Online Heterogeneous Data. (arXiv:1803.08010v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1803.08010</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence of online media has attracted researchers from various domains
to explore human behavior and make interesting predictions. In this research,
we leverage heterogeneous social media data collected from various online
platforms to predict Taiwan&apos;s 2016 presidential election. In contrast to most
existing research, we take a &quot;signal&quot; view of heterogeneous information and
adopt the Kalman filter to fuse multiple signals into daily vote predictions
for the candidates. We also consider events that influenced the election in a
quantitative manner based on the so-called event study model that originated in
the field of financial research. We obtained the following interesting
findings. First, public opinions in online media dominate traditional polls in
Taiwan election prediction in terms of both predictive power and timeliness.
But offline polls can still function on alleviating the sample bias of online
opinions. Second, although online signals converge as election day approaches,
the simple Facebook &quot;Like&quot; is consistently the strongest indicator of the
election result. Third, most influential events have a strong connection to
cross-strait relations, and the Chou Tzu-yu flag incident followed by the
apology video one day before the election increased the vote share of Tsai
Ing-Wen by 3.66%. This research justifies the predictive power of online media
in politics and the advantages of information fusion. The combined use of the
Kalman filter and the event study method contributes to the data-driven
political analytics paradigm for both prediction and attribution purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guannan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junjie Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02798">
<title>Bayesian Recurrent Neural Networks. (arXiv:1704.02798v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02798</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we explore a straightforward variational Bayes scheme for
Recurrent Neural Networks. Firstly, we show that a simple adaptation of
truncated backpropagation through time can yield good quality uncertainty
estimates and superior regularisation at only a small extra computational cost
during training, also reducing the amount of parameters by 80\%. Secondly, we
demonstrate how a novel kind of posterior approximation yields further
improvements to the performance of Bayesian RNNs. We incorporate local gradient
information into the approximate posterior to sharpen it around the current
batch statistics. We show how this technique is not exclusive to recurrent
neural networks and can be applied more widely to train Bayesian neural
networks. We also empirically demonstrate how Bayesian RNNs are superior to
traditional RNNs on a language modelling benchmark and an image captioning
task, as well as showing how each of these methods improve our model over a
variety of other schemes for training them. We also introduce a new benchmark
for studying uncertainty for language models so future methods can be easily
compared.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortunato_M/0/1/0/all/0/1&quot;&gt;Meire Fortunato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05335">
<title>Unsupervised Domain Adaptation with Random Walks on Target Labelings. (arXiv:1706.05335v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05335</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (DA) is used to automatize the task of
labeling data: an unlabeled dataset (target) is annotated using a labeled
dataset (source) from a related domain. We cast domain adaptation as the
problem of finding stable labels for target examples. A new definition of label
stability is proposed, motivated by a generalization error bound for large
margin linear classifiers: a target labeling is stable when, with high
probability, a classifier trained on a random subsample of the target with that
labeling yields the same labeling. We find stable labelings using a random walk
on a directed graph with transition probabilities based on labeling stability.
The majority vote of those labelings visited by the walk yields a stable label
for each target example. The resulting domain adaptation algorithm is
strikingly easy to implement and apply: It does not rely on data
transformations, which are in general computational prohibitive in the presence
of many input features, and does not need to access the source data, which is
advantageous when data sharing is restricted. By acting on the original feature
space, our method is able to take full advantage of deep features from external
pre-trained neural networks, as demonstrated by the results of our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laarhoven_T/0/1/0/all/0/1&quot;&gt;Twan van Laarhoven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01401">
<title>Demystifying MMD GANs. (arXiv:1801.01401v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01401</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the training and performance of generative adversarial
networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs.
As our main theoretical contribution, we clarify the situation with bias in GAN
loss functions raised by recent work: we show that gradient estimators used in
the optimization process for both MMD GANs and Wasserstein GANs are unbiased,
but learning a discriminator based on samples leads to biased gradients for the
generator parameters. We also discuss the issue of kernel choice for the MMD
critic, and characterize the kernel corresponding to the energy distance used
for the Cramer GAN critic. Being an integral probability metric, the MMD
benefits from training strategies recently developed for Wasserstein GANs. In
experiments, the MMD GAN is able to employ a smaller critic network than the
Wasserstein GAN, resulting in a simpler and faster-training algorithm with
matching performance. We also propose an improved measure of GAN convergence,
the Kernel Inception Distance, and show how to use it to dynamically adapt
learning rates during GAN training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Binkowski_M/0/1/0/all/0/1&quot;&gt;Miko&amp;#x142;aj Bi&amp;#x144;kowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1&quot;&gt;Dougal J. Sutherland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1&quot;&gt;Michael Arbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03360">
<title>Information Planning for Text Data. (arXiv:1802.03360v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Information planning enables faster learning with fewer training examples. It
is particularly applicable when training examples are costly to obtain. This
work examines the advantages of information planning for text data by focusing
on three supervised models: Naive Bayes, supervised LDA and deep neural
networks. We show that planning based on entropy and mutual information
outperforms random selection baseline and therefore accelerates learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smolyakov_V/0/1/0/all/0/1&quot;&gt;Vadim Smolyakov&lt;/a&gt;</dc:creator>
</item></rdf:RDF>