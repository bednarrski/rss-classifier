<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01174"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01262"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10454"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01128"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01132"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.03581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00525"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.07824">
<title>Autotune: A Derivative-free Optimization Framework for Hyperparameter Tuning. (arXiv:1804.07824v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07824</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning applications often require hyperparameter tuning. The
hyperparameters usually drive both the efficiency of the model training process
and the resulting model quality. For hyperparameter tuning, machine learning
algorithms are complex black-boxes. This creates a class of challenging
optimization problems, whose objective functions tend to be nonsmooth,
discontinuous, unpredictably varying in computational expense, and include
continuous, categorical, and/or integer variables. Further, function
evaluations can fail for a variety of reasons including numerical difficulties
or hardware failures. Additionally, not all hyperparameter value combinations
are compatible, which creates so called hidden constraints. Robust and
efficient optimization algorithms are needed for hyperparameter tuning. In this
paper we present an automated parallel derivative-free optimization framework
called \textbf{Autotune}, which combines a number of specialized sampling and
search methods that are very effective in tuning machine learning models
despite these challenges. Autotune provides significantly improved models over
using default hyperparameter settings with minimal user interaction on
real-world applications. Given the inherent expense of training numerous
candidate models, we demonstrate the effectiveness of Autotune&apos;s search methods
and the efficient distributed and parallel paradigms for training and tuning
models, and also discuss the resource trade-offs associated with the ability to
both distribute the training process and parallelize the tuning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1&quot;&gt;Patrick Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golovidov_O/0/1/0/all/0/1&quot;&gt;Oleg Golovidov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_S/0/1/0/all/0/1&quot;&gt;Steven Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wujek_B/0/1/0/all/0/1&quot;&gt;Brett Wujek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffin_J/0/1/0/all/0/1&quot;&gt;Joshua Griffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01174">
<title>Generalization Error in Deep Learning. (arXiv:1808.01174v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01174</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have lately shown great performance in various fields
such as computer vision, speech recognition, speech translation, and natural
language processing. However, alongside their state-of-the-art performance, it
is still generally unclear what is the source of their generalization ability.
Thus, an important question is what makes deep neural networks able to
generalize well from the training set to new data. In this article, we provide
an overview of the existing theory and bounds for the characterization of the
generalization error of deep neural networks, combining both classical and more
recent theoretical and empirical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakubovitz_D/0/1/0/all/0/1&quot;&gt;Daniel Jakubovitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1&quot;&gt;Miguel R. D. Rodrigues&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01262">
<title>The Text-Based Adventure AI Competition. (arXiv:1808.01262v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.01262</link>
<description rdf:parseType="Literal">&lt;p&gt;In 2016 and 2017 at the IEEE Conference on Computational Intelligence in
Games, the authors of this paper ran a competition for agents that can play
classic text-based adventure games. This competition fills a gap in existing
game AI competitions that have typically focussed on traditional card/board
games or modern video games with graphical interfaces. By providing a platform
for evaluating agents in text-based adventures, the competition provides a
novel benchmark for game AI with unique challenges for natural language
understanding and generation. This paper summarises the two competitions ran in
2016 and 2017 (including details of open source implementations of both the
competition framework and our competitors) and presents the results of an
improved evaluation of these competitors across 20 games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkinson_T/0/1/0/all/0/1&quot;&gt;Timothy Atkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baier_H/0/1/0/all/0/1&quot;&gt;Hendrik Baier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Copplestone_T/0/1/0/all/0/1&quot;&gt;Tara Copplestone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1&quot;&gt;Sam Devlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swan_J/0/1/0/all/0/1&quot;&gt;Jerry Swan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03642">
<title>Adversarial Contrastive Estimation. (arXiv:1805.03642v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.03642</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning by contrasting positive and negative samples is a general strategy
adopted by many methods. Noise contrastive estimation (NCE) for word embeddings
and translating embeddings for knowledge graphs are examples in NLP employing
this approach. In this work, we view contrastive learning as an abstraction of
all such methods and augment the negative sampler into a mixture distribution
containing an adversarially learned sampler. The resulting adaptive sampler
finds harder negative examples, which forces the main model to learn a better
representation of the data. We evaluate our proposal on learning word
embeddings, order embeddings and knowledge graph embeddings and observe both
faster convergence and improved results on multiple metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1&quot;&gt;Avishek Joey Bose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yanshuai Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10454">
<title>From Adversarial Training to Generative Adversarial Networks. (arXiv:1807.10454v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10454</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we are interested in two seemingly different concepts:
\textit{adversarial training} and \textit{generative adversarial networks
(GANs)}. Particularly, how these techniques help to improve each other. To this
end, we analyze the limitation of adversarial training as the defense method,
starting from questioning how well the robustness of a model can generalize.
Then, we successfully improve the generalizability via data augmentation by the
&quot;fake&quot; images sampled from generative adversarial networks. After that, we are
surprised to see that the resulting robust classifier leads to a better
generator, for free. We intuitively explain this interesting phenomenon and
leave the theoretical analysis for future work. Motivated by these
observations, we propose a system that combines generator, discriminator, and
adversarial attacker in a single network. After end-to-end training and fine
tuning, our method can simultaneously improve the robustness of classifiers,
measured by accuracy under strong adversarial attacks; and the quality of
generators, evaluated both aesthetically and quantitatively. In terms of the
classifier, we achieve better robustness than the state-of-the-art adversarial
training algorithm proposed in (Madry etla., 2017), while our generator
achieves competitive performance compared with SN-GAN (Miyato and Koyama,
2018). Source code is publicly available online at
\url{https://github.com/anonymous}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuanqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00967">
<title>Active model learning and diverse action sampling for task and motion planning. (arXiv:1803.00967v1 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.00967</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of this work is to augment the basic abilities of a robot by
learning to use new sensorimotor primitives to enable the solution of complex
long-horizon problems. Solving long-horizon problems in complex domains
requires flexible generative planning that can combine primitive abilities in
novel combinations to solve problems as they arise in the world. In order to
plan to combine primitive actions, we must have models of the preconditions and
effects of those actions: under what circumstances will executing this
primitive achieve some particular effect in the world?
&lt;/p&gt;
&lt;p&gt;We use, and develop novel improvements on, state-of-the-art methods for
active learning and sampling. We use Gaussian process methods for learning the
conditions of operator effectiveness from small numbers of expensive training
examples collected by experimentation on a robot. We develop adaptive sampling
methods for generating diverse elements of continuous sets (such as robot
configurations and object poses) during planning for solving a new task, so
that planning is as efficient as possible. We demonstrate these methods in an
integrated system, combining newly learned models with an efficient
continuous-space robot task and motion planner to learn to solve long horizon
problems more efficiently than was previously possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrett_C/0/1/0/all/0/1&quot;&gt;Caelan Reed Garrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00961">
<title>Impacts of Weather Conditions on District Heat System. (arXiv:1808.00961v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Using artificial neural network for the prediction of heat demand has
attracted more and more attention. Weather conditions, such as ambient
temperature, wind speed and direct solar irradiance, have been identified as
key input parameters. In order to further improve the model accuracy, it is of
great importance to understand the influence of different parameters. Based on
an Elman neural network (ENN), this paper investigates the impact of direct
solar irradiance and wind speed on predicting the heat demand of a district
heating network. Results show that including wind speed can generally result in
a lower overall mean absolute percentage error (MAPE) (6.43%) than including
direct solar irradiance (6.47%); while including direct solar irradiance can
achieve a lower maximum absolute deviation (71.8%) than including wind speed
(81.53%). In addition, even though including both wind speed and direct solar
irradiance shows the best overall performance (MAPE=6.35%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jun Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01048">
<title>Variational Information Bottleneck on Vector Quantized Autoencoders. (arXiv:1808.01048v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01048</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide an information-theoretic interpretation of the
Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss
function of the original VQ-VAE can be derived from the variational
deterministic information bottleneck (VDIB) principle. On the other hand, the
VQ-VAE trained by the Expectation Maximization (EM) algorithm can be viewed as
an approximation to the variational information bottleneck(VIB) principle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hanwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flierl_M/0/1/0/all/0/1&quot;&gt;Markus Flierl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01128">
<title>PHI Scrubber: A Deep Learning Approach. (arXiv:1808.01128v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01128</link>
<description rdf:parseType="Literal">&lt;p&gt;Confidentiality of patient information is an essential part of Electronic
Health Record System. Patient information, if exposed, can cause a serious
damage to the privacy of individuals receiving healthcare. Hence it is
important to remove such details from physician notes. A system is proposed
which consists of a deep learning model where a de-convolutional neural network
and bi-directional LSTM-CNN is used along with regular expressions to recognize
and eliminate the individually identifiable information. This information is
then removed from a medical practitioner&apos;s data which further allows the fair
usage of such information among researchers and in clinical trials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilip_A/0/1/0/all/0/1&quot;&gt;Abhai Kollara Dilip&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1&quot;&gt;Kamal Raj K&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1&quot;&gt;Malaikannan Sankarasubbu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01132">
<title>Generalized Spectral Mixture Kernels for Multi-Task Gaussian Processes. (arXiv:1808.01132v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01132</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Task Gaussian processes (MTGPs) have shown a significant progress both
in expressiveness and interpretation of the relatedness between different
tasks: from linear combinations of independent single-output Gaussian processes
(GPs), through the direct modeling of the cross-covariances such as spectral
mixture kernels with phase shift, to the design of multivariate covariance
functions based on spectral mixture kernels which model delays among tasks in
addition to phase differences, and which provide a parametric interpretation of
the relatedness across tasks. In this paper we further extend expressiveness
and interpretability of MTGPs models and introduce a new family of kernels
capable to model nonlinear correlations between tasks as well as dependencies
between spectral mixtures, including time and phase delay. Specifically, we use
generalized convolution spectral mixture kernels for modeling dependencies at
spectral mixture level, and coupling coregionalization for discovering task
level correlations. The proposed kernels for MTGP are validated on artificial
data and compared with existing MTGPs methods on three real-world experiments.
Results indicate the benefits of our more expressive representation with
respect to performance and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_P/0/1/0/all/0/1&quot;&gt;Perry Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01204">
<title>Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data. (arXiv:1808.01204v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01204</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have many successful applications, while much less
theoretical understanding has been gained. Towards bridging this gap, we study
the problem of learning a two-layer overparameterized ReLU neural network for
multi-class classification via stochastic gradient descent (SGD) from random
initialization. In the overparameterized setting, when the data comes from
mixtures of well-separated distributions, we prove that SGD learns a network
with a small generalization error, albeit the network has enough capacity to
fit arbitrary labels. Furthermore, the analysis provides interesting insights
into several aspects of learning neural networks and can be verified based on
empirical studies on synthetic data and on the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.03581">
<title>Polya Urn Latent Dirichlet Allocation: a doubly sparse massively parallel sampler. (arXiv:1704.03581v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.03581</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Dirichlet Allocation (LDA) is a topic model widely used in natural
language processing and machine learning. Most approaches to training the model
rely on iterative algorithms, which makes it difficult to run LDA on big
corpora that are best analyzed in parallel and distributed computational
environments. Indeed, current approaches to parallel inference either don&apos;t
converge to the correct posterior or require storage of large dense matrices in
memory. We present a novel sampler that overcomes both problems, and we show
that this sampler is faster, both empirically and theoretically, than previous
Gibbs samplers for LDA. We do so by employing a novel P\&apos;olya-urn-based
approximation in the sparse partially collapsed sampler for LDA. We prove that
the approximation error vanishes with data size, making our algorithm
asymptotically exact, a property of importance for large-scale topic models. In
addition, we show, via an explicit example, that -- contrary to popular belief
in the topic modeling literature -- partially collapsed samplers can be more
efficient than fully collapsed samplers. We conclude by comparing the
performance of our algorithm with that of other approaches on well-known
corpora.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1&quot;&gt;Alexander Terenin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Magnusson_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;ns Magnusson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jonsson_L/0/1/0/all/0/1&quot;&gt;Leif Jonsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Draper_D/0/1/0/all/0/1&quot;&gt;David Draper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05271">
<title>Adaptive Federated Learning in Resource Constrained Edge Computing Systems. (arXiv:1804.05271v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05271</link>
<description rdf:parseType="Literal">&lt;p&gt;Emerging technologies and applications including Internet of Things (IoT),
social networking, and crowd-sourcing generate large amounts of data at the
network edge. Machine learning models are often built from the collected data,
to enable the detection, classification, and prediction of future events. Due
to bandwidth, storage, and privacy concerns, it is often impractical to send
all the data to a centralized location. In this paper, we consider the problem
of learning model parameters from data distributed across multiple edge nodes,
without sending raw data to a centralized place. Our focus is on a generic
class of machine learning models that are trained using gradient-descent based
approaches. We analyze the convergence bound of distributed gradient descent
from a theoretical point of view, based on which we propose a control algorithm
that determines the best trade-off between local update and global parameter
aggregation to minimize the loss function under a given resource budget. The
performance of the proposed algorithm is evaluated via extensive experiments
with real datasets, both on a networked prototype system and in a larger-scale
simulated environment. The experimentation results show that our proposed
approach performs near to the optimum with various machine learning models and
different data distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuor_T/0/1/0/all/0/1&quot;&gt;Tiffany Tuor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salonidis_T/0/1/0/all/0/1&quot;&gt;Theodoros Salonidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_K/0/1/0/all/0/1&quot;&gt;Kin K. Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makaya_C/0/1/0/all/0/1&quot;&gt;Christian Makaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Ting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1&quot;&gt;Kevin Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01918">
<title>A Framework for the construction of upper bounds on the number of affine linear regions of ReLU feed-forward neural networks. (arXiv:1806.01918v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01918</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we present a new framework to derive upper bounds on the number
regions of feed-forward neural nets with ReLU activation functions. We derive
all existing such bounds as special cases, however in a different
representation in terms of matrices. This provides new insight and allows a
more detailed analysis of the corresponding bounds. In particular, we provide a
Jordan-like decomposition for the involved matrices and present new tighter
results for an asymptotic setting. Moreover, new even stronger bounds may be
obtained from our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hinz_P/0/1/0/all/0/1&quot;&gt;Peter Hinz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Geer_S/0/1/0/all/0/1&quot;&gt;Sara van de Geer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00525">
<title>The impact of imbalanced training data on machine learning for author name disambiguation. (arXiv:1808.00525v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00525</link>
<description rdf:parseType="Literal">&lt;p&gt;In supervised machine learning for author name disambiguation, negative
training data are often dominantly larger than positive training data. This
paper examines how the ratios of negative to positive training data can affect
the performance of machine learning algorithms to disambiguate author names in
bibliographic records. On multiple labeled datasets, three classifiers -
Logistic Regression, Na\&quot;ive Bayes, and Random Forest - are trained through
representative features such as coauthor names, and title words extracted from
the same training data but with various positive-negative training data ratios.
Results show that increasing negative training data can improve disambiguation
performance but with a few percent of performance gains and sometimes degrade
it. Logistic Regression and Na\&quot;ive Bayes learn optimal disambiguation models
even with a base ratio (1:1) of positive and negative training data. Also, the
performance improvement by Random Forest tends to quickly saturate roughly
after 1:10 ~ 1:15. These findings imply that contrary to the common practice
using all training data, name disambiguation algorithms can be trained using
part of negative training data without degrading much disambiguation
performance while increasing computational efficiency. This study calls for
more attention from author name disambiguation scholars to methods for machine
learning from imbalanced data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jenna Kim&lt;/a&gt;</dc:creator>
</item></rdf:RDF>