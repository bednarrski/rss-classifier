<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01940"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08591"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11359"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01610"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01910"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01911"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02091"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02308"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02311"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.07450"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.08475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00094"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01798"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07039"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01661"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01933"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01973"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01993"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02169"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02190"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02248"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05428"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11921"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01477"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01660"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.01940">
<title>EIGEN: Ecologically-Inspired GENetic Approach for Neural Network Structure Searching. (arXiv:1806.01940v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.01940</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing the structure of neural networks is considered one of the most
challenging tasks in deep learning. Recently, a few approaches have been
proposed to automatically search for the optimal structure of neural networks,
however, they suffer from either prohibitive computation cost (e.g., 256 Hours
on 250 GPU in [1]) or unsatisfactory performance compared to those of
hand-crafted neural networks. In this paper, we propose an
Ecologically-Inspired GENetic approach for neural network structure search
(EIGEN), that includes succession, mimicry and gene duplication. Specifically,
we first use primary succession to rapidly evolve a community of poor
initialized neural network structures into a more diverse community, followed
by a secondary succession stage for fine-grained searching based on the
networks from the primary succession. Extinction is applied in both stages to
reduce computational cost. Mimicry is employed during the entire evolution
process to help the inferior networks imitate the behavior of a superior
network and gene duplication is utilized to duplicate the learned blocks of
novel structures, both of which help to find the better network structures.
Extensive experimental results show that our proposed approach can achieve the
similar or better performance compared to the existing genetic approaches with
dramatically reduced computation cost. For example, the network discovered by
our approach on CIFAR-100 dataset achieves 78.1% test accuracy under 120 GPU
hours, compared to 77.0% test accuracy in more than 65, 536 GPU hours in [1].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jian Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianchao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foran_D/0/1/0/all/0/1&quot;&gt;David J. Foran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02003">
<title>Deep Algorithms: designs for networks. (arXiv:1806.02003v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02003</link>
<description rdf:parseType="Literal">&lt;p&gt;A new design methodology for neural networks that is guided by traditional
algorithm design is presented. To prove our point, we present two heuristics
and demonstrate an algorithmic technique for incorporating additional weights
in their signal-flow graphs. We show that with training the performance of
these networks can not only exceed the performance of the initial network, but
can match the performance of more-traditional neural network architectures. A
key feature of our approach is that these networks are initialized with
parameters that provide a known performance threshold for the architecture on a
given task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopal_A/0/1/0/all/0/1&quot;&gt;Abhejit Rajagopal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1&quot;&gt;Shivkumar Chandrasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1&quot;&gt;Hrushikesh N. Mhaskar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08591">
<title>The Shattered Gradients Problem: If resnets are the answer, then what is the question?. (arXiv:1702.08591v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08591</link>
<description rdf:parseType="Literal">&lt;p&gt;A long-standing obstacle to progress in deep learning is the problem of
vanishing and exploding gradients. Although, the problem has largely been
overcome via carefully constructed initializations and batch normalization,
architectures incorporating skip-connections such as highway and resnets
perform much better than standard feedforward architectures despite well-chosen
initialization and batch normalization. In this paper, we identify the
shattered gradients problem. Specifically, we show that the correlation between
gradients in standard feedforward networks decays exponentially with depth
resulting in gradients that resemble white noise whereas, in contrast, the
gradients in architectures with skip-connections are far more resistant to
shattering, decaying sublinearly. Detailed empirical evidence is presented in
support of the analysis, on both fully-connected networks and convnets.
Finally, we present a new &quot;looks linear&quot; (LL) initialization that prevents
shattering, with preliminary experiments showing the new initialization allows
to train very deep networks without the addition of skip-connections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balduzzi_D/0/1/0/all/0/1&quot;&gt;David Balduzzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frean_M/0/1/0/all/0/1&quot;&gt;Marcus Frean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leary_L/0/1/0/all/0/1&quot;&gt;Lennox Leary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;JP Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kurt Wan-Duo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McWilliams_B/0/1/0/all/0/1&quot;&gt;Brian McWilliams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09431">
<title>On the Long-Term Memory of Deep Recurrent Networks. (arXiv:1710.09431v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09431</link>
<description rdf:parseType="Literal">&lt;p&gt;A key attribute that drives the unprecedented success of modern Recurrent
Neural Networks (RNNs) on learning tasks which involve sequential data, is
their ability to model intricate long-term temporal dependencies. However, a
well established measure of RNNs long-term memory capacity is lacking, and thus
formal understanding of the effect of depth on their ability to correlate data
throughout time is limited. Specifically, existing depth efficiency results on
convolutional networks do not suffice in order to account for the success of
deep RNNs on data of varying lengths. In order to address this, we introduce a
measure of the network&apos;s ability to support information flow across time,
referred to as the Start-End separation rank, which reflects the distance of
the function realized by the recurrent network from modeling no dependency
between the beginning and end of the input sequence. We prove that deep
recurrent networks support Start-End separation ranks which are combinatorially
higher than those supported by their shallow counterparts. Thus, we establish
that depth brings forth an overwhelming advantage in the ability of recurrent
networks to model long-term dependencies, and provide an exemplar of
quantifying this key attribute which may be readily extended to other RNN
architectures of interest, e.g. variants of LSTM networks. We obtain our
results by considering a class of recurrent networks referred to as Recurrent
Arithmetic Circuits, which merge the hidden state with the input via the
Multiplicative Integration operation, and empirically demonstrate the discussed
phenomena on common RNNs. Finally, we employ the tool of quantum Tensor
Networks to gain additional graphic insight regarding the complexity brought
forth by depth in recurrent networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1&quot;&gt;Yoav Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1&quot;&gt;Or Sharir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziv_A/0/1/0/all/0/1&quot;&gt;Alon Ziv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1&quot;&gt;Amnon Shashua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05642">
<title>The Mechanics of n-Player Differentiable Games. (arXiv:1802.05642v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05642</link>
<description rdf:parseType="Literal">&lt;p&gt;The cornerstone underpinning deep learning is the guarantee that gradient
descent on an objective converges to local minima. Unfortunately, this
guarantee fails in settings, such as generative adversarial nets, where there
are multiple interacting losses. The behavior of gradient-based methods in
games is not well understood -- and is becoming increasingly important as
adversarial and multi-objective architectures proliferate. In this paper, we
develop new techniques to understand and control the dynamics in general games.
The key result is to decompose the second-order dynamics into two components.
The first is related to potential games, which reduce to gradient descent on an
implicit function; the second relates to Hamiltonian games, a new class of
games that obey a conservation law, akin to conservation laws in classical
mechanical systems. The decomposition motivates Symplectic Gradient Adjustment
(SGA), a new algorithm for finding stable fixed points in general games. Basic
experiments show SGA is competitive with recently proposed algorithms for
finding stable fixed points in GANs -- whilst at the same time being applicable
to -- and having guarantees in -- much more general games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balduzzi_D/0/1/0/all/0/1&quot;&gt;David Balduzzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Racaniere_S/0/1/0/all/0/1&quot;&gt;Sebastien Racaniere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martens_J/0/1/0/all/0/1&quot;&gt;James Martens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Foerster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuyls_K/0/1/0/all/0/1&quot;&gt;Karl Tuyls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graepel_T/0/1/0/all/0/1&quot;&gt;Thore Graepel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09254">
<title>A theory of the phenomenology of multipopulation genetic algorithm with an application to the Ising model. (arXiv:1803.09254v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09254</link>
<description rdf:parseType="Literal">&lt;p&gt;Genetic algorithm (GA) is a stochastic metaheuristic process consisting on
the evolution of a population of candidate solutions for a given optimization
problem. By extension, multipopulation genetic algorithm (MPGA) aims for
efficiency by evolving many populations, or islands, in parallel and performing
migrations between them periodically. The connectivity between islands
constrains the directions of migration and characterizes MPGA as a dynamic
process over a network. As such, predicting the evolution of the quality of the
solutions is a difficult challenge, implying in the waste of computer resources
and energy when the parameters are inadequate. By using models derived from
statistical mechanics, this work aims to estimate equations for the study of
dynamics in relation to the connectivity in MPGA. To illustrate the importance
of understanding MPGA, we show its application as an efficient alternative to
the thermalization phase of Metropolis-Hastings algorithm applied to the Ising
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messias_B/0/1/0/all/0/1&quot;&gt;Bruno Messias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morais_B/0/1/0/all/0/1&quot;&gt;Bruno W. D. Morais&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11359">
<title>Properties of interaction networks, structure coefficients, and benefit-to-cost ratios. (arXiv:1805.11359v2 [q-bio.PE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11359</link>
<description rdf:parseType="Literal">&lt;p&gt;In structured populations the spatial arrangement of cooperators and
defectors on the interaction graph together with the structure of the graph
itself determines the game dynamics and particularly whether or not fixation of
cooperation (or defection) is favored. For a single cooperator (and a single
defector) and a network described by a regular graph the question of fixation
can be addressed by a single parameter, the structure coefficient. As this
quantity is generic for any regular graph, we may call it the generic structure
coefficient. For two and more cooperators (or several defectors) fixation
properties can also be assigned by structure coefficients. These structure
coefficients, however, depend on the arrangement of cooperators and defectors
which we may interpret as a configuration of the game. Moreover, the
coefficients are specific for a given interaction network modeled as regular
graph, which is why we may call them specific structure coefficients. In this
paper, we study how specific structure coefficients vary over interaction
graphs and link the distributions obtained over different graphs to spectral
properties of interaction networks. We also discuss implications for the
benefit-to-cost ratios of donation games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Richter_H/0/1/0/all/0/1&quot;&gt;Hendrik Richter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01610">
<title>Generative Reversible Networks. (arXiv:1806.01610v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01610</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models with an encoding component such as autoencoders currently
receive great interest. However, training of autoencoders is typically
complicated by the need to train a separate encoder and decoder model that have
to be enforced to be reciprocal to each other. Here, we propose to use the
by-design reversible neural networks (RevNets) as a new class of generative
models. We investigate the generative performance of RevNets on the CelebA
dataset, showing that generative RevNets can generate coherent faces with
similar quality as Variational Autoencoders. This first attempt to use RevNets
as a generative model slightly underperformed relative to recent advanced
generative models using an autoencoder component on CelebA, but this gap may
diminish with further optimization of the training setup of generative RevNets.
In addition to the experiments on CelebA, we show a proof-of-principle
experiment on the MNIST dataset suggesting that adversary-free trained RevNets
can discover meaningful latent dimensions without pre-specifying the number of
dimensions of the latent sampling distribution. In summary, this study shows
that RevNets enable generative applications with an encoding component while
overcoming the need to train a separate encoder and decoder model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrabaszcz_P/0/1/0/all/0/1&quot;&gt;Patryk Chrab&amp;#x105;szcz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01910">
<title>Probabilistic Deep Learning using Random Sum-Product Networks. (arXiv:1806.01910v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.01910</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic deep learning currently receives an increased interest, as
consistent treatment of uncertainty is one of the most important goals in
machine learning and AI. Most current approaches, however, have severe
limitations concerning inference. Sum-Product networks (SPNs), although having
excellent properties in that regard, have so far not been explored as serious
deep learning models, likely due to their special structural requirements. In
this paper, we make a drastic simplification and use a random structure which
is trained in a &quot;classical deep learning manner&quot; such as automatic
differentiation, SGD, and GPU support. The resulting models, called RAT-SPNs,
yield comparable prediction results to deep neural networks, but maintain
well-calibrated uncertainty estimates which makes them highly robust against
missing data. Furthermore, they successfully capture uncertainty over their
inputs in a convincing manner, yielding robust outlier and peculiarity
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peharz_R/0/1/0/all/0/1&quot;&gt;Robert Peharz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1&quot;&gt;Antonio Vergari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stelzner_K/0/1/0/all/0/1&quot;&gt;Karl Stelzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1&quot;&gt;Alejandro Molina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1&quot;&gt;Martin Trapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01911">
<title>Adversarial Scene Editing: Automatic Object Removal from Weak Supervision. (arXiv:1806.01911v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.01911</link>
<description rdf:parseType="Literal">&lt;p&gt;While great progress has been made recently in automatic image manipulation,
it has been limited to object centric images like faces or structured scene
datasets. In this work, we take a step towards general scene-level image
editing by developing an automatic interaction-free object removal model. Our
model learns to find and remove objects from general scene images using
image-level labels and unpaired data in a generative adversarial network (GAN)
framework. We achieve this with two key contributions: a two-stage editor
architecture consisting of a mask generator and image in-painter that
co-operate to remove objects, and a novel GAN based prior for the mask
generator that allows us to flexibly incorporate knowledge about object shapes.
We experimentally show on two datasets that our method effectively removes a
wide variety of objects using weak supervision only
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_R/0/1/0/all/0/1&quot;&gt;Rakshith Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01946">
<title>Learning to Follow Language Instructions with Adversarial Reward Induction. (arXiv:1806.01946v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.01946</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has shown that deep reinforcement-learning agents can learn to
follow language-like instructions from infrequent environment rewards. However,
for many real-world natural language commands that involve a degree of
underspecification or ambiguity, such as &quot;tidy the room&quot;, it would be
challenging or impossible to program an appropriate reward function. To
overcome this, we present a method for learning to follow commands from a
training set of instructions and corresponding example goal-states, rather than
an explicit reward function. Importantly, the example goal-states are not seen
at test time. The approach effectively separates the representation of what
instructions require from how they can be executed. In a simple grid world, the
method enables an agent to learn a range of commands requiring interaction with
blocks and understanding of spatial relations and underspecified abstract
arrangements. We further show the method allows our agent to adapt to changes
in the environment without requiring new training examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1&quot;&gt;Dzmitry Bahdanau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1&quot;&gt;Felix Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1&quot;&gt;Jan Leike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_E/0/1/0/all/0/1&quot;&gt;Edward Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1&quot;&gt;Edward Grefenstette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02091">
<title>Can Machines Design? An Artificial General Intelligence Approach. (arXiv:1806.02091v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.02091</link>
<description rdf:parseType="Literal">&lt;p&gt;Can machines design? Can they come up with creative solutions to problems and
build tools and artifacts across a wide range of domains? Recent advances in
the field of computational creativity and formal Artificial General
Intelligence (AGI) provide frameworks for machines with the general ability to
design. In this paper we propose to integrate a formal computational creativity
framework into the G\&quot;odel machine framework. We call this machine a design
G\&quot;odel machine. Such a machine could solve a variety of design problems by
generating novel concepts. In addition, it could change the way these concepts
are generated by modifying itself. The design G\&quot;odel machine is able to
improve its initial design program, once it has proven that a modification
would increase its return on the utility function. Finally, we sketch out a
specific version of the design G\&quot;odel machine which specifically aims at the
design of complex software and hardware systems. Future work could be the
development of a more formal version of the Design G\&quot;odel machine and a
potential implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_A/0/1/0/all/0/1&quot;&gt;Andreas Makoto Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Condat_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;l&amp;#xe8;ne Condat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02127">
<title>Addendum to &quot;HTN Acting: A Formalism and an Algorithm&quot;. (arXiv:1806.02127v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.02127</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical Task Network (HTN) planning is a practical and efficient
approach to planning when the &apos;standard operating procedures&apos; for a domain are
available. Like Belief-Desire-Intention (BDI) agent reasoning, HTN planning
performs hierarchical and context-based refinement of goals into subgoals and
basic actions. However, while HTN planners &apos;lookahead&apos; over the consequences of
choosing one refinement over another, BDI agents interleave refinement with
acting. There has been renewed interest in making HTN planners behave more like
BDI agent systems, e.g. to have a unified representation for acting and
planning. However, past work on the subject has remained informal or
implementation-focused. This paper is a formal account of &apos;HTN acting&apos;, which
supports interleaved deliberation, acting, and failure recovery. We use the
syntax of the most general HTN planning formalism and build on its core
semantics, and we provide an algorithm which combines our new formalism with
the processing of exogenous events. We also study the properties of HTN acting
and its relation to HTN planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_L/0/1/0/all/0/1&quot;&gt;Lavindra de Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02180">
<title>Addressing Two Problems in Deep Knowledge Tracing via Prediction-Consistent Regularization. (arXiv:1806.02180v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.02180</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge tracing is one of the key research areas for empowering
personalized education. It is a task to model students&apos; mastery level of a
knowledge component (KC) based on their historical learning trajectories. In
recent years, a recurrent neural network model called deep knowledge tracing
(DKT) has been proposed to handle the knowledge tracing task and literature has
shown that DKT generally outperforms traditional methods. However, through our
extensive experimentation, we have noticed two major problems in the DKT model.
The first problem is that the model fails to reconstruct the observed input. As
a result, even when a student performs well on a KC, the prediction of that
KC&apos;s mastery level decreases instead, and vice versa. Second, the predicted
performance for KCs across time-steps is not consistent. This is undesirable
and unreasonable because student&apos;s performance is expected to transit gradually
over time. To address these problems, we introduce regularization terms that
correspond to reconstruction and waviness to the loss function of the original
DKT model to enhance the consistency in prediction. Experiments show that the
regularized loss function effectively alleviates the two problems without
degrading the original task of DKT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_C/0/1/0/all/0/1&quot;&gt;Chun-Kit Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02215">
<title>Spectral Inference Networks: Unifying Spectral Methods With Deep Learning. (arXiv:1806.02215v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02215</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Spectral Inference Networks, a framework for learning
eigenfunctions of linear operators by stochastic optimization. Spectral
Inference Networks generalize Slow Feature Analysis to generic symmetric
operators, and are closely related to Variational Monte Carlo methods from
computational physics. As such, they can be a powerful tool for unsupervised
representation learning from video or pairs of data. We derive a training
algorithm for Spectral Inference Networks that addresses the bias in the
gradients due to finite batch size and allows for online learning of multiple
eigenfunctions. We show results of training Spectral Inference Networks on
problems in quantum mechanics and feature learning for videos on synthetic
datasets as well as the Arcade Learning Environment. Our results demonstrate
that Spectral Inference Networks accurately recover eigenfunctions of linear
operators, can discover interpretable representations from video and find
meaningful subgoals in reinforcement learning environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfau_D/0/1/0/all/0/1&quot;&gt;David Pfau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_S/0/1/0/all/0/1&quot;&gt;Stig Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Ashish Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_D/0/1/0/all/0/1&quot;&gt;David Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stachenfeld_K/0/1/0/all/0/1&quot;&gt;Kim Stachenfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02308">
<title>Model-free, Model-based, and General Intelligence. (arXiv:1806.02308v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.02308</link>
<description rdf:parseType="Literal">&lt;p&gt;During the 60s and 70s, AI researchers explored intuitions about intelligence
by writing programs that displayed intelligent behavior. Many good ideas came
out from this work but programs written by hand were not robust or general.
After the 80s, research increasingly shifted to the development of learners
capable of inferring behavior and functions from experience and data, and
solvers capable of tackling well-defined but intractable models like SAT,
classical planning, Bayesian networks, and POMDPs. The learning approach has
achieved considerable success but results in black boxes that do not have the
flexibility, transparency, and generality of their model-based counterparts.
Model-based approaches, on the other hand, require models and scalable
algorithms. Model-free learners and model-based solvers have close parallels
with Systems 1 and 2 in current theories of the human mind: the first, a fast,
opaque, and inflexible intuitive mind; the second, a slow, transparent, and
flexible analytical mind. In this paper, I review developments in AI and draw
on these theories to discuss the gap between model-free learners and
model-based solvers, a gap that needs to be bridged in order to have
intelligent systems that are robust and general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geffner_H/0/1/0/all/0/1&quot;&gt;Hector Geffner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02311">
<title>Unsupervised Attention-guided Image to Image Translation. (arXiv:1806.02311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.02311</link>
<description rdf:parseType="Literal">&lt;p&gt;Current unsupervised image-to-image translation techniques struggle to focus
their attention on individual objects without altering the background or the
way multiple objects interact within a scene. Motivated by the important role
of attention in human perception, we tackle this limitation by introducing
unsupervised attention mechanisms that are jointly adversarialy trained with
the generators and discriminators. We demonstrate qualitatively and
quantitatively that our approach is able to attend to relevant regions in the
image without requiring supervision, and that by doing so it achieves more
realistic mappings compared to recent approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejjati_Y/0/1/0/all/0/1&quot;&gt;Youssef A. Mejjati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1&quot;&gt;Christian Richardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1&quot;&gt;James Tompkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1&quot;&gt;Darren Cosker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang In Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02322">
<title>Learning Kolmogorov Models for Binary Random Variables. (arXiv:1806.02322v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02322</link>
<description rdf:parseType="Literal">&lt;p&gt;We summarize our recent findings, where we proposed a framework for learning
a Kolmogorov model, for a collection of binary random variables. More
specifically, we derive conditions that link outcomes of specific random
variables, and extract valuable relations from the data. We also propose an
algorithm for computing the model and show its first-order optimality, despite
the combinatorial nature of the learning problem. We apply the proposed
algorithm to recommendation systems, although it is applicable to other
scenarios. We believe that the work is a significant step toward interpretable
machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1&quot;&gt;Hadi Ghauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skoglund_M/0/1/0/all/0/1&quot;&gt;Mikael Skoglund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shokri_Ghadikolaei_H/0/1/0/all/0/1&quot;&gt;Hossein Shokri-Ghadikolaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischione_C/0/1/0/all/0/1&quot;&gt;Carlo Fischione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_A/0/1/0/all/0/1&quot;&gt;Ali H. Sayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.07450">
<title>Strongly-Typed Agents are Guaranteed to Interact Safely. (arXiv:1702.07450v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.07450</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial agents proliferate, it is becoming increasingly important to
ensure that their interactions with one another are well-behaved. In this
paper, we formalize a common-sense notion of when algorithms are well-behaved:
an algorithm is safe if it does no harm. Motivated by recent progress in deep
learning, we focus on the specific case where agents update their actions
according to gradient descent. The paper shows that that gradient descent
converges to a Nash equilibrium in safe games. The main contribution is to
define strongly-typed agents and show they are guaranteed to interact safely,
thereby providing sufficient conditions to guarantee safe interactions. A
series of examples show that strong-typing generalizes certain key features of
convexity, is closely related to blind source separation, and introduces a new
perspective on classical multilinear games based on tensor decomposition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balduzzi_D/0/1/0/all/0/1&quot;&gt;David Balduzzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.08475">
<title>DARLA: Improving Zero-Shot Transfer in Reinforcement Learning. (arXiv:1707.08475v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.08475</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation is an important open problem in deep reinforcement learning
(RL). In many scenarios of interest data is hard to obtain, so agents may learn
a source policy in a setting where data is readily available, with the hope
that it generalises well to the target domain. We propose a new multi-stage RL
agent, DARLA (DisentAngled Representation Learning Agent), which learns to see
before learning to act. DARLA&apos;s vision is based on learning a disentangled
representation of the observed environment. Once DARLA can see, it is able to
acquire source policies that are robust to many domain shifts - even with no
access to the target domain. DARLA significantly outperforms conventional
baselines in zero-shot domain adaptation scenarios, an effect that holds across
a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms
(DQN, A3C and EC).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Higgins_I/0/1/0/all/0/1&quot;&gt;Irina Higgins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Arka Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rusu_A/0/1/0/all/0/1&quot;&gt;Andrei A. Rusu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matthey_L/0/1/0/all/0/1&quot;&gt;Loic Matthey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burgess_C/0/1/0/all/0/1&quot;&gt;Christopher P Burgess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pritzel_A/0/1/0/all/0/1&quot;&gt;Alexander Pritzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botvinick_M/0/1/0/all/0/1&quot;&gt;Matthew Botvinick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lerchner_A/0/1/0/all/0/1&quot;&gt;Alexander Lerchner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10928">
<title>Optimization Landscape and Expressivity of Deep CNNs. (arXiv:1710.10928v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10928</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the loss landscape and expressiveness of practical deep
convolutional neural networks (CNNs) with shared weights and max pooling
layers. We show that such CNNs produce linearly independent features at a
&quot;wide&quot; layer which has more neurons than the number of training samples. This
condition holds e.g. for the VGG network. Furthermore, we provide for such wide
CNNs necessary and sufficient conditions for global minima with zero training
error. For the case where the wide layer is followed by a fully connected layer
we show that almost every critical point of the empirical loss is a global
minimum with zero training error. Our analysis suggests that both depth and
width are very important in deep learning. While depth brings more
representational power and allows the network to learn high level features,
width smoothes the optimization landscape of the loss function in the sense
that a sufficiently wide network has a well-behaved loss surface with almost no
bad local minima.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quynh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00094">
<title>Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions. (arXiv:1803.00094v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00094</link>
<description rdf:parseType="Literal">&lt;p&gt;In the recent literature the important role of depth in deep learning has
been emphasized. In this paper we argue that sufficient width of a feedforward
network is equally important by answering the simple question under which
conditions the decision regions of a neural network are connected. It turns out
that for a class of activation functions including leaky ReLU, neural networks
having a pyramidal structure, that is no layer has more hidden units than the
input dimension, produce necessarily connected decision regions. This implies
that a sufficiently wide hidden layer is necessary to guarantee that the
network can produce disconnected decision regions. We discuss the implications
of this result for the construction of neural networks, in particular the
relation to the problem of adversarial manipulation of classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quynh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukkamala_M/0/1/0/all/0/1&quot;&gt;Mahesh Chandra Mukkamala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01798">
<title>One-Class Adversarial Nets for Fraud Detection. (arXiv:1803.01798v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01798</link>
<description rdf:parseType="Literal">&lt;p&gt;Many online applications, such as online social networks or knowledge bases,
are often attacked by malicious users who commit different types of actions
such as vandalism on Wikipedia or fraudulent reviews on eBay. Currently, most
of the fraud detection approaches require a training dataset that contains
records of both benign and malicious users. However, in practice, there are
often no or very few records of malicious users. In this paper, we develop
one-class adversarial nets (OCAN) for fraud detection using training data with
only benign users. OCAN first uses LSTM-Autoencoder to learn the
representations of benign users from their sequences of online activities. It
then detects malicious users by training a discriminator with a complementary
GAN model that is different from the regular GAN model. Experimental results
show that our OCAN outperforms the state-of-the-art one-class classification
models and achieves comparable performance with the latest multi-source LSTM
model that requires both benign and malicious users in the training phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1&quot;&gt;Panpan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Shuhan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1&quot;&gt;Aidong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07039">
<title>A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations. (arXiv:1805.07039v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07039</link>
<description rdf:parseType="Literal">&lt;p&gt;Backpropagation-based visualizations have been proposed to interpret
convolutional neural networks (CNNs), however a theory is missing to justify
their behaviors: Guided backpropagation(GBP) and deconvolutional network
(DeconvNet) generate more human-interpretable but less class-sensitive
visualizations than saliency map. Motivated by this, we develop a theoretical
explanation revealing that GBP and DeconvNet are essentially doing (partial)
image recovery and thus are unrelated to the network decisions. Specifically,
our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and
the local connections in CNNs are the two main causes of compelling
visualizations. Extensive experiments are provided that support the theoretical
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1&quot;&gt;Weili Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Ankit Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01661">
<title>Experimental Tests of Spirituality. (arXiv:1806.01661v1 [physics.pop-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.01661</link>
<description rdf:parseType="Literal">&lt;p&gt;We currently harness technologies that could shed new light on old
philosophical questions, such as whether our mind entails anything beyond our
body or whether our moral values reflect universal truth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Loeb_A/0/1/0/all/0/1&quot;&gt;Abraham Loeb&lt;/a&gt; (Harvard)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01845">
<title>Deep Neural Networks with Multi-Branch Architectures Are Less Non-Convex. (arXiv:1806.01845v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.01845</link>
<description rdf:parseType="Literal">&lt;p&gt;Several recently proposed architectures of neural networks such as ResNeXt,
Inception, Xception, SqueezeNet and Wide ResNet are based on the designing idea
of having multiple branches and have demonstrated improved performance in many
applications. We show that one cause for such success is due to the fact that
the multi-branch architecture is less non-convex in terms of duality gap. The
duality gap measures the degree of intrinsic non-convexity of an optimization
problem: smaller gap in relative value implies lower degree of intrinsic
non-convexity. The challenge is to quantitatively measure the duality gap of
highly non-convex problems such as deep neural networks. In this work, we
provide strong guarantees of this quantity for two classes of network
architectures. For the neural networks with arbitrary activation functions,
multi-branch architecture and a variant of hinge loss, we show that the duality
gap of both population and empirical risks shrinks to zero as the number of
branches increases. This result sheds light on better understanding the power
of over-parametrization where increasing the network width tends to make the
loss surface less non-convex. For the neural networks with linear activation
function and $\ell_2$ loss, we show that the duality gap of empirical risk is
zero. Our two results work for arbitrary depths and adversarial data, while the
analytical techniques might be of independent interest to non-convex
optimization more broadly. Experiments on both synthetic and real-world
datasets validate our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Junru Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01875">
<title>EEG-GAN: Generative adversarial networks for electroencephalograhic (EEG) brain signals. (arXiv:1806.01875v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1806.01875</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) are recently highly successful in
generative applications involving images and start being applied to time series
data. Here we describe EEG-GAN as a framework to generate
electroencephalographic (EEG) brain signals. We introduce a modification to the
improved training of Wasserstein GANs to stabilize training and investigate a
range of architectural choices critical for time series generation (most
notably up- and down-sampling). For evaluation we consider and compare
different metrics such as Inception score, Frechet inception distance and
sliced Wasserstein distance, together showing that our EEG-GAN framework
generated naturalistic EEG examples. It thus opens up a range of new generative
application scenarios in the neuroscientific and neurological context, such as
data augmentation in brain-computer interfacing tasks, EEG super-sampling, or
restoration of corrupted data segments. The possibility to generate signals of
a certain class and/or with specific properties may also open a new avenue for
research into the underlying structure of brain signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hartmann_K/0/1/0/all/0/1&quot;&gt;Kay Gregor Hartmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01918">
<title>A Framework for the construction of upper bounds on the number of affine linear regions of ReLU feed-forward neural networks. (arXiv:1806.01918v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.01918</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we present a new framework to derive upper bounds on the number
regions of feed-forward neural nets with ReLU activation functions. We derive
all existing such bounds as special cases, however in a different
representation in terms of matrices. This provides new insight and allows a
more detailed analysis of the corresponding bounds. In particular, we provide a
Jordan-like decomposition for the involved matrices and present new tighter
results for an asymptotic setting. Moreover, new even stronger bounds may be
obtained from our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hinz_P/0/1/0/all/0/1&quot;&gt;Peter Hinz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Geer_S/0/1/0/all/0/1&quot;&gt;Sara van de Geer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01933">
<title>Explainable Neural Networks based on Additive Index Models. (arXiv:1806.01933v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.01933</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning algorithms are increasingly being used in recent years due
to their flexibility in model fitting and increased predictive performance.
However, the complexity of the models makes them hard for the data analyst to
interpret the results and explain them without additional tools. This has led
to much research in developing various approaches to understand the model
behavior. In this paper, we present the Explainable Neural Network (xNN), a
structured neural network designed especially to learn interpretable features.
Unlike fully connected neural networks, the features engineered by the xNN can
be extracted from the network in a relatively straightforward manner and the
results displayed. With appropriate regularization, the xNN provides a
parsimonious explanation of the relationship between the features and the
output. We illustrate this interpretable feature--engineering property on
simulated examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vaughan_J/0/1/0/all/0/1&quot;&gt;Joel Vaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sudjianto_A/0/1/0/all/0/1&quot;&gt;Agus Sudjianto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brahimi_E/0/1/0/all/0/1&quot;&gt;Erind Brahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nair_V/0/1/0/all/0/1&quot;&gt;Vijayan N. Nair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01969">
<title>Reverse iterative volume sampling for linear regression. (arXiv:1806.01969v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.01969</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the following basic machine learning task: Given a fixed set of
$d$-dimensional input points for a linear regression problem, we wish to
predict a hidden response value for each of the points. We can only afford to
attain the responses for a small subset of the points that are then used to
construct linear predictions for all points in the dataset. The performance of
the predictions is evaluated by the total square loss on all responses (the
attained as well as the hidden ones). We show that a good approximate solution
to this least squares problem can be obtained from just dimension $d$ many
responses by using a joint sampling technique called volume sampling. Moreover,
the least squares solution obtained for the volume sampled subproblem is an
unbiased estimator of optimal solution based on all n responses. This
unbiasedness is a desirable property that is not shared by other common subset
selection techniques.
&lt;/p&gt;
&lt;p&gt;Motivated by these basic properties, we develop a theoretical framework for
studying volume sampling, resulting in a number of new matrix expectation
equalities and statistical guarantees which are of importance not only to least
squares regression but also to numerical linear algebra in general. Our methods
also lead to a regularized variant of volume sampling, and we propose the first
efficient algorithms for volume sampling which make this technique a practical
tool in the machine learning toolbox. Finally, we provide experimental evidence
which confirms our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warmuth_M/0/1/0/all/0/1&quot;&gt;Manfred K. Warmuth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01973">
<title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems. (arXiv:1806.01973v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1806.01973</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in deep neural networks for graph-structured data have
led to state-of-the-art performance on recommender system benchmarks. However,
making these methods practical and scalable to web-scale recommendation tasks
with billions of items and hundreds of millions of users remains a challenge.
Here we describe a large-scale deep recommendation engine that we developed and
deployed at Pinterest. We develop a data-efficient Graph Convolutional Network
(GCN) algorithm PinSage, which combines efficient random walks and graph
convolutions to generate embeddings of nodes (i.e., items) that incorporate
both graph structure as well as node feature information. Compared to prior GCN
approaches, we develop a novel method based on highly efficient random walks to
structure the convolutions and design a novel training strategy that relies on
harder-and-harder training examples to improve robustness and convergence of
the model. We also develop an efficient MapReduce model inference algorithm to
generate embeddings using a trained model. We deploy PinSage at Pinterest and
train it on 7.5 billion examples on a graph with 3 billion nodes representing
pins and boards, and 18 billion edges. According to offline metrics, user
studies and A/B tests, PinSage generates higher-quality recommendations than
comparable deep learning and graph-based alternatives. To our knowledge, this
is the largest application of deep graph embeddings to date and paves the way
for a new generation of web-scale recommender systems based on graph
convolutional architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1&quot;&gt;Rex Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruining He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eksombatchai_P/0/1/0/all/0/1&quot;&gt;Pong Eksombatchai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1&quot;&gt;William L. Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01993">
<title>Semiparametric Classification of Forest Graphical Models. (arXiv:1806.01993v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.01993</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new semiparametric approach to binary classification that
exploits the modeling flexibility of sparse graphical models. Specifically, we
assume that each class can be represented by a forest-structured graphical
model. Under this assumption, the optimal classifier is linear in the log of
the one- and two-dimensional marginal densities. Our proposed procedure
non-parametrically estimates the univariate and bivariate marginal densities,
maps each sample to the logarithm of these estimated densities and constructs a
linear SVM in the transformed space. We prove convergence of the resulting
classifier to an oracle SVM classifier and give finite sample bounds on its
excess risk. Experiments with simulated and real data indicate that the
resulting classifier is competitive with several popular methods across a range
of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorn_M/0/1/0/all/0/1&quot;&gt;Mary Frances Dorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moscovich_A/0/1/0/all/0/1&quot;&gt;Amit Moscovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nadler_B/0/1/0/all/0/1&quot;&gt;Boaz Nadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spiegelman_C/0/1/0/all/0/1&quot;&gt;Clifford Spiegelman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02012">
<title>A Peek Into the Hidden Layers of a Convolutional Neural Network Through a Factorization Lens. (arXiv:1806.02012v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02012</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their increasing popularity and success in a variety of supervised
learning problems, deep neural networks are extremely hard to interpret and
debug: Given and already trained Deep Neural Net, and a set of test inputs, how
can we gain insight into how those inputs interact with different layers of the
neural network? Furthermore, can we characterize a given deep neural network
based on it&apos;s observed behavior on different inputs? In this paper we propose a
novel factorization based approach on understanding how different deep neural
networks operate. In our preliminary results, we identify fascinating patterns
that link the factorization rank (typically used as a measure of
interestingness in unsupervised data analysis) with how well or poorly the deep
network has been trained. Finally, our proposed approach can help provide
visual insights on how high-level. interpretable patterns of the network&apos;s
input behave inside the hidden layers of the deep network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_U/0/1/0/all/0/1&quot;&gt;Uday Singh Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02078">
<title>Convolutional Sequence to Sequence Non-intrusive Load Monitoring. (arXiv:1806.02078v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02078</link>
<description rdf:parseType="Literal">&lt;p&gt;A convolutional sequence to sequence non-intrusive load monitoring model is
proposed in this paper. Gated linear unit convolutional layers are used to
extract information from the sequences of aggregate electricity consumption.
Residual blocks are also introduced to refine the output of the neural network.
The partially overlapped output sequences of the network are averaged to
produce the final output of the model. We apply the proposed model to the REDD
dataset and compare it with the convolutional sequence to point model in the
literature. Results show that the proposed model is able to give satisfactory
disaggregation performance for appliances with varied characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kunjin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Ziyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kunlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jinliang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02169">
<title>StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks. (arXiv:1806.02169v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.02169</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a method that allows for non-parallel many-to-many voice
conversion (VC) by using a variant of generative adversarial networks (GANs)
called StarGAN. Our method, which we term StarGAN-VC, is remarkable in that it
(1) requires neither parallel utterances, transcriptions, nor time alignment
procedures for speech generator training, (2) simultaneously learns
many-to-many mappings across different attribute domains using a single
generator network, (3) is able to generate signals of converted speech quickly
enough to allow for real-time implementations and (4) requires only several
minutes of training examples to generate reasonably realistic-sounding speech.
Subjective evaluation experiments on a non-parallel many-to-many speaker
identity conversion task revealed that the proposed method obtained higher
sound quality and speaker similarity than a state-of-the-art method based on
variational autoencoding GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1&quot;&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kou Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hojo_N/0/1/0/all/0/1&quot;&gt;Nobukatsu Hojo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02190">
<title>Mitigation of Policy Manipulation Attacks on Deep Q-Networks with Parameter-Space Noise. (arXiv:1806.02190v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02190</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments have established the vulnerability of deep reinforcement
learning to policy manipulation attacks via intentionally perturbed inputs,
known as adversarial examples. In this work, we propose a technique for
mitigation of such attacks based on addition of noise to the parameter space of
deep reinforcement learners during training. We experimentally verify the
effect of parameter-space noise in reducing the transferability of adversarial
examples, and demonstrate the promising performance of this technique in
mitigating the impact of whitebox and blackbox attacks at both test and
training times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behzadan_V/0/1/0/all/0/1&quot;&gt;Vahid Behzadan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munir_A/0/1/0/all/0/1&quot;&gt;Arslan Munir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02193">
<title>GraKeL: A Graph Kernel Library in Python. (arXiv:1806.02193v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02193</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of accurately measuring the similarity between graphs is at the
core of many applications in a variety of disciplines. Graph kernels have
recently emerged as a promising approach to this problem. There are now many
kernels, each focusing on different structural aspects of graphs. Here, we
present GraKeL, a library that unifies several graph kernels into a common
framework. The library is written in Python and is build on top of
scikit-learn. It is simple to use and can be naturally combined with
scikit-learn&apos;s modules to build a complete machine learning pipeline for tasks
such as graph classification and clustering. The code is BSD licensed and is
available at: https://github.com/ysig/GraKeL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siglidis_G/0/1/0/all/0/1&quot;&gt;Giannis Siglidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nikolentzos_G/0/1/0/all/0/1&quot;&gt;Giannis Nikolentzos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Limnios_S/0/1/0/all/0/1&quot;&gt;Stratis Limnios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giatsidis_C/0/1/0/all/0/1&quot;&gt;Christos Giatsidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Skianis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Skianis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vazirgianis_M/0/1/0/all/0/1&quot;&gt;Michalis Vazirgianis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02248">
<title>TopRank: A practical algorithm for online stochastic ranking. (arXiv:1806.02248v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02248</link>
<description rdf:parseType="Literal">&lt;p&gt;Online learning to rank is a sequential decision-making problem where in each
round the learning agent chooses a list of items and receives feedback in the
form of clicks from the user. Many sample-efficient algorithms have been
proposed for this problem that assume a specific click model connecting
rankings and user behavior. We propose a generalized click model that
encompasses many existing models, including the position-based and cascade
models. Our generalization motivates a novel online learning algorithm based on
topological sort, which we call TopRank. TopRank is (a) more natural than
existing algorithms, (b) has stronger regret guarantees than existing
algorithms with comparable generality, (c) has a more insightful proof that
leaves the door open to many generalizations, (d) outperforms existing
algorithms empirically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1&quot;&gt;Tor Lattimore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesvari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02326">
<title>Conditional Linear Regression. (arXiv:1806.02326v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02326</link>
<description rdf:parseType="Literal">&lt;p&gt;Work in machine learning and statistics commonly focuses on building models
that capture the vast majority of data, possibly ignoring a segment of the
population as outliers. However, there does not often exist a good model on the
whole dataset, so we seek to find a small subset where there exists a useful
model. We are interested in finding a linear rule capable of achieving more
accurate predictions for just a segment of the population. We give an efficient
algorithm with theoretical analysis for the conditional linear regression task,
which is the joint task of identifying a significant segment of the population,
described by a k-DNF, along with its linear regression fit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderon_D/0/1/0/all/0/1&quot;&gt;Diego Calderon&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1&quot;&gt;Brendan Juba&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sirui Li&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongyi Li&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_L/0/1/0/all/0/1&quot;&gt;Lisa Ruan&lt;/a&gt; (3) ((1) University of Arkansas, (2) Washington University in St. Louis, (3) MIT)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02329">
<title>Mitigating Bias in Adaptive Data Gathering via Differential Privacy. (arXiv:1806.02329v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;Data that is gathered adaptively --- via bandit algorithms, for example ---
exhibits bias. This is true both when gathering simple numeric valued data ---
the empirical means kept track of by stochastic bandit algorithms are biased
downwards --- and when gathering more complicated data --- running hypothesis
tests on complex data gathered via contextual bandit algorithms leads to false
discovery. In this paper, we show that this problem is mitigated if the data
collection procedure is differentially private. This lets us both bound the
bias of simple numeric valued quantities (like the empirical means of
stochastic bandit algorithms), and correct the p-values of hypothesis tests run
on the adaptively gathered data. Moreover, there exist differentially private
bandit algorithms with near optimal regret bounds: we apply existing theorems
in the simple stochastic case, and give a new analysis for linear contextual
bandits. We complement our theoretical results with experiments validating our
theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1&quot;&gt;Seth Neel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1&quot;&gt;Aaron Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03744">
<title>Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?. (arXiv:1801.03744v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03744</link>
<description rdf:parseType="Literal">&lt;p&gt;We give a rigorous analysis of the statistical behavior of gradients in a
randomly initialized fully connected network N with ReLU activations. Our
results show that the empirical variance of the squares of the entries in the
input-output Jacobian of N is exponential in the sum of the reciprocals of the
hidden layer widths. Our approach complements the mean field theory analysis of
random neural nets. From this point of view, we rigorously compute the finite
width corrections to gradients at the edge of chaos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05428">
<title>A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music. (arXiv:1803.05428v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05428</link>
<description rdf:parseType="Literal">&lt;p&gt;The Variational Autoencoder (VAE) has proven to be an effective model for
producing semantically meaningful latent representations for natural data.
However, it has thus far seen limited application to sequential data, and, as
we demonstrate, existing recurrent VAE models have difficulty modeling
sequences with long-term structure. To address this issue, we propose the use
of a hierarchical decoder, which first outputs embeddings for subsequences of
the input and then uses these embeddings to generate each subsequence
independently. This structure encourages the model to utilize its latent code,
thereby avoiding the &quot;posterior collapse&quot; problem which remains an issue for
recurrent VAEs. We apply this architecture to modeling sequences of musical
notes and find that it exhibits dramatically better sampling, interpolation,
and reconstruction performance than a &quot;flat&quot; baseline model. An implementation
of our &quot;MusicVAE&quot; is available online at &lt;a href=&quot;http://g.co/magenta/musicvae-code.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1&quot;&gt;Adam Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1&quot;&gt;Jesse Engel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawthorne_C/0/1/0/all/0/1&quot;&gt;Curtis Hawthorne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1&quot;&gt;Douglas Eck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01553">
<title>A Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation. (arXiv:1805.01553v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01553</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach to interactive-predictive neural machine translation
that attempts to reduce human effort from three directions: Firstly, instead of
requiring humans to select, correct, or delete segments, we employ the idea of
learning from human reinforcements in form of judgments on the quality of
partial translations. Secondly, human effort is further reduced by using the
entropy of word predictions as uncertainty criterion to trigger feedback
requests. Lastly, online updates of the model parameters after every
interaction allow the model to adapt quickly. We show in simulation experiments
that reward signals on partial translations significantly improve character
F-score and BLEU compared to feedback on full translations only, while human
effort can be reduced to an average number of $5$ feedback requests for every
input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1&quot;&gt;Tsz Kin Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1&quot;&gt;Julia Kreutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1&quot;&gt;Stefan Riezler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09298">
<title>Learning towards Minimum Hyperspherical Energy. (arXiv:1805.09298v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09298</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are a powerful class of nonlinear functions that can be
trained end-to-end on various applications. While the over-parametrization
nature in many neural networks renders the ability to fit complex functions and
the strong representation power to handle challenging tasks, it also leads to
highly correlated neurons that can hurt the generalization ability and incur
unnecessary computation cost. As a result, how to regularize the network to
avoid undesired representation redundancy becomes an important issue. To this
end, we draw inspiration from a well-known problem in physics -- Thomson
problem, where one seeks to find a state that distributes N electrons on a unit
sphere as even as possible with minimum potential energy. In light of this
intuition, we reduce the redundancy regularization problem to generic energy
minimization, and propose a minimum hyperspherical energy (MHE) objective as
generic regularization for neural networks. We also propose a few novel
variants of MHE, and provide some insights from a theoretical point of view.
Finally, we apply networks with MHE regularization to several challenging
tasks. Extensive experiments demonstrate the effectiveness of our method, by
showing the superior performance with MHE regularization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1&quot;&gt;Rongmei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11921">
<title>Anonymous Walk Embeddings. (arXiv:1805.11921v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11921</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of representing entire graphs has seen a surge of prominent results,
mainly due to learning convolutional neural networks (CNNs) on graph-structured
data. While CNNs demonstrate state-of-the-art performance in graph
classification task, such methods are supervised and therefore steer away from
the original problem of network representation in task-agnostic manner. Here,
we coherently propose an approach for embedding entire graphs and show that our
feature representations with SVM classifier increase classification accuracy of
CNN algorithms and traditional graph kernels. For this we describe a recently
discovered graph object, anonymous walk, on which we design task-independent
algorithms for learning graph representations in explicit and distributed way.
Overall, our work represents a new scalable unsupervised learning of
state-of-the-art representations of entire graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanov_S/0/1/0/all/0/1&quot;&gt;Sergey Ivanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01477">
<title>An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks. (arXiv:1806.01477v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01477</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks(DNN) have excessively advanced the field of computer
vision by achieving state of the art performance in various vision tasks. These
results are not limited to the field of vision but can also be seen in speech
recognition and machine translation tasks. Recently, DNNs are found to poorly
fail when tested with samples that are crafted by making imperceptible changes
to the original input images. This causes a gap between the validation and
adversarial performance of a DNN. An effective and generalizable robustness
metric for evaluating the performance of DNN on these adversarial inputs is
still missing from the literature. In this paper, we propose Noise Sensitivity
Score (NSS), a metric that quantifies the performance of a DNN on a specific
input under different forms of fix-directional attacks. An insightful
mathematical explanation is provided for deeply understanding the proposed
metric. By leveraging the NSS, we also proposed a skewness based dataset
robustness metric for evaluating a DNN&apos;s adversarial performance on a given
dataset. Extensive experiments using widely used state of the art architectures
along with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100,
and ImageNet, are used to validate the effectiveness and generalization of our
proposed metrics. Instead of simply measuring a DNN&apos;s adversarial robustness in
the input domain, as previous works, the proposed NSS is built on top of
insightful mathematical understanding of the adversarial attack and gives a
more explicit explanation of the robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1&quot;&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bo Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonfeld_D/0/1/0/all/0/1&quot;&gt;Dan Schonfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoogs_A/0/1/0/all/0/1&quot;&gt;Anthony Hoogs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01660">
<title>Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization. (arXiv:1806.01660v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01660</link>
<description rdf:parseType="Literal">&lt;p&gt;Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD)
have been widely used in distributed machine learning, e.g., training large
collaborative filtering systems and deep neural networks. Due to current
technical limit, however, establishing convergence properties of Async-MSGD for
these highly complicated nonoconvex problems is generally infeasible.
Therefore, we propose to analyze the algorithm through a simpler but nontrivial
nonconvex problem - streaming PCA. This allows us to make progress toward
understanding Aync-MSGD and gaining new insights for more general problems.
Specifically, by exploiting the diffusion approximation of stochastic
optimization, we establish the asymptotic rate of convergence of Async-MSGD for
streaming PCA. Our results indicate a fundamental tradeoff between asynchrony
and momentum: To ensure convergence and acceleration through asynchrony, we
have to reduce the momentum (compared with Sync-MSGD). To the best of our
knowledge, this is the first theoretical attempt on understanding Async-MSGD
for distributed nonconvex stochastic optimization. Numerical experiments on
both streaming PCA and training deep neural networks are provided to support
our findings for Async-MSGD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jianping Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enlu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item></rdf:RDF>