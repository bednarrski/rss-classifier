<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01516"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01837"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01509"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08807"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01532"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01626"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01867"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.01516">
<title>How deep should be the depth of convolutional neural networks: a backyard dog case study. (arXiv:1805.01516v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01516</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a straightforward non-iterative method for shallowing of deep
Convolutional Neural Network (CNN) by combination of several layers of CNNs
with Advanced Supervised Principal Component Analysis (ASPCA) of their outputs.
We tested this new method on a practically important case of `friend-or-foe&apos;
face recognition. This is the backyard dog problem: the dog should (i)
distinguish the members of the family from possible strangers and (ii) identify
the members of the family. Our experiments revealed that the method is capable
of drastically reducing the depth of deep learning CNNs, albeit at the cost of
mild performance deterioration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1&quot;&gt;A.N. Gorban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1&quot;&gt;E.M. Mirkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tukin_I/0/1/0/all/0/1&quot;&gt;I.Y. Tukin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01623">
<title>Recent Progress on Graph Partitioning Problems Using Evolutionary Computation. (arXiv:1805.01623v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;The graph partitioning problem (GPP) is a representative combinatorial
optimization problem which is NP-hard. Currently, various approaches to solve
GPP have been introduced. Among these, the GPP solution using evolutionary
computation (EC) is an effective approach. There has not been any survey on the
research applying EC to GPP since 2011. In this survey, we introduce various
attempts to apply EC to GPP made in the recent seven years.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hye-Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yong-Hyuk Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01837">
<title>Towards a Spectrum of Graph Convolutional Networks. (arXiv:1805.01837v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01837</link>
<description rdf:parseType="Literal">&lt;p&gt;We present our ongoing work on understanding the limitations of graph
convolutional networks (GCNs) as well as our work on generalizations of graph
convolutions for representing more complex node attribute dependencies. Based
on an analysis of GCNs with the help of the corresponding computation graphs,
we propose a generalization of existing GCNs where the aggregation operations
are (a) determined by structural properties of the local neighborhood graphs
and (b) not restricted to weighted averages. We show that the proposed approach
is strictly more expressive while requiring only a modest increase in the
number of parameters and computations. We also show that the proposed
generalization is identical to standard convolutional layers when applied to
regular grid graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garcia-Duran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01509">
<title>SURREAL: SUbgraph Robust REpresentAtion Learning. (arXiv:1805.01509v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01509</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of graph embeddings or node representation learning in a variety
of downstream tasks, such as node classification, link prediction, and
recommendation systems, has led to their popularity in recent years.
Representation learning algorithms aim to preserve local and global network
structure by identifying node neighborhood notions. However, many existing
algorithms generate embeddings that fail to properly preserve the network
structure, or lead to unstable representations due to random processes (e.g.,
random walks to generate context) and, thus, cannot generate to multi-graph
problems. In this paper, we propose a robust graph embedding using connection
subgraphs algorithm, entitled: SURREAL, a novel, stable graph embedding
algorithmic framework. SURREAL learns graph representations using connection
subgraphs by employing the analogy of graphs with electrical circuits. It
preserves both local and global connectivity patterns, and addresses the issue
of high-degree nodes. Further, it exploits the strength of weak ties and
meta-data that have been neglected by baselines. The experiments show that
SURREAL outperforms state-of-the-art algorithms by up to 36.85% on multi-label
classification problem. Further, in contrast to baselines, SURREAL, being
deterministic, is completely stable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Sayouri_S/0/1/0/all/0/1&quot;&gt;Saba A. Al-Sayouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1&quot;&gt;Danai Koutra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_S/0/1/0/all/0/1&quot;&gt;Sarah S. Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08807">
<title>When Will AI Exceed Human Performance? Evidence from AI Experts. (arXiv:1705.08807v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08807</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in artificial intelligence (AI) will transform modern life by
reshaping transportation, health, science, finance, and the military. To adapt
public policy, we need to better anticipate these advances. Here we report the
results from a large survey of machine learning researchers on their beliefs
about progress in AI. Researchers predict AI will outperform humans in many
activities in the next ten years, such as translating languages (by 2024),
writing high-school essays (by 2026), driving a truck (by 2027), working in
retail (by 2031), writing a bestselling book (by 2049), and working as a
surgeon (by 2053). Researchers believe there is a 50% chance of AI
outperforming humans in all tasks in 45 years and of automating all human jobs
in 120 years, with Asian respondents expecting these dates much sooner than
North Americans. These results will inform discussion amongst researchers and
policymakers about anticipating and managing trends in AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grace_K/0/1/0/all/0/1&quot;&gt;Katja Grace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatier_J/0/1/0/all/0/1&quot;&gt;John Salvatier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dafoe_A/0/1/0/all/0/1&quot;&gt;Allan Dafoe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baobao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1&quot;&gt;Owain Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01276">
<title>Learning Conceptual Space Representations of Interrelated Concepts. (arXiv:1805.01276v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01276</link>
<description rdf:parseType="Literal">&lt;p&gt;Several recently proposed methods aim to learn conceptual space
representations from large text collections. These learned representations
asso- ciate each object from a given domain of interest with a point in a
high-dimensional Euclidean space, but they do not model the concepts from this
do- main, and can thus not directly be used for catego- rization and related
cognitive tasks. A natural solu- tion is to represent concepts as Gaussians,
learned from the representations of their instances, but this can only be
reliably done if sufficiently many in- stances are given, which is often not
the case. In this paper, we introduce a Bayesian model which addresses this
problem by constructing informative priors from background knowledge about how
the concepts of interest are interrelated with each other. We show that this
leads to substantially better pre- dictions in a knowledge base completion
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouraoui_Z/0/1/0/all/0/1&quot;&gt;Zied Bouraoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01500">
<title>Noisin: Unbiased Regularization for Recurrent Neural Networks. (arXiv:1805.01500v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01500</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are powerful models of sequential data. They
have been successfully used in domains such as text and speech. However, RNNs
are susceptible to overfitting; regularization is important. In this paper we
develop Noisin, a new method for regularizing RNNs. Noisin injects random noise
into the hidden states of the RNN and then maximizes the corresponding marginal
likelihood of the data. We show how Noisin applies to any RNN and we study many
different types of noise. Noisin is unbiased--it preserves the underlying RNN
on average. We characterize how Noisin regularizes its RNN both theoretically
and empirically. On language modeling benchmarks, Noisin improves over dropout
by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We
also compared the state-of-the-art language model of Yang et al. 2017, both
with and without Noisin. On the Penn Treebank, the method with Noisin more
quickly reaches state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dieng_A/0/1/0/all/0/1&quot;&gt;Adji B. Dieng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1&quot;&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Altosaar_J/0/1/0/all/0/1&quot;&gt;Jaan Altosaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01532">
<title>Lifted Neural Networks. (arXiv:1805.01532v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01532</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a novel family of models of multi- layer feedforward neural
networks in which the activation functions are encoded via penalties in the
training problem. Our approach is based on representing a non-decreasing
activation function as the argmin of an appropriate convex optimiza- tion
problem. The new framework allows for algo- rithms such as block-coordinate
descent methods to be applied, in which each step is composed of a simple (no
hidden layer) supervised learning problem that is parallelizable across data
points and/or layers. Experiments indicate that the pro- posed models provide
excellent initial guesses for weights for standard neural networks. In addi-
tion, the model provides avenues for interesting extensions, such as robustness
against noisy in- puts and optimizing over parameters in activation functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askari_A/0/1/0/all/0/1&quot;&gt;Armin Askari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negiar_G/0/1/0/all/0/1&quot;&gt;Geoffrey Negiar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sambharya_R/0/1/0/all/0/1&quot;&gt;Rajiv Sambharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaoui_L/0/1/0/all/0/1&quot;&gt;Laurent El Ghaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01553">
<title>A Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation. (arXiv:1805.01553v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.01553</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach to interactive-predictive neural machine translation
that attempts to reduce human effort from three directions: Firstly, instead of
requiring humans to select, correct, or delete segments, we employ the idea of
learning from human reinforcements in form of judgments on the quality of
partial translations. Secondly, human effort is further reduced by using the
entropy of word predictions as uncertainty criterion to trigger feedback
requests. Lastly, online updates of the model parameters after every
interaction allow the model to adapt quickly. We show in simulation experiments
that reward signals on partial translations significantly improve character
F-score and BLEU compared to feedback on full translations only, while human
effort can be reduced to an average number of $5$ feedback requests for every
input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1&quot;&gt;Tsz Kin Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1&quot;&gt;Julia Kreutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1&quot;&gt;Stefan Riezler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01554">
<title>A Deep Learning Model with Hierarchical LSTMs and Supervised Attention for Anti-Phishing. (arXiv:1805.01554v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1805.01554</link>
<description rdf:parseType="Literal">&lt;p&gt;Anti-phishing aims to detect phishing content/documents in a pool of textual
data. This is an important problem in cybersecurity that can help to guard
users from fraudulent information. Natural language processing (NLP) offers a
natural solution for this problem as it is capable of analyzing the textual
content to perform intelligent recognition. In this work, we investigate
state-of-the-art techniques for text categorization in NLP to address the
problem of anti-phishing for emails (i.e, predicting if an email is phishing or
not). These techniques are based on deep learning models that have attracted
much attention from the community recently. In particular, we present a
framework with hierarchical long short-term memory networks (H-LSTMs) and
attention mechanisms to model the emails simultaneously at the word and the
sentence level. Our expectation is to produce an effective model for
anti-phishing and demonstrate the effectiveness of deep learning for problems
in cybersecurity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Toan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thien Huu Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01626">
<title>Estimating Learnability in the Sublinear Data Regime. (arXiv:1805.01626v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01626</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of estimating how well a model class is capable of
fitting a distribution of labeled data. We show that it is often possible to
accurately estimate this &quot;learnability&quot; even when given an amount of data that
is too small to reliably learn any accurate model. Our first result applies to
the setting where the data is drawn from a $d$-dimensional distribution with
isotropic covariance, and the label of each datapoint is an arbitrary noisy
function of the datapoint. In this setting, we show that with $O(\sqrt{d})$
samples, one can accurately estimate the fraction of the variance of the label
that can be explained via the best linear function of the data. We extend these
techniques to the setting of binary classification, where we show that in an
analogous setting, the prediction error of the best linear classifier can be
accurately estimated given $O(\sqrt{d})$ labeled samples. Note that in both the
linear regression and binary classification settings, even if there is no noise
in the labels, a sample size linear in the dimension, $d$, is required to
\emph{learn} any function correlated with the underlying model. We further
extend our estimation approach to the setting where the data distribution has
an (unknown) arbitrary covariance matrix, allowing these techniques to be
applied to settings where the model class consists of a linear function applied
to a nonlinear embedding of the data. Finally, we demonstrate the practical
viability of these approaches on synthetic and real data. This ability to
estimate the explanatory value of a set of features (or dataset), even in the
regime in which there is too little data to realize that explanatory value, may
be relevant to the scientific and industrial settings for which data collection
is expensive and there are many potentially relevant feature sets that could be
collected.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1&quot;&gt;Weihao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1&quot;&gt;Gregory Valiant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01867">
<title>Bayesian active learning for choice models with deep Gaussian processes. (arXiv:1805.01867v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01867</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an active learning algorithm and models which can
gradually learn individual&apos;s preference through pairwise comparisons. The
active learning scheme aims at finding individual&apos;s most preferred choice with
minimized number of pairwise comparisons. The pairwise comparisons are encoded
into probabilistic models based on assumptions of choice models and deep
Gaussian processes. The next-to-compare decision is determined by a novel
acquisition function. We benchmark the proposed algorithm and models using
functions with multiple local optima and one public airline itinerary dataset.
The experiments indicate the effectiveness of our active learning algorithm and
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>