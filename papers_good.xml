<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00227"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08219"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00094"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00512"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.05597"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07736"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10026"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00184"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.05470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03882"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07124"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.00227">
<title>WRPN &amp; Apprentice: Methods for Training and Inference using Low-Precision Numerics. (arXiv:1803.00227v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.00227</link>
<description rdf:parseType="Literal">&lt;p&gt;Today&apos;s high performance deep learning architectures involve large models
with numerous parameters. Low precision numerics has emerged as a popular
technique to reduce both the compute and memory requirements of these large
models. However, lowering precision often leads to accuracy degradation. We
describe three schemes whereby one can both train and do efficient inference
using low precision numerics without hurting accuracy. Finally, we describe an
efficient hardware accelerator that can take advantage of the proposed low
precision numerics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Asit Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marr_D/0/1/0/all/0/1&quot;&gt;Debbie Marr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08219">
<title>Tensor Field Networks: Rotation- and Translation-Equivariant Neural Networks for 3D Point Clouds. (arXiv:1802.08219v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08219</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce tensor field networks, which are locally equivariant to 3D
rotations, translations, and permutations of points at every layer. 3D rotation
equivariance removes the need for data augmentation to identify features in
arbitrary orientations. Our network uses filters built from spherical
harmonics; due to the mathematical consequences of this filter choice, each
layer accepts as input (and guarantees as output) scalars, vectors, and
higher-order tensors, in the geometric sense of these terms. We demonstrate how
tensor field networks learn to model simple physics (Newtonian gravitation and
moment of inertia), classify simple 3D shapes (trained on one orientation and
tested on shapes in arbitrary orientations), and, given a small organic
molecule with an atom removed, replace the correct element at the correct
location in space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_N/0/1/0/all/0/1&quot;&gt;Nathaniel Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1&quot;&gt;Tess Smidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kearnes_S/0/1/0/all/0/1&quot;&gt;Steven Kearnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lusann Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohlhoff_K/0/1/0/all/0/1&quot;&gt;Kai Kohlhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1&quot;&gt;Patrick Riley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00094">
<title>Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions. (arXiv:1803.00094v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00094</link>
<description rdf:parseType="Literal">&lt;p&gt;In the recent literature the important role of depth in deep learning has
been emphasized. In this paper we argue that sufficient width of a feedforward
network is equally important by answering the simple question under which
conditions the decision regions of a neural network are connected. It turns out
that for a class of activation functions including leaky ReLU, neural networks
having a pyramidal structure, that is no layer has more hidden units than the
input dimension, produce necessarily connected decision regions. This implies
that a sufficiently wide layer is necessary to produce disconnected decision
regions. We discuss the implications of this result for the construction of
neural networks, in particular the relation to the problem of adversarial
manipulation of classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quynh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukkamala_M/0/1/0/all/0/1&quot;&gt;Mahesh Mukkamala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00144">
<title>Learning Longer-term Dependencies in RNNs with Auxiliary Losses. (arXiv:1803.00144v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00144</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent advances in training recurrent neural networks (RNNs),
capturing long-term dependencies in sequences remains a fundamental challenge.
Most approaches use backpropagation through time (BPTT), which is difficult to
scale to very long sequences. This paper proposes a simple method that improves
the ability to capture long term dependencies in RNNs by adding an unsupervised
auxiliary loss to the original objective. This auxiliary loss forces RNNs to
either reconstruct previous events or predict next events in a sequence, making
truncated backpropagation feasible for long sequences and also improving full
BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel
image classification with sequence lengths up to 16\,000, and a real document
classification benchmark. Our results highlight good performance and resource
efficiency of this approach over competitive baselines, including other
recurrent models and a comparable sized Transformer. Further analyses reveal
beneficial effects of the auxiliary loss on optimization and regularization, as
well as extreme cases where there is little to no backpropagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trinh_T/0/1/0/all/0/1&quot;&gt;Trieu H. Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luong_T/0/1/0/all/0/1&quot;&gt;Thang Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00158">
<title>Modeling reverse thinking for machine learning. (arXiv:1803.00158v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.00158</link>
<description rdf:parseType="Literal">&lt;p&gt;Human inertial thinking schemes can be formed through learning, which are
then applied to quickly solve similar problems later. However, when problems
are significantly different, inertial thinking generally presents the solutions
that are definitely imperfect. In such cases, people will apply creative
thinking, such as reverse thinking, to solve problems. Similarly, machine
learning methods also form inertial thinking schemes through learning the
knowledge from a large amount of data. However, when the testing data are
vastly difference, the formed inertial thinking schemes will inevitably
generate errors. This kind of inertial thinking is called illusion inertial
thinking. Because all machine learning methods do not consider illusion
inertial thinking, in this paper we propose a new method that uses reverse
thinking to correct illusion inertial thinking, which increases the
generalization ability of machine learning methods. Experimental results on
benchmark datasets are used to validate the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huihui_L/0/1/0/all/0/1&quot;&gt;Li Huihui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guihua_W/0/1/0/all/0/1&quot;&gt;Wen Guihua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00512">
<title>Composable Planning with Attributes. (arXiv:1803.00512v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.00512</link>
<description rdf:parseType="Literal">&lt;p&gt;The tasks that an agent will need to solve often are not known during
training. However, if the agent knows which properties of the environment are
important then, after learning how its actions affect those properties, it may
be able to use this knowledge to solve complex tasks without training
specifically for them. Towards this end, we consider a setup in which an
environment is augmented with a set of user defined attributes that
parameterize the features of interest. We propose a method that learns a policy
for transitioning between &quot;nearby&quot; sets of attributes, and maintains a graph of
possible transitions. Given a task at test time that can be expressed in terms
of a target set of attributes, and a current state, our model infers the
attributes of the current state and searches over paths through attribute space
to get a high level plan, and then uses its low level policy to execute the
plan. We show in 3D block stacking, grid-world games, and StarCraft that our
model is able to generalize to longer, more complex tasks at test time by
composing simpler learned policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1&quot;&gt;Adam Lerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1&quot;&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1&quot;&gt;Arthur Szlam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.05597">
<title>Adding Context to Concept Trees. (arXiv:1606.05597v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1606.05597</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept Trees are a type of database that can organise arbitrary textual
information using a very simple rule. Each tree tries to represent a single
cohesive concept and the trees can link with each other for navigation and
semantic purposes. The trees are therefore a type of semantic network and would
benefit from having a consistent level of context for each of the nodes. The
Concept Tree nodes have a mathematical basis allowing for a consistent build
process. These would represent nouns or verbs in a text sentence, for example.
New to the design can then be lists of descriptive elements for each of the
nodes. The descriptors can also be weighted, but do not have to follow the
strict counting rule of the tree nodes. With the new descriptive layers, a much
richer type of knowledge can be achieved and a consistent method for adding
context is suggested. It is also suggested to use the linking structure of the
licas system as basis for the context links. The mathematical model is extended
further and to finish, a query language is suggested for practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07736">
<title>MaskGAN: Better Text Generation via Filling in the______. (arXiv:1801.07736v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07736</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural text generation models are often autoregressive language models or
seq2seq models. These models generate text by sampling words sequentially, with
each word conditioned on the previous word, and are state-of-the-art for
several machine translation and summarization benchmarks. These benchmarks are
often defined by validation perplexity even though this is not a direct measure
of the quality of the generated text. Additionally, these models are typically
trained via maxi- mum likelihood and teacher forcing. These methods are
well-suited to optimizing perplexity but can result in poor sample quality
since generating text requires conditioning on sequences of words that may have
never been observed at training time. We propose to improve sample quality
using Generative Adversarial Networks (GANs), which explicitly train the
generator to produce high quality samples and have shown a lot of success in
image generation. GANs were originally designed to output differentiable
values, so discrete language generation is challenging for them. We claim that
validation perplexity alone is not indicative of the quality of text generated
by a model. We introduce an actor-critic conditional GAN that fills in missing
text conditioned on the surrounding context. We show qualitatively and
quantitatively, evidence that this produces more realistic conditional and
unconditional text samples compared to a maximum likelihood trained model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedus_W/0/1/0/all/0/1&quot;&gt;William Fedus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06070">
<title>Diversity is All You Need: Learning Skills without a Reward Function. (arXiv:1802.06070v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06070</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent creatures can explore their environments and learn useful skills
without supervision. In this paper, we propose DIAYN (&quot;Diversity is All You
Need&quot;), a method for learning useful skills without a reward function. Our
proposed method learns skills by maximizing an information theoretic objective
using a maximum entropy policy. On a variety of simulated robotic tasks, we
show that this simple objective results in the unsupervised emergence of
diverse skills, such as walking and jumping. In a number of reinforcement
learning benchmark environments, our method is able to learn a skill that
solves the benchmark task despite never receiving the true task reward. In
these environments, some of the learned skills correspond to solving the task,
and each skill that solves the task does so in a distinct manner. Our results
suggest that unsupervised discovery of skills can serve as an effective
pretraining mechanism for overcoming challenges of exploration and data
efficiency in reinforcement learning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1&quot;&gt;Benjamin Eysenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1&quot;&gt;Julian Ibarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10026">
<title>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. (arXiv:1802.10026v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.10026</link>
<description rdf:parseType="Literal">&lt;p&gt;The loss functions of deep neural networks are complex and their geometric
properties are not well understood. We show that the optima of these complex
loss functions are in fact connected by a simple polygonal chain with only one
bend, over which training and test accuracy are nearly constant. We introduce a
training procedure to discover these high-accuracy pathways between modes.
Inspired by this new geometric insight, we also propose a new ensembling method
entitled Fast Geometric Ensembling (FGE). Using FGE we can train
high-performing ensembles in the time required to train a single model. We
achieve improved performance compared to the recent state-of-the-art Snapshot
Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual
networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet
by 0.56% by running FGE for just 5 epochs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garipov_T/0/1/0/all/0/1&quot;&gt;Timur Garipov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Izmailov_P/0/1/0/all/0/1&quot;&gt;Pavel Izmailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Podoprikhin_D/0/1/0/all/0/1&quot;&gt;Dmitrii Podoprikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry P. Vetrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06794">
<title>Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering. (arXiv:1711.06794v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1711.06794</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the Visual Question Answering (VQA) task has gained increasing
attention in artificial intelligence. Existing VQA methods mainly adopt the
visual attention mechanism to associate the input question with corresponding
image regions for effective question answering. The free-form region based and
the detection-based visual attention mechanisms are mostly investigated, with
the former ones attending free-form image regions and the latter ones attending
pre-specified detection-box regions. We argue that the two attention mechanisms
are able to provide complementary information and should be effectively
integrated to better solve the VQA problem. In this paper, we propose a novel
deep neural network for VQA that integrates both attention mechanisms. Our
proposed framework effectively fuses features from free-form image regions,
detection boxes, and question representations via a multi-modal multiplicative
feature embedding scheme to jointly attend question-related free-form image
regions and detection boxes for more accurate question answering. The proposed
method is extensively evaluated on two publicly available datasets, COCO-QA and
VQA, and outperforms state-of-the-art approaches. Source code is available at
https://github.com/lupantech/dual-mfa-vqa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00156">
<title>Autoencoding topology. (arXiv:1803.00156v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00156</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of learning a manifold structure on a dataset is framed in terms
of a generative model, to which we use ideas behind autoencoders (namely
adversarial/Wasserstein autoencoders) to fit deep neural networks. From a
machine learning perspective, the resulting structure, an atlas of a manifold,
may be viewed as a combination of dimensionality reduction and &quot;fuzzy&quot;
clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Korman_E/0/1/0/all/0/1&quot;&gt;Eric O. Korman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00184">
<title>Learning Sparse Structured Ensembles with SG-MCMC and Network Pruning. (arXiv:1803.00184v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00184</link>
<description rdf:parseType="Literal">&lt;p&gt;An ensemble of neural networks is known to be more robust and accurate than
an individual network, however usually with linearly-increased cost in both
training and testing. In this work, we propose a two-stage method to learn
Sparse Structured Ensembles (SSEs) for neural networks. In the first stage, we
run SG-MCMC with group sparse priors to draw an ensemble of samples from the
posterior distribution of network parameters. In the second stage, we apply
weight-pruning to each sampled network and then perform retraining over the
remained connections. In this way of learning SSEs with SG-MCMC and pruning, we
not only achieve high prediction accuracy since SG-MCMC enhances exploration of
the model-parameter space, but also reduce memory and computation cost
significantly in both training and testing of NN ensembles. This is thoroughly
evaluated in the experiments of learning SSE ensembles of both FNNs and LSTMs.
For example, in LSTM based language modeling (LM), we obtain 21% relative
reduction in LM perplexity by learning a SSE of 4 large LSTM models, which has
only 30% of model parameters and 70% of computations in total, as compared to
the baseline large LSTM LM. To the best of our knowledge, this work represents
the first methodology and empirical study of integrating SG-MCMC, group sparse
prior and network pruning together for learning NN ensembles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ou_Z/0/1/0/all/0/1&quot;&gt;Zhijian Ou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00195">
<title>The Regularization Effects of Anisotropic Noise in Stochastic Gradient Descent. (arXiv:1803.00195v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00195</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the generalization of deep learning has raised lots of concerns
recently, where the learning algorithms play an important role in
generalization performance, such as stochastic gradient descent (SGD). Along
this line, we particularly study the anisotropic noise introduced by SGD, and
investigate its importance for the generalization in deep neural networks.
Through a thorough empirical analysis, it is shown that the anisotropic
diffusion of SGD tends to follow the curvature information of the loss
landscape, and thus is beneficial for escaping from sharp and poor minima
effectively, towards more stable and flat minima. We verify our understanding
through comparing this anisotropic diffusion with full gradient descent plus
isotropic diffusion (i.e. Langevin dynamics) and other types of
position-dependent noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingfeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jinwen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bing Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00387">
<title>A General Pipeline for 3D Detection of Vehicles. (arXiv:1803.00387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.00387</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving requires 3D perception of vehicles and other objects in
the in environment. Much of the current methods support 2D vehicle detection.
This paper proposes a flexible pipeline to adopt any 2D detection network and
fuse it with a 3D point cloud to generate 3D information with minimum changes
of the 2D detection networks. To identify the 3D box, an effective model
fitting algorithm is developed based on generalised car models and score maps.
A two-stage convolutional neural network (CNN) is proposed to refine the
detected 3D box. This pipeline is tested on the KITTI dataset using two
different 2D detection networks. The 3D detection results based on these two
networks are similar, demonstrating the flexibility of the proposed pipeline.
The results rank second among the 3D detection algorithms, indicating its
competencies in 3D detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinxin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1&quot;&gt;Marcelo H. Ang Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1&quot;&gt;Sertac Karaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00502">
<title>Pairwise Inner Product Distance: Metric for Functionality, Stability, Dimensionality of Vector Embedding. (arXiv:1803.00502v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.00502</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a theoretical framework for understanding vector
embedding, a fundamental building block of many deep learning models,
especially in NLP. We discover a natural unitary-invariance in vector
embeddings, which is required by the distributional hypothesis. This
unitary-invariance states the fact that two embeddings are essentially
equivalent if one can be obtained from the other by performing a
relative-geometry preserving transformation, for example a rotation. This idea
leads to the Pairwise Inner Product (PIP) loss, a natural unitary-invariant
metric for the distance between two embeddings. We demonstrate that the PIP
loss captures the difference in functionality between embeddings. By
formulating the embedding training process as matrix factorization under noise,
we reveal a fundamental bias-variance tradeoff in dimensionality selection.
With tools from perturbation and stability theory, we provide an upper bound on
the PIP loss using the signal spectrum and noise variance, both of which can be
readily inferred from data. Our framework sheds light on many empirical
phenomena, including the existence of an optimal dimension, and the robustness
of embeddings against over-parametrization. The bias-variance tradeoff of PIP
loss explicitly answers the fundamental open problem of dimensionality
selection for vector embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00530">
<title>Online Feature Ranking for Intrusion Detection Systems. (arXiv:1803.00530v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1803.00530</link>
<description rdf:parseType="Literal">&lt;p&gt;Many current approaches to the design of intrusion detec- tion systems apply
feature selection in a static, non-adaptive fashion. These methods often
neglect the dynamic nature of network data which requires to use adaptive
feature selection techniques. In this paper, we present a simple technique
based on incremental learning of support vector machines in order to rank the
features in real time within a streaming model for network data. Some
illustrative numerical experiments with two popular benchmark datasets show
that our approach al- lows to adapt to the changes in normal network behaviour
and novel attack patterns which have not been experienced before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atli_B/0/1/0/all/0/1&quot;&gt;Buse Gul Atli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.05470">
<title>DeepProbe: Information Directed Sequence Understanding and Chatbot Design via Recurrent Neural Networks. (arXiv:1707.05470v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.05470</link>
<description rdf:parseType="Literal">&lt;p&gt;Information extraction and user intention identification are central topics
in modern query understanding and recommendation systems. In this paper, we
propose DeepProbe, a generic information-directed interaction framework which
is built around an attention-based sequence to sequence (seq2seq) recurrent
neural network. DeepProbe can rephrase, evaluate, and even actively ask
questions, leveraging the generative ability and likelihood estimation made
possible by seq2seq models. DeepProbe makes decisions based on a derived
uncertainty (entropy) measure conditioned on user inputs, possibly with
multiple rounds of interactions. Three applications, namely a rewritter, a
relevance scorer and a chatbot for ad recommendation, were built around
DeepProbe, with the first two serving as precursory building blocks for the
third. We first use the seq2seq model in DeepProbe to rewrite a user query into
one of standard query form, which is submitted to an ordinary recommendation
system. Secondly, we evaluate DeepProbe&apos;s seq2seq model-based relevance
scoring. Finally, we build a chatbot prototype capable of making active user
interactions, which can ask questions that maximize information gain, allowing
for a more efficient user intention idenfication process. We evaluate first two
applications by 1) comparing with baselines by BLEU and AUC, and 2) human judge
evaluation. Both demonstrate significant improvements compared with current
state-of-the-art systems, proving their values as useful tools on their own,
and at the same time laying a good foundation for the ongoing chatbot
application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Keng-hao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruofei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04062">
<title>MINE: Mutual Information Neural Estimation. (arXiv:1801.04062v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04062</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that the estimation of mutual information between high dimensional
continuous random variables can be achieved by gradient descent over neural
networks. We present a Mutual Information Neural Estimator (MINE) that is
linearly scalable in dimensionality as well as in sample size, trainable
through back-prop, and strongly consistent. We present a handful of
applications on which MINE can be used to minimize or maximize mutual
information. We apply MINE to improve adversarially trained generative models.
We also use MINE to implement Information Bottleneck, applying it in tasks
related to supervised classification; our results demonstrate substantial
improvement in flexibility and performance in these settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belghazi_M/0/1/0/all/0/1&quot;&gt;Mohamed Ishmael Belghazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1&quot;&gt;Aristide Baratin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1&quot;&gt;Sai Rajeswar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozair_S/0/1/0/all/0/1&quot;&gt;Sherjil Ozair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benhio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Benhio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00912">
<title>AFT*: Integrating Active Learning and Transfer Learning to Reduce Annotation Efforts. (arXiv:1802.00912v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00912</link>
<description rdf:parseType="Literal">&lt;p&gt;The splendid success of convolutional neural networks (CNNs) in computer
vision is largely attributed to the availability of large annotated datasets,
such as ImageNet and Places. However, in biomedical imaging, it is very
challenging to create such large annotated datasets, as annotating biomedical
images is not only tedious, laborious, and time consuming, but also demanding
of costly, specialty-oriented skills, which are not easily accessible. To
dramatically reduce annotation cost, this paper presents a novel method to
naturally integrate active learning and transfer learning (fine-tuning) into a
single framework, called AFT*, which starts directly with a pre-trained CNN to
seek &quot;worthy&quot; samples for annotation and gradually enhance the (fine-tuned) CNN
via continuous fine-tuning. We have evaluated our method in three distinct
biomedical imaging applications, demonstrating that it can cut the annotation
cost by at least half, in comparison with the state-of-the-art method. This
performance is attributed to the several advantages derived from the advanced
active, continuous learning capability of our method. Although AFT* was
initially conceived in the context of computer-aided diagnosis in biomedical
imaging, it is generic and applicable to many tasks in computer vision and
image analysis; we illustrate the key ideas behind AFT* with the Places
database for scene interpretation in natural images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zongwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jae Y. Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurudu_S/0/1/0/all/0/1&quot;&gt;Suryakanth R. Gurudu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotway_M/0/1/0/all/0/1&quot;&gt;Michael B. Gotway&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jianming Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03882">
<title>Random Hinge Forest for Differentiable Learning. (arXiv:1802.03882v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03882</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose random hinge forests, a simple, efficient, and novel variant of
decision forests. Importantly, random hinge forests can be readily incorporated
as a general component within arbitrary computation graphs that are optimized
end-to-end with stochastic gradient descent or variants thereof. We derive
random hinge forest and ferns, focusing on their sparse and efficient nature,
their min-max margin property, strategies to initialize them for arbitrary
network architectures, and the class of optimizers most suitable for optimizing
random hinge forest. The performance and versatility of random hinge forests
are demonstrated by experiments incorporating a variety of of small and large
UCI machine learning data sets and also ones involving the MNIST, Letter, and
USPS image datasets. We compare random hinge forests with random forests and
the more recent backpropagating deep neural decision forests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lay_N/0/1/0/all/0/1&quot;&gt;Nathan Lay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harrison_A/0/1/0/all/0/1&quot;&gt;Adam P. Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schreiber_S/0/1/0/all/0/1&quot;&gt;Sharon Schreiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dawer_G/0/1/0/all/0/1&quot;&gt;Gitesh Dawer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barbu_A/0/1/0/all/0/1&quot;&gt;Adrian Barbu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07124">
<title>Out-distribution training confers robustness to deep neural networks. (arXiv:1802.07124v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07124</link>
<description rdf:parseType="Literal">&lt;p&gt;The easiness at which adversarial instances can be generated in deep neural
networks raises some fundamental questions on their functioning and concerns on
their use in critical systems. In this paper, we draw a connection between
over-generalization and adversaries: a possible cause of adversaries lies in
models designed to make decisions all over the input space, leading to
inappropriate high-confidence decisions in parts of the input space not
represented in the training set. We empirically show an augmented neural
network, which is not trained on any types of adversaries, can increase the
robustness by detecting black-box one-step adversaries, i.e. assimilated to
out-distribution samples, and making generation of white-box one-step
adversaries harder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_M/0/1/0/all/0/1&quot;&gt;Mahdieh Abbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;</dc:creator>
</item></rdf:RDF>