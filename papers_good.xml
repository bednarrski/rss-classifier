<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.01780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.09219"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06015"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05878"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05882"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06006"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06246"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06283"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06559"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07469"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05016"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.05954">
<title>An Artificial Neural Network Architecture Based on Context Transformations in Cortical Minicolumns. (arXiv:1712.05954v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.05954</link>
<description rdf:parseType="Literal">&lt;p&gt;Cortical minicolumns are considered a model of cortical organization. Their
function is still a source of research and not reflected properly in modern
architecture of nets in algorithms of Artificial Intelligence. We assume its
function and describe it in this article. Furthermore, we show how this
proposal allows to construct a new architecture, that is not based on
convolutional neural networks, test it on MNIST data and receive close to
Convolutional Neural Network accuracy. We also show that the proposed
architecture possesses an ability to train on a small quantity of samples. To
achieve these results, we enable the minicolumns to remember context
transformations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morzhakov_V/0/1/0/all/0/1&quot;&gt;Vasily Morzhakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redozubov_A/0/1/0/all/0/1&quot;&gt;Alexey Redozubov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06070">
<title>Self-adaptation of Genetic Operators Through Genetic Programming Techniques. (arXiv:1712.06070v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.06070</link>
<description rdf:parseType="Literal">&lt;p&gt;Here we propose an evolutionary algorithm that self modifies its operators at
the same time that candidate solutions are evolved. This tackles convergence
and lack of diversity issues, leading to better solutions. Operators are
represented as trees and are evolved using genetic programming (GP) techniques.
The proposed approach is tested with real benchmark functions and an analysis
of operator evolution is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salinas_A/0/1/0/all/0/1&quot;&gt;Andres Felipe Cruz Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1&quot;&gt;Jonatan Gomez Perdomo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06272">
<title>Automated flow for compressing convolution neural networks for efficient edge-computation with FPGA. (arXiv:1712.06272v1 [cs.AR])</title>
<link>http://arxiv.org/abs/1712.06272</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (CNN) based solutions are the current
state- of-the-art for computer vision tasks. Due to the large size of these
models, they are typically run on clusters of CPUs or GPUs. However, power
requirements and cost budgets can be a major hindrance in adoption of CNN for
IoT applications. Recent research highlights that CNN contain significant
redundancy in their structure and can be quantized to lower bit-width
parameters and activations, while maintaining acceptable accuracy. Low
bit-width and especially single bit-width (binary) CNN are particularly
suitable for mobile applications based on FPGA implementation, due to the
bitwise logic operations involved in binarized CNN. Moreover, the transition to
lower bit-widths opens new avenues for performance optimizations and model
improvement. In this paper, we present an automatic flow from trained
TensorFlow models to FPGA system on chip implementation of binarized CNN. This
flow involves quantization of model parameters and activations, generation of
network and model in embedded-C, followed by automatic generation of the FPGA
accelerator for binary convolutions. The automated flow is demonstrated through
implementation of binarized &quot;YOLOV2&quot; on the low cost, low power Cyclone- V FPGA
device. Experiments on object detection using binarized YOLOV2 demonstrate
significant performance benefit in terms of model size and inference speed on
FPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire
automated flow from trained models to FPGA synthesis can be completed within
one hour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiq_F/0/1/0/all/0/1&quot;&gt;Farhan Shafiq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_T/0/1/0/all/0/1&quot;&gt;Takato Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilchez_A/0/1/0/all/0/1&quot;&gt;Antonio T. Vilchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sakyasingha Dasgupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06563">
<title>Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients. (arXiv:1712.06563v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.06563</link>
<description rdf:parseType="Literal">&lt;p&gt;While neuroevolution (evolving neural networks) has a successful track record
across a variety of domains from reinforcement learning to artificial life, it
is rarely applied to large, deep neural networks. A central reason is that
while random mutation generally works in low dimensions, a random perturbation
of thousands or millions of weights is likely to break existing functionality,
providing no learning signal even if some individual weight changes were
beneficial. This paper proposes a solution by introducing a family of safe
mutation (SM) operators that aim within the mutation operator itself to find a
degree of change that does not alter network behavior too much, but still
facilitates exploration. Importantly, these SM operators do not require any
additional interactions with the environment. The most effective SM variant
capitalizes on the intriguing opportunity to scale the degree of mutation of
each individual weight according to the sensitivity of the network&apos;s outputs to
that weight, which requires computing the gradient of outputs with respect to
the weights (instead of the gradient of error, as in conventional deep
learning). This safe mutation through gradients (SM-G) operator dramatically
increases the ability of a simple genetic algorithm-based neuroevolution method
to find solutions in high-dimensional domains that require deep and/or
recurrent neural networks (which tend to be particularly brittle to mutation),
including domains that require processing raw pixels. By improving our ability
to evolve deep neural networks, this new safer approach to mutation expands the
scope of domains amenable to neuroevolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jay Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06567">
<title>Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning. (arXiv:1712.06567v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.06567</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep artificial neural networks (DNNs) are typically trained via
gradient-based learning algorithms, namely backpropagation. Evolution
strategies (ES) can rival backprop-based algorithms such as Q-learning and
policy gradients on challenging deep reinforcement learning (RL) problems.
However, ES can be considered a gradient-based algorithm because it performs
stochastic gradient descent via an operation similar to a finite-difference
approximation of the gradient. That raises the question of whether
non-gradient-based evolutionary algorithms can work at DNN scales. Here we
demonstrate they can: we evolve the weights of a DNN with a simple,
gradient-free, population-based genetic algorithm (GA) and it performs well on
hard deep RL problems, including Atari and humanoid locomotion. The Deep GA
successfully evolves networks with over four million free parameters, the
largest neural networks ever evolved with a traditional evolutionary algorithm.
These results (1) expand our sense of the scale at which GAs can operate, (2)
suggest intriguingly that in some cases following the gradient is not the best
choice for optimizing performance, and (3) make immediately available the
multitude of techniques that have been developed in the neuroevolution
community to improve performance on RL problems. To demonstrate the latter, we
show that combining DNNs with novelty search, which was designed to encourage
exploration on tasks with deceptive or sparse reward functions, can solve a
high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C,
ES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES,
A3C, and DQN, and enables a state-of-the-art compact encoding technique that
can represent million-parameter DNNs in thousands of bytes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1&quot;&gt;Felipe Petroski Such&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhavan_V/0/1/0/all/0/1&quot;&gt;Vashisht Madhavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_E/0/1/0/all/0/1&quot;&gt;Edoardo Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.01780">
<title>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. (arXiv:1703.01780v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1703.01780</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently proposed Temporal Ensembling has achieved state-of-the-art
results in several semi-supervised learning benchmarks. It maintains an
exponential moving average of label predictions on each training example, and
penalizes predictions that are inconsistent with this target. However, because
the targets change only once per epoch, Temporal Ensembling becomes unwieldy
when learning large datasets. To overcome this problem, we propose Mean
Teacher, a method that averages model weights instead of label predictions. As
an additional benefit, Mean Teacher improves test accuracy and enables training
with fewer labels than Temporal Ensembling. Without changing the network
architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250
labels, outperforming Temporal Ensembling trained with 1000 labels. We also
show that a good network architecture is crucial to performance. Combining Mean
Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with
4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels
from 35.24% to 9.11%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarvainen_A/0/1/0/all/0/1&quot;&gt;Antti Tarvainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valpola_H/0/1/0/all/0/1&quot;&gt;Harri Valpola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.09219">
<title>Recurrent Ladder Networks. (arXiv:1707.09219v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1707.09219</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a recurrent extension of the Ladder networks whose structure is
motivated by the inference required in hierarchical latent variable models. We
demonstrate that the recurrent Ladder is able to handle a wide variety of
complex learning tasks that benefit from iterative inference and temporal
modeling. The architecture shows close-to-optimal results on temporal modeling
of video data, competitive results on music modeling, and improved perceptual
grouping based on higher order abstractions, such as stochastic textures and
motion cues. We present results for fully supervised, semi-supervised, and
unsupervised tasks. The results suggest that the proposed architecture and
principles are powerful tools for learning a hierarchy of abstractions,
learning iterative inference and handling temporal information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Premont_Schwarz_I/0/1/0/all/0/1&quot;&gt;Isabeau Pr&amp;#xe9;mont-Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1&quot;&gt;Alexander Ilin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1&quot;&gt;Tele Hotloo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasmus_A/0/1/0/all/0/1&quot;&gt;Antti Rasmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1&quot;&gt;Rinu Boney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valpola_H/0/1/0/all/0/1&quot;&gt;Harri Valpola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05881">
<title>Morphology dictates a robot&apos;s ability to ground crowd-proposed language. (arXiv:1712.05881v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.05881</link>
<description rdf:parseType="Literal">&lt;p&gt;As more robots act in physical proximity to people, it is essential to ensure
they make decisions and execute actions that align with human values. To do so,
robots need to understand the true intentions behind human-issued commands. In
this paper, we define a safe robot as one that receives a natural-language
command from humans, considers an action in response to that command, and
accurately predicts how humans will judge that action is executed in reality.
Our contribution is two-fold: First, we introduce a web platform for human
users to propose commands to simulated robots. The robots receive commands and
act based on those proposed commands, and then the users provide positive
and/or negative reinforcement. Next, we train a critic for each robot to
predict the crowd&apos;s responses to one of the crowd-proposed commands. Second, we
show that the morphology of a robot plays a role in the way it grounds
language: The critics show that two of the robots used in the experiment
achieve a lower prediction error than the others. Thus, those two robots are
safer, according to our definition, since they ground the proposed command more
accurately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoor_Z/0/1/0/all/0/1&quot;&gt;Zahra Mahoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felag_J/0/1/0/all/0/1&quot;&gt;Jack Felag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06015">
<title>StackInsights: Cognitive Learning for Hybrid Cloud Readiness. (arXiv:1712.06015v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.06015</link>
<description rdf:parseType="Literal">&lt;p&gt;Hybrid cloud is an integrated cloud computing environment utilizing a mix of
public cloud, private cloud, and on-premise traditional IT infrastructures.
Workload awareness, defined as a detailed full range understanding of each
individual workload, is essential in implementing the hybrid cloud. While it is
critical to perform an accurate analysis to determine which workloads are
appropriate for on-premise deployment versus which workloads can be migrated to
a cloud off-premise, the assessment is mainly performed by rule or policy based
approaches. In this paper, we introduce StackInsights, a novel cognitive system
to automatically analyze and predict the cloud readiness of workloads for an
enterprise. Our system harnesses the critical metrics across the entire stack:
1) infrastructure metrics, 2) data relevance metrics, and 3) application
taxonomy, to identify workloads that have characteristics of a) low sensitivity
with respect to business security, criticality and compliance, and b) low
response time requirements and access patterns. Since the capture of the data
relevance metrics involves an intrusive and in-depth scanning of the content of
storage objects, a machine learning model is applied to perform the business
relevance classification by learning from the meta level metrics harnessed
across stack. In contrast to traditional methods, StackInsights significantly
reduces the total time for hybrid cloud readiness assessment by orders of
magnitude.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1&quot;&gt;Mu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bathen_L/0/1/0/all/0/1&quot;&gt;Luis Bathen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genot_S/0/1/0/all/0/1&quot;&gt;Simon-Pierre G&amp;#xe9;not&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sunhwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Routray_R/0/1/0/all/0/1&quot;&gt;Ramani Routray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06180">
<title>Towards a Deep Reinforcement Learning Approach for Tower Line Wars. (arXiv:1712.06180v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.06180</link>
<description rdf:parseType="Literal">&lt;p&gt;There have been numerous breakthroughs with reinforcement learning in the
recent years, perhaps most notably on Deep Reinforcement Learning successfully
playing and winning relatively advanced computer games. There is undoubtedly an
anticipation that Deep Reinforcement Learning will play a major role when the
first AI masters the complicated game plays needed to beat a professional
Real-Time Strategy game player. For this to be possible, there needs to be a
game environment that targets and fosters AI research, and specifically Deep
Reinforcement Learning. Some game environments already exist, however, these
are either overly simplistic such as Atari 2600 or complex such as Starcraft II
from Blizzard Entertainment. We propose a game environment in between Atari
2600 and Starcraft II, particularly targeting Deep Reinforcement Learning
algorithm research. The environment is a variant of Tower Line Wars from
Warcraft III, Blizzard Entertainment. Further, as a proof of concept that the
environment can harbor Deep Reinforcement algorithms, we propose and apply a
Deep Q-Reinforcement architecture. The architecture simplifies the state space
so that it is applicable to Q-learning, and in turn improves performance
compared to current state-of-the-art methods. Our experiments show that the
proposed architecture can learn to play the environment well, and score 33%
better than standard Deep Q-learning which in turn proves the usefulness of the
game environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1&quot;&gt;Per-Arne Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06536">
<title>Nonparametric Inference for Auto-Encoding Variational Bayes. (arXiv:1712.06536v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.06536</link>
<description rdf:parseType="Literal">&lt;p&gt;We would like to learn latent representations that are low-dimensional and
highly interpretable. A model that has these characteristics is the Gaussian
Process Latent Variable Model. The benefits and negative of the GP-LVM are
complementary to the Variational Autoencoder, the former provides interpretable
low-dimensional latent representations while the latter is able to handle large
amounts of data and can use non-Gaussian likelihoods. Our inspiration for this
paper is to marry these two approaches and reap the benefits of both. In order
to do so we will introduce a novel approximate inference scheme inspired by the
GP-LVM and the VAE. We show experimentally that the approximation allows the
capacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large
without losing a highly interpretable representation, allowing reconstruction
quality to be unlimited by Z at the same time as a low-dimensional space can be
used to perform ancestral sampling from as well as a means to reason about the
embedded data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bodin_E/0/1/0/all/0/1&quot;&gt;Erik Bodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Malik_I/0/1/0/all/0/1&quot;&gt;Iman Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ek_C/0/1/0/all/0/1&quot;&gt;Carl Henrik Ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1&quot;&gt;Neill D. F. Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05813">
<title>Realistic Traffic Generation for Web Robots. (arXiv:1712.05813v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1712.05813</link>
<description rdf:parseType="Literal">&lt;p&gt;Critical to evaluating the capacity, scalability, and availability of web
systems are realistic web traffic generators. Web traffic generation is a
classic research problem, no generator accounts for the characteristics of web
robots or crawlers that are now the dominant source of traffic to a web server.
Administrators are thus unable to test, stress, and evaluate how their systems
perform in the face of ever increasing levels of web robot traffic. To resolve
this problem, this paper introduces a novel approach to generate synthetic web
robot traffic with high fidelity. It generates traffic that accounts for both
the temporal and behavioral qualities of robot traffic by statistical and
Bayesian models that are fitted to the properties of robot traffic seen in web
logs from North America and Europe. We evaluate our traffic generator by
comparing the characteristics of generated traffic to those of the original
data. We look at session arrival rates, inter-arrival times and session
lengths, comparing and contrasting them between generated and real traffic.
Finally, we show that our generated traffic affects cache performance similarly
to actual traffic, using the common LRU and LFU eviction policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1&quot;&gt;Kyle Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doran_D/0/1/0/all/0/1&quot;&gt;Derek Doran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05878">
<title>An MPI-Based Python Framework for Distributed Training with Keras. (arXiv:1712.05878v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1712.05878</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a lightweight Python framework for distributed training of neural
networks on multiple GPUs or CPUs. The framework is built on the popular Keras
machine learning library. The Message Passing Interface (MPI) protocol is used
to coordinate the training process, and the system is well suited for job
submission at supercomputing sites. We detail the software&apos;s features, describe
its use, and demonstrate its performance on systems of varying sizes on a
benchmark problem drawn from high-energy physics research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Dustin Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlimant_J/0/1/0/all/0/1&quot;&gt;Jean-Roch Vlimant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spiropulu_M/0/1/0/all/0/1&quot;&gt;Maria Spiropulu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05882">
<title>On reproduction of On the regularization of Wasserstein GANs. (arXiv:1712.05882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.05882</link>
<description rdf:parseType="Literal">&lt;p&gt;This report has several purposes. First, our report is written to investigate
the reproducibility of the submitted paper On the regularization of Wasserstein
GANs (2018). Second, among the experiments performed in the submitted paper,
five aspects were emphasized and reproduced: learning speed, stability,
robustness against hyperparameter, estimating the Wasserstein distance, and
various sampling method. Finally, we identify which parts of the contribution
can be reproduced, and at what cost in terms of resources. All source code for
reproduction is open to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Junghoon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_T/0/1/0/all/0/1&quot;&gt;Taegyun Jeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06006">
<title>How well does your sampler really work?. (arXiv:1712.06006v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.06006</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new data-driven benchmark system to evaluate the performance of
new MCMC samplers. Taking inspiration from the COCO benchmark in optimization,
we view this task as having critical importance to machine learning and
statistics given the rate at which new samplers are proposed. The common
hand-crafted examples to test new samplers are unsatisfactory; we take a
meta-learning-like approach to generate benchmark examples from a large corpus
of data sets and models. Surrogates of posteriors found in real problems are
created using highly flexible density models including modern neural network
based approaches. We provide new insights into the real effective sample size
of various samplers per unit time and the estimation efficiency of the samplers
per sample. Additionally, we provide a meta-analysis to assess the predictive
utility of various MCMC diagnostics and perform a nonparametric regression to
combine them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Ryan Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neal_B/0/1/0/all/0/1&quot;&gt;Brady Neal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06199">
<title>Structured Optimal Transport. (arXiv:1712.06199v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.06199</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal Transport has recently gained interest in machine learning for
applications ranging from domain adaptation, sentence similarities to deep
learning. Yet, its ability to capture frequently occurring structure beyond the
&quot;ground metric&quot; is limited. In this work, we develop a nonlinear generalization
of (discrete) optimal transport that is able to reflect much additional
structure. We demonstrate how to leverage the geometry of this new model for
fast algorithms, and explore connections and properties. Illustrative
experiments highlight the benefit of the induced structured couplings for tasks
in domain adaptation and natural language processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alvarez_Melis_D/0/1/0/all/0/1&quot;&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi S. Jaakkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06246">
<title>A Survey on Multi-View Clustering. (arXiv:1712.06246v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.06246</link>
<description rdf:parseType="Literal">&lt;p&gt;With the fast development of information technology, especially the
popularization of internet, multi-view learning becomes more and more popular
in machine learning and data mining fields. As we all know that, multi-view
semi-supervised learning, such as co-training, co-regularization has gained
considerable attentions. Although recently, multi-view clustering (MVC) has
developed rapidly, there are not a survey or review to summarize and analyze
the current progress. Therefore, this paper sums up the common strategies of
combining multiple views and based on that we proposed a novel taxonomy of the
MVC approaches. We also discussed the relationships between MVC and multi-view
representation, ensemble clustering, multi-task clustering, multi-view
supervised and multi-view semi-supervised learning. Several representative
real-world applications are elaborated. To promote the further development of
MVC, we pointed out several open problems that are worth exploring in the
future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_G/0/1/0/all/0/1&quot;&gt;Guoqing Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shiliang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1&quot;&gt;Jinbo Bi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06283">
<title>A Bridge Between Hyperparameter Optimization and Larning-to-learn. (arXiv:1712.06283v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.06283</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a class of a nested optimization problems involving inner and
outer objectives. We observe that by taking into explicit account the
optimization dynamics for the inner objective it is possible to derive a
general framework that unifies gradient-based hyperparameter optimization and
meta-learning (or learning-to-learn). Depending on the specific setting, the
variables of the outer objective take either the meaning of hyperparameters in
a supervised learning problem or parameters of a meta-learner. We show that
some recently proposed methods in the latter setting can be instantiated in our
framework and tackled with the same gradient-based algorithms. Finally, we
discuss possible design patterns for learning-to-learn and present encouraging
preliminary experiments for few-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franceschi_L/0/1/0/all/0/1&quot;&gt;Luca Franceschi&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Donini_M/0/1/0/all/0/1&quot;&gt;Michele Donini&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frasconi_P/0/1/0/all/0/1&quot;&gt;Paolo Frasconi&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimiliano Pontil&lt;/a&gt; (1 and 2) ((1) University College London, (2) Istituto Italiano di Teconologia, (3) Universita&amp;#x27; degli Studi di Firenze)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06559">
<title>The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning. (arXiv:1712.06559v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.06559</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gradient Descent (SGD) with small mini-batch is a key component in
modern large-scale machine learning. However, its efficiency has not been easy
to analyze as most theoretical results require adaptive rates and show
convergence rates far slower than that for gradient descent, making
computational comparisons difficult.
&lt;/p&gt;
&lt;p&gt;In this paper we aim to clarify the issue of fast SGD convergence. The key
observation is that most modern architectures are over-parametrized and are
trained to interpolate the data by driving the empirical loss (classification
and regression) close to zero. While it is still unclear why these interpolated
solutions perform well on test data, these regimes allow for very fast
convergence of SGD, comparable in the number of iterations to gradient descent.
&lt;/p&gt;
&lt;p&gt;Specifically, consider the setting with quadratic objective function, or near
a minimum, where the quadratic term is dominant. We show that: (1) Mini-batch
size $1$ with constant step size is optimal in terms of computations to achieve
a given error. (2) There is a critical mini-batch size such that: (a. linear
scaling) SGD iteration with mini-batch size $m$ smaller than the critical size
is nearly equivalent to $m$ iterations of mini-batch size $1$. (b. saturation)
SGD iteration with mini-batch larger than the critical size is nearly
equivalent to a gradient descent step.
&lt;/p&gt;
&lt;p&gt;The critical mini-batch size can be viewed as the limit for effective
mini-batch parallelization. It is also nearly independent of the data size,
implying $O(n)$ acceleration over GD per unit of computation.
&lt;/p&gt;
&lt;p&gt;We give experimental evidence on real data, with the results closely
following our theoretical analyses.
&lt;/p&gt;
&lt;p&gt;Finally, we show how the interpolation perspective and our results fit with
recent developments in training deep neural networks and discuss connections to
adaptive rates for SGD and variance reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Siyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassily_R/0/1/0/all/0/1&quot;&gt;Raef Bassily&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Belkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07469">
<title>DGM: A deep learning algorithm for solving partial differential equations. (arXiv:1708.07469v3 [q-fin.MF] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07469</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional PDEs have been a longstanding computational challenge. We
propose to solve high-dimensional PDEs by approximating the solution with a
deep neural network which is trained to satisfy the differential operator,
initial condition, and boundary conditions. We prove that the neural network
converges to the solution of the partial differential equation as the number of
hidden units increases. Our algorithm is meshfree, which is key since meshes
become infeasible in higher dimensions. Instead of forming a mesh, the neural
network is trained on batches of randomly sampled time and space points. We
implement the approach for American options (a type of free-boundary PDE which
is widely used in finance) in up to $200$ dimensions. We call the algorithm a
&quot;Deep Galerkin Method (DGM)&quot; since it is similar in spirit to Galerkin methods,
with the solution approximated by a neural network instead of a linear
combination of basis functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sirignano_J/0/1/0/all/0/1&quot;&gt;Justin Sirignano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Spiliopoulos_K/0/1/0/all/0/1&quot;&gt;Konstantinos Spiliopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00679">
<title>GANGs: Generative Adversarial Network Games. (arXiv:1712.00679v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00679</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GAN) have become one of the most successful
frameworks for unsupervised generative modeling. As GANs are difficult to train
much research has focused on this. However, very little of this research has
directly exploited game-theoretic techniques. We introduce Generative
Adversarial Network Games (GANGs), which explicitly model a finite zero-sum
game between a generator ($G$) and classifier ($C$) that use mixed strategies.
The size of these games precludes exact solution methods, therefore we define
resource-bounded best responses (RBBRs), and a resource-bounded Nash
Equilibrium (RB-NE) as a pair of mixed strategies such that neither $G$ or $C$
can find a better RBBR. The RB-NE solution concept is richer than the notion of
`local Nash equilibria&apos; in that it captures not only failures of escaping local
optima of gradient descent, but applies to any approximate best response
computations, including methods with random restarts. To validate our approach,
we solve GANGs with the Parallel Nash Memory algorithm, which provably
monotonically converges to an RB-NE. We compare our results to standard GAN
setups, and demonstrate that our method deals well with typical GAN problems
such as mode collapse, partial mode coverage and forgetting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliehoek_F/0/1/0/all/0/1&quot;&gt;Frans A. Oliehoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Savani_R/0/1/0/all/0/1&quot;&gt;Rahul Savani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gallego_Posada_J/0/1/0/all/0/1&quot;&gt;Jose Gallego-Posada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pol_E/0/1/0/all/0/1&quot;&gt;Elise van der Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jong_E/0/1/0/all/0/1&quot;&gt;Edwin D. de Jong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gross_R/0/1/0/all/0/1&quot;&gt;Roderich Gross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05016">
<title>Deep Prior. (arXiv:1712.05016v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.05016</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent literature on deep learning offers new tools to learn a rich
probability distribution over high dimensional data such as images or sounds.
In this work we investigate the possibility of learning the prior distribution
over neural network parameters using such tools. Our resulting variational
Bayes algorithm generalizes well to new tasks, even when very few training
examples are provided. Furthermore, this learned prior allows the model to
extrapolate correctly far from a given task&apos;s training data on a meta-dataset
of periodic signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boquet_T/0/1/0/all/0/1&quot;&gt;Thomas Boquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rostamzadeh_N/0/1/0/all/0/1&quot;&gt;Negar Rostamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oreshkin_B/0/1/0/all/0/1&quot;&gt;Boris Oreshkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chung_W/0/1/0/all/0/1&quot;&gt;Wonchang Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;</dc:creator>
</item></rdf:RDF>