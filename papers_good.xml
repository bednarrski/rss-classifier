<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06092"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05976"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06030"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06084"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06272"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1702.05639">
<title>Deep Stochastic Configuration Networks with Universal Approximation Property. (arXiv:1702.05639v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05639</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops a randomized approach for incrementally building deep
neural networks, where a supervisory mechanism is proposed to constrain the
random assignment of the weights and biases, and all the hidden layers have
direct links to the output layer. A fundamental result on the universal
approximation property is established for such a class of randomized leaner
models, namely deep stochastic configuration networks (DeepSCNs). A learning
algorithm is presented to implement DeepSCNs with either specific architecture
or self-organization. The read-out weights attached with all direct links from
each hidden layer to the output layer are evaluated by the least squares
method. Given a set of training examples, DeepSCNs can speedily produce a
learning representation, that is, a collection of random basis functions with
the cascaded inputs together with the read-out weights. An empirical study on a
function approximation is carried out to demonstrate some properties of the
proposed deep learner model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dianhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06024">
<title>Deep Learning Reconstruction of Ultra-Short Pulses. (arXiv:1803.06024v1 [physics.optics])</title>
<link>http://arxiv.org/abs/1803.06024</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultra-short laser pulses with femtosecond to attosecond pulse duration are
the shortest systematic events humans can create. Characterization (amplitude
and phase) of these pulses is a key ingredient in ultrafast science, e.g.,
exploring chemical reactions and electronic phase transitions. Here, we propose
and demonstrate, numerically and experimentally, the first deep neural network
technique to reconstruct ultra-short optical pulses. We anticipate that this
approach will extend the range of ultrashort laser pulses that can be
characterized, e.g., enabling to diagnose very weak attosecond pulses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zahavy_T/0/1/0/all/0/1&quot;&gt;Tom Zahavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dikopoltsev_A/0/1/0/all/0/1&quot;&gt;Alex Dikopoltsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cohen_O/0/1/0/all/0/1&quot;&gt;Oren Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Segev_M/0/1/0/all/0/1&quot;&gt;Mordechai Segev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06064">
<title>A Meaning-based Statistical English Math Word Problem Solver. (arXiv:1803.06064v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.06064</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MeSys, a meaning-based approach to solving English math word
problems (MWPs) via understanding and reasoning in this paper. It first
analyzes the text, transforms both body and question parts into their
corresponding logic forms, and then performs inference on them. The associated
context of each quantity is represented with proposed role-tags (e.g., nsubj,
verb, etc.), which provides the flexibility for annotating an extracted math
quantity with its associated context information (i.e., the physical meaning of
this quantity). Statistical models are proposed to select the operator and
operands. A noisy dataset is designed to assess if a solver solves MWPs mainly
via understanding or pattern matching. Experimental results show that our
approach outperforms existing systems on both benchmark datasets and the noisy
dataset, which demonstrates that the proposed approach more understands the
meaning of each quantity in the text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chao-Chun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1&quot;&gt;Yu-Shiang Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi-Chung Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Keh-Yih Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06092">
<title>A dataset and architecture for visual reasoning with a working memory. (arXiv:1803.06092v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.06092</link>
<description rdf:parseType="Literal">&lt;p&gt;A vexing problem in artificial intelligence is reasoning about events that
occur in complex, changing visual stimuli such as in video analysis or game
play. Inspired by a rich tradition of visual reasoning and memory in cognitive
psychology and neuroscience, we developed an artificial, configurable visual
question and answer dataset (COG) to parallel experiments in humans and
animals. COG is much simpler than the general problem of video analysis, yet it
addresses many of the problems relating to visual and logical reasoning and
memory -- problems that remain challenging for modern deep learning
architectures. We additionally propose a deep learning architecture that
performs competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as
easy settings of the COG dataset. However, several settings of COG result in
datasets that are progressively more challenging to learn. After training, the
network can zero-shot generalize to many new tasks. Preliminary analyses of the
network architectures trained on COG demonstrate that the network accomplishes
the task in a manner interpretable to humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guangyu Robert Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganichev_I/0/1/0/all/0/1&quot;&gt;Igor Ganichev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao-Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1&quot;&gt;Jonathon Shlens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sussillo_D/0/1/0/all/0/1&quot;&gt;David Sussillo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05976">
<title>Deep Choice Model Using Pointer Networks for Airline Itinerary Prediction. (arXiv:1803.05976v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05976</link>
<description rdf:parseType="Literal">&lt;p&gt;Travel providers such as airlines and on-line travel agents are becoming more
and more interested in understanding how passengers choose among alternative
itineraries when searching for flights. This knowledge helps them better
display and adapt their offer, taking into account market conditions and
customer needs. Some common applications are not only filtering and sorting
alternatives, but also changing certain attributes in real-time (e.g., changing
the price). In this paper, we concentrate with the problem of modeling air
passenger choices of flight itineraries. This problem has historically been
tackled using classical Discrete Choice Modelling techniques. Traditional
statistical approaches, in particular the Multinomial Logit model (MNL), is
widely used in industrial applications due to its simplicity and general good
performance. However, MNL models present several shortcomings and assumptions
that might not hold in real applications. To overcome these difficulties, we
present a new choice model based on Pointer Networks. Given an input sequence,
this type of deep neural architecture combines Recurrent Neural Networks with
the Attention Mechanism to learn the conditional probability of an output whose
values correspond to positions in an input sequence. Therefore, given a
sequence of different alternatives presented to a customer, the model can learn
to point to the one most likely to be chosen by the customer. The proposed
method was evaluated on a real dataset that combines on-line user search logs
and airline flight bookings. Experimental results show that the proposed model
outperforms the traditional MNL model on several metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mottini_A/0/1/0/all/0/1&quot;&gt;Alejandro Mottini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Acuna_Agost_R/0/1/0/all/0/1&quot;&gt;Rodrigo Acuna-Agost&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06030">
<title>Estimation of lactate threshold with machine learning techniques in recreational runners. (arXiv:1803.06030v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.06030</link>
<description rdf:parseType="Literal">&lt;p&gt;Lactate threshold is considered an essential parameter when assessing
performance of elite and recreational runners and prescribing training
intensities in endurance sports. However, the measurement of blood lactate
concentration requires expensive equipment and the extraction of blood samples,
which are inconvenient for frequent monitoring. Furthermore, most recreational
runners do not have access to routine assessment of their physical fitness by
the aforementioned equipment so they are not able to calculate the lactate
threshold without resorting to an expensive and specialized centre. Therefore,
the main objective of this study is to create an intelligent system capable of
estimating the lactate threshold of recreational athletes participating in
endurance running sports. The solution here proposed is based on a machine
learning system which models the lactate evolution using recurrent neural
networks and includes the proposal of standardization of the temporal axis as
well as a modification of the stratified sampling method. The results show that
the proposed system accurately estimates the lactate threshold of 89.52% of the
athletes and its correlation with the experimentally measured lactate threshold
is very high (R=0,89). Moreover, its behaviour with the test dataset is as good
as with the training set, meaning that the generalization power of the model is
high. Therefore, in this study a machine learning based system is proposed as
alternative to the traditional invasive lactate threshold measurement tests for
recreational runners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Etxegarai_U/0/1/0/all/0/1&quot;&gt;Urtats Etxegarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Portillo_E/0/1/0/all/0/1&quot;&gt;Eva Portillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Irazusta_J/0/1/0/all/0/1&quot;&gt;Jon Irazusta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arriandiaga_A/0/1/0/all/0/1&quot;&gt;Ander Arriandiaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cabanes_I/0/1/0/all/0/1&quot;&gt;Itziar Cabanes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06084">
<title>A Kernel Theory of Modern Data Augmentation. (arXiv:1803.06084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06084</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation, a technique in which a training set is expanded with
class-preserving transformations, is ubiquitous in modern machine learning
pipelines. In this paper, we seek to establish a theoretical framework for
understanding modern data augmentation techniques. We start by showing that for
kernel classifiers, data augmentation can be approximated by first-order
feature averaging and second-order variance regularization components. We
connect this general approximation framework to prior work in invariant
kernels, tangent propagation, and robust optimization. Next, we explicitly
tackle the compositional aspect of modern data augmentation techniques,
proposing a novel model of data augmentation as a Markov process. Under this
model, we show that performing $k$-nearest neighbors with data augmentation is
asymptotically equivalent to a kernel classifier. Finally, we illustrate ways
in which our theoretical framework can be leveraged to accelerate machine
learning workflows in practice, including reducing the amount of computation
needed to train on augmented data, and predicting the utility of a
transformation prior to training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1&quot;&gt;Tri Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1&quot;&gt;Albert Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1&quot;&gt;Alexander J. Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1&quot;&gt;Virginia Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06272">
<title>Graph Partition Neural Networks for Semi-Supervised Classification. (arXiv:1803.06272v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06272</link>
<description rdf:parseType="Literal">&lt;p&gt;We present graph partition neural networks (GPNN), an extension of graph
neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate
between locally propagating information between nodes in small subgraphs and
globally propagating information between the subgraphs. To efficiently
partition graphs, we experiment with several partitioning algorithms and also
propose a novel variant for fast processing of large scale graphs. We
extensively test our model on a variety of semi-supervised node classification
tasks. Experimental results indicate that GPNNs are either superior or
comparable to state-of-the-art methods on a wide variety of datasets for
graph-based semi-supervised classification. We also show that GPNNs can achieve
similar performance as standard GNNs with fewer propagation steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1&quot;&gt;Marc Brockschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarlow_D/0/1/0/all/0/1&quot;&gt;Daniel Tarlow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaunt_A/0/1/0/all/0/1&quot;&gt;Alexander L. Gaunt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item></rdf:RDF>