<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07426"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07384"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07442"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.03430"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07301"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07569"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07714"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.09458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05799"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01012"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.07426">
<title>Generalization in Machine Learning via Analytical Learning Theory. (arXiv:1802.07426v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07426</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel measure-theoretic learning theory to analyze
generalization behaviors of practical interest. The proposed learning theory
has the following abilities: 1) to utilize the qualities of each learned
representation on the path from raw inputs to outputs in representation
learning, 2) to guarantee good generalization errors possibly with arbitrarily
rich hypothesis spaces (e.g., arbitrarily large capacity and Rademacher
complexity) and non-stable/non-robust learning algorithms, and 3) to clearly
distinguish each individual problem instance from each other. Our
generalization bounds are relative to a representation of the data, and hold
true even if the representation is learned. We discuss several consequences of
our results on deep learning, one-shot learning and curriculum learning. Unlike
statistical learning theory, the proposed learning theory analyzes each problem
instance individually via measure theory, rather than a set of problem
instances via statistics. Because of the differences in the assumptions and the
objectives, the proposed learning theory is meant to be complementary to
previous learning theory and is not designed to compete with it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07384">
<title>Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections. (arXiv:1802.07384v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07384</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper describes a new algorithm to generate minimal, stable, and symbolic
corrections to an input that will cause a neural network with ReLU neurons to
change its output. We argue that such a correction is a useful way to provide
feedback to a user when the neural network produces an output that is different
from a desired output. Our algorithm generates such a correction by solving a
series of linear constraint satisfaction problems. The technique is evaluated
on a neural network that has been trained to predict whether an applicant will
pay a mortgage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1&quot;&gt;Armando Solar-Lezama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07442">
<title>Learning to Play with Intrinsically-Motivated Self-Aware Agents. (arXiv:1802.07442v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07442</link>
<description rdf:parseType="Literal">&lt;p&gt;Infants are experts at playing, with an amazing ability to generate novel
structured behaviors in unstructured environments that lack clear extrinsic
reward signals. We seek to mathematically formalize these abilities using a
neural network that implements curiosity-driven intrinsic motivation. Using a
simple but ecologically naturalistic simulated environment in which an agent
can move and interact with objects it sees, we propose a &quot;world-model&quot; network
that learns to predict the dynamic consequences of the agent&apos;s actions.
Simultaneously, we train a separate explicit &quot;self-model&quot; that allows the agent
to track the error map of its own world-model, and then uses the self-model to
adversarially challenge the developing world-model. We demonstrate that this
policy causes the agent to explore novel and informative interactions with its
environment, leading to the generation of a spectrum of complex behaviors,
including ego-motion prediction, object attention, and object gathering.
Moreover, the world-model that the agent learns supports improved performance
on object dynamics prediction, detection, localization and recognition tasks.
Taken together, our results are initial steps toward creating flexible
autonomous agents that self-supervise in complex novel physical environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1&quot;&gt;Nick Haber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mrowca_D/0/1/0/all/0/1&quot;&gt;Damian Mrowca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1&quot;&gt;Daniel L. K. Yamins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07623">
<title>Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives. (arXiv:1802.07623v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.07623</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a novel method that provides contrastive
explanations justifying the classification of an input by a black box
classifier such as a deep neural network. Given an input we find what should be
minimally and sufficiently present (viz. important object pixels in an image)
to justify its classification and analogously what should be minimally and
necessarily \emph{absent} (viz. certain background pixels). We argue that such
explanations are natural for humans and are used commonly in domains such as
health care and criminology. What is minimally but critically \emph{absent} is
an important part of an explanation, which to the best of our knowledge, has
not been touched upon by current explanation methods that attempt to explain
predictions of neural networks. We validate our approach on three real datasets
obtained from diverse domains; namely, a handwritten digits dataset MNIST, a
large procurement fraud dataset and an fMRI brain imaging dataset. In all three
cases, we witness the power of our approach in generating precise explanations
that are also easy for human experts to understand and evaluate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1&quot;&gt;Amit Dhurandhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1&quot;&gt;Ronny Luss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Chun-Chen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting_P/0/1/0/all/0/1&quot;&gt;Paishun Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Payel Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.06207">
<title>Cooperating with Machines. (arXiv:1703.06207v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06207</link>
<description rdf:parseType="Literal">&lt;p&gt;Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major
driving force behind technical progress has been competition with human
cognition. Historical milestones have been frequently associated with computers
matching or outperforming humans in difficult cognitive tasks (e.g. face
recognition [2], personality classification [3], driving cars [4], or playing
video games [5]), or defeating humans in strategic zero-sum encounters (e.g.
Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast,
less attention has been given to developing autonomous machines that establish
mutually cooperative relationships with people who may not share the machine&apos;s
preferences. A main challenge has been that human cooperation does not require
sheer computational power, but rather relies on intuition [11], cultural norms
[12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions
toward cooperation [17], common-sense mechanisms that are difficult to encode
in machines for arbitrary contexts. Here, we combine a state-of-the-art
machine-learning algorithm with novel mechanisms for generating and acting on
signals to produce a new learning algorithm that cooperates with people and
other machines at levels that rival human cooperation in a variety of
two-player repeated stochastic games. This is the first general-purpose
algorithm that is capable, given a description of a previously unseen game
environment, of learning to cooperate with people within short timescales in
scenarios previously unanticipated by algorithm designers. This is achieved
without complex opponent modeling or higher-order theories of mind, thus
showing that flexible, fast, and general human-machine cooperation is
computationally achievable using a non-trivial, but ultimately simple, set of
algorithmic mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_J/0/1/0/all/0/1&quot;&gt;Jacob W. Crandall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudah_M/0/1/0/all/0/1&quot;&gt;Mayada Oudah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennom/0/1/0/all/0/1&quot;&gt;Tennom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishowo_Oloko_F/0/1/0/all/0/1&quot;&gt;Fatimah Ishowo-Oloko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdallah_S/0/1/0/all/0/1&quot;&gt;Sherief Abdallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnefon_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Bonnefon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cebrian_M/0/1/0/all/0/1&quot;&gt;Manuel Cebrian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariff_A/0/1/0/all/0/1&quot;&gt;Azim Shariff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodrich_M/0/1/0/all/0/1&quot;&gt;Michael A. Goodrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahwan_I/0/1/0/all/0/1&quot;&gt;Iyad Rahwan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.03430">
<title>Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering. (arXiv:1710.03430v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1710.03430</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel end-to-end neural architecture for ranking
candidate answers, that adapts a hierarchical recurrent neural network and a
latent topic clustering module. With our proposed model, a text is encoded to a
vector representation from an word-level to a chunk-level to effectively
capture the entire meaning. In particular, by adapting the hierarchical
structure, our model shows very small performance degradations in longer text
comprehension while other state-of-the-art recurrent neural network models
suffer from it. Additionally, the latent topic clustering module extracts
semantic information from target samples. This clustering module is useful for
any text related tasks by allowing each data sample to find its nearest topic
cluster, thus helping the neural network model analyze the entire data. We
evaluate our models on the Ubuntu Dialogue Corpus and consumer electronic
domain question answering dataset, which is related to Samsung products. The
proposed model shows state-of-the-art results for ranking question-answer
pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Seunghyun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Joongbo Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kyomin Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03544">
<title>To the problem of &quot;The Instrumental complex for ontological engineering purpose&quot; software system design. (arXiv:1802.03544v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03544</link>
<description rdf:parseType="Literal">&lt;p&gt;The given work describes methodological principles of design instrumental
complex of ontological purpose. Instrumental complex intends for the
implementation of the integrated information technologies automated build of
domain ontologies. Results focus on enhancing the effectiveness of the
automatic analysis and understanding of natural-language texts, building of
knowledge description of subject areas (primarily in the area of science and
technology) and for interdisciplinary research in conjunction with the solution
of complex problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palagin_A/0/1/0/all/0/1&quot;&gt;A.V. Palagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrenko_N/0/1/0/all/0/1&quot;&gt;N.G. Petrenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velychko_V/0/1/0/all/0/1&quot;&gt;V.Yu. Velychko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1&quot;&gt;K.S. Malakhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhonov_Y/0/1/0/all/0/1&quot;&gt;Yu.L. Tikhonov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07301">
<title>On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition. (arXiv:1802.07301v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07301</link>
<description rdf:parseType="Literal">&lt;p&gt;We establish connections between the problem of learning a two-layers neural
network with good generalization error and tensor decomposition. We consider a
model with input $\boldsymbol x \in \mathbb R^d$, $r$ hidden units with weights
$\{\boldsymbol w_i\}_{1\le i \le r}$ and output $y\in \mathbb R$, i.e.,
$y=\sum_{i=1}^r \sigma(\left \langle \boldsymbol x, \boldsymbol w_i\right
\rangle)$, where $\sigma$ denotes the activation function.
&lt;/p&gt;
&lt;p&gt;First, we show that, if we cannot learn the weights $\{\boldsymbol
w_i\}_{1\le i \le r}$ accurately, then the neural network does not generalize
well. More specifically, the generalization error is close to that of a trivial
predictor with access only to the norm of the input. This result holds for any
activation function, and it requires that the weights are roughly isotropic and
the input distribution is Gaussian, which is a typical assumption in the
theoretical literature. Then, we show that the problem of learning the weights
$\{\boldsymbol w_i\}_{1\le i \le r}$ is at least as hard as the problem of
tensor decomposition. This result holds for any input distribution and assumes
that the activation function is a polynomial whose degree is related to the
order of the tensor to be decomposed. By putting everything together, we prove
that learning a two-layers neural network that generalizes well is at least as
hard as tensor decomposition. It has been observed that neural network models
with more parameters than training samples often generalize well, even if the
problem is highly underdetermined. This means that the learning algorithm does
not estimate the weights accurately and yet is able to yield a good
generalization error. This paper shows that such a phenomenon cannot occur when
the input distribution is Gaussian and the weights are roughly isotropic. We
also provide numerical evidence supporting our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondelli_M/0/1/0/all/0/1&quot;&gt;Marco Mondelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Andrea Montanari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07369">
<title>On the Statistical Challenges of Echo State Networks and Some Potential Remedies. (arXiv:1802.07369v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07369</link>
<description rdf:parseType="Literal">&lt;p&gt;Echo state networks are powerful recurrent neural networks. However, they are
often unstable and shaky, making the process of finding an good ESN for a
specific dataset quite hard. Obtaining a superb accuracy by using the Echo
State Network is a challenging task. We create, develop and implement a family
of predictably optimal robust and stable ensemble of Echo State Networks via
regularizing the training and perturbing the input. Furthermore, several
distributions of weights have been tried based on the shape to see if the shape
of the distribution has the impact for reducing the error. We found ESN can
track in short term for most dataset, but it collapses in the long run.
Short-term tracking with large size reservoir enables ESN to perform strikingly
with superior prediction. Based on this scenario, we go a further step to
aggregate many of ESNs into an ensemble to lower the variance and stabilize the
system by stochastic replications and bootstrapping of input data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiuyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fokoue_E/0/1/0/all/0/1&quot;&gt;Ernest Fokoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kudithipudi_D/0/1/0/all/0/1&quot;&gt;Dhireesha Kudithipudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07401">
<title>A Study into the similarity in generator and discriminator in GAN architecture. (arXiv:1802.07401v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07401</link>
<description rdf:parseType="Literal">&lt;p&gt;One popular generative model that has high-quality results is the Generative
Adversarial Networks(GAN). This type of architecture consists of two separate
networks that play against each other. The generator creates an output from the
input noise that is given to it. The discriminator has the task of determining
if the input to it is real or fake. This takes place constantly eventually
leads to the generator modeling the target distribution. This paper includes a
study into the actual weights learned by the network and a study into the
similarity of the discriminator and generator networks. The paper also tries to
leverage the similarity between these networks and shows that indeed both the
networks may have a similar structure with experimental evidence with a novel
shared architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karuvally_A/0/1/0/all/0/1&quot;&gt;Arjun Karuvally&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07535">
<title>A Generative Deep Recurrent Model for Exchangeable Data. (arXiv:1802.07535v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07535</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel model architecture which leverages deep learning tools to
perform exact Bayesian inference on sets of high dimensional, complex
observations. Our model is provably exchangeable, meaning that the joint
distribution over observations is invariant under permutation: this property
lies at the heart of Bayesian inference. The model does not require variational
approximations to train, and new samples can be generated conditional on
previous samples, with cost linear in the size of the conditioning set. The
advantages of our architecture are demonstrated on learning tasks requiring
generalisation from short observed sequences while modelling sequence
variability, such as conditional image generation, few-shot learning, set
completion, and anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Korshunova_I/0/1/0/all/0/1&quot;&gt;Iryna Korshunova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Degrave_J/0/1/0/all/0/1&quot;&gt;Jonas Degrave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huszar_F/0/1/0/all/0/1&quot;&gt;Ferenc Husz&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yarin Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dambre_J/0/1/0/all/0/1&quot;&gt;Joni Dambre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07543">
<title>The Many Faces of Exponential Weights in Online Learning. (arXiv:1802.07543v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07543</link>
<description rdf:parseType="Literal">&lt;p&gt;A standard introduction to online learning might place Online Gradient
Descent at its center and then proceed to develop generalizations and
extensions like Online Mirror Descent and second-order methods. Here we explore
the alternative approach of putting exponential weights (EW) first. We show
that many standard methods and their regret bounds then follow as a special
case by plugging in suitable surrogate losses and playing the EW posterior
mean. For instance, we easily recover Online Gradient Descent by using EW with
a Gaussian prior on linearized losses, and, more generally, all instances of
Online Mirror Descent based on regular Bregman divergences also correspond to
EW with a prior that depends on the mirror map. Furthermore, appropriate
quadratic surrogate losses naturally give rise to Online Gradient Descent for
strongly convex losses and to Online Newton Step. We further interpret several
recent adaptive methods (iProd, Squint, and a variation of Coin Betting for
experts) as a series of closely related reductions to exp-concave surrogate
losses that are then handled by Exponential Weights. Finally, a benefit of our
EW interpretation is that it opens up the possibility of sampling from the EW
posterior distribution instead of playing the mean. As already observed by
Bubeck and Eldan, this recovers the best-known rate in Online Bandit Linear
Optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoeven_D/0/1/0/all/0/1&quot;&gt;Dirk van der Hoeven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Erven_T/0/1/0/all/0/1&quot;&gt;Tim van Erven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kotlowski_W/0/1/0/all/0/1&quot;&gt;Wojciech Kot&amp;#x142;owski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07569">
<title>Continual Lifelong Learning with Neural Networks: A Review. (arXiv:1802.07569v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07569</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans and animals have the ability to continually acquire and fine-tune
knowledge throughout their lifespan. This ability is mediated by a rich set of
neurocognitive functions that together contribute to the early development and
experience-driven specialization of our sensorimotor skills. Consequently, the
ability to learn from continuous streams of information is crucial for
computational learning systems and autonomous agents (inter)acting in the real
world. However, continual lifelong learning remains a long-standing challenge
for machine learning and neural network models since the incremental
acquisition of new skills from non-stationary data distributions generally
leads to catastrophic forgetting or interference. This limitation represents a
major drawback also for state-of-the-art deep neural network models that
typically learn representations from stationary batches of training data, thus
without accounting for situations in which the number of tasks is not known a
priori and the information becomes incrementally available over time. In this
review, we critically summarize the main challenges linked to continual
lifelong learning for artificial learning systems and compare existing neural
network approaches that alleviate, to different extents, catastrophic
interference. Although significant advances have been made in domain-specific
continual lifelong learning with neural networks, extensive research efforts
are required for the development of general-purpose artificial intelligence and
autonomous agents. We discuss well-established research and recent
methodological trends motivated by experimentally observed lifelong learning
factors in biological systems. Such factors include principles of neurosynaptic
stability-plasticity, critical developmental stages, intrinsically motivated
exploration, transfer learning, and crossmodal integration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisi_G/0/1/0/all/0/1&quot;&gt;German I. Parisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemker_R/0/1/0/all/0/1&quot;&gt;Ronald Kemker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Part_J/0/1/0/all/0/1&quot;&gt;Jose L. Part&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07714">
<title>Detecting Learning vs Memorization in Deep Neural Networks using Shared Structure Validation Sets. (arXiv:1802.07714v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07714</link>
<description rdf:parseType="Literal">&lt;p&gt;The roles played by learning and memorization represent an important topic in
deep learning research. Recent work on this subject has shown that the
optimization behavior of DNNs trained on shuffled labels is qualitatively
different from DNNs trained with real labels. Here, we propose a novel
permutation approach that can differentiate memorization from learning in deep
neural networks (DNNs) trained as usual (i.e., using the real labels to guide
the learning, rather than shuffled labels). The evaluation of weather the DNN
has learned and/or memorized, happens in a separate step where we compare the
predictive performance of a shallow classifier trained with the features
learned by the DNN, against multiple instances of the same classifier, trained
on the same input, but using shuffled labels as outputs. By evaluating these
shallow classifiers in validation sets that share structure with the training
set, we are able to tell apart learning from memorization. Application of our
permutation approach to multi-layer perceptrons and convolutional neural
networks trained on image data corroborated many findings from other groups.
Most importantly, our illustrations also uncovered interesting dynamic patterns
about how DNNs memorize over increasing numbers of training epochs, and support
the surprising result that DNNs are still able to learn, rather than only
memorize, when trained with pure Gaussian noise as input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neto_E/0/1/0/all/0/1&quot;&gt;Elias Chaibub Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.09458">
<title>Vote-boosting ensembles. (arXiv:1606.09458v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1606.09458</link>
<description rdf:parseType="Literal">&lt;p&gt;Vote-boosting is a sequential ensemble learning method in which the
individual classifiers are built on different weighted versions of the training
data. To build a new classifier, the weight of each training instance is
determined in terms of the degree of disagreement among the current ensemble
predictions for that instance. For low class-label noise levels, especially
when simple base learners are used, emphasis should be made on instances for
which the disagreement rate is high. When more flexible classifiers are used
and as the noise level increases, the emphasis on these uncertain instances
should be reduced. In fact, at sufficiently high levels of class-label noise,
the focus should be on instances on which the ensemble classifiers agree. The
optimal type of emphasis can be automatically determined using
cross-validation. An extensive empirical analysis using the beta distribution
as emphasis function illustrates that vote-boosting is an effective method to
generate ensembles that are both accurate and robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabzevari_M/0/1/0/all/0/1&quot;&gt;Maryam Sabzevari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Munoz_G/0/1/0/all/0/1&quot;&gt;Gonzalo Mart&amp;#xed;nez-Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suarez_A/0/1/0/all/0/1&quot;&gt;Alberto Su&amp;#xe1;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00489">
<title>Active Learning for Convolutional Neural Networks: A Core-Set Approach. (arXiv:1708.00489v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00489</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have been successfully applied to many
recognition and learning tasks using a universal recipe; training a deep model
on a very large dataset of supervised examples. However, this approach is
rather restrictive in practice since collecting a large set of labeled images
is very expensive. One way to ease this problem is coming up with smart ways
for choosing images to be labelled from a very large collection (ie. active
learning).
&lt;/p&gt;
&lt;p&gt;Our empirical study suggests that many of the active learning heuristics in
the literature are not effective when applied to CNNs in batch setting.
Inspired by these limitations, we define the problem of active learning as
core-set selection, ie. choosing set of points such that a model learned over
the selected subset is competitive for the remaining data points. We further
present a theoretical result characterizing the performance of any selected
subset using the geometry of the datapoints. As an active learning algorithm,
we choose the subset which is expected to yield best result according to our
characterization. Our experiments show that the proposed method significantly
outperforms existing approaches in image classification experiments by a large
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sener_O/0/1/0/all/0/1&quot;&gt;Ozan Sener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08446">
<title>Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step. (arXiv:1710.08446v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08446</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) are a family of generative models that
do not minimize a single training criterion. Unlike other generative models,
the data distribution is learned via a game between a generator (the generative
model) and a discriminator (a teacher providing training signal) that each
minimize their own cost. GANs are designed to reach a Nash equilibrium at which
each player cannot reduce their cost without changing the other players&apos;
parameters. One useful approach for the theory of GANs is to show that a
divergence between the training distribution and the model distribution obtains
its minimum value at equilibrium. Several recent research directions have been
motivated by the idea that this divergence is the primary guide for the
learning process and that every step of learning should decrease the
divergence. We show that this view is overly restrictive. During GAN training,
the discriminator provides learning signal in situations where the gradients of
the divergences between distributions would not be useful. We provide empirical
counterexamples to the view of GAN training as divergence minimization.
Specifically, we demonstrate that GANs are able to learn distributions in
situations where the divergence minimization point of view predicts they would
fail. We also show that gradient penalties motivated from the divergence
minimization perspective are equally helpful when applied in other contexts in
which the divergence minimization perspective does not predict they would be
helpful. This contributes to a growing body of evidence that GAN training may
be more usefully viewed as approaching Nash equilibria via trajectories that do
not necessarily minimize a specific divergence at each step.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedus_W/0/1/0/all/0/1&quot;&gt;William Fedus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1&quot;&gt;Mihaela Rosca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lakshminarayanan_B/0/1/0/all/0/1&quot;&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Shakir Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05799">
<title>Horovod: fast and easy distributed deep learning in TensorFlow. (arXiv:1802.05799v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05799</link>
<description rdf:parseType="Literal">&lt;p&gt;Training modern deep learning models requires large amounts of computation,
often provided by GPUs. Scaling computation from one GPU to many can enable
much faster training and research progress but entails two complications.
First, the training library must support inter-GPU communication. Depending on
the particular methods employed, this communication may entail anywhere from
negligible to significant overhead. Second, the user must modify his or her
training code to take advantage of inter-GPU communication. Depending on the
training library&apos;s API, the modification required may be either significant or
minimal.
&lt;/p&gt;
&lt;p&gt;Existing methods for enabling multi-GPU training under the TensorFlow library
entail non-negligible communication overhead and require users to heavily
modify their model-building code, leading many researchers to avoid the whole
mess and stick with slower single-GPU training. In this paper we introduce
Horovod, an open source library that improves on both obstructions to scaling:
it employs efficient inter-GPU communication via ring reduction and requires
only a few lines of modification to user code, enabling faster, easier
distributed training in TensorFlow. Horovod is available under the Apache 2.0
license at https://github.com/uber/horovod
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sergeev_A/0/1/0/all/0/1&quot;&gt;Alexander Sergeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balso_M/0/1/0/all/0/1&quot;&gt;Mike Del Balso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07124">
<title>Out-distribution training confers robustness to deep neural networks. (arXiv:1802.07124v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07124</link>
<description rdf:parseType="Literal">&lt;p&gt;The easiness at which adversarial instances can be generated in deep neural
networks raises some fundamental questions on their functioning and concerns on
their use in critical systems. In this paper, we draw a connection between
over-generalization and adversaries: a possible cause of adversaries lies in
models designed to make decisions all over the input space, leading to
inappropriate high-confidence decisions in parts of the input space not
represented in the training set. We empirically show an augmented neural
network, which is not trained on any types of adversaries, can increase the
robustness by detecting black-box one-step adversaries, i.e. assimilated to
out-distribution samples, and making generation of white-box one-step
adversaries harder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_M/0/1/0/all/0/1&quot;&gt;Mahdieh Abbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Christian Gagn&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01012">
<title>On the convergence properties of a $K$-step averaging stochastic gradient descent algorithm for nonconvex optimization. (arXiv:1708.01012v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1708.01012</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their popularity, the practical performance of asynchronous
stochastic gradient descent methods (ASGD) for solving large scale machine
learning problems are not as good as theoretical results indicate. We adopt and
analyze a synchronous K-step averaging stochastic gradient descent algorithm
which we call K-AVG. We establish the convergence results of K-AVG for
nonconvex objectives and explain why the K-step delay is necessary and leads to
better performance than traditional parallel stochastic gradient descent which
is a special case of K-AVG with $K=1$. We also show that K-AVG scales better
than ASGD. Another advantage of K-AVG over ASGD is that it allows larger
stepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD
implementations and achieves better accuracies and faster convergence for
\cifar dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1&quot;&gt;Guojing Cong&lt;/a&gt;</dc:creator>
</item></rdf:RDF>