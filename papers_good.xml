<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08853"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07134"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07147"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07255"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07281"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07490"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07545"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07187"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07282"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07291"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.07463"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05565"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.01662"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06576"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1709.08853">
<title>Object-oriented Neural Programming (OONP) for Document Understanding. (arXiv:1709.08853v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08853</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Object-oriented Neural Programming (OONP), a framework for
semantically parsing documents in specific domains. Basically, OONP reads a
document and parses it into a predesigned object-oriented data structure
(referred to as ontology in this paper) that reflects the domain-specific
semantics of the document. An OONP parser models semantic parsing as a decision
process: a neural net-based Reader sequentially goes through the document, and
during the process it builds and updates an intermediate ontology to summarize
its partial understanding of the text it covers. OONP supports a rich family of
operations (both symbolic and differentiable) for composing the ontology, and a
big variety of forms (both symbolic and differentiable) for representing the
state and the document. An OONP parser can be trained with supervision of
different forms and strength, including supervised learning (SL) ,
reinforcement learning (RL) and hybrid of the two. Our experiments on both
synthetic and real-world document parsing tasks have shown that OONP can learn
to handle fairly complicated ontology with training data of modest sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhengdong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Haotian Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianggen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yukun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Daqi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06687">
<title>A Directionally Selective Small Target Motion Detecting Visual Neural Network in Cluttered Backgrounds. (arXiv:1801.06687v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06687</link>
<description rdf:parseType="Literal">&lt;p&gt;Discriminating targets moving against a cluttered background is a huge
challenge, let alone detecting a target as small as one or a few pixels and
tracking it in flight. In the fly&apos;s visual system, a class of specific neurons,
called small target motion detectors (STMDs), have been identified as showing
exquisite selectivity for small target motion. Some of the STMDs have also
demonstrated directional selectivity which means these STMDs respond strongly
only to their preferred motion direction. Directional selectivity is an
important property of these STMD neurons which could contribute to tracking
small targets such as mates in flight. However, little has been done on
systematically modeling these directional selective STMD neurons. In this
paper, we propose a directional selective STMD-based neural network (DSTMD) for
small target detection in a cluttered background. In the proposed neural
network, a new correlation mechanism is introduced for direction selectivity
via correlating signals relayed from two pixels. Then, a lateral inhibition
mechanism is implemented on the spatial field for size selectivity of STMD
neurons. Extensive experiments showed that the proposed neural network not only
is in accord with current biological findings, i.e. showing directional
preferences, but also worked reliably in detecting small targets against
cluttered backgrounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jigen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1&quot;&gt;Shigang Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06446">
<title>Self-Attentive Neural Collaborative Filtering. (arXiv:1806.06446v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06446</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper has been withdrawn as we discovered a bug in our tensorflow
implementation that involved accidental mixing of vectors across batches. This
lead to different inference results given different batch sizes which is
completely strange. The performance scores still remain the same but we
concluded that it was not the self-attention that contributed to the
performance. We are withdrawing the paper because this renders the main claim
of the paper false. Thanks to Guan Xinyu from NUS for discovering this issue in
our previously open source code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siu Cheung Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07134">
<title>Representational efficiency outweighs action efficiency in human program induction. (arXiv:1807.07134v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.07134</link>
<description rdf:parseType="Literal">&lt;p&gt;The importance of hierarchically structured representations for tractable
planning has long been acknowledged. However, the questions of how people
discover such abstractions and how to define a set of optimal abstractions
remain open. This problem has been explored in cognitive science in the problem
solving literature and in computer science in hierarchical reinforcement
learning. Here, we emphasize an algorithmic perspective on learning
hierarchical representations in which the objective is to efficiently encode
the structure of the problem, or, equivalently, to learn an algorithm with
minimal length. We introduce a novel problem-solving paradigm that links
problem solving and program induction under the Markov Decision Process (MDP)
framework. Using this task, we target the question of whether humans discover
hierarchical solutions by maximizing efficiency in number of actions they
generate or by minimizing the complexity of the resulting representation and
find evidence for the primacy of representational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanborn_S/0/1/0/all/0/1&quot;&gt;Sophia Sanborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bourgin_D/0/1/0/all/0/1&quot;&gt;David D. Bourgin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1&quot;&gt;Michael Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07147">
<title>Guess who? Multilingual approach for the automated generation of author-stylized poetry. (arXiv:1807.07147v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.07147</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of stylized text generation in a
multilingual setup. A version of a language model based on a long short-term
memory (LSTM) artificial neural network with extended phonetic and semantic
embeddings is used for stylized poetry generation. Phonetics is shown to have
comparable importance for the task of stylized poetry generation as the
information on the target author. The quality of the resulting poems generated
by the network is estimated through bilingual evaluation understudy (BLEU), a
survey and a new cross-entropy based metric that is suggested for the problems
of such type. The experiments show that the proposed model consistently
outperforms random sample and vanilla-LSTM baselines, humans also tend to
attribute machine generated texts to the target author.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1&quot;&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1&quot;&gt;Ivan P. Yamshchikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07186">
<title>Evaluating Word Embeddings in Multi-label Classification Using Fine-grained Name Typing. (arXiv:1807.07186v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.07186</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding models typically associate each word with a single real-valued
vector, representing its different properties. Evaluation methods, therefore,
need to analyze the accuracy and completeness of these properties in
embeddings. This requires fine-grained analysis of embedding subspaces.
Multi-label classification is an appropriate way to do so. We propose a new
evaluation method for word embeddings based on multi-label classification given
a word embedding. The task we use is fine-grained name typing: given a large
corpus, find all types that a name can refer to based on the name embedding.
Given the scale of entities in knowledge bases, we can build datasets for this
task that are complementary to the current embedding evaluation datasets in:
they are very large, contain fine-grained classes, and allow the direct
evaluation of embeddings without confounding factors like sentence context
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1&quot;&gt;Yadollah Yaghoobzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1&quot;&gt;Katharina Kann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1&quot;&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07255">
<title>Towards Explainable and Controllable Open Domain Dialogue Generation with Dialogue Acts. (arXiv:1807.07255v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.07255</link>
<description rdf:parseType="Literal">&lt;p&gt;We study open domain dialogue generation with dialogue acts designed to
explain how people engage in social chat. To imitate human behavior, we propose
managing the flow of human-machine interactions with the dialogue acts as
policies. The policies and response generation are jointly learned from
human-human conversations, and the former is further optimized with a
reinforcement learning approach. With the dialogue acts, we achieve significant
improvement over state-of-the-art methods on response quality for given
contexts and dialogue length in both machine-machine simulation and
human-machine conversation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Can Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07281">
<title>ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech. (arXiv:1807.07281v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.07281</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an alternative solution for parallel wave generation
by WaveNet. In contrast to parallel WaveNet (Oord et al., 2018), we distill a
Gaussian inverse autoregressive flow from the autoregressive WaveNet by
minimizing a novel regularized KL divergence between their highly-peaked output
distributions. Our method computes the KL divergence in closed-form, which
simplifies the training algorithm and provides very efficient distillation. In
addition, we propose the first text-to-wave neural architecture for speech
synthesis, which is fully convolutional and enables fast end-to-end training
from scratch. It significantly outperforms the previous pipeline that connects
a text-to-spectrogram model to a separately trained WaveNet (Ping et al.,
2017). We also successfully distill a parallel waveform synthesizer conditioned
on the hidden representation in this end-to-end model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kainan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jitong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07389">
<title>Fuzzy quantification for linguistic data analysis and data mining. (arXiv:1807.07389v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.07389</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzy quantification is a subtopic of fuzzy logic which deals with the
modelling of the quantified expressions we can find in natural language. Fuzzy
quantifiers have been successfully applied in several fields like fuzzy,
control, fuzzy databases, information retrieval, natural language generation,
etc. Their ability to model and evaluate linguistic expressions in a
mathematical way, makes fuzzy quantifiers very powerful for data analytics and
data mining applications. In this paper we will give a general overview of the
main applications of fuzzy quantifiers in this field as well as some ideas to
use them in new application contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_Hermida_F/0/1/0/all/0/1&quot;&gt;F. D&amp;#xed;az-Hermida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1&quot;&gt;Juan. C. Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07490">
<title>FuzzerGym: A Competitive Framework for Fuzzing and Learning. (arXiv:1807.07490v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1807.07490</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzing is a commonly used technique designed to test software by
automatically crafting program inputs. Currently, the most successful fuzzing
algorithms emphasize simple, low-overhead strategies with the ability to
efficiently monitor program state during execution. Through compile-time
instrumentation, these approaches have access to numerous aspects of program
state including coverage, data flow, and heterogeneous fault detection and
classification. However, existing approaches utilize blind random mutation
strategies when generating test inputs. We present a different approach that
uses this state information to optimize mutation operators using reinforcement
learning (RL). By integrating OpenAI Gym with libFuzzer we are able to
simultaneously leverage advancements in reinforcement learning as well as
fuzzing to achieve deeper coverage across several varied benchmarks. Our
technique connects the rich, efficient program monitors provided by LLVM
Santizers with a deep neural net to learn mutation selection strategies
directly from the input data. The cross-language, asynchronous architecture we
developed enables us to apply any OpenAI Gym compatible deep reinforcement
learning algorithm to any fuzzing problem with minimal slowdown.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drozd_W/0/1/0/all/0/1&quot;&gt;William Drozd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Michael D. Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07530">
<title>Self-Organizing Maps as a Storage and Transfer Mechanism in Reinforcement Learning. (arXiv:1807.07530v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.07530</link>
<description rdf:parseType="Literal">&lt;p&gt;The idea of reusing information from previously learned tasks (source tasks)
for the learning of new tasks (target tasks) has the potential to significantly
improve the sample efficiency reinforcement learning agents. In this work, we
describe an approach to concisely store and represent learned task knowledge,
and reuse it by allowing it to guide the exploration of an agent while it
learns new tasks. In order to do so, we use a measure of similarity that is
defined directly in the space of parameterized representations of the value
functions. This similarity measure is also used as a basis for a variant of the
growing self-organizing map algorithm, which is simultaneously used to enable
the storage of previously acquired task knowledge in an adaptive and scalable
manner.We empirically validate our approach in a simulated navigation
environment and discuss possible extensions to this approach along with
potential applications where it could be particularly useful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimpanal_T/0/1/0/all/0/1&quot;&gt;Thommen George Karimpanal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouffanais_R/0/1/0/all/0/1&quot;&gt;Roland Bouffanais&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07545">
<title>Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks. (arXiv:1807.07545v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1807.07545</link>
<description rdf:parseType="Literal">&lt;p&gt;Systematic compositionality is the ability to recombine meaningful units with
regular and predictable outcomes, and it&apos;s seen as key to humans&apos; capacity for
generalization in language. Recent work has studied systematic compositionality
in modern seq2seq models using generalization to novel navigation instructions
in a grounded environment as a probing tool, requiring models to quickly
bootstrap the meaning of new words. We extend this framework here to settings
where the model needs only to recombine well-trained functional words (such as
&quot;around&quot; and &quot;right&quot;) in novel contexts. Our findings confirm and strengthen
the earlier ones: seq2seq models can be impressively good at generalizing to
novel combinations of previously-seen input, but only when they receive
extensive training on the specific pattern to be generalized (e.g.,
generalizing from many examples of &quot;X around right&quot; to &quot;jump around right&quot;),
while failing when generalization requires novel application of compositional
rules (e.g., inferring the meaning of &quot;around right&quot; from those of &quot;right&quot; and
&quot;around&quot;).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loula_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Loula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1&quot;&gt;Marco Baroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1&quot;&gt;Brenden M. Lake&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07187">
<title>Efficient Training on Very Large Corpora via Gramian Estimation. (arXiv:1807.07187v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07187</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning similarity functions over very large corpora
using neural network embedding models. These models are typically trained using
SGD with sampling of random observed and unobserved pairs, with a number of
samples that grows quadratically with the corpus size, making it expensive to
scale to very large corpora. We propose new efficient methods to train these
models without having to sample unobserved pairs. Inspired by matrix
factorization, our approach relies on adding a global quadratic penalty to all
pairs of examples and expressing this term as the matrix-inner-product of two
generalized Gramians. We show that the gradient of this term can be efficiently
computed by maintaining estimates of the Gramians, and develop variance
reduction schemes to improve the quality of the estimates. We conduct
large-scale experiments that show a significant improvement in training time
and generalization quality compared to traditional sampling methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krichene_W/0/1/0/all/0/1&quot;&gt;Walid Krichene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mayoraz_N/0/1/0/all/0/1&quot;&gt;Nicolas Mayoraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rendle_S/0/1/0/all/0/1&quot;&gt;Steffen Rendle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xinyang Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lichan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_E/0/1/0/all/0/1&quot;&gt;Ed Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anderson_J/0/1/0/all/0/1&quot;&gt;John Anderson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07215">
<title>A Machine Learning Approach for Detecting Students at Risk of Low Academic Achievement. (arXiv:1807.07215v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07215</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to predict whether a primary school student will perform in the `below
standard&apos; band of a standardized test based on a set of individual,
school-level, and family-level observables. We exploit a data set containing
test performance on the National Assessment Program - Literacy and Numeracy
(NAPLAN); a test given annually to all Australian primary school students in
grades 3, 5, 7, and 9. Students who perform in the `below standard&apos; band
constitute approximately 3% of the sample, with the remainder performing at or
above standard, requiring that a proposed classifier be robust to imbalanced
classes. Observations for students in grades 5, 7, and 9 contain data on
previous achievement in NAPLAN. We separate the analysis into students in grade
5 and above, for which previous achievement may be used as a predictor; and
students in grade 3, which must rely on family and school-level predictors
only. On each subset of the data, we train and compare a set of classifiers in
order to predict below standard performance in reading and numeracy learning
areas respectively. The best classifiers for grades 5 and above achieve an area
under the ROC curve of approximately 95%, and for grade 3 achieve an AUC of
approximately 80%. Our results suggest that it is feasible for schools to
screen a large number of students for their risk of obtaining below standard
achievement a full two years before they are identified as achieving below
standard on their next NAPLAN test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cornell_Farrow_S/0/1/0/all/0/1&quot;&gt;Sarah Cornell-Farrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garrard_R/0/1/0/all/0/1&quot;&gt;Robert Garrard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07282">
<title>Anomaly Detection for Water Treatment System based on Neural Network with Automatic Architecture Optimization. (arXiv:1807.07282v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07282</link>
<description rdf:parseType="Literal">&lt;p&gt;We continue to develop our neural network (NN) based forecasting approach to
anomaly detection (AD) using the Secure Water Treatment (SWaT) industrial
control system (ICS) testbed dataset. We propose genetic algorithms (GA) to
find the best NN architecture for a given dataset, using the NAB metric to
assess the quality of different architectures. The drawbacks of the F1-metric
are analyzed. Several techniques are proposed to improve the quality of AD:
exponentially weighted smoothing, mean p-powered error measure, individual
error weight for each variable, disjoint prediction windows. Based on the
techniques used, an approach to anomaly interpretation is introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalyga_D/0/1/0/all/0/1&quot;&gt;Dmitry Shalyga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filonov_P/0/1/0/all/0/1&quot;&gt;Pavel Filonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavrentyev_A/0/1/0/all/0/1&quot;&gt;Andrey Lavrentyev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07291">
<title>Label Aggregation via Finding Consensus Between Models. (arXiv:1807.07291v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07291</link>
<description rdf:parseType="Literal">&lt;p&gt;Label aggregation is an efficient and low cost way to make large datasets for
supervised learning. It takes the noisy labels provided by non-experts and
infers the unknown true labels. In this paper, we propose a novel label
aggregation algorithm which includes a label aggregation neural network. The
learning task in this paper is unsupervised. In order to train the neural
network, we try to design a suitable guiding model to define the loss function.
The optimization goal of our algorithm is to find the consensus between the
predictions of the neural network and the guiding model. This algorithm is easy
to optimize using mini-batch stochastic optimization methods. Since the choices
of the neural network and the guiding model are very flexible, our label
aggregation algorithm is easy to extend. According to the algorithm framework,
we design two novel models to aggregate noisy labels. Experimental results show
that our models achieve better results than state-of-the-art label aggregation
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Chi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07333">
<title>Sequence to Logic with Copy and Cache. (arXiv:1807.07333v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.07333</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating logical form equivalents of human language is a fresh way to
employ neural architectures where long short-term memory effectively captures
dependencies in both encoder and decoder units.
&lt;/p&gt;
&lt;p&gt;The logical form of the sequence usually preserves information from the
natural language side in the form of similar tokens, and recently a copying
mechanism has been proposed which increases the probability of outputting
tokens from the source input through decoding.
&lt;/p&gt;
&lt;p&gt;In this paper we propose a caching mechanism as a more general form of the
copying mechanism which also weighs all the words from the source vocabulary
according to their relation to the current decoding context.
&lt;/p&gt;
&lt;p&gt;Our results confirm that the proposed method achieves improvements in
sequence/token-level accuracy on sequence to logical form tasks. Further
experiments on cross-domain adversarial attacks show substantial improvements
when using the most influential examples of other domains for training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadashkarimi_J/0/1/0/all/0/1&quot;&gt;Javid Dadashkarimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatikonda_S/0/1/0/all/0/1&quot;&gt;Sekhar Tatikonda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.07463">
<title>Sequence Modeling via Segmentations. (arXiv:1702.07463v7 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.07463</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmental structure is a common pattern in many types of sequences such as
phrases in human languages. In this paper, we present a probabilistic model for
sequences via their segmentations. The probability of a segmented sequence is
calculated as the product of the probabilities of all its segments, where each
segment is modeled using existing tools such as recurrent neural networks.
Since the segmentation of a sequence is usually unknown in advance, we sum over
all valid segmentations to obtain the final probability for the sequence. An
efficient dynamic programming algorithm is developed for forward and backward
computations without resorting to any approximation. We demonstrate our
approach on text segmentation and speech recognition tasks. In addition to
quantitative results, we also show that our approach can discover meaningful
segments in their respective application contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Sen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dengyong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Li Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05565">
<title>Towards Neural Phrase-based Machine Translation. (arXiv:1706.05565v7 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05565</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our
method explicitly models the phrase structures in output sequences using
Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence
modeling method. To mitigate the monotonic alignment requirement of SWAN, we
introduce a new layer to perform (soft) local reordering of input sequences.
Different from existing neural machine translation (NMT) approaches, NPMT does
not use attention-based decoding mechanisms. Instead, it directly outputs
phrases in a sequential order and can decode in linear time. Our experiments
show that NPMT achieves superior performances on IWSLT 2014
German-English/English-German and IWSLT 2015 English-Vietnamese machine
translation tasks compared with strong NMT baselines. We also observe that our
method produces meaningful phrases in output languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Sen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sitao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dengyong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Li Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.01662">
<title>Unsupervised Generative Modeling Using Matrix Product States. (arXiv:1709.01662v3 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/1709.01662</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative modeling, which learns joint probability distribution from data
and generates samples according to it, is an important task in machine learning
and artificial intelligence. Inspired by probabilistic interpretation of
quantum physics, we propose a generative model using matrix product states,
which is a tensor network originally proposed for describing (particularly
one-dimensional) entangled quantum states. Our model enjoys efficient learning
analogous to the density matrix renormalization group method, which allows
dynamically adjusting dimensions of the tensors and offers an efficient direct
sampling approach for generative tasks. We apply our method to generative
modeling of several standard datasets including the Bars and Stripes, random
binary patterns and the MNIST handwritten digits to illustrate the abilities,
features and drawbacks of our model over popular generative models such as
Hopfield model, Boltzmann machines and generative adversarial networks. Our
work sheds light on many interesting directions of future exploration on the
development of quantum-inspired algorithms for unsupervised machine learning,
which are promisingly possible to be realized on quantum devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhao-Yu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09401">
<title>Generative Temporal Models with Spatial Memory for Partially Observed Environments. (arXiv:1804.09401v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09401</link>
<description rdf:parseType="Literal">&lt;p&gt;In model-based reinforcement learning, generative and temporal models of
environments can be leveraged to boost agent performance, either by tuning the
agent&apos;s representations during training or via use as part of an explicit
planning mechanism. However, their application in practice has been limited to
simplistic environments, due to the difficulty of training such models in
larger, potentially partially-observed and 3D environments. In this work we
introduce a novel action-conditioned generative model of such challenging
environments. The model features a non-parametric spatial memory system in
which we store learned, disentangled representations of the environment.
Low-dimensional spatial updates are computed using a state-space model that
makes use of knowledge on the prior dynamics of the moving agent, and
high-dimensional visual observations are modelled with a Variational
Auto-Encoder. The result is a scalable architecture capable of performing
coherent predictions over hundreds of time steps across a range of partially
observed 2D and 3D environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fraccaro_M/0/1/0/all/0/1&quot;&gt;Marco Fraccaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rezende_D/0/1/0/all/0/1&quot;&gt;Danilo Jimenez Rezende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zwols_Y/0/1/0/all/0/1&quot;&gt;Yori Zwols&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pritzel_A/0/1/0/all/0/1&quot;&gt;Alexander Pritzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eslami_S/0/1/0/all/0/1&quot;&gt;S. M. Ali Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Viola_F/0/1/0/all/0/1&quot;&gt;Fabio Viola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10627">
<title>Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning. (arXiv:1805.10627v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10627</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a study on reinforcement learning (RL) from human bandit feedback
for sequence-to-sequence learning, exemplified by the task of bandit neural
machine translation (NMT). We investigate the reliability of human bandit
feedback, and analyze the influence of reliability on the learnability of a
reward estimator, and the effect of the quality of reward estimates on the
overall RL task. Our analysis of cardinal (5-point ratings) and ordinal
(pairwise preferences) feedback shows that their intra- and inter-annotator
$\alpha$-agreement is comparable. Best reliability is obtained for standardized
cardinal feedback, and cardinal feedback is also easiest to learn and
generalize from. Finally, improvements of over 1 BLEU can be obtained by
integrating a regression-based reward estimator trained on cardinal feedback
for 800 translations into RL for NMT. This shows that RL is possible even from
small amounts of fairly reliable human feedback, pointing to a great potential
for applications at larger scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1&quot;&gt;Julia Kreutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uyheng_J/0/1/0/all/0/1&quot;&gt;Joshua Uyheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1&quot;&gt;Stefan Riezler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06576">
<title>Comparison of RNN Encoder-Decoder Models for Anomaly Detection. (arXiv:1807.06576v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06576</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we compare different types of Recurrent Neural Network (RNN)
Encoder-Decoders in anomaly detection viewpoint. We focused on finding the
model that can learn the same data more effectively. We compared multiple
models under the same conditions, such as the number of parameters, optimizer,
and learning rate. However, the difference is whether to predict the future
sequence or restore the current sequence. We constructed the dataset with
simple vectors and used them for the experiment. Finally, we experimentally
confirmed that the model performs better when the model restores the current
sequence, rather than predict the future sequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;YeongHyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_I/0/1/0/all/0/1&quot;&gt;Il Dong Yun&lt;/a&gt;</dc:creator>
</item></rdf:RDF>