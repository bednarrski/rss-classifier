<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Good papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Good Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06871"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07336"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06950"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07135"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07297"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07371"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07377"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.06616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01320"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06913"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06915"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06940"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06949"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07108"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07259"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1607.00662"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.09520"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08241"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01719"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.06871">
<title>Continuous-variable quantum neural networks. (arXiv:1806.06871v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1806.06871</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a general method for building neural networks on quantum
computers. The quantum neural network is a variational quantum circuit built in
the continuous-variable (CV) architecture, which encodes quantum information in
continuous degrees of freedom such as the amplitudes of the electromagnetic
field. This circuit contains a layered structure of continuously parameterized
gates which is universal for CV quantum computation. Affine transformations and
nonlinear activation functions, two key elements in neural networks, are
enacted in the quantum network using Gaussian and non-Gaussian gates,
respectively. The non-Gaussian gates provide both the nonlinearity and the
universality of the model. Due to the structure of the CV model, the CV quantum
neural network can encode highly nonlinear transformations while remaining
completely unitary. We show how a classical network can be embedded into the
quantum formalism and propose quantum versions of various specialized model
such as convolutional, recurrent, and residual networks. Finally, we present
numerous modeling experiments built with the Strawberry Fields software
library. These experiments, including a classifier for fraud detection, a
network which generates Tetris images, and a hybrid classical-quantum
autoencoder, demonstrate the capability and adaptability of CV quantum neural
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Killoran_N/0/1/0/all/0/1&quot;&gt;Nathan Killoran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bromley_T/0/1/0/all/0/1&quot;&gt;Thomas R. Bromley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Arrazola_J/0/1/0/all/0/1&quot;&gt;Juan Miguel Arrazola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Schuld_M/0/1/0/all/0/1&quot;&gt;Maria Schuld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Quesada_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Quesada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lloyd_S/0/1/0/all/0/1&quot;&gt;Seth Lloyd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06926">
<title>Understanding Patch-Based Learning by Explaining Predictions. (arXiv:1806.06926v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06926</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep networks are able to learn highly predictive models of video data. Due
to video length, a common strategy is to train them on small video snippets. We
apply the deep Taylor / LRP technique to understand the deep network&apos;s
classification decisions, and identify a &quot;border effect&quot;: a tendency of the
classifier to look mainly at the bordering frames of the input. This effect
relates to the step size used to build the video snippet, which we can then
tune in order to improve the classifier&apos;s accuracy without retraining the
model. To our knowledge, this is the the first work to apply the deep Taylor /
LRP technique on any video analyzing neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1&quot;&gt;Christopher Anders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montavon_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Montavon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06928">
<title>Meta Continual Learning. (arXiv:1806.06928v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06928</link>
<description rdf:parseType="Literal">&lt;p&gt;Using neural networks in practical settings would benefit from the ability of
the networks to learn new tasks throughout their lifetimes without forgetting
the previous tasks. This ability is limited in the current deep neural networks
by a problem called catastrophic forgetting, where training on new tasks tends
to severely degrade performance on previous tasks. One way to lessen the impact
of the forgetting problem is to constrain parameters that are important to
previous tasks to stay close to the optimal parameters. Recently, multiple
competitive approaches for computing the importance of the parameters with
respect to the previous tasks have been presented. In this paper, we propose a
learning to optimize algorithm for mitigating catastrophic forgetting. Instead
of trying to formulate a new constraint function ourselves, we propose to train
another neural network to predict parameter update steps that respect the
importance of parameters to the previous tasks. In the proposed meta-training
scheme, the update predictor is trained to minimize loss on a combination of
current and past tasks. We show experimentally that the proposed approach works
in the continual learning setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuorio_R/0/1/0/all/0/1&quot;&gt;Risto Vuorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1&quot;&gt;Dong-Yeon Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daejoong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07073">
<title>Transfer Learning with Human Corneal Tissues: An Analysis of Optimal Cut-Off Layer. (arXiv:1806.07073v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.07073</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning is a powerful tool to adapt trained neural networks to new
tasks. Depending on the similarity of the original task to the new task, the
selection of the cut-off layer is critical. For medical applications like
tissue classification, the last layers of an object classification network
might not be optimal. We found that on real data of human corneal tissues the
best feature representation can be found in the middle layers of the
Inception-v3 and in the rear layers of the VGG-19 architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prodanova_N/0/1/0/all/0/1&quot;&gt;Nadezhda Prodanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1&quot;&gt;Johannes Stegmaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allgeier_S/0/1/0/all/0/1&quot;&gt;Stephan Allgeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohn_S/0/1/0/all/0/1&quot;&gt;Sebastian Bohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stachs_O/0/1/0/all/0/1&quot;&gt;Oliver Stachs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_B/0/1/0/all/0/1&quot;&gt;Bernd K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1&quot;&gt;Ralf Mikut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartschat_A/0/1/0/all/0/1&quot;&gt;Andreas Bartschat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07336">
<title>Neural Code Comprehension: A Learnable Representation of Code Semantics. (arXiv:1806.07336v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07336</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent success of embeddings in natural language processing,
research has been conducted into applying similar methods to code analysis.
Most works attempt to process the code directly or use a syntactic tree
representation, treating it like sentences written in a natural language.
However, none of the existing methods are sufficient to comprehend program
semantics robustly, due to structural features such as function calls,
branching, and interchangeable order of statements. In this paper, we propose a
novel processing technique to learn code semantics, and apply it to a variety
of program analysis tasks. In particular, we stipulate that a robust
distributional hypothesis of code applies to both human- and machine-generated
programs. Following this hypothesis, we define an embedding space, inst2vec,
based on an Intermediate Representation (IR) of the code that is independent of
the source programming language. We provide a novel definition of contextual
flow for this IR, leveraging both the underlying data- and control-flow of the
program. We then analyze the embeddings qualitatively using analogies and
clustering, and evaluate the learned representation on three different
high-level tasks. We show that with a single RNN architecture and pre-trained
fixed embeddings, inst2vec outperforms specialized approaches for performance
prediction (compute device mapping, optimal thread coarsening); and algorithm
classification from raw code (104 classes), where we set a new
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Nun_T/0/1/0/all/0/1&quot;&gt;Tal Ben-Nun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakobovits_A/0/1/0/all/0/1&quot;&gt;Alice Shoshana Jakobovits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07239">
<title>Continual Reinforcement Learning with Complex Synapses. (arXiv:1802.07239v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07239</link>
<description rdf:parseType="Literal">&lt;p&gt;Unlike humans, who are capable of continual learning over their lifetimes,
artificial neural networks have long been known to suffer from a phenomenon
known as catastrophic forgetting, whereby new learning can lead to abrupt
erasure of previously acquired knowledge. Whereas in a neural network the
parameters are typically modelled as scalar values, an individual synapse in
the brain comprises a complex network of interacting biochemical components
that evolve at different timescales. In this paper, we show that by equipping
tabular and deep reinforcement learning agents with a synaptic model that
incorporates this biological complexity (Benna &amp;amp; Fusi, 2016), catastrophic
forgetting can be mitigated at multiple timescales. In particular, we find that
as well as enabling continual learning across sequential training of two simple
tasks, it can also be used to overcome within-task forgetting by reducing the
need for an experience replay database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1&quot;&gt;Christos Kaplanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1&quot;&gt;Murray Shanahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clopath_C/0/1/0/all/0/1&quot;&gt;Claudia Clopath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02464">
<title>Differentiable plasticity: training plastic neural networks with backpropagation. (arXiv:1804.02464v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02464</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we build agents that keep learning from experience, quickly and
efficiently, after their initial training? Here we take inspiration from the
main mechanism of learning in biological brains: synaptic plasticity, carefully
tuned by evolution to produce efficient lifelong learning. We show that
plasticity, just like connection weights, can be optimized by gradient descent
in large (millions of parameters) recurrent networks with Hebbian plastic
connections. First, recurrent plastic networks with more than two million
parameters can be trained to memorize and reconstruct sets of novel,
high-dimensional 1000+ pixels natural images not seen during training.
Crucially, traditional non-plastic recurrent networks fail to solve this task.
Furthermore, trained plastic networks can also solve generic meta-learning
tasks such as the Omniglot task, with competitive results and little parameter
overhead. Finally, in reinforcement learning settings, plastic networks
outperform a non-plastic equivalent in a maze exploration task. We conclude
that differentiable plasticity may provide a powerful novel approach to the
learning-to-learn problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miconi_T/0/1/0/all/0/1&quot;&gt;Thomas Miconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06946">
<title>Semantic Image Retrieval by Uniting Deep Neural Networks and Cognitive Architectures. (arXiv:1806.06946v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1806.06946</link>
<description rdf:parseType="Literal">&lt;p&gt;Image and video retrieval by their semantic content has been an important and
challenging task for years, because it ultimately requires bridging the
symbolic/subsymbolic gap. Recent successes in deep learning enabled detection
of objects belonging to many classes greatly outperforming traditional computer
vision techniques. However, deep learning solutions capable of executing
retrieval queries are still not available. We propose a hybrid solution
consisting of a deep neural network for object detection and a cognitive
architecture for query execution. Specifically, we use YOLOv2 and OpenCog.
Queries allowing the retrieval of video frames containing objects of specified
classes and specified spatial arrangement are implemented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potapov_A/0/1/0/all/0/1&quot;&gt;Alexey Potapov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhdanov_I/0/1/0/all/0/1&quot;&gt;Innokentii Zhdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherbakov_O/0/1/0/all/0/1&quot;&gt;Oleg Scherbakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skorobogatko_N/0/1/0/all/0/1&quot;&gt;Nikolai Skorobogatko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenoglio_E/0/1/0/all/0/1&quot;&gt;Enzo Fenoglio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06950">
<title>GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking. (arXiv:1806.06950v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.06950</link>
<description rdf:parseType="Literal">&lt;p&gt;Model compression is essential for serving large deep neural nets on devices
with limited resources or applications that require real-time responses. As a
case study, a state-of-the-art neural language model usually consists of one or
more recurrent layers sandwiched between an embedding layer used for
representing input tokens and a softmax layer for generating output tokens. For
problems with a very large vocabulary size, the embedding and the softmax
matrices can account for more than half of the model size. For instance, the
bigLSTM model achieves state-of- the-art performance on the One-Billion-Word
(OBW) dataset with around 800k vocabulary, and its word embedding and softmax
matrices use more than 6GBytes space, and are responsible for over 90% of the
model parameters. In this paper, we propose GroupReduce, a novel compression
method for neural language models, based on vocabulary-partition (block) based
low-rank matrix approximation and the inherent frequency distribution of tokens
(the power-law distribution of words). The experimental results show our method
can significantly outperform traditional compression methods such as low-rank
approximation and pruning. On the OBW dataset, our method achieved 6.6 times
compression rate for the embedding and softmax matrices, and when combined with
quantization, our method can achieve 26 times compression rate, which
translates to a factor of 12.8 times compression for the entire model with very
little degradation in perplexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Patrick H. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1&quot;&gt;Si Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chelba_C/0/1/0/all/0/1&quot;&gt;Ciprian Chelba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06972">
<title>Comparative Analysis of Neural QA models on SQuAD. (arXiv:1806.06972v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.06972</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Question Answering has gained prominence in the past few decades
for testing the ability of machines to understand natural language. Large
datasets for Machine Reading have led to the development of neural models that
cater to deeper language understanding compared to information retrieval tasks.
Different components in these neural architectures are intended to tackle
different challenges. As a first step towards achieving generalization across
multiple domains, we attempt to understand and compare the peculiarities of
existing end-to-end neural models on the Stanford Question Answering Dataset
(SQuAD) by performing quantitative as well as qualitative analysis of the
results attained by each of them. We observed that prediction errors reflect
certain model-specific biases, which we further discuss in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1&quot;&gt;Soumya Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1&quot;&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1&quot;&gt;Eric Nyberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07037">
<title>Translating MFM into FOL: towards plant operation planning. (arXiv:1806.07037v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07037</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a method to translate multilevel flow modeling (MFM) into
a first-order language (FOL), which enables the utilisation of logical
techniques, such as inference engines and abductive reasoners. An example of
this is a planning task for a toy plant that can be solved in FOL using
abduction. In addition, owing to the expressivity of FOL, the language is
capable of describing actions and their preconditions. This allows the
derivation of procedures consisting of multiple actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motoura_S/0/1/0/all/0/1&quot;&gt;Shota Motoura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1&quot;&gt;Kazeto Yamamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubosawa_S/0/1/0/all/0/1&quot;&gt;Shumpei Kubosawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onishi_T/0/1/0/all/0/1&quot;&gt;Takashi Onishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07135">
<title>SMarTplan: a Task Planner for Smart Factories. (arXiv:1806.07135v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07135</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart factories are on the verge of becoming the new industrial paradigm,
wherein optimization permeates all aspects of production, from concept
generation to sales. To fully pursue this paradigm, flexibility in the
production means as well as in their timely organization is of paramount
importance. AI is planning a major role in this transition, but the scenarios
encountered in practice might be challenging for current tools. Task planning
is one example where AI enables more efficient and flexible operation through
an online automated adaptation and rescheduling of the activities to cope with
new operational constraints and demands.
&lt;/p&gt;
&lt;p&gt;In this paper we present SMarTplan, a task planner specifically conceived to
deal with real-world scenarios in the emerging smart factory paradigm.
Including both special-purpose and general-purpose algorithms, SMarTplan is
based on current automated reasoning technology and it is designed to tackle
complex application domains. In particular, we show its effectiveness on a
logistic scenario, by comparing its specialized version with the general
purpose one, and extending the comparison to other state-of-the-art task
planners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bit_Monnot_A/0/1/0/all/0/1&quot;&gt;Arthur Bit-Monnot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leofante_F/0/1/0/all/0/1&quot;&gt;Francesco Leofante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pulina_L/0/1/0/all/0/1&quot;&gt;Luca Pulina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abraham_E/0/1/0/all/0/1&quot;&gt;Erika Abraham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tacchella_A/0/1/0/all/0/1&quot;&gt;Armando Tacchella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07297">
<title>Canonical Tensor Decomposition for Knowledge Base Completion. (arXiv:1806.07297v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07297</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of Knowledge Base Completion can be framed as a 3rd-order binary
tensor completion problem. In this light, the Canonical Tensor Decomposition
(CP) (Hitchcock, 1927) seems like a natural solution; however, current
implementations of CP on standard Knowledge Base Completion benchmarks are
lagging behind their competitors. In this work, we attempt to understand the
limits of CP for knowledge base completion. First, we motivate and test a novel
regularizer, based on tensor nuclear $p$-norms. Then, we present a
reformulation of the problem that makes it invariant to arbitrary choices in
the inclusion of predicates or their reciprocals in the dataset. These two
methods combined allow us to beat the current state of the art on several
datasets with a CP decomposition, and obtain even better results using the more
advanced ComplEx model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacroix_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Lacroix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Usunier_N/0/1/0/all/0/1&quot;&gt;Nicolas Usunier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Obozinski_G/0/1/0/all/0/1&quot;&gt;Guillaume Obozinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07366">
<title>Neural Ordinary Differential Equations. (arXiv:1806.07366v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07366</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new family of deep neural network models. Instead of
specifying a discrete sequence of hidden layers, we parameterize the derivative
of the hidden state using a neural network. The output of the network is
computed using a blackbox differential equation solver. These continuous-depth
models have constant memory cost, adapt their evaluation strategy to each
input, and can explicitly trade numerical precision for speed. We demonstrate
these properties in continuous-depth residual networks and continuous-time
latent variable models. We also construct continuous normalizing flows, a
generative model that can train by maximum likelihood, without partitioning or
ordering the data dimensions. For training, we show how to scalably
backpropagate through any ODE solver, without access to its internal
operations. This allows end-to-end training of ODEs within larger models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tian Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubanova_Y/0/1/0/all/0/1&quot;&gt;Yulia Rubanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bettencourt_J/0/1/0/all/0/1&quot;&gt;Jesse Bettencourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07371">
<title>Object-Oriented Dynamics Predictor. (arXiv:1806.07371v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07371</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization has been one of the major challenges for learning dynamics
models in model-based reinforcement learning. However, previous work on
action-conditioned dynamics prediction focuses on learning the pixel-level
motion and thus does not generalize well to novel environments with different
object layouts. In this paper, we present a novel object-oriented framework,
called object-oriented dynamics predictor (OODP), which decomposes the
environment into objects and predicts the dynamics of objects conditioned on
both actions and object-to-object relations. It is an end-to-end neural network
and can be trained in an unsupervised manner. To enable the generalization
ability of dynamics learning, we design a novel CNN-based relation mechanism
that is class-specific (rather than object-specific) and exploits the locality
principle. Empirical results show that OODP significantly outperforms previous
methods in terms of generalization over novel environments with various object
layouts. OODP is able to learn from very few environments and accurately
predict dynamics in a large number of unseen environments. In addition, OODP
learns semantically and visually interpretable dynamics models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guangxiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07377">
<title>Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation. (arXiv:1806.07377v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07377</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Reinforcement Learning has managed to achieve state-of-the-art results
in learning control policies directly from raw pixels. However, despite its
remarkable success, it fails to generalize, a fundamental component required in
a stable Artificial Intelligence system. Using the Atari game Breakout, we
demonstrate the difficulty of a trained agent in adjusting to simple
modifications in the raw image, ones that a human could adapt to trivially. In
transfer learning, the goal is to use the knowledge gained from the source task
to make the training of the target task faster and better. We show that using
various forms of fine-tuning, a common method for transfer learning, is not
effective for adapting to such small visual changes. In fact, it is often
easier to re-train the agent from scratch than to fine-tune a trained agent. We
suggest that in some cases transfer learning can be improved by adding a
dedicated component whose goal is to learn to visually map between the known
domain and the new one. Concretely, we use Generative Adversarial Networks
(GANs) to create a mapping function to translate images in the target task to
corresponding images in the source task, allowing us to transform between the
different tasks. We show that learning this mapping is substantially more
efficient than re-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gamrian_S/0/1/0/all/0/1&quot;&gt;Shani Gamrian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1&quot;&gt;Yoav Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07380">
<title>Deep Sequence Learning with Auxiliary Information for Traffic Prediction. (arXiv:1806.07380v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07380</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting traffic conditions from online route queries is a challenging task
as there are many complicated interactions over the roads and crowds involved.
In this paper, we intend to improve traffic prediction by appropriate
integration of three kinds of implicit but essential factors encoded in
auxiliary information. We do this within an encoder-decoder sequence learning
framework that integrates the following data: 1) offline geographical and
social attributes. For example, the geographical structure of roads or public
social events such as national celebrations; 2) road intersection information.
In general, traffic congestion occurs at major junctions; 3) online crowd
queries. For example, when many online queries issued for the same destination
due to a public performance, the traffic around the destination will
potentially become heavier at this location after a while. Qualitative and
quantitative experiments on a real-world dataset from Baidu have demonstrated
the effectiveness of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Binbing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIlwraith_D/0/1/0/all/0/1&quot;&gt;Douglas McIlwraith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shengwen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.06616">
<title>Accurately and Efficiently Interpreting Human-Robot Instructions of Varying Granularities. (arXiv:1704.06616v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1704.06616</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans can ground natural language commands to tasks at both abstract and
fine-grained levels of specificity. For instance, a human forklift operator can
be instructed to perform a high-level action, like &quot;grab a pallet&quot; or a
low-level action like &quot;tilt back a little bit.&quot; While robots are also capable
of grounding language commands to tasks, previous methods implicitly assume
that all commands and tasks reside at a single, fixed level of abstraction.
Additionally, methods that do not use multiple levels of abstraction encounter
inefficient planning and execution times as they solve tasks at a single level
of abstraction with large, intractable state-action spaces closely resembling
real world complexity. In this work, by grounding commands to all the tasks or
subtasks available in a hierarchical planning framework, we arrive at a model
capable of interpreting language at multiple levels of specificity ranging from
coarse to more granular. We show that the accuracy of the grounding procedure
is improved when simultaneously inferring the degree of abstraction in language
used to communicate the task. Leveraging hierarchy also improves efficiency:
our proposed approach enables a robot to respond to a command within one second
on 90% of our tasks, while baselines take over twenty seconds on half the
tasks. Finally, we demonstrate that a real, physical robot can ground commands
at multiple levels of abstraction allowing it to efficiently plan different
subtasks within the same planning hierarchy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1&quot;&gt;Dilip Arumugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karamcheti_S/0/1/0/all/0/1&quot;&gt;Siddharth Karamcheti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1&quot;&gt;Nakul Gopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1&quot;&gt;Lawson L.S. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1&quot;&gt;Stefanie Tellex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01320">
<title>3D Pathfinding and Collision Avoidance Using Uneven Search-space Quantization and Visual Cone Search. (arXiv:1706.01320v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01320</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathfinding is a very popular area in computer game development. While
two-dimensional (2D) pathfinding is widely applied in most of the popular game
engines, little implementation of real three-dimensional (3D) pathfinding can
be found. This research presents a dynamic search space optimization algorithm
which can be applied to tessellate 3D search space unevenly, significantly
reducing the total number of resulting nodes. The algorithm can be used with
popular pathfinding algorithms in 3D game engines. Furthermore, a simplified
standalone 3D pathfinding algorithm is proposed in this paper. The proposed
algorithm relies on ray-casting or line vision to generate a feasible path
during runtime without requiring division of the search space into a 3D grid.
Both of the proposed algorithms are simulated on Unreal Engine to show
innerworkings and resultant path comparison with A*. The advantages and
shortcomings of the proposed algorithms are also discussed along with future
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandit_D/0/1/0/all/0/1&quot;&gt;Diptangshu Pandit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05049">
<title>Fractal AI: A fragile theory of intelligence. (arXiv:1803.05049v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05049</link>
<description rdf:parseType="Literal">&lt;p&gt;Fractal AI is a theory for general artificial intelligence. It allows to
derive new mathematical tools that constitute the foundations for a new kind of
stochastic calculus, by modelling information using cellular automaton-like
structures instead of smooth functions.
&lt;/p&gt;
&lt;p&gt;In the repository included we are presenting a new Agent, derived from the
first principles of the theory, which is capable of solving Atari games several
orders of magnitude more efficiently than other similar techniques, like Monte
Carlo Tree Search.
&lt;/p&gt;
&lt;p&gt;The code provided shows how it is now possible to beat some of the current
state of the art benchmarks on Atari games, without previous learning and using
less than 1000 samples to calculate each one of the actions when standard MCTS
uses 3 Million samples. Among other things, Fractal AI makes it possible to
generate a huge database of top performing examples with very little amount of
computation required, transforming Reinforcement Learning into a supervised
problem.
&lt;/p&gt;
&lt;p&gt;The algorithm presented is capable of solving the exploration vs exploitation
dilemma on both the discrete and continuous cases, while maintaining control
over any aspect of the behavior of the Agent. From a general approach, new
techniques presented here have direct applications to other areas such as:
Non-equilibrium thermodynamics, chemistry, quantum physics, economics,
information theory, and non-linear control theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerezo_S/0/1/0/all/0/1&quot;&gt;Sergio Hernandez Cerezo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballester_G/0/1/0/all/0/1&quot;&gt;Guillem Duran Ballester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06877">
<title>A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress. (arXiv:1806.06877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06877</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning is the problem of inferring the reward
function of an observed agent, given its policy or behavior. Researchers
perceive IRL both as a problem and as a class of methods. By categorically
surveying the current literature in IRL, this article serves as a reference for
researchers and practitioners in machine learning to understand the challenges
of IRL and select the approaches best suited for the problem on hand. The
survey formally introduces the IRL problem along with its central challenges
which include accurate inference, generalizability, correctness of prior
knowledge, and growth in solution complexity with problem size. The article
elaborates how the current methods mitigate these challenges. We further
discuss the extensions of traditional IRL methods: (i) inaccurate and
incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv)
non-linear reward functions. This discussion concludes with some broad advances
in the research area and currently open research questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1&quot;&gt;Saurabh Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1&quot;&gt;Prashant Doshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06913">
<title>Deep Learning based Estimation of Weaving Target Maneuvers. (arXiv:1806.06913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06913</link>
<description rdf:parseType="Literal">&lt;p&gt;In target tracking, the estimation of an unknown weaving target frequency is
crucial for improving the miss distance. The estimation process is commonly
carried out in a Kalman framework. The objective of this paper is to examine
the potential of using neural networks in target tracking applications. To that
end, we propose estimating the weaving frequency using deep neural networks,
instead of classical Kalman framework based estimation. Particularly, we focus
on the case where a set of possible constant target frequencies is known.
Several neural network architectures, requiring low computational resources
were designed to estimate the unknown frequency out of the known set of
frequencies. The proposed approach performance is compared with the multiple
model adaptive estimation algorithm. Simulation results show that in the
examined scenarios, deep neural network outperforms multiple model adaptive
estimation in terms of accuracy and the amount of required measurements to
convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalumov_V/0/1/0/all/0/1&quot;&gt;Vitaly Shalumov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_I/0/1/0/all/0/1&quot;&gt;Itzik Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06915">
<title>A One-Sided Classification Toolkit with Applications in the Analysis of Spectroscopy Data. (arXiv:1806.06915v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06915</link>
<description rdf:parseType="Literal">&lt;p&gt;This dissertation investigates the use of one-sided classification algorithms
in the application of separating hazardous chlorinated solvents from other
materials, based on their Raman spectra. The experimentation is carried out
using a new one-sided classification toolkit that was designed and developed
from the ground up. In the one-sided classification paradigm, the objective is
to separate elements of the target class from all outliers. These one-sided
classifiers are generally chosen, in practice, when there is a deficiency of
some sort in the training examples. Sometimes outlier examples can be rare,
expensive to label, or even entirely absent. However, this author would like to
note that they can be equally applicable when outlier examples are plentiful
but nonetheless not statistically representative of the complete outlier
concept. It is this scenario that is explicitly dealt with in this research
work. In these circumstances, one-sided classifiers have been found to be more
robust that conventional multi-class classifiers. The term &quot;unexpected&quot;
outliers is introduced to represent outlier examples, encountered in the test
set, that have been taken from a different distribution to the training set
examples. These are examples that are a result of an inadequate representation
of all possible outliers in the training set. It can often be impossible to
fully characterise outlier examples given the fact that they can represent the
immeasurable quantity of &quot;everything else&quot; that is not a target. The findings
from this research have shown the potential drawbacks of using conventional
multi-class classification algorithms when the test data come from a completely
different distribution to that of the training samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavin_F/0/1/0/all/0/1&quot;&gt;Frank G. Glavin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06924">
<title>On the Metric Distortion of Embedding Persistence Diagrams into Reproducing Kernel Hilbert Spaces. (arXiv:1806.06924v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06924</link>
<description rdf:parseType="Literal">&lt;p&gt;Persistence diagrams are important feature descriptors in Topological Data
Analysis. Due to the nonlinearity of the space of persistence diagrams equipped
with their {\em diagram distances}, most of the recent attempts at using
persistence diagrams in Machine Learning have been done through kernel methods,
i.e., embeddings of persistence diagrams into Reproducing Kernel Hilbert Spaces
(RKHS), in which all computations can be performed easily. Since persistence
diagrams enjoy theoretical stability guarantees for the diagram distances, the
{\em metric properties} of a kernel $k$, i.e., the relationship between the
RKHS distance $d_k$ and the diagram distances, are of central interest for
understanding if the persistence diagram guarantees carry over to the
embedding. In this article, we study the possibility of embedding persistence
diagrams into RKHS with bi-Lipschitz maps. In particular, we show that when the
RKHS is infinite dimensional, any lower bound must depend on the cardinalities
of the persistence diagrams, and that when the RKHS is finite dimensional,
finding a bi-Lipschitz embedding is impossible, even when restricting the
persistence diagrams to have bounded cardinalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carriere_M/0/1/0/all/0/1&quot;&gt;Mathieu Carriere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_U/0/1/0/all/0/1&quot;&gt;Ulrich Bauer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06940">
<title>Pressure Predictions of Turbine Blades with Deep Learning. (arXiv:1806.06940v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06940</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has been used in many areas, such as feature detections in
images and the game of go. This paper presents a study that attempts to use the
deep learning method to predict turbomachinery performance. Three different
deep neural networks are built and trained to predict the pressure
distributions of turbine airfoils. The performance of a library of turbine
airfoils were firstly predicted using methods based on Euler equations, which
were then used to train and validate the deep learning neural networks. The
results show that network with four layers of convolutional neural network and
two layers of fully connected neural network provides the best predictions. For
the best neural network architecture, the pressure prediction on more than 99%
locations are better than 3% and 90% locations are better than 1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1&quot;&gt;Cheng&amp;#x27;an Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chao Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06949">
<title>DropBack: Continuous Pruning During Training. (arXiv:1806.06949v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06949</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a technique that compresses deep neural networks both during and
after training by constraining the total number of weights updated during
backpropagation to those with the highest total gradients. The remaining
weights are forgotten and their initial value is regenerated at every access to
avoid storing them in memory. This dramatically reduces the number of off-chip
memory accesses during both training and inference, a key component of the
energy needs of DNN accelerators.
&lt;/p&gt;
&lt;p&gt;By ensuring that the total weight diffusion remains close to that of baseline
unpruned SGD, networks pruned using DropBack are able to maintain high accuracy
across network architectures. We observe weight compression of 25x with
LeNet-300-100 on MNIST while maintaining accuracy. On CIFAR-10, we see an
approximately 5x weight compression on 3 models: an already 9x-reduced VGG-16,
Densenet, and WRN-28-10 - all with zero or negligible accuracy loss. On
Densenet and WRN, which are particularly challenging to compress, Both Densenet
and WRN improve on the state of the art, achieving higher compression with
better accuracy than prior pruning techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golub_M/0/1/0/all/0/1&quot;&gt;Maximilian Golub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemieux_G/0/1/0/all/0/1&quot;&gt;Guy Lemieux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lis_M/0/1/0/all/0/1&quot;&gt;Mieszko Lis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06975">
<title>Towards Gene Expression Convolutions using Gene Interaction Graphs. (arXiv:1806.06975v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/1806.06975</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the challenges of applying deep learning to gene expression data. We
find experimentally that there exists non-linear signal in the data, however is
it not discovered automatically given the noise and low numbers of samples used
in most research. We discuss how gene interaction graphs (same pathway,
protein-protein, co-expression, or research paper text association) can be used
to impose a bias on a deep model similar to the spatial bias imposed by
convolutions on an image. We explore the usage of Graph Convolutional Neural
Networks coupled with dropout and gene embeddings to utilize the graph
information. We find this approach provides an advantage for particular tasks
in a low data regime but is very dependent on the quality of the graph used. We
conclude that more work should be done in this direction. We design experiments
that show why existing methods fail to capture signal that is present in the
data when features are added which clearly isolates the problem that needs to
be addressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dutil_F/0/1/0/all/0/1&quot;&gt;Francis Dutil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Joseph Paul Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Weiss_M/0/1/0/all/0/1&quot;&gt;Martin Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Derevyanko_G/0/1/0/all/0/1&quot;&gt;Georgy Derevyanko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06988">
<title>Deep Neural Decision Trees. (arXiv:1806.06988v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.06988</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have been proven powerful at processing perceptual data,
such as images and audio. However for tabular data, tree-based models are more
popular. A nice property of tree-based models is their natural
interpretability. In this work, we present Deep Neural Decision Trees (DNDT) --
tree models realised by neural networks. A DNDT is intrinsically interpretable,
as it is a tree. Yet as it is also a neural network (NN), it can be easily
implemented in NN toolkits, and trained with gradient descent rather than
greedy splitting. We evaluate DNDT on several tabular datasets, verify its
efficacy, and investigate similarities and differences between DNDT and vanilla
decision trees. Interestingly, DNDT self-prunes at both split and
feature-level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morillo_I/0/1/0/all/0/1&quot;&gt;Irene Garcia Morillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy M. Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07001">
<title>Theoretical Analysis of Image-to-Image Translation with Adversarial Learning. (arXiv:1806.07001v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07001</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a unified model for image-to-image translation tasks within
adversarial learning framework has aroused widespread research interests in
computer vision practitioners. Their reported empirical success however lacks
solid theoretical interpretations for its inherent mechanism. In this paper, we
reformulate their model from a brand-new geometrical perspective and have
eventually reached a full interpretation on some interesting but unclear
empirical phenomenons from their experiments. Furthermore, by extending the
definition of generalization for generative adversarial nets to a broader
sense, we have derived a condition to control the generalization capability of
their model. According to our derived condition, several practical suggestions
have also been proposed on model design and dataset construction as a guidance
for further empirical researches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xudong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_D/0/1/0/all/0/1&quot;&gt;Daizong Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07057">
<title>Effect of Hyper-Parameter Optimization on the Deep Learning Model Proposed for Distributed Attack Detection in Internet of Things Environment. (arXiv:1806.07057v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07057</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the effect of various hyper-parameters and their selection
for the best performance of the deep learning model proposed in [1] for
distributed attack detection in the Internet of Things (IoT). The findings show
that there are three hyper-parameters that have more influence on the best
performance achieved by the model. As a consequence, this study shows that the
model&apos;s accuracy as reported in the paper is not achievable, based on the best
selections of parameters, which is also supported by another recent publication
[2].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohaimenuzzaman_M/0/1/0/all/0/1&quot;&gt;Md Mohaimenuzzaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdallah_Z/0/1/0/all/0/1&quot;&gt;Zahraa Said Abdallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamruzzaman_J/0/1/0/all/0/1&quot;&gt;Joarder Kamruzzaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1&quot;&gt;Bala Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07104">
<title>Online Linear Quadratic Control. (arXiv:1806.07104v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07104</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of controlling linear time-invariant systems with known
noisy dynamics and adversarially chosen quadratic losses. We present the first
efficient online learning algorithms in this setting that guarantee
$O(\sqrt{T})$ regret under mild assumptions, where $T$ is the time horizon. Our
algorithms rely on a novel SDP relaxation for the steady-state distribution of
the system. Crucially, and in contrast to previously proposed relaxations, the
feasible solutions of our SDP all correspond to &quot;strongly stable&quot; policies that
mix exponentially fast to a steady state.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Alon Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1&quot;&gt;Avinatan Hassidim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1&quot;&gt;Tomer Koren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazic_N/0/1/0/all/0/1&quot;&gt;Nevena Lazic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1&quot;&gt;Yishay Mansour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1&quot;&gt;Kunal Talwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07108">
<title>Improving brain computer interface performance by data augmentation with conditional Deep Convolutional Generative Adversarial Networks. (arXiv:1806.07108v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1806.07108</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the big restrictions in brain computer interface field is the very
limited training samples, it is difficult to build a reliable and usable system
with such limited data. Inspired by generative adversarial networks, we propose
a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks
method to generate more artificial EEG signal automatically for data
augmentation to improve the performance of convolutional neural networks in
brain computer interface field and overcome the small training dataset
problems. We evaluate the proposed cDCGAN method on BCI competition dataset of
motor imagery. The results show that the generated artificial EEG data from
Gaussian noise can learn the features from raw EEG data and has no less than
the classification accuracy of raw EEG data in the testing dataset. Also by
using generated artificial data can effectively improve classification accuracy
at the same model with limited training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ying Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07185">
<title>Mixed batches and symmetric discriminators for GAN training. (arXiv:1806.07185v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07185</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) are pow- erful generative models based
on providing feed- back to a generative network via a discriminator network.
However, the discriminator usually as- sesses individual samples. This prevents
the dis- criminator from accessing global distributional statistics of
generated samples, and often leads to mode dropping: the generator models only
part of the target distribution. We propose to feed the discriminator with
mixed batches of true and fake samples, and train it to predict the ratio of
true samples in the batch. The latter score does not depend on the order of
samples in a batch. Rather than learning this invariance, we introduce a
generic permutation-invariant discriminator ar- chitecture. This architecture
is provably a uni- versal approximator of all symmetric functions.
Experimentally, our approach reduces mode col- lapse in GANs on two synthetic
datasets, and obtains good results on the CIFAR10 and CelebA datasets, both
qualitatively and quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_T/0/1/0/all/0/1&quot;&gt;Thomas Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tallec_C/0/1/0/all/0/1&quot;&gt;Corentin Tallec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1&quot;&gt;Yann Ollivier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07259">
<title>Learning Equations for Extrapolation and Control. (arXiv:1806.07259v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07259</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach to identify concise equations from data using a
shallow neural network approach. In contrast to ordinary black-box regression,
this approach allows understanding functional relations and generalizing them
from observed data to unseen parts of the parameter space. We show how to
extend the class of learnable equations for a recently proposed equation
learning network to include divisions, and we improve the learning and model
selection strategy to be useful for challenging real-world data. For systems
governed by analytical expressions, our method can in many cases identify the
true underlying equation and extrapolate to unseen domains. We demonstrate its
effectiveness by experiments on a cart-pendulum system, where only 2 random
rollouts are required to learn the forward dynamics and successfully achieve
the swing-up task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1&quot;&gt;Subham S. Sahoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1&quot;&gt;Christoph H. Lampert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1&quot;&gt;Georg Martius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07268">
<title>Beyond Local Nash Equilibria for Adversarial Networks. (arXiv:1806.07268v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07268</link>
<description rdf:parseType="Literal">&lt;p&gt;Save for some special cases, current training methods for Generative
Adversarial Networks (GANs) are at best guaranteed to converge to a `local Nash
equilibrium` (LNE). Such LNEs, however, can be arbitrarily far from an actual
Nash equilibrium (NE), which implies that there are no guarantees on the
quality of the found generator or classifier. This paper proposes to model GANs
explicitly as finite games in mixed strategies, thereby ensuring that every LNE
is an NE. With this formulation, we propose a solution method that is proven to
monotonically converge to a resource-bounded Nash equilibrium (RB-NE): by
increasing computational resources we can find better solutions. We empirically
demonstrate that our method is less prone to typical GAN problems such as mode
collapse, and produces solutions that are less exploitable than those produced
by GANs and MGANs, and closely resemble theoretical predictions about NEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliehoek_F/0/1/0/all/0/1&quot;&gt;Frans A. Oliehoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savani_R/0/1/0/all/0/1&quot;&gt;Rahul Savani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_J/0/1/0/all/0/1&quot;&gt;Jose Gallego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pol_E/0/1/0/all/0/1&quot;&gt;Elise van der Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_R/0/1/0/all/0/1&quot;&gt;Roderich Gro&amp;#xdf;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1607.00662">
<title>Unsupervised Learning of 3D Structure from Images. (arXiv:1607.00662v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1607.00662</link>
<description rdf:parseType="Literal">&lt;p&gt;A key goal of computer vision is to recover the underlying 3D structure from
2D observations of the world. In this paper we learn strong deep generative
models of 3D structures, and recover these structures from 3D and 2D images via
probabilistic inference. We demonstrate high-quality samples and report
log-likelihoods on several datasets, including ShapeNet [2], and establish the
first benchmarks in the literature. We also show how these models and their
inference networks can be trained end-to-end from 2D images. This demonstrates
for the first time the feasibility of learning to infer 3D representations of
the world in a purely unsupervised manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezende_D/0/1/0/all/0/1&quot;&gt;Danilo Jimenez Rezende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1&quot;&gt;S. M. Ali Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Shakir Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1&quot;&gt;Peter Battaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaderberg_M/0/1/0/all/0/1&quot;&gt;Max Jaderberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1&quot;&gt;Nicolas Heess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.09520">
<title>Orthogonal Recurrent Neural Networks with Scaled Cayley Transform. (arXiv:1707.09520v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.09520</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent Neural Networks (RNNs) are designed to handle sequential data but
suffer from vanishing or exploding gradients. Recent work on Unitary Recurrent
Neural Networks (uRNNs) have been used to address this issue and in some cases,
exceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose
a simpler and novel update scheme to maintain orthogonal recurrent weight
matrices without using complex valued matrices. This is done by parametrizing
with a skew-symmetric matrix using the Cayley transform. Such a parametrization
is unable to represent matrices with negative one eigenvalues, but this
limitation is overcome by scaling the recurrent weight matrix by a diagonal
matrix consisting of ones and negative ones. The proposed training scheme
involves a straightforward gradient calculation and update step. In several
experiments, the proposed scaled Cayley orthogonal recurrent neural network
(scoRNN) achieves superior results with fewer trainable parameters than other
unitary RNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Helfrich_K/0/1/0/all/0/1&quot;&gt;Kyle Helfrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Willmott_D/0/1/0/all/0/1&quot;&gt;Devin Willmott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qiang Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08241">
<title>Hessian-based Analysis of Large Batch Training and Robustness to Adversaries. (arXiv:1802.08241v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08241</link>
<description rdf:parseType="Literal">&lt;p&gt;Large batch size training of Neural Networks has been shown to incur accuracy
loss when trained with the current methods. The precise underlying reasons for
this are still not completely understood. Here, we study large batch size
training through the lens of the Hessian operator and robust optimization. In
particular, we perform a Hessian based study to analyze how the landscape of
the loss functional is different for large batch size training. We compute the
true Hessian spectrum, without approximation, by back-propagating the second
derivative. Our results on multiple networks show that, when training at large
batch sizes, one tends to stop at points in the parameter space with noticeably
higher/larger Hessian spectrum, i.e., where the eigenvalues of the Hessian are
much larger. We then study how batch size affects robustness of the model in
the face of adversarial attacks. All the results show that models trained with
large batches are more susceptible to adversarial attacks, as compared to
models trained with small batch sizes. Furthermore, we prove a theoretical
result which shows that the problem of finding an adversarial perturbation is a
saddle-free optimization problem. Finally, we show empirical results that
demonstrate that adversarial training leads to areas with smaller Hessian
spectrum. We present detailed experiments with five different network
architectures tested on MNIST, CIFAR-10, and CIFAR-100 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1&quot;&gt;Amir Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1&quot;&gt;Qi Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08768">
<title>Is Generator Conditioning Causally Related to GAN Performance?. (arXiv:1802.08768v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08768</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work (Pennington et al, 2017) suggests that controlling the entire
distribution of Jacobian singular values is an important design consideration
in deep learning. Motivated by this, we study the distribution of singular
values of the Jacobian of the generator in Generative Adversarial Networks
(GANs). We find that this Jacobian generally becomes ill-conditioned at the
beginning of training. Moreover, we find that the average (with z from p(z))
conditioning of the generator is highly predictive of two other ad-hoc metrics
for measuring the &apos;quality&apos; of trained GANs: the Inception Score and the
Frechet Inception Distance (FID). We test the hypothesis that this relationship
is causal by proposing a &apos;regularization&apos; technique (called Jacobian Clamping)
that softly penalizes the condition number of the generator Jacobian. Jacobian
Clamping improves the mean Inception Score and the mean FID for GANs trained on
several datasets. It also greatly reduces inter-run variance of the
aforementioned scores, addressing (at least partially) one of the main
criticisms of GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Odena_A/0/1/0/all/0/1&quot;&gt;Augustus Odena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buckman_J/0/1/0/all/0/1&quot;&gt;Jacob Buckman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Olsson_C/0/1/0/all/0/1&quot;&gt;Catherine Olsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_T/0/1/0/all/0/1&quot;&gt;Tom B. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Olah_C/0/1/0/all/0/1&quot;&gt;Christopher Olah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raffel_C/0/1/0/all/0/1&quot;&gt;Colin Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01719">
<title>How to Start Training: The Effect of Initialization and Architecture. (arXiv:1803.01719v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01719</link>
<description rdf:parseType="Literal">&lt;p&gt;We identify and study two common failure modes for early training in deep
ReLU nets. For each we give a rigorous proof of when it occurs and how to avoid
it, for fully connected and residual architectures. The first failure mode,
exploding/vanishing mean activation length, can be avoided by initializing
weights from a symmetric distribution with variance 2/fan-in and, for ResNets,
by correctly weighting the residual modules. We prove that the second failure
mode, exponentially large variance of activation length, never occurs in
residual nets once the first failure mode is avoided. In contrast, for fully
connected nets, we prove that this failure mode can happen and is avoided by
keeping constant the sum of the reciprocals of layer widths. We demonstrate
empirically the effectiveness of our theoretical results in predicting when
networks are able to start training. In particular, we note that many popular
initializations fail our criteria, whereas correct initialization and
architecture allows much deeper networks to be trained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt;</dc:creator>
</item></rdf:RDF>